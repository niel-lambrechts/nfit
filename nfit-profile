#!/usr/bin/env perl

# NAME     : nfit-profile
# AUTHOR   : Niël Lambrechts (https://github.com/niel-lambrechts)
# PURPOSE  : Runs 'nfit' multiple times with user-defined profiles.
#            Applies RunQ modifiers, generates hints, logs rationale, and aggregates to CSV.
# REQUIRES : Perl, nfit, Time::Piece, List::Util, IPC::Open3, version

use strict;
use warnings;
use Getopt::Long qw(GetOptions :config bundling);
use Cwd qw(abs_path);
use File::Basename qw(dirname basename);
use File::Spec qw(catfile);
use File::Path qw(make_path);
use File::Temp qw(tempfile);
use Time::Piece;
use Time::Seconds;
use List::Util qw(sum sum0 min max uniq); # sum0 is available from List::Util 1.33+
use Scalar::Util qw(looks_like_number);
use IPC::Open3;
use IO::Select;
use version;
use JSON;
use Fcntl qw(:DEFAULT :flock);
use constant ONE_SECOND => 1;
use Data::Dumper;
use Archive::Tar;
use IO::Zlib; # Standard in Perl core since 5.9.3

# --- Store original ARGV for logging ---
my @original_argv = @ARGV;

# --- Capture nfit-profile.pl start time ---
my $PROFILE_SCRIPT_START_TIME_EPOCH = time();
my $PROFILE_SCRIPT_START_TIME_STR = localtime($PROFILE_SCRIPT_START_TIME_EPOCH)->strftime("%Y-%m-%d %H:%M:%S %Z");

# --- Version ---
my $VERSION = '6.25.306.0';

# --- Seasonal Engine Version (Phase 4: Idempotency) ---
# Bump this constant whenever anchoring semantics, fingerprint logic, or
# model computation behaviour changes. This ensures stale cached results
# are correctly invalidated.
my $SEASONAL_ENGINE_VERSION = '1.0.0';

# --- Configuration ---
my $DEFAULT_AVG_METHOD     = 'ema';
my $DEFAULT_DECAY_LEVEL    = 'medium';
my $DEFAULT_WINDOW_MINUTES = 15;
my $DEFAULT_PERCENTILE = 95;
my $DEFAULT_ROUND_INCREMENT = 0.05;
my $DEFAULT_SMT            = 8;
my $DEFAULT_VM_CONFIG_FILE = "config-all.csv";
my $DEFAULT_PROFILES_CONFIG_FILE = "nfit.profiles.cfg";
my $DEFAULT_SMT_VALUE_PROFILE = 8;
my $DEFAULT_RUNQ_NORM_PERCS = "50,90"; # Global default for nfit-profile if not in profile's flags
my $DEFAULT_RUNQ_ABS_PERCS  = "90";    # Global default for nfit-profile if not in profile's flags
my $DEFAULT_NFIT_RUNQ_AVG_METHOD = "ema";

# This profile's CPU value is used for MaxCPU pressure checks,
# and its RunQ metrics will now be used for global RunQ pressure hints.
my $MANDATORY_PEAK_PROFILE_FOR_HINT = "Peak_P-99W1";

# Default flags for the mandatory peak helper profile, used when the profiles
# configuration is incomplete. This profile is an evidence source (peak + RunQ)
# and is intentionally hidden from the planner CSV by default when injected.
my $DEFAULT_MANDATORY_PEAK_PROFILE_FLAGS = "-p 99.75 -w 1 --filter-above-perc 30 --decay high --runq-decay medium --runq-abs-perc 90 --runq-norm-perc \"50,90\" --enable-growth-prediction --max-growth-inflation-percent 15";

my $LEGACY_MANDATORY_PEAK_PROFILE_FOR_HINT = "P-99W1"; # Back-compat alias
my $MISSION_CRITICAL_PROFILE_NAME = "P-99W1";          # Actionable peak-coverage profile (runs full modifier chain)

# Windowed Decay Defaults (for when nfit-profile instructs nfit to use its internal decay)
my $DEFAULT_PROCESS_WINDOW_UNIT_FOR_NFIT = "weeks";
my $DEFAULT_PROCESS_WINDOW_SIZE_FOR_NFIT = 1;
my $DEFAULT_DECAY_HALF_LIFE_DAYS_FOR_NFIT = 30;

# Heuristic Thresholds (for sizing hints and RunQ modifiers)
my $PATTERN_RATIO_THRESHOLD = 2.0;     # For O vs B pattern determination
my $HIGH_PEAK_RATIO_THRESHOLD = 5.0;   # For "Very Peaky" shape
my $LOW_PEAK_RATIO_THRESHOLD  = 2.0;   # For "Moderately Peaky" shape
my $LIMIT_THRESHOLD_PERC = 0.98;       # P99W1 vs MaxCPU for pressure detection

# RunQ Modifier Thresholds (for calculate_runq_modified_physc)
my $WORKLOAD_PRESSURE_NORM_P90_TRIGGER_THRESHOLD = 2.0; # NormRunQ P90 above this may indicate workload pressure (NOT adaptive)
my $RUNQ_PRESSURE_P90_SATURATION_THRESHOLD = 1.5;       # AbsRunQ P90 / (MaxCPU * SMT) above this indicates RunQ pressure (ADAPTIVE)
my $RUNQ_ADDITIVE_TOLERANCE_FACTOR = 1.8;               # Tolerate AbsRunQ up to this Factor x Base_Profile_PhysC's LCPU capacity (NOT adaptive)
my $RUNQ_TARGET_HEADROOM_FACTOR = 0.80;                 # Target headroom below the tolerance ceiling (aim below the red line)

# VCPU sizing policy (stress-driven, slot-aware, rENT-bounded)
# Note: rVCPU is emitted per profile. The envelope is computed from the profile's rENT (growth-aware).
my $VCPU_RATIO_MIN = 1.00;                              # Minimum allowable VCPU >= ~1.00x entitlement (rounded up to nearest whole number)
my $VCPU_RATIO_MAX = 2.00;                              # IBM guidance (typical): keep VCPU <= ~2.00x entitlement
my $VCPU_MIN_ABS = 1;                                   # Never recommend below 1 VP
my $VCPU_MIN_FOR_ENT_GE_0_5_LT_1 = 1;                    # For 0.5 <= rENT < 1.0, 1 VP is sufficient
my $VCPU_ROUND_UP_TO_EVEN = 1;                           # If true: round rVCPU up to nearest even (within envelope)
my $VCPU_PEAK_HEADROOM_FACTOR = 1.20;   # rVCPU floor based on P-99W1 (smoothed peak cores); 1.00 => at least ceil(P-99W1)
# Optional dynamic scaling of peak headroom based on stress/volatility diagnostics.
# Multiplier is clamped to keep behaviour bounded.
my $VCPU_PEAK_HEADROOM_DYNAMIC = 1;
my $VCPU_PEAK_HEADROOM_MULT_MIN = 0.90;
my $VCPU_PEAK_HEADROOM_MULT_MAX = 1.20;

# RunQ credibility adjustment for P99W1 peak floor.
# When RunQ-derived core demand is well below PhysC peak, the gap likely reflects
# VP dispatch overhead (idle spin, cache warming, folding) rather than true workload.
# The adjustment discounts PhysC peak proportionally, bounded by a minimum credibility.
my $VCPU_PEAK_CRED_BIAS    = 0.15;   # Benefit-of-doubt for PhysC (I/O, interrupts, cache affinity)
my $VCPU_PEAK_CRED_MIN     = 0.65;   # Never discount PhysC peak by more than 35%

# Micro-partition RunQ envelope override.
# When a tiny partition (rENT < threshold) has strong RunQ evidence (cred_ratio > threshold)
# that exceeds the PhysC-derived envelope, allow VP envelope to expand to meet RunQ demand.
# This addresses scheduling disadvantage in micro-partitions where PhysC under-reports true demand.
my $MICRO_PARTITION_RUNQ_OVERRIDE_ENT_THRESH  = 0.50;  # rENT_P must be below this
my $MICRO_PARTITION_RUNQ_OVERRIDE_CRED_THRESH = 1.50;  # cred_ratio must exceed this
my $MICRO_PARTITION_RUNQ_MAX_VP_CAP           = 6;     # Absolute VP ceiling for override

# Tier-aware rVCPU headroom factors
# Higher tiers (lower latency tolerance) receive more conservative (wider) VP allocation.
# Applied to both the RunQ operating-point and the P99W1 peak floor.
# Split into separate maps if independent tuning is required later.
my %VCPU_TIER_HEADROOM_FACTORS = (
    1 => 1.30,  # Tier 1: +30% more width (low latency tolerance)
    2 => 1.15,  # Tier 2: +15% more width
    3 => 1.00,  # Tier 3: Baseline (no adjustment)
    4 => 0.95,  # Tier 4: Slightly moderated width
);


my $ENTITLEMENT_BURST_ALLOWANCE_FACTOR = 0.25;          # Allow uncapped VMs to burst 25% over their entitlement before pressure is assessed
my $BURST_ALLOWANCE_MIN_FACTOR = 0.0;
my $BURST_ALLOWANCE_MAX_FACTOR = 0.5;                   # Clamp at 50%

# Internal constants for growth heuristics (not user-configurable initially)
my $GROWTH_MIN_HISTORICAL_PERIODS       = 5;    # Min number of windowed periods to attempt trend
my $GROWTH_MAX_CV_THRESHOLD             = 0.50; # Max Coefficient of Variation (StdDev/Mean); if > this, data too volatile
my $GROWTH_MIN_POSITIVE_SLOPE_THRESHOLD = 0.01; # Min slope (units/period) to consider as actual growth for inflation
my $GROWTH_MAX_PROJECTION_HISTORY_RATIO = 2.0;  # Max ratio of projection duration to history duration used for trend
my $DEFAULT_GROWTH_PROJECTION_DAYS         = 90;
my $DEFAULT_MAX_GROWTH_INFLATION_PERCENT   = 25;

# RunQ Volatility Confidence Factor Thresholds & Values (adjusts additive CPU based on P90/P50 RunQ ratio)
my $VOLATILITY_SPIKY_THRESHOLD = 0.5;    my $VOLATILITY_SPIKY_FACTOR = 0.70;    # Very stable/spiky, less confidence in adding CPU
my $VOLATILITY_MODERATE_THRESHOLD = 0.8; my $VOLATILITY_MODERATE_FACTOR = 0.85; # Moderately stable
my $RUNQ_PRESSURE_SATURATION_CONFIDENCE_FACTOR = 1.0; # Full confidence if RunQ saturation is high

# --- Advanced Efficiency Adjustment Tunable Parameters ---
# Guard Rail for High Existing Constraint (CPU Downsizing Skip)
my $BASE_PHYSC_VS_MAXCPU_THRESH_FOR_CONSTRAINT_GUARD = 0.90; # If Base PhysC > 90% of MaxCPU
my $RUNQ_PRESSURE_FOR_CONSTRAINT_GUARD_FACTOR = 0.80;      # And RunQ Pressure > (this factor * saturation_threshold)

# Dynamic Blending Weights for Efficient Target (P_efficient_target_raw vs. BasePhysC)
my $NORM_P50_LOW_THRESH_FOR_BLEND1 = 0.25;       # If NormP50 < this, give more weight to raw target
my $BLEND_WEIGHT_BASE_FOR_LOW_P50_1 = 0.70;      #   70% Base / 30% Raw Target
my $NORM_P50_MODERATE_THRESH_FOR_BLEND2 = 0.40;  # If NormP50 < this (but >= BLEND1), moderate blend
my $BLEND_WEIGHT_BASE_FOR_LOW_P50_2 = 0.75;      #   75% Base / 25% Raw Target (original idea)
# If NormP50 >= MODERATE_THRESH_FOR_BLEND2 (but still low enough for efficiency consideration),
# lean more heavily on BasePhysC by default (e.g., 85% Base / 15% Target)
my $BLEND_WEIGHT_BASE_DEFAULT_LOW_P50 = 0.85;

# Volatility-Sensitive Cap for MAX_EFFICIENCY_REDUCTION_PERCENTAGE
# These thresholds are for Volatility Ratio (P90/P50)
my $VOLATILITY_MODERATE_LOW_CAP_THRESH = 1.2;  # Volatility above this starts to reduce the reduction cap
my $VOLATILITY_MODERATE_MEDIUM_CAP_THRESH = 1.5; # Intermediate threshold
my $VOLATILITY_MODERATE_HIGH_CAP_THRESH = 1.8; # Volatility above this reduces cap more significantly

# Factors to scale down MAX_EFFICIENCY_REDUCTION_PERCENTAGE
my $REDUCTION_CAP_SCALE_FOR_MODERATE_VOLATILITY = 0.66; # e.g., 15% * 0.66 = ~10% max cut
my $REDUCTION_CAP_SCALE_FOR_MODERATE_MEDIUM_VOLATILITY = 0.50; # e.g., 15% * 0.50 = 7.5% max cut
my $REDUCTION_CAP_SCALE_FOR_MODERATE_HIGH_VOLATILITY = 0.33; # e.g., 15% * 0.33 = ~5% max cut

# --- Single-Thread-Dominated (STD) Workload Heuristics ---
# These constants are used to detect bursting workloads that are likely inefficient
# and should still be considered for strategic (but not tactical) downsizing.
my $STD_NORM_P90_THRESH = 0.5; # NormRunQ P90 must be below this to be considered non-concurrent.
my $STD_IQRC_THRESH     = 0.3; # Volatility must be below this (very steady).
my $STD_PHYSC_STABILITY_THRESH = 0.15; # PhysC P90/P50 ratio must be less than 1.15.
my $STD_LCPU_TIER1_MAX = 8;     # For small VMs (less dampening)
my $STD_LCPU_TIER2_MAX = 32;    # For medium VMs (base dampening)
                                # VMs > TIER2_MAX are large (more dampening)

# --- Downsizing Guardrail Banding (AAE / PAE / IQRC) ---
# Replaces the legacy "Base PhysC > Entitlement" veto for tactical downsizing.
#
# AAE: Average Above Entitlement (daily diagnostic; entitlement-relative)
#   - negative means typical utilisation is below entitlement
#   - slightly positive means typical utilisation is above entitlement
#
# PAE: Peak Above Entitlement (daily diagnostic; entitlement-relative)
#   - negative means peak is below entitlement
#   - positive means tail frequently exceeds entitlement
#
# IQRC: Normalised RunQ volatility (NormRunQ IQRC)
#
# Bands:
#   GREEN: allow analytical downsizing to proceed normally
#   AMBER: allow downsizing, but dampen analytical reduction
#   RED:   block tactical downsizing (retain conservative behaviour)
my $DOWNSIZE_GUARD_GREEN_MAX_PAE   = 0.25;  # up to +25% tail exceedance allowed
my $DOWNSIZE_GUARD_AMBER_MAX_PAE   = 0.50;  # up to +50% tail exceedance allowed (limited)
my $DOWNSIZE_GUARD_GREEN_MAX_IQRC  = 0.60;  # moderate variability OK
my $DOWNSIZE_GUARD_AMBER_MAX_IQRC  = 1.00;  # higher variability acceptable with dampening
my $DOWNSIZE_GUARD_AMBER_MAX_AAE   = 0.05;  # allow slight typical exceedance (+5%) in AMBER
my $DOWNSIZE_GUARD_AMBER_CAP_SCALE = 0.50;  # halve the analytical reduction in AMBER band

# --- New: 2D boundary helpers (AAE x PAE) ---
#
# "Frequent but shallow" lane:
#   - PAE can exceed AMBER_MAX_PAE, but ONLY if AAE remains shallow.
#   - We intentionally align this with AMBER_MAX_AAE so you don't create a new
#     interpretation of "shallow typical exceedance".
my $DOWNSIZE_GUARD_AMBER_SHALLOW_AAE = $DOWNSIZE_GUARD_AMBER_MAX_AAE;  # 0.05

# Hard RED magnitude veto:
#   - If typical above-entitlement magnitude is meaningfully high, don't downsize,
#     regardless of frequency. This matches your earlier "AAE > 0.10 => RED" idea.
my $DOWNSIZE_GUARD_RED_HARD_AAE = 0.10;

# Frequent exceedance threshold:
#   - This is the line beyond which "frequency" is considered high. You already use
#     0.50 as the AMBER ceiling, so re-using it keeps behaviour intuitive.
my $DOWNSIZE_GUARD_RED_FREQUENT_PAE = $DOWNSIZE_GUARD_AMBER_MAX_PAE;   # 0.50

# --- Dispatch-Bound Workload (DBW) Anomaly Heuristics ---
# Detects workloads with very low CPU usage but extremely high dispatch contention.
my $DBW_LOW_UTIL_FACTOR = 0.5; # BasePhysC must be less than 50% of Entitlement.
my $DBW_DSR_THRESHOLD = 10.0;  # Dispatch Stress Ratio must be > 10.0.
my $DBW_MEDIAN_PRESSURE_THRESHOLD = 1.5; # NormRunQ P50 must be > 1.5.

# --- Enhanced Efficiency Factor Constants (for calculate_runq_modified_physc) ---
my $VOLATILITY_CAUTION_THRESHOLD = 2.5; # If NormRunQ P90/P50 ratio >= this, skip efficiency reduction
my $NORM_P50_THRESHOLD_FOR_EFFICIENCY_CONSIDERATION = 0.5; # NormRunQ P50 must be below this to consider efficiency
my $MAX_EFFICIENCY_REDUCTION_PERCENTAGE  = 0.25; # Max % a profile can be reduced by efficiency logic (ADAPTIVE)
my $MIN_P50_DENOMINATOR_FOR_VOLATILITY = 0.1;    # Min P50 value to avoid division by zero in volatility calc
my $DEFAULT_TARGET_NORM_RUNQ_FOR_EFFICIENCY_CALC = 0.8; # Base, SMT-dependent adjustments in calc sub (ADAPTIVE)

# --- Hot Thread Workload (HTW) Additive Dampening Heuristics ---
# These constants are used to detect and dampen additive CPU for workloads
# that appear constrained (e.g., single-threaded) despite high normalized RunQ.
my $HOT_THREAD_WL_ENT_FACTOR = 0.80;    # BasePhysC < Entitlement * this_factor
my $HOT_THREAD_WL_MAXCPU_FACTOR = 0.25;   # OR BasePhysC < MaxCPU * this_factor
my $HOT_THREAD_WL_HIGH_NORM_P50_THRESHOLD = 3.0; # NormRunQ P50 > this_threshold
# RUNQ_PRESSURE_P90_SATURATION_THRESHOLD (1.8) is an existing constant, also used here.
my $HOT_THREAD_WL_IQRC_THRESHOLD = 1.0;    # NormRunQ_IQRC > this_threshold (tune based on data)
my $HOT_THREAD_WL_DETECTION_MIN_CONDITIONS_MET = 4; # Minimum number of detection conditions to be met

# Dynamic Dampening Multipliers for HTW
my $HOT_THREAD_WL_BASE_DAMPENING_FACTOR = 0.25; # Base factor for the dynamic dampening calculation
my $HOT_THREAD_WL_MIN_DYNAMIC_DAMPENING = 0.05; # Floor for the final dynamic dampening factor
my $HOT_THREAD_WL_MAX_DYNAMIC_DAMPENING = 0.75; # Ceiling for the final dynamic dampening factor

# Enhanced Safety Caps for Additive CPU (applied AFTER all other factors)
my $ADDITIVE_CPU_SAFETY_CAP_FACTOR_OF_BASE = 2.0; # Max additive CPU as a multiple of BasePhysC
my $ADDITIVE_CPU_SAFETY_CAP_ABSOLUTE = 0.5;       # Absolute maximum additive CPU in cores

my $POOL_CONSTRAINT_CONFIDENCE_FACTOR = 0.80; # Reduction factor for additive CPU if VM is in a non-default pool

# --- Built-in default Target NormRunQ values for the adaptive heuristic ---
my %DEFAULT_RUNQ_TARGETS = (
    'P'  => 0.25,
    'O1' => 0.30, 'O2' => 0.40, 'O3' => 0.50, 'O4' => 0.65,
    'G1' => 0.80, 'G2' => 0.90, 'G3' => 1.00, 'G4' => 1.10,
    'B1' => 1.20, 'B2' => 1.30, 'B3' => 1.40, 'B4' => 1.50,
);
my %custom_runq_targets;
my %tier_override_for_csv;

my $CACHE_STATES_FILE = ".nfit.cache.states";
my $DATA_CACHE_FILE   = ".nfit.cache.data";
my $UNIFIED_HISTORY_FILE = ".nfit.history.json";    # Unified history file

# --- Profile Definitions (Loaded from file) ---
my @profiles;
my @csv_visible_profiles;
my $PEAK_PROFILE_NAME = "Peak"; # Standardized name for the peak metric column in output
my @output_header_cols_csv;

# --- Argument Parsing ---
my $physc_data_file;
my $runq_data_file_arg;
my $vm_config_file_arg;
my $profiles_config_file_arg;
my $start_date_str;
my $end_date_str;
my $target_vm_name;
my $round_arg;                     # For nfit's -r (round to nearest)
my $roundup_arg;                   # For nfit's -u (round up)
my $default_smt_arg = $DEFAULT_SMT_VALUE_PROFILE;
my $runq_norm_perc_list_str = $DEFAULT_RUNQ_NORM_PERCS; # Global default for nfit-profile itself
my $runq_abs_perc_list_str  = $DEFAULT_RUNQ_ABS_PERCS;  # Global default for nfit-profile itself
my $runq_perc_behavior_mode = 'fixed';                  # Default behavior: use fixed P90 for RunQ. Alternative: 'match'.
my $help = 0;
my $show_version = 0;
my $script_dir = dirname(abs_path($0));

my $LOG_FILE_PATH = "$script_dir/output/nfit-profile.log";        # Default log file path
my $LOG_FILE_DIR = dirname(abs_path($LOG_FILE_PATH));
my $log_file_path_for_run = $LOG_FILE_PATH;

my $nfit_script_path = "$script_dir/nfit"; # Default path to nfit script
my $nfit_enable_windowed_decay = 0;        # Flag to instruct nfit to use its internal decay
my $nfit_window_unit_str = $DEFAULT_PROCESS_WINDOW_UNIT_FOR_NFIT;
my $nfit_window_size_val = $DEFAULT_PROCESS_WINDOW_SIZE_FOR_NFIT;
my $nfit_decay_half_life_days_val = $DEFAULT_DECAY_HALF_LIFE_DAYS_FOR_NFIT;
my $nfit_analysis_reference_date_str;
my $nfit_runq_avg_method_str = $DEFAULT_NFIT_RUNQ_AVG_METHOD; # 'none', 'sma', 'ema' for nfit's RunQ processing
my $nfit_decay_over_states = 0;
my $nmon_dir;
my $excel_formulas_flag = "false";
my $mgsys_filter;
my $verbose = 0;
my $debug = 0;
my $force_update = 0;
my $use_l1_baseline = 0;  # Phase 6: Emergency escape hatch — forces legacy L1 cache baseline

# Seasonality related variables
my $apply_seasonality_event;
my $update_history_flag = 0; # New unified history update flag
my $min_history_days_arg;    # New flag for partial month processing
my $reset_seasonal_cache = 0;
my $seasonal_auto_flag = 0; # Automatic Seasonal Discovery Flag
my $DEFAULT_SEASONALITY_CONFIG_FILE = "nfit.seasonality.cfg";
my $seasonal_scope = 'latest'; # Phase 3: 'latest' (default) or 'all'

# Global flags for seasonality execution context
my $is_multiplicative_forecast_run = 0;
my $is_predictive_peak_model_run = 0;

# The Global Lock ensures atomic access to the history directory.
my $HISTORY_LOCK_FILENAME = ".nfit.history.lock";

GetOptions(
    'nmondir=s'                  => \$nmon_dir,
    'config=s'                   => \$vm_config_file_arg,
    'profiles-config=s'          => \$profiles_config_file_arg,
    'startdate|s=s'              => \$start_date_str,
    'enddate|e=s'                => \$end_date_str,
    'vm|lpar=s'                  => \$target_vm_name,
    'round|r:f'                  => \$round_arg,
    'roundup|u:f'                => \$roundup_arg,
    'default-smt|smt=i'          => \$default_smt_arg,
    'runq-norm-percentiles=s'    => \$runq_norm_perc_list_str,      # Global default list for NormRunQ
    'runq-abs-percentiles=s'     => \$runq_abs_perc_list_str,       # Global default list for AbsRunQ
    'runq-perc-behavior=s'       => \$runq_perc_behavior_mode,      # default: use 'AbsRunQ_P90' for all profiles ('match': match the RunQ percentile to the PhysC profile percentile)
    'help|h'                     => \$help,
    'nfit-path=s'                => \$nfit_script_path,
    'version'                    => \$show_version,
    'enable-windowed-decay'      => \$nfit_enable_windowed_decay,
    'process-window-unit=s'      => \$nfit_window_unit_str,
    'process-window-size=i'      => \$nfit_window_size_val,
    'decay-half-life-days=i'     => \$nfit_decay_half_life_days_val,
    'analysis-reference-date=s'  => \$nfit_analysis_reference_date_str,
    'runq-avg-method=s'          => \$nfit_runq_avg_method_str,
    'decay-over-states'          => \$nfit_decay_over_states,
    'excel-formulas=s'           => \$excel_formulas_flag,
    'mgsys|system|serial|host=s' => \$mgsys_filter,
    'apply-seasonality=s'        => \$apply_seasonality_event,
    'seasonal'                   => \$seasonal_auto_flag,
    'seasonal-scope=s'           => \$seasonal_scope,
    'update-history'             => \$update_history_flag,
    'min-history-days=i'         => \$min_history_days_arg,
    'reset-seasonal-cache'       => \$reset_seasonal_cache,
    "v|verbose+"                 => \$verbose,
    "d|debug"                    => \$debug,
    'force'                      => \$force_update,
    'use-l1-baseline'            => \$use_l1_baseline,
) or die usage_wrapper();

print STDERR "nfit-profile version $VERSION\n" if (!defined $show_version);

unless($help or $show_version) {
    _phase("Initialisation");
};

# --- Validation ---
my $nfit_ver = "N/A"; # Store nfit version
# This block now runs every time to get the nfit version for logging
if (-x $nfit_script_path) {
	my $nfit_ver_output = `$nfit_script_path --version 2>&1`;
	my ($parsed_nfit_ver) = ($nfit_ver_output =~ /nfit version\s*([0-9.a-zA-Z-]+)/i);
	if (defined $parsed_nfit_ver) {
		$nfit_ver = $parsed_nfit_ver;
		# Check nfit version compatibility for certain features
		my $required_nfit_ver_for_windowing = "2.27.0";
		my $required_nfit_ver_for_runq_avg_and_decay = "2.28.0.4";

		if ($nfit_enable_windowed_decay && version->parse($nfit_ver) < version->parse($required_nfit_ver_for_windowing)) {
			print STDERR " [WARN] The `--enable-windowed-decay` option requires nfit version $required_nfit_ver_for_windowing or higher. Your nfit version ($nfit_ver) may not support this\n";
		}
		if (defined $nfit_runq_avg_method_str && $nfit_runq_avg_method_str ne 'none' && version->parse($nfit_ver) < version->parse($required_nfit_ver_for_runq_avg_and_decay)) {
			print STDERR " [WARN] The `--runq-avg-method (sma/ema)` option requires features from nfit version $required_nfit_ver_for_runq_avg_and_decay or higher. Your nfit version ($nfit_ver) behavior might differ for RunQ processing, especially if `--runq-decay` is intended\n";
		}
	} else {
		print STDERR "  [WARN] Unable to determine nfit version from output: $nfit_ver_output\n";
		if ($nfit_enable_windowed_decay || (defined $nfit_runq_avg_method_str && $nfit_runq_avg_method_str ne 'none')) {
			print STDERR " [WARN] Advanced nfit features are enabled but nfit version cannot be verified\n";
		}
	}
} else {
    print STDERR "  [ERROR] nFit engine ($nfit_script_path) not found or not executable\n";
    exit 2;
}

if ($show_version)
{
    print STDERR "nfit-profile version $VERSION\n";
    print STDERR "  nfit version $nfit_ver\n";
    exit 0;
}

if (defined $seasonal_scope && length $seasonal_scope) {
    $seasonal_scope = lc($seasonal_scope);
    if ($seasonal_scope ne 'latest' && $seasonal_scope ne 'all') {
        die "  [ERROR] Invalid --seasonal-scope '$seasonal_scope' (expected: latest|all)\n";
    }
}

# --- Validation for runq-perc-behavior ---
$runq_perc_behavior_mode = lc($runq_perc_behavior_mode);
unless ($runq_perc_behavior_mode eq 'fixed' || $runq_perc_behavior_mode eq 'match')
{
    die "  [ERROR] Invalid value for --runq-perc-behavior. Must be 'fixed' or 'match' instead of '$runq_perc_behavior_mode'.\n";
}

# --- Data Source Validation ---
# The script now operates exclusively on pre-built cache directories.
my $DEFAULT_BASE_STAGE_DIR = File::Spec->catfile($script_dir, 'stage');

if ($help)
{
    print STDERR usage_wrapper();
    exit 0;
}

# A cache source must be specified via --nmondir or --mgsys.
if (!defined $nmon_dir && !defined $mgsys_filter)
{
    print STDERR usage_wrapper();
    die "[ERROR] No data source cache specified. Please use --nmondir or --mgsys.\n";
}

# If --mgsys is provided without a base --nmondir, set --nmondir to the default.
# This allows the Smart Dispatcher to find the system-specific cache.
if (defined $mgsys_filter && !defined $nmon_dir)
{
    $nmon_dir = $DEFAULT_BASE_STAGE_DIR;
    # Only print the info message if it's a regular run, not a staging/snapshotting run.
    if (!$update_history_flag) {
        print STDERR "  ↳  Staging Directory: '$nmon_dir'\n";
    }
}

# Final check to ensure the base directory exists.
if (defined $nmon_dir && !-d $nmon_dir)
{
    die "[ERROR] The specified cache directory (--nmondir) was not found: '$nmon_dir'\n";
}


if ($default_smt_arg <= 0)
{
    die "[ERROR] --default-smt value must be a positive integer (e.g., 4, 8).\n";
}
if (! -x $nfit_script_path)
{
    die "[ERROR] Cannot find or execute 'nfit' script at '$nfit_script_path'. Use --nfit-path.\n";
}
if (defined($round_arg) && defined($roundup_arg))
{
    die "[ERROR] -round (-r) and -roundup (-u) options are mutually exclusive.\n";
}
if (defined $start_date_str && $start_date_str !~ /^\d{4}-\d{2}-\d{2}$/)
{
    die "[ERROR] Invalid startdate (-s) format '$start_date_str'. Use YYYY-MM-DD.\n";
}
if (defined $end_date_str && $end_date_str !~ /^\d{4}-\d{2}-\d{2}$/)
{
    die "[ERROR] Invalid enddate (-e) format '$end_date_str'. Use YYYY-MM-DD.\n";
}
if (defined $start_date_str && defined $end_date_str)
{
    my ($s_tp, $e_tp);
    eval { $s_tp = Time::Piece->strptime($start_date_str, "%Y-%m-%d"); };
    eval { $e_tp = Time::Piece->strptime($end_date_str, "%Y-%m-%d"); };

    if ($s_tp && $e_tp && $e_tp < $s_tp)
    {
        die "[ERROR] --enddate ($end_date_str) cannot be before --startdate ($start_date_str).\n";
    }
}

# Validate the relationship between enddate and analysis-reference-date
if (defined $end_date_str && defined $nfit_analysis_reference_date_str)
{
    my ($e_tp, $ref_tp);
    eval { $e_tp = Time::Piece->strptime($end_date_str, "%Y-%m-%d"); };
    eval { $ref_tp = Time::Piece->strptime($nfit_analysis_reference_date_str, "%Y-%m-%d"); };

    if ($e_tp && $ref_tp && $ref_tp > $e_tp)
    {
        die "[ERROR] --analysis-reference-date ($nfit_analysis_reference_date_str) cannot be after --enddate ($end_date_str).\n" .
            "       The reference date is used to calculate recency weights and should not be beyond the data cutoff.\n" .
            "       If you want to analyse data up to $end_date_str, either:\n" .
            "         1. Omit --analysis-reference-date (it will default to the last data point), or\n" .
            "         2. Set --analysis-reference-date to $end_date_str or earlier.\n";
    }
}

# Validations for nfit's windowed decay options, if enabled by nfit-profile
if ($nfit_enable_windowed_decay && $nfit_decay_over_states)
{
    die "[ERROR] --enable-windowed-decay and --decay-over-states are mutually exclusive analysis modes.\n";
}
if ($nfit_enable_windowed_decay)
{
    if ($nfit_window_unit_str ne "days" && $nfit_window_unit_str ne "weeks")
    {
        die "[ERROR] --process-window-unit must be 'days' or 'weeks'.\n";
    }
    if ($nfit_window_size_val < 1)
    {
        die "[ERROR] --process-window-size must be at least 1.\n";
    }
    if ($nfit_decay_half_life_days_val < 1)
    {
        die "[ERROR] --decay-half-life-days must be at least 1.\n";
    }
    if (defined $nfit_analysis_reference_date_str && $nfit_analysis_reference_date_str !~ /^\d{4}-\d{2}-\d{2}$/)
    {
        die "[ERROR] Invalid --analysis-reference-date format. Use YYYY-MM-DD.\n";
    }
    print STDERR "  Φ Enabled Windowed Decay processing (window size: $nfit_window_size_val $nfit_window_unit_str)\n";
}

# Validate nfit's RunQ averaging method, if specified
if (defined $nfit_runq_avg_method_str)
{
    $nfit_runq_avg_method_str = lc($nfit_runq_avg_method_str);
    unless ($nfit_runq_avg_method_str eq 'none' || $nfit_runq_avg_method_str eq 'sma' || $nfit_runq_avg_method_str eq 'ema')
    {
        die "[ERROR] The `--runq-avg-method` option requires 'none', 'sma', or 'ema'. Invalid option: '$nfit_runq_avg_method_str'.\n";
    }
}

# --- Excel Formula Validation ---
my $add_excel_formulas = 0; # Default to off
if (defined $excel_formulas_flag)
{
    # If flag is just --excel-formulas, $excel_formulas_flag will be empty string.
    # Default to 'true' if flag is present but no value is given.
    my $val = lc($excel_formulas_flag // 'true');
    if ($val eq 'true' || $val eq '1')
    {
        $add_excel_formulas = 1;
    }
    elsif ($val eq 'false' || $val eq '0')
    {
        $add_excel_formulas = 0;
    }
    else
    {
        die "[ERROR] Invalid value for --excel-formulas. Must be 'true', 'false', or omitted. Got '$excel_formulas_flag'.\n";
    }
}

my $output_dir = File::Spec->catfile($script_dir, 'output');
make_path($output_dir);
if (! -d $output_dir) {
    die "Unable to create output directory '$output_dir': $!";
}

my @generated_files; # Global array to store names of all generated files

# --- Smart Dispatcher Logic ---
# This block determines which cache directories to process. It can handle
# being pointed at a single cache directory or a parent directory containing
# multiple system-specific caches.

my @target_systems_to_process;
my $base_cache_dir = $nmon_dir; # Assume the provided dir is the base by default.

# First, check if the provided --nmondir is ITSELF a valid cache directory.
if (-f File::Spec->catfile($nmon_dir, '.nfit_stage_id'))
{
    # This is a singular run targeting a specific cache directory.
    print STDERR "  ↳  Single-cache mode enabled ($nmon_dir)\n";
    push @target_systems_to_process, undef; # 'undef' signals a single run.
    $base_cache_dir = dirname($nmon_dir); # The base is the parent of the cache dir.
}
else
{
    # If not, check if it's a PARENT directory containing multiple caches.
    opendir(my $dh, $nmon_dir) or die "Cannot open directory $nmon_dir: $!";
    my @subdirs = grep { -d File::Spec->catfile($nmon_dir, $_) && !/^\./ } readdir($dh);
    closedir($dh);

    my @found_serials;
    foreach my $subdir (@subdirs)
    {
        if (-f File::Spec->catfile($nmon_dir, $subdir, '.nfit_stage_id'))
        {
            push @found_serials, $subdir;
        }
    }

    if (@found_serials)
    {
        # This is a multi-system run.
        print STDERR "  ↳  Multi-cache mode enabled ($nmon_dir)\n";
        $base_cache_dir = $nmon_dir; # The provided dir is the base.

        # Apply --mgsys filter if provided, otherwise target all found systems.
        if (defined $mgsys_filter)
        {
            my %serials_from_args = map { $_ => 1 } split /,/, $mgsys_filter;
            @target_systems_to_process = grep { exists $serials_from_args{$_} } @found_serials;
        }
        else
        {
            @target_systems_to_process = @found_serials;
        }
    }
    else
    {
        # The directory is neither a cache itself, nor does it contain any caches.
        die "[ERROR] The directory '$nmon_dir' is not a valid nFit cache and does not contain any cache subdirectories.\n";
    }
}

# If a --vm filter was provided, we must refine our list of target systems.
# This only makes sense in a multi-system context where we can check each one.
if (scalar(@target_systems_to_process) > 1 && defined $target_vm_name)
{
    my %vms_to_find = map { $_ => 1 } split /,/, $target_vm_name;
    my %systems_with_target_vms;

    foreach my $serial (@target_systems_to_process)
    {
        my $states_file = File::Spec->catfile($base_cache_dir, $serial, $CACHE_STATES_FILE);
        next unless -f $states_file;

        my $json_text = do { open my $fh, '<:encoding(utf8)', $states_file or next; local $/; my $content = <$fh>; close $fh; $content; };
        next unless defined $json_text;

        my $states = eval { decode_json($json_text) };
        if ($@) { warn " [WARN] Could not decode JSON from '$states_file': $@. Skipping for VM discovery."; next; }

        foreach my $vm_in_state (keys %$states)
        {
            if (exists $vms_to_find{$vm_in_state})
            {
                $systems_with_target_vms{$serial} = 1;
                last; # Found a match for this system.
            }
        }
    }
    # The new target list is only the systems that contain the specified VMs.
    @target_systems_to_process = sort keys %systems_with_target_vms;
}

if (!@target_systems_to_process)
{
    die "[ERROR] No target managed systems could be identified for processing based on the provided filters.\n";
}

my $s = (scalar(@target_systems_to_process) != 1) ? 's' : '';
print STDERR "  ↳  Dispatcher detected " . scalar(@target_systems_to_process) . " managed system$s\n";

# --- Load Profile Definitions ---
# The log file will now be created inside the main processing loop for each system.
print STDERR "\n[+] Loading Profile Definitions\n";
my $profiles_config_path_to_load;
if (defined $profiles_config_file_arg)
{
    if (-f $profiles_config_file_arg)
    {
        $profiles_config_path_to_load = $profiles_config_file_arg;
    }
    else
    {
        die "[ERROR] Specified profiles config (--profiles-config) not found: $profiles_config_file_arg\n";
    }
}
else # Attempt to find default profiles config
{
    $profiles_config_path_to_load = "$script_dir/etc/$DEFAULT_PROFILES_CONFIG_FILE";
    unless (-f $profiles_config_path_to_load)
    {
        $profiles_config_path_to_load = "$script_dir/$DEFAULT_PROFILES_CONFIG_FILE"; # Try in script's root
    }
    unless (-f $profiles_config_path_to_load)
    {
        die "[ERROR] Default profiles config '$DEFAULT_PROFILES_CONFIG_FILE' not found in '$script_dir/etc/' or '$script_dir/'. Use --profiles-config.\n";
    }
}
print STDERR "  ↳  Profile configuration: $profiles_config_path_to_load\n";
@profiles = load_profile_definitions($profiles_config_path_to_load);
if (scalar @profiles == 0)
{
    die "[ERROR] No profiles loaded from '$profiles_config_path_to_load'.\n";
}

# Ensure the mandatory peak helper profile exists even if the configuration is incomplete.
# This profile is required for peak/RunQ evidence, credibility-adjusted peak floors, and
# global pressure hints. When injected, it is hidden from the planner CSV by default.
my $mandatory_peak_present = 0;
foreach my $p (@profiles) {
    next if (!defined $p->{name});
    if ($p->{name} eq $MANDATORY_PEAK_PROFILE_FOR_HINT || $p->{name} eq $LEGACY_MANDATORY_PEAK_PROFILE_FOR_HINT) {
        $mandatory_peak_present = 1;
        last;
    }
}

if (!$mandatory_peak_present) {
    push @profiles, {
        name          => $MANDATORY_PEAK_PROFILE_FOR_HINT,
        nfit_flags    => $DEFAULT_MANDATORY_PEAK_PROFILE_FLAGS,
        runq_behavior => 'default',
        csv_output    => 0,   # keep helper transparent by default
    };
    print STDERR "  [WARN] Mandatory profile '$MANDATORY_PEAK_PROFILE_FOR_HINT' was missing; injected built-in default (hidden from CSV).\n";
}

# --- Peak helper + mission-critical peak profile normalisation (must run before CSV profile selection) ---
my $peak_helper_ref;
my $legacy_peak_helper_ref;
my $p99w1_profile_ref;

foreach my $p (@profiles) {
    $peak_helper_ref        = $p if (defined $p->{name} && $p->{name} eq $MANDATORY_PEAK_PROFILE_FOR_HINT);
    $legacy_peak_helper_ref = $p if (defined $p->{name} && $p->{name} eq $LEGACY_MANDATORY_PEAK_PROFILE_FOR_HINT);
    $p99w1_profile_ref      = $p if (defined $p->{name} && $p->{name} eq $MISSION_CRITICAL_PROFILE_NAME);
}

# Back-compat: treat legacy helper name as the new helper name
if (!$peak_helper_ref && $legacy_peak_helper_ref) {
    $legacy_peak_helper_ref->{name} = $MANDATORY_PEAK_PROFILE_FOR_HINT;
    $peak_helper_ref = $legacy_peak_helper_ref;
    print STDERR "  [WARN] Profile '$LEGACY_MANDATORY_PEAK_PROFILE_FOR_HINT' is deprecated; treating it as '$MANDATORY_PEAK_PROFILE_FOR_HINT'.\n";
}

# IMPORTANT: after aliasing/renaming, re-evaluate whether an actionable P-99W1 exists.
# The legacy helper rename can otherwise trick the earlier scan into thinking P-99W1 exists.
$p99w1_profile_ref = undef;
foreach my $p (@profiles) {
    $p99w1_profile_ref = $p if (defined $p->{name} && $p->{name} eq $MISSION_CRITICAL_PROFILE_NAME);
}

# Ensure actionable P-tier profile exists and is CSV-visible
if (!$p99w1_profile_ref && $peak_helper_ref) {
    my %clone = %{$peak_helper_ref};
    $clone{name} = $MISSION_CRITICAL_PROFILE_NAME;  # "P-99W1"
    $clone{csv_output} = 1;                         # ensure it appears in CSV for all standard models
    push @profiles, \%clone;
    print STDERR "    ✓ Added built-in profile '$MISSION_CRITICAL_PROFILE_NAME' (CSV-visible)\n";
} elsif ($p99w1_profile_ref) {
    # If config defines it, force CSV visibility unless explicitly disabled elsewhere
    $p99w1_profile_ref->{csv_output} = 1;
}

# Defensive: deduplicate profile list by name (keep first occurrence)
my %seen_profile_name;
@profiles = grep {
    my $n = $_->{name};
    defined $n && !$seen_profile_name{$n}++
} @profiles;

# Enforce ordering: Peak helper first, then actionable P-tier profile, then the rest.
my (@ordered, @rest);
foreach my $p (@profiles) {
    next if (!defined $p->{name});
    if ($p->{name} eq $MANDATORY_PEAK_PROFILE_FOR_HINT) {
        push @ordered, $p;
    } elsif ($p->{name} eq $MISSION_CRITICAL_PROFILE_NAME) {
        push @ordered, $p;
    } else {
        push @rest, $p;
    }
}
@profiles = (@ordered, @rest);

# Filter to only profiles with csv_output enabled
@csv_visible_profiles = grep { $_->{csv_output} } @profiles;

if (scalar @csv_visible_profiles < scalar @profiles) {
    my $invisible = scalar @profiles - scalar @csv_visible_profiles;
    print STDERR "    ✓ Loaded " . scalar(@profiles) . " workload profiles ($invisible invisible)\n";
} else {
    print STDERR "    ✓ Loaded " . scalar(@profiles) . " workload profiles\n";
}

# load custom VM tiers
my %vm_tier_overrides = %{parse_vm_tier_overrides("$script_dir/etc/nfit.vms.cfg")};

# --- Generate CSV header columns ---

# Define the order of columns for the CSV output
@output_header_cols_csv = (
    "VM", "TIER", "Hint", "Pattern", "Pressure", "PressureDetail", "SMT",
    "Serial", "SystemType", "Pool Name", "Pool ID",
    "RunQ_Tactical", "RunQ_Strategic", "RunQ_Potential", "RunQ_Source", $PEAK_PROFILE_NAME
);

# Add interleaved profile + rVCPU columns (rVCPU is emitted for all visible profiles
# EXCEPT the mandatory peak helper profile (P99W1), which is not a workload profile).
foreach my $p (@csv_visible_profiles) {
    push @output_header_cols_csv, $p->{name};
    if (defined $p->{name} && $p->{name} ne $MANDATORY_PEAK_PROFILE_FOR_HINT) {
        push @output_header_cols_csv, $p->{name} . "_rVCPU";
    }
}

# Add current entitlement + current VCPU + constraint flags
push @output_header_cols_csv, ("Current - ENT", "Current - VCPU", "NFIT - ENT", "VCPU_Constraints");

if ($add_excel_formulas) {
    push @output_header_cols_csv, ("NETT", "NETT%");
}

# --- Ensure the Mandatory P-99W1 Profile is Defined ---
# This check occurs after profiles are loaded.
my $mandatory_profile_is_present = 0;
foreach my $profile_entry (@profiles)
{
    if (defined $profile_entry->{name} && $profile_entry->{name} eq $MANDATORY_PEAK_PROFILE_FOR_HINT)
    {
        $mandatory_profile_is_present = 1;
        last;
    }
}

unless ($mandatory_profile_is_present)
{
    die "[ERROR] Mandatory peak helper profile '$MANDATORY_PEAK_PROFILE_FOR_HINT' is not defined in: '$profiles_config_path_to_load'.\n" .
    "  This profile is essential for RunQ pressure detection and peak-floor calculations.\n" .
    "  Back-compat note: older configurations used '$LEGACY_MANDATORY_PEAK_PROFILE_FOR_HINT' for this purpose.\n" .
    "  Please add '$MANDATORY_PEAK_PROFILE_FOR_HINT' (preferred) or '$LEGACY_MANDATORY_PEAK_PROFILE_FOR_HINT' (legacy) to your profiles config.\n";
}

# --- Load Seasonality Definitions ---
my $seasonality_config_path;
my $seasonality_config_file_arg; # Assume this could be a future flag
my $seasonality_config;

if (defined $seasonality_config_file_arg && -f $seasonality_config_file_arg) {
    $seasonality_config_path = $seasonality_config_file_arg;
} else {
    my $default_path_etc = "$script_dir/etc/$DEFAULT_SEASONALITY_CONFIG_FILE";
    my $default_path_root = "$script_dir/$DEFAULT_SEASONALITY_CONFIG_FILE";
    if (-f $default_path_etc) {
        $seasonality_config_path = $default_path_etc;
    } elsif (-f $default_path_root) {
        $seasonality_config_path = $default_path_root;
    }
}

if ($seasonality_config_path) {
    print STDERR "  ↳  Seasonality rules: $seasonality_config_path\n";
    # Replace Config::Tiny->read() with our custom, dependency-free parser.
    $seasonality_config = parse_seasonality_config($seasonality_config_path);
    unless (defined $seasonality_config) {
        # The custom sub will die on error, but this is a safeguard.
        die "[ERROR] Could not parse seasonality configuration file '$seasonality_config_path'.\n";
    }
    my $event_count = scalar(keys %$seasonality_config);
    print STDERR "    ✓ Loaded $event_count seasonal event definitions\n";
} elsif ($apply_seasonality_event || $update_history_flag) {
    # It's an error to request seasonality if the config file doesn't exist.
    die "  [ERROR] A seasonality operation was requested, but '$DEFAULT_SEASONALITY_CONFIG_FILE' could not be found in '$script_dir/etc/' or '$script_dir/'.\n";
}

# --- Determine Minimum Days for History Processing ---
my $min_days_for_history;
my $MIN_HISTORY_DAYS_DEFAULT = 28; # Hard-coded default

if (defined $min_history_days_arg) {
    $min_days_for_history = $min_history_days_arg; # Command line takes precedence
} elsif (defined $seasonality_config && exists $seasonality_config->{Global}{min_history_days}) {
    $min_days_for_history = $seasonality_config->{Global}{min_history_days}; # Config file is second
} else {
    $min_days_for_history = $MIN_HISTORY_DAYS_DEFAULT; # Fallback to default
}

# --- Validation for Seasonality and Decay Model Interaction ---
if (defined $apply_seasonality_event) {
    # Abort if the specified event doesn't exist in the loaded configuration.
    unless (exists $seasonality_config->{$apply_seasonality_event}) {
        die "  [ERROR] Seasonal event '$apply_seasonality_event' could not be found as a valid section in $seasonality_config_path\n";
    }

    my $event_config = $seasonality_config->{$apply_seasonality_event} // {};
    my $model_type = $event_config->{model} // '';

    # The multiplicative_seasonal model is a complete workflow and cannot be
    # combined with the top-level decay flags.
    if ($model_type eq 'multiplicative_seasonal' && ($nfit_enable_windowed_decay || $nfit_decay_over_states)) {
        die "  [ERROR] Incompatible arguments. The '--apply-seasonality' flag for a 'multiplicative_seasonal' event cannot be used with '--enable-windowed-decay' or '--decay-over-states'.\n";
    }
    # Automatically enable the correct decay model if needed
    elsif ($model_type eq 'recency_decay') {
        # Silently enable wndowed decay processing for recency_decay
        $nfit_enable_windowed_decay = 1; # Use the time-based windowed decay engine.
        $nfit_decay_over_states = 0;     # Ensure the state-based engine is disabled.
    }
}

# --- Locate and Load VM Configuration Data ---
# This data provides SMT, MaxCPU, Entitlement, etc., per VM.
my $vm_config_file_path = undef;
my $vm_config_found = 0;
my %vm_config_data;         # Stores parsed VM config: $vm_config_data{hostname}{key} = value
my %vm_config_col_idx;      # Maps column names (lowercase) to their index in the CSV
my $vm_config_header_count = 0; # Number of columns in VM config header

if (defined $vm_config_file_arg) # User specified a VM config file
{
    if (-f $vm_config_file_arg)
    {
        $vm_config_file_path = $vm_config_file_arg;
        $vm_config_found = 1;
        print STDERR "  ↳  VM configuration: $vm_config_file_path\n";
    }
    else
    {
        die "[ERROR] Specified VM configuration file (-config) not found: $vm_config_file_arg\n";
    }
}
elsif (!defined $apply_seasonality_event)
{
    # No VM config file specified, try default locations
    my $dp_etc = "$script_dir/etc/$DEFAULT_VM_CONFIG_FILE";  # Default path: script_dir/etc/
    my $dp_root = "$script_dir/$DEFAULT_VM_CONFIG_FILE"; # Alternative path: script_dir/
    if (-f $dp_etc)
    {
        $vm_config_file_path = $dp_etc;
        $vm_config_found = 1;
        print STDERR "  ↳  VM configuration: $vm_config_file_path\n";
    }
    elsif (-f $dp_root)
    {
        $vm_config_file_path = $dp_root;
        $vm_config_found = 1;
        print STDERR "  ↳  VM configuration: $vm_config_file_path\n";
    }
    else {
        print STDERR "    [WARN] Default VM configuration file not found ($DEFAULT_VM_CONFIG_FILE); VM metadata will be incomplete\n";
    }
}

if ($vm_config_found)
{
    open my $cfg_fh, '<:encoding(utf8)', $vm_config_file_path or die "[ERROR] Cannot open VM config file '$vm_config_file_path': $!\n";
    my $hdr = <$cfg_fh>; # Read header line
    unless (defined $hdr)
    {
        die "    [ERROR] Could not read header from VM config '$vm_config_file_path'\n";
    }
    chomp $hdr;
    $hdr =~ s/\r$//;        # Remove CR if present (Windows line endings)
    $hdr =~ s/^\x{FEFF}//;  # Remove BOM if present (UTF-8 Byte Order Mark)
    my @rhdrs = split /,/, $hdr;
    $vm_config_header_count = scalar @rhdrs;
    my %hmap; # Map lowercase header name to index
    for my $i (0 .. $#rhdrs)
    {
        my $cn = $rhdrs[$i];
        $cn =~ s/^\s*"?|"?\s*$//g; # Trim spaces and quotes from column name
        if ($cn ne '')
        {
            $hmap{lc($cn)} = $i; # Store lowercase column name
        }
    }

    # Check for required columns
    my @req_cols = qw(hostname serial systemtype procpool_name procpool_id entitledcpu maxcpu);
    my $has_smt_col = exists $hmap{'smt'}; # Check if SMT column exists
    unless (exists $hmap{'maxcpu'})
    {
        print STDERR "    [WARN] 'maxcpu' column not present in VM configuration file. This will affect MaxCPU capping logic\n";
    }
    if (!$has_smt_col)
    {
        print STDERR "    [WARN] 'SMT' column not present in VM configuration file. Using default SMT: $default_smt_arg for RunQ calculations\n";
    }

    foreach my $rc (@req_cols)
    {
        unless (exists $hmap{$rc})
        {
            die "    [ERROR] Required column '$rc' could not be found in VM configuration file ($vm_config_file_path).\n";
        }
        $vm_config_col_idx{$rc} = $hmap{$rc};
    }
    if ($has_smt_col) # If SMT column exists, store its index
    {
        $vm_config_col_idx{'smt'} = $hmap{'smt'};
    }

    # Read data lines from VM config
    while (my $ln = <$cfg_fh>)
    {
        chomp $ln;
        $ln =~ s/\r$//;
        next if $ln =~ /^\s*$/; # Skip empty lines

        # Attempt to parse CSV with quoted fields (handles commas within quotes)
        my @rvals = ($ln =~ /"([^"]*)"/g);
        if (scalar @rvals != $vm_config_header_count)
        { # Fallback to simple comma split if quote parsing fails or count mismatches
            @rvals = split /,/, $ln;
            if (scalar @rvals != $vm_config_header_count)
            {
                warn "    [WARN] Mismatched field count on VM config line $. Skipping: $ln\n";
                next;
            }
            # Trim whitespace for values from simple split
            $_ =~ s/^\s+|\s+$//g for @rvals;
        }
        # Else, if quote parsing worked, values in @rvals are already unquoted and trimmed by regex.

        my $hn = $rvals[ $vm_config_col_idx{'hostname'} ]; # Get hostname
        if (defined $hn && $hn ne '')
        {
            my $smt_v = $default_smt_arg; # Default SMT value
            if ($has_smt_col && defined $rvals[$vm_config_col_idx{'smt'}] && $rvals[$vm_config_col_idx{'smt'}] ne '')
            {
                my $sf = $rvals[$vm_config_col_idx{'smt'}];
                if ($sf =~ /(\d+)$/) # Extract trailing digits for SMT value (e.g., "SMT4" -> 4)
                {
                    $smt_v = $1;
                    if ($smt_v <= 0) # Validate SMT
                    {
                        warn "    [WARN] Invalid SMT '$sf' for '$hn'. Using default $default_smt_arg.\n";
                        $smt_v = $default_smt_arg;
                    }
                }
                else
                {
                    warn "    [WARN] Could not parse SMT '$sf' for '$hn'. Using default $default_smt_arg.\n";
                }
            }

            my $max_cpu_val = $rvals[$vm_config_col_idx{'maxcpu'}];
            # Validate MaxCPU value
            unless (defined $max_cpu_val && $max_cpu_val =~ /^[0-9.]+$/ && ($max_cpu_val+0) > 0)
            {
                $max_cpu_val = 0; # Default to 0 if invalid, meaning no effective MaxCPU cap from config
            }

            # Store parsed VM config data
            $vm_config_data{$hn} = {
                serial      => $rvals[$vm_config_col_idx{'serial'}],
                systemtype  => $rvals[$vm_config_col_idx{'systemtype'}],
                pool_name   => $rvals[$vm_config_col_idx{'procpool_name'}],
                pool_id     => $rvals[$vm_config_col_idx{'procpool_id'}],
                entitlement => $rvals[$vm_config_col_idx{'entitledcpu'}],
                max_capacity      => ($max_cpu_val + 0), # Store as number
                smt         => $smt_v,
            };
        }
        else # Hostname missing or empty
        {
            warn "    [WARN] Hostname field is missing in VM configuration file line $. Skipping.\n";
        }
    }
    close $cfg_fh;
    print STDERR "    ✓ Loaded " . scalar(keys %vm_config_data) . " VM configuration entries\n";
}
else # VM config file not found or not specified
{
    print STDERR "    [WARN] VM configuration file not loaded. MaxCPU capping logic will be affected, and SMT will use default\n";
}

# --- Construct Common Flags for nfit ---
# These flags are common to ALL nfit runs initiated by nfit-profile.
# Note: RunQ percentile flags (--runq-norm-perc, --runq-abs-perc) are now handled PER PROFILE run.
my $common_nfit_flags_base = "-q";
if ($nmon_dir)
{
    $common_nfit_flags_base .= " -k --nmondir \"$nmon_dir\"";
    # When using --nmondir, runq data comes from within the NMON files, so --runq-data is not used.
    # However, we still need to pass the RunQ averaging method to nfit if specified.
    if (defined $nfit_runq_avg_method_str)
    {
        $common_nfit_flags_base .= " --runq-avg-method \"$nfit_runq_avg_method_str\"";
    }
}
else # The original path using --physc-data
{
    $common_nfit_flags_base .= " -k --physc-data \"$physc_data_file\"";
    if (defined $runq_data_file_arg)
    {
        $common_nfit_flags_base .= " --runq-data \"$runq_data_file_arg\"";
        if (defined $nfit_runq_avg_method_str)
        {
            $common_nfit_flags_base .= " --runq-avg-method \"$nfit_runq_avg_method_str\"";
        }
    }
}
if (defined $start_date_str) # Global start date for all nfit runs
{
    $common_nfit_flags_base .= " -s $start_date_str";
}

# Common rounding flags (passed to nfit for its output formatting)
my $rounding_flags_for_nfit = ""; # These are applied by nfit itself
if (defined $round_arg)
{
    $rounding_flags_for_nfit .= " -r";
    if (length $round_arg && $round_arg !~ /^\s*$/) # Check if round_arg has a value (e.g. -r=0.1)
    {
        $rounding_flags_for_nfit .= "=$round_arg";
    }
}
elsif (defined $roundup_arg)
{
    $rounding_flags_for_nfit .= " -u";
    if (length $roundup_arg && $roundup_arg !~ /^\s*$/) # Check if roundup_arg has a value (e.g. -u=0.1)
    {
        $rounding_flags_for_nfit .= "=$roundup_arg";
    }
}
$common_nfit_flags_base .= $rounding_flags_for_nfit; # Add rounding to common flags if specified

# --- Main Logic: Run nfit Profiles ---
my %results_table; # Stores PhysC values from nfit for each profile: $results_table{vm_name}{profile_name}
my %runq_modifier_values;
my %nfit_growth_adjustments;
my %runq_uncapped_values;
my %runq_potential_values;
my %hint_tier_for_csv;
my %hint_pattern_for_csv;
my %hint_pressure_for_csv;
my @vm_order;      # To maintain CSV output order consistent with first nfit run that reports VMs
my %vm_seen;       # Tracks VMs seen to populate @vm_order correctly
my %primary_runq_metrics_captured_for_vm; # Tracks if global P50/P90 RunQ metrics captured for hints
my %source_profile_for_global_runq; # Which profile's output sourced the global RunQ P50/P90 for hints
my %per_profile_runq_metrics; # Stores ALL RunQ metrics (e.g. AbsRunQ_P80, AbsRunQ_P98) from EACH profile's nfit run
# Structure: $per_profile_runq_metrics{vm_name}{profile_name}{runq_metric_key}
my %per_profile_nfit_raw_results;
my %pressure_details_for_csv;
my %seasonal_debug_info;
my %outlier_warnings;

my %parsed_growth_adj_values;      # Stores GrowthAdj from nfit output
my %parsed_growth_adj_abs_values;  # Stores GrowthAdjAbs from nfit output
my $FLOAT_EPSILON = 1e-9;

my $LOG_FH;
my %open_log_files;

my $is_seasonal_run = $apply_seasonality_event || $update_history_flag;
my $o_nfit_analysis_reference_date_str = $nfit_analysis_reference_date_str;
my $o_start_date_str = $start_date_str;
my $o_end_date_str = $end_date_str;

# --- Main Processing Loop (iterates through systems for InfluxDB cache, runs once for standard file source modes) ---
foreach my $system_serial (@target_systems_to_process)
{
    my $system_analysis_start_time = time();

    # Restore user-supplied CLI date options for each managed system, to prevent
    # seasonal execution and per-event resets from leaking into subsequent systems.
    $start_date_str = $o_start_date_str;
    $end_date_str = $o_end_date_str;
    $nfit_analysis_reference_date_str = $o_nfit_analysis_reference_date_str;

    # To store the unique YYYYMMDD[-N] suffix
    my $unique_date_suffix;

    # Flag for the multiplicative seasonal model
    my $is_multiplicative_forecast_run = 0;
    my $is_predictive_peak_model_run = 0;
    my $historic_data_for_csv_href;

    my $effective_event_name;
    my $effective_config;
    my $exclusions;
    my ($start_date, $end_date);
    my ($effective_start_date_obj, $effective_end_date_obj);

    # First, determine the full path to the cache we are processing in this iteration.
    my $current_cache_path = defined($system_serial) ? File::Spec->catfile($base_cache_dir, $system_serial) : $nmon_dir;

    # Now, robustly determine the system identifier for logging.
    # Default to the directory name, but prefer the canonical name from the ID file.
    my $system_identifier = basename($current_cache_path); # Tier 2 Fallback identifier
    my $id_file_path = File::Spec->catfile($current_cache_path, '.nfit_stage_id');

    if (-f $id_file_path) {
        eval {
            open my $id_fh, '<:encoding(utf8)', $id_file_path;
            my $id_content = <$id_fh>;
            close $id_fh;

            # Tier 1 Attempt: Extract the system name from the file.
            if ($id_content && $id_content =~ /for system\s+(.+)/) {
                my $candidate_name = $1;
                $candidate_name =~ s/^\s+|\s+$//g; # Trim leading/trailing whitespace

                # --> ADDED: Validate the extracted name.
                # It must not be empty and must not contain illegal filename characters.
                if ($candidate_name ne '' && $candidate_name !~ /[\\\/:\*\?"<>\|]/) {
                    # Validation passed. Use the canonical name.
                    $system_identifier = $candidate_name;
                } else {
                    # Validation failed. The fallback (directory name) will be used.
                    warn " [WARN] Unusable system identifier ('$candidate_name') found in '$id_file_path'. Reverting to directory name.";
                }
            }
        };
        if ($@) {
            # This catches errors during file open/read. The fallback will be used.
            warn " [WARN] Could not read or parse '$id_file_path'. Using directory name for log. Error: $@";
        }
    }

    my $report_type_for_log = 'state-based'; # Default for the "no flags" forensic model
    if ($seasonal_auto_flag) {
        $report_type_for_log = 'seasonal';
    } elsif (defined $apply_seasonality_event && $apply_seasonality_event ne '') {
        $report_type_for_log = $apply_seasonality_event;
    } elsif ($nfit_decay_over_states) {
        $report_type_for_log = 'hybrid-state-decay';
    } elsif ($nfit_enable_windowed_decay) {
        $report_type_for_log = 'windowed-decay';
    }

    # Initialise L1 data cache
    my $data_cache_file = File::Spec->catfile($current_cache_path, $DATA_CACHE_FILE);

    # Calculate cache time-spam
    ($start_date, $end_date) = _get_cache_date_range($data_cache_file);

    #  --- Resolve effective analysis window (CLI -s/-e clamped to cache span) ---
    my $cli_start_obj = undef;
    my $cli_end_obj   = undef;

    if (defined $start_date_str) {
        $cli_start_obj = Time::Piece->strptime($start_date_str, "%Y-%m-%d");
    }
    if (defined $end_date_str) {
        $cli_end_obj = Time::Piece->strptime($end_date_str, "%Y-%m-%d");
    }

    ($effective_start_date_obj, $effective_end_date_obj) =
        _resolve_effective_window($start_date, $end_date, $cli_start_obj, $cli_end_obj);

    # Initialise the run end date to the effective end date by default
    my $run_end_date_obj = $effective_end_date_obj;

    if ($system_identifier && $system_identifier ne '.')
    {
        _phase("Processing Managed System: $system_identifier");

        if ($start_date && $end_date) {
            print STDERR "  ⧗ Data cache timespan: " . $start_date->strftime('%Y-%m-%d') . " - " . $end_date->strftime('%Y-%m-%d') . "\n";
        }

        # Determine and print the analysis type for clarity
        my $analysis_type_desc = 'Workload Profiling';
        if ($apply_seasonality_event) {
            my $event_config = $seasonality_config->{$apply_seasonality_event} // {};
            my $model_type = $event_config->{model} // '';

            if ($model_type eq 'multiplicative_seasonal') {
                $analysis_type_desc = "Seasonal Forecast (Multiplicative, event $apply_seasonality_event)";
            } elsif ($model_type eq 'recency_decay') {
                 $analysis_type_desc = "Seasonal Forecast (Recency-Decay, event $apply_seasonality_event)";
            } elsif ($model_type eq 'predictive_peak') {
                 $analysis_type_desc = "Seasonal Forecast (Predictive Peak, event $apply_seasonality_event)";
            } else {
                $analysis_type_desc = "Seasonal Forecast (event $apply_seasonality_event)";
            }
        } elsif ($nfit_decay_over_states) {
            $analysis_type_desc = "Hybrid State-Time Decay Analysis (--decay-over-states)";
        } elsif ($nfit_enable_windowed_decay) {
            $analysis_type_desc = "Time-Based Windowed Decay Analysis (--enable-windowed-decay)";
        } elsif ($seasonal_auto_flag) {
            $analysis_type_desc = 'Automatic Seasonal Discovery';
        }
        print STDERR "  ◆ Analysis Mode: $analysis_type_desc\n";

        # --- Generate unique, collision-resistant, paired filenames ---
        my $date_str = localtime->strftime('%Y%m%d');

        # Sanitise system identifiers for use in the filename.
        my $log_suffix_system = $system_identifier;
        $log_suffix_system =~ s/[^a-zA-Z0-9_.-]//g;
        my $log_suffix_report = $report_type_for_log;
        $log_suffix_report =~ s/[^a-zA-Z0-9_.-]//g;

        my $base_name_prefix = "nfit-profile.$log_suffix_system.$log_suffix_report";
        my $date_suffix = $date_str;
        my $counter = 0;
        # Check for collision on either file extension
        while (
            -f File::Spec->catfile($output_dir, "$base_name_prefix.$date_suffix.log") ||
            -f File::Spec->catfile($output_dir, "$base_name_prefix.$date_suffix.csv")
        ) {
            $counter++;
            $date_suffix = "$date_str-$counter";
        }
        $unique_date_suffix = $date_suffix; # Store for CSV naming
        $log_file_path_for_run = File::Spec->catfile($output_dir, "$base_name_prefix.$unique_date_suffix.log");
    }
    else
    {

        # Fallback to a default log path inside the output directory for single/non-serial runs.
        my $date_str = localtime->strftime('%Y%m%d');
        my $base_name_prefix = "nfit-profile.default_system.$report_type_for_log";
        my $date_suffix = $date_str;
        my $counter = 0;
        while (
            -f File::Spec->catfile($output_dir, "$base_name_prefix.$date_suffix.log") ||
            -f File::Spec->catfile($output_dir, "$base_name_prefix.$date_suffix.csv")
        ) {
            $counter++;
            $date_suffix = "$date_str-$counter";
        }
        $unique_date_suffix = $date_suffix; # Store for CSV naming
        $log_file_path_for_run = File::Spec->catfile($output_dir, "$base_name_prefix.$unique_date_suffix.log");
    }

    # --- Open Log File for this specific system ---
    my $log_fh_for_system = IO::File->new($log_file_path_for_run, '>:encoding(UTF-8)')
        or warn "[ERROR] Cannot open rationale log for '$system_serial' at '$log_file_path_for_run': $!. Rationale logging will be skipped for this system.\n";

    if ($log_fh_for_system)
    {
        $open_log_files{$system_identifier} = $log_fh_for_system;
        my $LOG_FH = $log_fh_for_system; # Use a lexical variable for printing the header

        $LOG_FH->autoflush(1); # Ensure immediate writing to log
        print {$LOG_FH} "======================================================================\n";
        print {$LOG_FH} "nFit Profile Rationale Log\n";
        print {$LOG_FH} "======================================================================\n";
        print {$LOG_FH} "nfit-profile.pl Run Started: $PROFILE_SCRIPT_START_TIME_STR\n";
        print {$LOG_FH} "nfit-profile.pl Version  : $VERSION\n";
        print {$LOG_FH} "nfit Version Used        : $nfit_ver\n";
        print {$LOG_FH} "----------------------------------------------------------------------\n";
        my @quoted_original_argv_log = map { $_ =~ /\s/ ? qq/"$_"/ : $_ } @original_argv;
        print {$LOG_FH} "Invocation: $0 " . join(" ", @quoted_original_argv_log) . "\n";
        print {$LOG_FH} "----------------------------------------------------------------------\n";
        print {$LOG_FH} "Key Global Settings for System: " . ($system_identifier // 'N/A') . "\n";
        print {$LOG_FH} "  - Profiles Config File       : $profiles_config_path_to_load\n";
        print {$LOG_FH} "  - VM Config File             : " . ($vm_config_file_path // "Not Provided/Default Attempted") . "\n";
        print {$LOG_FH} "  - Common Flags               : -q, -k, rounding, smt, runq-avg-method\n";
        print {$LOG_FH} "  - Dynamic Flags              : Date filters, RunQ percentiles, and Decay models (dynamic).\n";
        print {$LOG_FH} "  - RunQ Avg Method            : $nfit_runq_avg_method_str\n";
        print {$LOG_FH} "  - Default SMT for Profile    : $default_smt_arg\n";
        print {$LOG_FH} "======================================================================\n\n";

        print {$LOG_FH} "Key Terminology Reference (Sections C, E, F)\n";
        print {$LOG_FH} "----------------------------------------------------------------------\n";
        print {$LOG_FH} "  PhysC              Physical CPU consumed (hypervisor-reported). Includes active\n";
        print {$LOG_FH} "                     execution plus scheduling effects (e.g. idle VP spin, cache\n";
        print {$LOG_FH} "                     warming). Excess or poorly matched VP counts can inflate PhysC\n";
        print {$LOG_FH} "                     without an equivalent increase in application work.\n";
        print {$LOG_FH} "\n";
        print {$LOG_FH} "  Entitlement vs     Entitlement is guaranteed CPU capacity (cores). rVCPU is the\n";
        print {$LOG_FH} "  rVCPU              recommended number of virtual processors for dispatch capacity.\n";
        print {$LOG_FH} "                     Entitlement sizes steady-state allocation; rVCPU sizes concurrency\n";
        print {$LOG_FH} "                     to service bursts with low dispatch latency.\n";
        print {$LOG_FH} "\n";
        print {$LOG_FH} "  Tier / Tier Factor Policy classification (1-4) reflecting latency sensitivity.\n";
        print {$LOG_FH} "                     Tier 1 (most conservative) adds headroom for latency-intolerant\n";
        print {$LOG_FH} "                     workloads; higher tiers favour efficiency. Tier influences dispatch\n";
        print {$LOG_FH} "                     sizing headroom and peak-floor padding.\n";
        print {$LOG_FH} "\n";
        print {$LOG_FH} "  STD                Standard capacity-planning profile family (e.g. G3-95W15), intended\n";
        print {$LOG_FH} "                     to represent typical sustained demand rather than peak-coverage.\n";
        print {$LOG_FH} "\n";
        print {$LOG_FH} "  GrowthAdj          Growth adjustment (e.g. Theil-Sen) applied to the baseline PhysC\n";
        print {$LOG_FH} "                     recommendation for growth-aware profiles. Expressed as a delta or\n";
        print {$LOG_FH} "                     described in the rationale as the trend method and confidence.\n";
        print {$LOG_FH} "\n";
        print {$LOG_FH} "  Peak               Maximum observed PhysC over the analysis window (measurement-only).\n";
        print {$LOG_FH} "                     No smoothing, no modifiers.\n";
        print {$LOG_FH} "\n";
        print {$LOG_FH} "  Peak_P-99W1        Statistical ceiling: the 99th percentile of 1-minute-smoothed (SMA)\n";
        print {$LOG_FH} "                     PhysC. A measurement-only ceiling intended to represent sustained\n";
        print {$LOG_FH} "                     peak demand (filters sub-minute spikes). No modifiers are applied.\n";
        print {$LOG_FH} "                     Used as the baseline for peak-floor and credibility calculations.\n";
        print {$LOG_FH} "\n";
        print {$LOG_FH} "  P-99W1             Peak-coverage planning profile. Built to service observed peaks.\n";
        print {$LOG_FH} "                     Uses Peak_P-99W1 as the peak signal, participates in the planning\n";
        print {$LOG_FH} "                     pipeline, and produces an rVCPU recommendation (P-99W1_rVCPU).\n";
        print {$LOG_FH} "                     Downsizing is blocked for this profile by design.\n";
        print {$LOG_FH} "\n";
        print {$LOG_FH} "  PAE                Proportion of Above-Entitlement days. Fraction of measured days\n";
        print {$LOG_FH} "                     where daily PhysC exceeded entitlement. High PAE indicates frequent\n";
        print {$LOG_FH} "                     reliance on burst CPU.\n";
        print {$LOG_FH} "\n";
        print {$LOG_FH} "  AAE                Aggregate Above-Entitlement excess. Total CPU consumption above\n";
        print {$LOG_FH} "                     entitlement as a fraction of total consumption. High AAE indicates\n";
        print {$LOG_FH} "                     large burst magnitude. PAE measures how often; AAE measures how much.\n";
        print {$LOG_FH} "\n";
        print {$LOG_FH} "  NormRunQ_IQRC      Normalised Run Queue volatility metric: (P75-P25)/P50.\n";
        print {$LOG_FH} "                     Low values (e.g. <0.30) indicate steady demand; high values indicate\n";
        print {$LOG_FH} "                     bursty or unstable queuing behaviour.\n";
        print {$LOG_FH} "\n";
        print {$LOG_FH} "  AAE/PAE/IQRC       CPU downsizing guardrail. Prevents tactical downsizing when the\n";
        print {$LOG_FH} "  Banding Veto       workload exhibits above-entitlement operation and does not match a\n";
        print {$LOG_FH} "                     recognised low-risk pattern.\n";
        print {$LOG_FH} "\n";
        print {$LOG_FH} "  vCPU-to-Entitlement\n";
        print {$LOG_FH} "  Ratio (EVR)        EVR = vCPU / max(Entitlement, 1.0). Used as an intent signal for\n";
        print {$LOG_FH} "                     uncapped headroom: larger EVR implies the configuration is designed\n";
        print {$LOG_FH} "                     to burst above entitlement. EVR is floored for tiny entitlements to\n";
        print {$LOG_FH} "                     avoid distortion when Ent < 1.0.\n";
        print {$LOG_FH} "\n";
        print {$LOG_FH} "  Uncapped Headroom  EVR-driven planning allowance (Base), expressed as a percentage of\n";
        print {$LOG_FH} "  Factor (Base)      entitlement. It represents the dispatch capacity the LPAR is assumed\n";
        print {$LOG_FH} "                     to sustain (in uncapped mode) before pool/share effects become limiting.\n";
        print {$LOG_FH} "\n";
        print {$LOG_FH} "  Pool Headroom /\n";
        print {$LOG_FH} "  Contention Factor  Multiplier applied to the base headroom allowance when the LPAR is in\n";
        print {$LOG_FH} "                     a non-default pool. Larger pool share implies a higher likelihood of\n";
        print {$LOG_FH} "                     contention above entitlement, so the multiplier reduces effective headroom.\n";
        print {$LOG_FH} "\n";
        print {$LOG_FH} "  Theoretical Dispatch\n";
        print {$LOG_FH} "  Capacity            Entitlement expanded into logical dispatch threads using the effective\n";
        print {$LOG_FH} "                     uncapped headroom allowance and SMT. Used as one input to the overall\n";
        print {$LOG_FH} "                     pressure allowance decision.\n";
        print {$LOG_FH} "\n";
        print {$LOG_FH} "  Observed Dispatch\n";
        print {$LOG_FH} "  Capacity            Evidence-based dispatch threads derived from observed peak PhysC (P-99W1)\n";
        print {$LOG_FH} "                     and SMT, optionally reduced by a confidence modifier based on headroom.\n";
        print {$LOG_FH} "                     Used as the second input to the overall pressure allowance decision.\n";
        print {$LOG_FH} "\n";
        print {$LOG_FH} "  VP limits          Platform-safe VP band derived from rENT_P and policy ratios.\n";
        print {$LOG_FH} "                     Expressed as env_min..env_max (even-rounded). Used as guardrails to\n";
        print {$LOG_FH} "                     avoid extreme VP under/over-provisioning.\n";
        print {$LOG_FH} "\n";
        print {$LOG_FH} "  Peak floor         Minimum rVCPU required to service the observed peak signal.\n";
        print {$LOG_FH} "                     Converts the credible peak (cores) to an rVCPU floor, with headroom\n";
        print {$LOG_FH} "                     influenced by policy/tier and evidence bands.\n";
        print {$LOG_FH} "\n";
        print {$LOG_FH} "  Credible peak      Peak_P-99W1 after RunQ corroboration adjustment.\n";
        print {$LOG_FH} "                     If run queue evidence implies materially fewer runnable threads than\n";
        print {$LOG_FH} "                     PhysC suggests, the gap may reflect scheduler effects (including folding)\n";
        print {$LOG_FH} "                     or bursts not visible in RunQ indicators. Credible peak discounts the\n";
        print {$LOG_FH} "                     peak signal proportionally (bounded by minimum credibility).\n";
        print {$LOG_FH} "\n";
        print {$LOG_FH} "  RunQ dispatch      rVCPU derived from observed run queue depth.\n";
        print {$LOG_FH} "  sizing             Converts AbsRunQ tail percentile (threads) to core-equivalents via SMT,\n";
        print {$LOG_FH} "                     applies target utilisation, then scales by tier policy.\n";
        print {$LOG_FH} "                     Answers: how many VPs are required to service observed concurrency with\n";
        print {$LOG_FH} "                     low dispatch latency?\n";
        print {$LOG_FH} "\n";
        print {$LOG_FH} "  RunQ anomalies     Pathological-state detection for cases where RunQ signals should not\n";
        print {$LOG_FH} "                     be interpreted as additive CPU demand:\n";
        print {$LOG_FH} "                       - ANOMALY (DBW): High RunQ with low CPU utilisation. Suggests blocking\n";
        print {$LOG_FH} "                         or application behaviour; investigate before adding CPU.\n";
        print {$LOG_FH} "                       - ANOMALY (XRQ): Irrational RunQ demand (e.g. far beyond plausible\n";
        print {$LOG_FH} "                         capacity). Indicates a pathological state; manual intervention required.\n";
        print {$LOG_FH} "\n";
        print {$LOG_FH} "  VCPU_Constraints   CSV binding-status token for the governing planning profile.\n";
        print {$LOG_FH} "                     Emitted only when a hard constraint binds:\n";
        print {$LOG_FH} "                       - GovernedByVPMin: rVCPU raised to VP band minimum\n";
        print {$LOG_FH} "                       - GovernedByVPMax: rVCPU capped to VP band maximum\n";
        print {$LOG_FH} "                       - BlockedByMaxCPU: recommendation exceeds MaxCPU (feasibility)\n";
        print {$LOG_FH} "                     Note: BlockedByMaxCPU does not change rVCPU; it highlights that MaxCPU\n";
        print {$LOG_FH} "                     must be raised to apply the recommendation.\n";
        print {$LOG_FH} "======================================================================\n\n";
    }

    # --- Adaptive Threshold Initialisation for this system ---
    # This block is self-contained and does not mutate global variables.
    # It detects the interval and gets the appropriate thresholds for this system's run.
    my $detected_interval_secs = detect_sampling_interval($data_cache_file);

    # The _build_adaptive_thresholds_hash function returns the new values, which are stored
    # in local variables. This avoids modifying the global defaults.
    my $adaptive_thresholds_href = _build_adaptive_thresholds_hash($detected_interval_secs, $log_fh_for_system);
    my $adaptive_runq_saturation_thresh     = $adaptive_thresholds_href->{saturation};
    my $adaptive_target_norm_runq           = $adaptive_thresholds_href->{target};
    my $adaptive_max_efficiency_reduction   = $adaptive_thresholds_href->{max_reduction};

    # --- Seasonality Engine: Main Controller ---
    # This block determines if a seasonal model should be applied and orchestrates the analysis.
    # This block handles all three modes:
    # 1. Updating a snapshot.
    # 2. Applying a recency_decay forecast.
    # 3. Applying a multiplicative_seasonal forecast.

    if ($update_history_flag) {
        # --- Mode 1: Update the Unified Monthly History Cache ---
        # This is a special run mode. Its only purpose is to analyse completed
        # months in the data cache and save the results to the new history file.
        # It does not produce a CSV output.
        # Call the new subroutine to handle the history generation.
        update_monthly_history($current_cache_path, $system_identifier, $seasonality_config, $min_days_for_history, $adaptive_runq_saturation_thresh, $force_update);

        # After updating the history, we skip the rest of the processing for this system.
        print STDERR "✓ History update complete for system $system_identifier\n";
        next; # Proceed to the next system in the loop.
    } elsif ($seasonal_auto_flag) {
        # --- Automatic Seasonal Discovery ---
        _execute_automatic_seasonal_discovery($seasonality_config, $current_cache_path, \@profiles, {
            system_identifier => $system_identifier,
            nfit_path => $nfit_script_path,
            rounding_flags => $rounding_flags_for_nfit,
            default_smt => $default_smt_arg,
            runq_avg_method => $nfit_runq_avg_method_str,
            runq_perc_behavior => $runq_perc_behavior_mode,
            adaptive_saturation_thresh => $adaptive_runq_saturation_thresh,
            adaptive_target_norm_runq => $adaptive_target_norm_runq,
            adaptive_max_efficiency_reduction => $adaptive_max_efficiency_reduction,
            adaptive_thresholds_href => $adaptive_thresholds_href,
            sampling_interval => $detected_interval_secs,
            cache_start_date  => $start_date,
            cache_end_date    => $end_date,
            effective_start_date => $effective_start_date_obj,
            effective_end_date   => $effective_end_date_obj,
        }, $effective_end_date_obj, undef);
        next; # Skip standard processing
    } elsif ($apply_seasonality_event) {

        # Preserve explicit guard: adaptive_peak is owned by nfit-forecast
        my $requested_event_config = $seasonality_config->{$apply_seasonality_event} // {};
        my $requested_model_type   = $requested_event_config->{model} // '';
        if ($requested_model_type eq 'adaptive_peak') {
            die "[ERROR] The 'adaptive_peak' forecasting model can only be executed using the `nfit-forecast` program.\n" .
                "        Please use the command: ./nfit-forecast --nmondir $current_cache_path --apply-seasonality $apply_seasonality_event\n";
        }

        _execute_automatic_seasonal_discovery(
            $seasonality_config,
            $current_cache_path,
            \@profiles,
            {
                system_identifier => $system_identifier,
                nfit_path => $nfit_script_path,
                rounding_flags => $rounding_flags_for_nfit,
                default_smt => $default_smt_arg,
                runq_avg_method => $nfit_runq_avg_method_str,
                runq_perc_behavior => $runq_perc_behavior_mode,
                adaptive_saturation_thresh => $adaptive_runq_saturation_thresh,
                adaptive_target_norm_runq => $adaptive_target_norm_runq,
                adaptive_max_efficiency_reduction => $adaptive_max_efficiency_reduction,
                adaptive_thresholds_href => $adaptive_thresholds_href,
                sampling_interval => $detected_interval_secs,
                cache_start_date  => $start_date,
                cache_end_date    => $end_date,
                effective_start_date => $effective_start_date_obj,
                effective_end_date   => $effective_end_date_obj,
            },
            $effective_end_date_obj,
            $apply_seasonality_event
        );

        next; # Skip standard processing for this system (matches --seasonal behaviour)
    }

    # --- Standard (non-seasonal) processing path ---
    # Executes the single-pass engine and any model-specific post-processing
    # for non-seasonal runs. Seasonal execution paths MUST `next` before this block.

    _phase("Standard Analysis (Non-Seasonal)");

    my $analysis_context = {
        analysis_start  => $system_analysis_start_time,
        start       => $effective_start_date_obj,
        end         => $run_end_date_obj,
        cache_start => $start_date,
        cache_end   => $end_date,
        interval    => $detected_interval_secs
    };

    # A single, clean call to the new single-pass engine.
    my $parsed_nfit_results = run_single_pass_analysis(
        $current_cache_path,
        \@profiles,
        {
            # ... (all necessary global arguments are passed in this hash) ...
            nfit_path               => $nfit_script_path,
            rounding_flags          => $rounding_flags_for_nfit,
            start_date              => $effective_start_date_obj->ymd,
            end_date                => $run_end_date_obj->ymd,
            vm_name                 => $target_vm_name,
            default_smt             => $default_smt_arg,
            runq_avg_method         => $nfit_runq_avg_method_str,
            enable_windowed_decay   => $nfit_enable_windowed_decay,
            decay_over_states       => $nfit_decay_over_states,
            analysis_reference_date => $nfit_analysis_reference_date_str,
            enable_growth_prediction => $nfit_enable_windowed_decay || $nfit_decay_over_states,
        }
    );

    _phase("Assimilating Engine Results");

    # This single function call replaces the previous complex assimilation logic.
    # It consumes the raw JSON from nfit and produces a clean, predictable map.
    # The global @profiles array is passed to assist with per-state averaging logic.
    my $assimilation_map_ref = build_assimilation_map($parsed_nfit_results, \@profiles, $adaptive_runq_saturation_thresh);

    # Capture Context Variables (Already present in scope)
    # $start_date (Time::Piece), $end_date (Time::Piece), $detected_interval_secs (Scalar)

    # The @vm_order array, which drives the main processing loop,
    # must be populated from the keys of our newly created assimilation map.
    @vm_order = sort keys %{$assimilation_map_ref};

    # --- DEBUG CHECKPOINT ---
    # As requested, this block will print the contents of the newly created
    # assimilation map and then halt execution. This allows for a clean
    # evaluation of the data structure before proceeding with further refactoring.
    #print STDERR "\n\n==================== nFit Profile Debug Checkpoint ====================\n\n";
    #print STDERR "--- STAGE 1: Assimilation Map Contents ---\n";
    #print STDERR "This map is the new single source of truth for all downstream logic.\n\n";
    #print STDERR Dumper($assimilation_map_ref);
    #print STDERR "\n================== End Debug. Halting Execution. ==================\n\n";
    #die "Exiting after assimilation map generation for review.";
    # --- END DEBUG CHECKPOINT ---

    # --- Rationale Logging and Hint Generation ---
    # This block now uses a conditional to call the correct logging subroutine
    # based on the analysis model that was run. It also now captures all hint
    # components for later use in the CSV report, improving efficiency
    if ($is_multiplicative_forecast_run) {
        # We pass the current assimilation map (initial_results) to the forecast engine.
        my ($forecast_results, $historic_data) = calculate_multiplicative_forecast(
            $current_cache_path,
            $system_identifier,
            $effective_event_name, # defined in the scope above (line 333)
            $effective_config,     # defined in the scope above (line 334)
            $seasonality_config,
            $assimilation_map_ref,  # Pass current results if needed
            $exclusions,
            $effective_start_date_obj,
            $effective_end_date_obj
        );

        # Guard against empty results from failed baseline calculation
        unless ($forecast_results && scalar keys %$forecast_results > 0) {
            warn "  [WARN] Multiplicative forecast returned no results for event '$effective_event_name'. Skipping.\n";
            # Skip the rest of the multiplicative processing
        }
        else {

            # Merge the forecast results into the main results table
            # (calculate_multiplicative_forecast already populates global %seasonal_debug_info)
            $historic_data_for_csv_href = $historic_data;

            # Calculate Horizon Metadata
            my ($next_start, $next_end) = determine_event_period($effective_config, $effective_end_date_obj, 'forecast');
            my $horizon_meta = undef;
            if ($next_end) {
                my $gen_time = Time::Piece->new();
                my $days_diff = int(($next_end->epoch - $gen_time->epoch) / 86400);
                $days_diff = 0 if $days_diff < 0;
                $horizon_meta = { target_date => $next_end->ymd, days => $days_diff };
            }

            # Store the multiplicative_seasonal forecast results to unified history
            _store_model_forecast_to_history(
                    $current_cache_path,
                    $effective_event_name,
                    'multiplicative_seasonal',
                    $forecast_results,
                    $effective_config,
                    $horizon_meta,
                    $analysis_context
                    );

            # Overwrite the main results table with the forecast results
            %results_table = %$forecast_results;

            # Populate the global vm_order array so the reporter knows which VMs to process
            @vm_order = sort keys %results_table;

            # Update assimilation map with forecast values for CSV reporter
            foreach my $vm_name (@vm_order) {
                $assimilation_map_ref->{$vm_name} //= {};
                $assimilation_map_ref->{$vm_name}{CoreResults} //= {};
                $assimilation_map_ref->{$vm_name}{CoreResults}{ProfileValues} = $forecast_results->{$vm_name};
            }

            # Path for the multiplicative seasonal model, which needs a unique log format.
            # We must populate the assimilation map with hints so the CSV writer can read them.
            foreach my $vm_name (@vm_order) {
                # Get the authoritative map entry for this VM
                my $vm_map_ref = $assimilation_map_ref->{$vm_name};

                # 1. Self-Reference & Metrics
                $vm_map_ref->{Configuration}{vm_name} = $vm_name;
                $vm_map_ref->{RunQMetrics} //= $per_profile_runq_metrics{$vm_name} || {};

                # 2. Generate Hints
                # Call generate_sizing_hint using the assimilation map (not a temp hash).
                my ($hint_type_tier, $hint_pattern_shape, $hint_pressure_bool, $pressure_detail_str, $rationale, $has_abs, $has_norm) =
                    generate_sizing_hint(
                        $vm_map_ref,
                        undef,  # No log file handle needed here
                        $adaptive_runq_saturation_thresh
                    );

                # 3. Store Hints
                $vm_map_ref->{Hinting}{AutoTier}       = $hint_type_tier;
                $vm_map_ref->{Hinting}{Pattern}        = $hint_pattern_shape;
                $vm_map_ref->{Hinting}{Pressure}       = $hint_pressure_bool;
                $vm_map_ref->{Hinting}{PressureDetail} = $pressure_detail_str // 'N/A';

                # 4. Populate legacy globals (for safety)
                $hint_tier_for_csv{$vm_name}      = $hint_type_tier;
                $hint_pattern_for_csv{$vm_name}   = $hint_pattern_shape;
                $hint_pressure_for_csv{$vm_name}  = $hint_pressure_bool;
                $pressure_details_for_csv{$vm_name} = $pressure_detail_str // 'N/A';

                # Populate Tier Override for CSV and Logic
                $vm_map_ref->{Hinting}{FinalTierForVM} = $vm_tier_overrides{$vm_name} // $hint_type_tier;
                $tier_override_for_csv{$vm_name}       = $vm_tier_overrides{$vm_name} // "";

                # 5. Calculate RunQ Modifiers (Tactical/Strategic/Potential)
                # Determine source profile (Logic borrowed from main loop)
                my $user_tier = $vm_tier_overrides{$vm_name} // "";
                my $pattern_source = ($user_tier ne "") ? $user_tier : $hint_type_tier;
                my ($pattern_char) = ($pattern_source =~ /^([A-Z])/);
                $pattern_char //= 'G';
                my %pat_map = ('O'=>'O3-95W15', 'B'=>'B3-95W15', 'G'=>'G3-95W15', 'P'=>'G3-95W15');
                my $runq_source_profile_name = $pat_map{$pattern_char} // 'G3-95W15';

                # 6. Iterate all profiles to apply modifiers
                foreach my $profile (@profiles) {
                    my $p_name = $profile->{name};

                    # Guard P-99W1 (Pure Signal)
                    # Ensure we do not apply RunQ modifiers to the statistical ceiling.
                    next if ($p_name eq $MANDATORY_PEAK_PROFILE_FOR_HINT);

                    my %pressure_flags = ( abs_pressure => $has_abs, norm_pressure => $has_norm );
                    my %adaptive_thresholds = %$adaptive_thresholds_href;

                    # Calculate modified value (Forecast + RunQ)
                    my ($adjusted_physc, $debug_info_ref) = calculate_runq_modified_physc(
                        $vm_name, $vm_map_ref, $profile, \%pressure_flags, \%adaptive_thresholds
                    );

                    # Update the CoreResults with the FINAL adjusted value
                    # This ensures the CSV shows the Forecast INCLUDING the RunQ modifier
                    if (looks_like_number($adjusted_physc)) {
                        $vm_map_ref->{CoreResults}{ProfileValues}{$p_name} = sprintf("%.4f", $adjusted_physc);
                    }

                    # Step 3: Per-profile rVCPU (stress-driven, slot-aware, rENT-bounded, growth-aware)
                    # - RunQ_hi_threads: AbsRunQ value actually used for this profile’s logic (already selected)
                    # - rENT_P: final entitlement recommendation for this profile (growth-adjusted, plus any narrow RunQ additive)
                    # - rVCPU envelope: clamp to vendor-safe band relative to rENT_P
                    my $rvcpu_final;
                    my $rvcpu_raw;
                    my $rvcpu_min;
                    my $rvcpu_max;

                    my $runq_hi_threads = undef;
                    if (defined $debug_info_ref && ref($debug_info_ref) eq 'HASH') {
                        my $v = $debug_info_ref->{'AbsRunQValueUsedForCalc'};
                        $runq_hi_threads = (defined $v && looks_like_number($v)) ? ($v + 0) : undef;
                    }

                    my $rENT_P = (defined $adjusted_physc && looks_like_number($adjusted_physc)) ? ($adjusted_physc + 0) : undef;
                    my $smt_for_vcpu = (defined $vm_map_ref->{Configuration}{smt} && looks_like_number($vm_map_ref->{Configuration}{smt}))
                                     ? ($vm_map_ref->{Configuration}{smt} + 0)
                                     : undef;

                    # Tier-aware headroom for rVCPU
                    my $ml_tier_str = $p_name;
                    $ml_tier_str =~ s/-.*//;
                    my $rvcpu_tier_num = 3;
                    if ($ml_tier_str =~ /(\d)$/) {
                        $rvcpu_tier_num = $1;
                    } elsif ($ml_tier_str eq 'P') {
                        $rvcpu_tier_num = 1;
                    }

                    my $rvcpu_tier_factor = $VCPU_TIER_HEADROOM_FACTORS{$rvcpu_tier_num} // 1.00;

                    # Interval-adaptive divisor for Gate 1 (dispatch sizing).
                    # This is a divisor (target utilisation), not a multiplier.
                    my $runq_width_divisor = $RUNQ_TARGET_HEADROOM_FACTOR;
                    $runq_width_divisor = 0.80 if (!defined $runq_width_divisor || !looks_like_number($runq_width_divisor) || $runq_width_divisor <= 0);

                    if (defined $adaptive_thresholds_href && ref($adaptive_thresholds_href) eq 'HASH'
                        && defined $adaptive_thresholds_href->{runq_width_divisor}
                        && looks_like_number($adaptive_thresholds_href->{runq_width_divisor})
                        && $adaptive_thresholds_href->{runq_width_divisor} > 0) {
                        $runq_width_divisor = $adaptive_thresholds_href->{runq_width_divisor} + 0;
                    }

                    # Hard safety clamps (never allow divide-by-zero or absurd divisors).
                    my $MIN_RUNQ_WIDTH_DIVISOR = 0.50;
                    my $MAX_RUNQ_WIDTH_DIVISOR = 1.00;
                    $runq_width_divisor = 0.80 if (!defined $runq_width_divisor || !looks_like_number($runq_width_divisor) || $runq_width_divisor <= 0);
                    $runq_width_divisor = $MIN_RUNQ_WIDTH_DIVISOR if ($runq_width_divisor < $MIN_RUNQ_WIDTH_DIVISOR);
                    $runq_width_divisor = $MAX_RUNQ_WIDTH_DIVISOR if ($runq_width_divisor > $MAX_RUNQ_WIDTH_DIVISOR);

                    if (defined $runq_hi_threads && defined $smt_for_vcpu && $smt_for_vcpu > 0 && defined $rENT_P && $rENT_P > 0) {
                        # Workload-driven width requirement (threads -> cores -> vCPUs), tier-adjusted
                        my $required_threads = ($runq_hi_threads / $runq_width_divisor) * $rvcpu_tier_factor;
                        my $required_cores   = $required_threads / $smt_for_vcpu;
                        $rvcpu_raw = int($required_cores);
                        $rvcpu_raw++ if ($required_cores > $rvcpu_raw); # ceil for positive values

                        # Envelope from rENT_P (growth-aware)
                        my $min_v = $rENT_P * $VCPU_RATIO_MIN;
                        my $max_v = $rENT_P * $VCPU_RATIO_MAX;
                        $rvcpu_min = int($min_v); $rvcpu_min++ if ($min_v > $rvcpu_min);
                        $rvcpu_max = int($max_v); $rvcpu_max++ if ($max_v > $rvcpu_max);

                        # Clamp
                        $rvcpu_final = $rvcpu_raw;
                        $rvcpu_final = $rvcpu_min if (defined $rvcpu_min && $rvcpu_final < $rvcpu_min);
                        $rvcpu_final = $rvcpu_max if (defined $rvcpu_max && $rvcpu_final > $rvcpu_max);
                    }

                    $vm_map_ref->{CoreResults}{Profile_rVCPU}{$p_name} = defined($rvcpu_final) ? $rvcpu_final : '';

                    # If this is the source profile, save the modifiers for the CSV columns
                    if ($p_name eq $runq_source_profile_name) {
                        $vm_map_ref->{CSVModifiers}{RunQ_Tactical}  = $debug_info_ref->{'FinalAdditive'};
                        $vm_map_ref->{CSVModifiers}{RunQ_Strategic} = $debug_info_ref->{'RunQ_Strategic'};
                        $vm_map_ref->{CSVModifiers}{RunQ_Potential} = $debug_info_ref->{'RunQ_Potential'};
                        $vm_map_ref->{CSVModifiers}{RunQ_Source}    = $runq_source_profile_name;
                    }
                }

            }
            log_multiplicative_seasonal_rationale($open_log_files{$system_identifier}, $seasonality_config->{$apply_seasonality_event}, $analysis_context);
        }
    } elsif ($is_predictive_peak_model_run) {
        # Path for the new predictive peak model
        # The predictive_peak model uses linear regression on historical seasonal peaks.
        my $forecast_results = calculate_predictive_peak_forecast(
            $current_cache_path,
            $system_identifier,
            $effective_event_name,
            $effective_config,
            $seasonality_config,
            $adaptive_runq_saturation_thresh,
            $exclusions,
            $effective_start_date_obj,
            $effective_end_date_obj
        );

        # Determine Forecast Horizon for Metadata (anchored to effective end date)
        my $asof_end = $effective_end_date_obj // $end_date;
        my ($next_start, $next_end) = determine_event_period($effective_config, $asof_end, 'forecast');
        my $horizon_meta = undef;
        if ($next_end) {
            my $ref_time = (defined $asof_end && ref($asof_end) eq 'Time::Piece') ? $asof_end : Time::Piece->new();

            my $days_diff = int(($next_end->epoch - $ref_time->epoch) / 86400);
            $days_diff = 0 if $days_diff < 0;

            $horizon_meta = {
                target_date => $next_end->ymd,
                days        => $days_diff
            };
        }

        # Store the predictive_peak forecast results to unified history
        _store_model_forecast_to_history(
            $current_cache_path,
            $effective_event_name,
            'predictive_peak',
            $forecast_results,
            $effective_config,
            $horizon_meta,
            $analysis_context
        );

        # Overwrite the main results table with the forecast
        %results_table = %$forecast_results;

        # Populate the global vm_order array so the reporter knows which VMs to process.
        @vm_order = sort keys %results_table;

        # Generate and store the necessary hint/pressure data for each VM for the CSV.
        # fix: Hydrate the Assimilation Map Correctly ---
        foreach my $vm_name (@vm_order) {

            # 1. Update CoreResults in the map with the forecasted values
            $assimilation_map_ref->{$vm_name} //= {};
            $assimilation_map_ref->{$vm_name}{CoreResults}{ProfileValues} = $forecast_results->{$vm_name};

            # 2. Ensure self-reference exists
            $assimilation_map_ref->{$vm_name}{Configuration}{vm_name} = $vm_name;
            $assimilation_map_ref->{$vm_name}{RunQMetrics} //= $per_profile_runq_metrics{$vm_name} || {};

            # 3. Generate Hints using the MAP (Single Source of Truth)
            my ($hint_type_tier, $hint_pattern_shape, $hint_pressure_bool, $pressure_detail_str, $rationale, $has_abs, $has_norm) =
                generate_sizing_hint(
                    $assimilation_map_ref->{$vm_name},
                    undef,  # No log file handle needed here
                    $adaptive_runq_saturation_thresh
                );

            # 4. Store hints back into the map
            # Ensure per-VM Configuration has a stable vm_name for downstream hinting/CSV logic.
            $assimilation_map_ref->{$vm_name}{Configuration} //= {};
            $assimilation_map_ref->{$vm_name}{Configuration}{vm_name} //= $vm_name;

            $assimilation_map_ref->{$vm_name}{Hinting}{Pattern}        = $hint_pattern_shape;
            $assimilation_map_ref->{$vm_name}{Hinting}{FinalTierForVM} = $vm_tier_overrides{$vm_name} // $hint_type_tier;
            $assimilation_map_ref->{$vm_name}{Hinting}{AutoTier}       = $hint_type_tier;
            $assimilation_map_ref->{$vm_name}{Hinting}{Pressure}       = $hint_pressure_bool;
            $assimilation_map_ref->{$vm_name}{Hinting}{PressureDetail} = $pressure_detail_str // 'N/A';

            # 5. Populate legacy globals for safety
            $hint_tier_for_csv{$vm_name}      = $hint_type_tier;
            $hint_pattern_for_csv{$vm_name}   = $hint_pattern_shape;
            $hint_pressure_for_csv{$vm_name}  = $hint_pressure_bool;
            $pressure_details_for_csv{$vm_name} = $pressure_detail_str // 'N/A';
            # Populate Tier Override for CSV and Logic
            $tier_override_for_csv{$vm_name}       = $vm_tier_overrides{$vm_name} // "";

            # 6. Calculate RunQ Modifiers (Copy of Multiplicative Logic)
            my $user_tier = $vm_tier_overrides{$vm_name} // "";
            my $pattern_source = ($user_tier ne "") ? $user_tier : $hint_type_tier;
            my ($pattern_char) = ($pattern_source =~ /^([A-Z])/);
            $pattern_char //= 'G';
            my %pat_map = ('O'=>'O3-95W15', 'B'=>'B3-95W15', 'G'=>'G3-95W15', 'P'=>'G3-95W15');
            my $runq_source_profile_name = $pat_map{$pattern_char} // 'G3-95W15';

            my ($runq_source_profile_obj) = grep { $_->{name} eq $runq_source_profile_name } @profiles;

            if ($runq_source_profile_obj) {
                my %pressure_flags = ( abs_pressure => $has_abs, norm_pressure => $has_norm );
                my %adaptive_thresholds = %$adaptive_thresholds_href;

                my (undef, $debug_info_ref) = calculate_runq_modified_physc(
                    $vm_name, $assimilation_map_ref->{$vm_name}, $runq_source_profile_obj, \%pressure_flags, \%adaptive_thresholds
                );

                $assimilation_map_ref->{$vm_name}{CSVModifiers}{RunQ_Tactical}  = $debug_info_ref->{'FinalAdditive'};
                $assimilation_map_ref->{$vm_name}{CSVModifiers}{RunQ_Strategic} = $debug_info_ref->{'RunQ_Strategic'};
                $assimilation_map_ref->{$vm_name}{CSVModifiers}{RunQ_Potential} = $debug_info_ref->{'RunQ_Potential'};
                $assimilation_map_ref->{$vm_name}{CSVModifiers}{RunQ_Source}    = $runq_source_profile_name;
            }

            # 7. Iterate all profiles to apply modifiers (FIX: Was missing in manual path)
            foreach my $profile (@profiles) {
                my $p_name = $profile->{name};

                # --- GUARD: Skip RunQ Modifiers for P-99W1 ---
                next if ($p_name eq $MANDATORY_PEAK_PROFILE_FOR_HINT);

                my %pressure_flags = ( abs_pressure => $has_abs, norm_pressure => $has_norm );
                my %adaptive_thresholds = %$adaptive_thresholds_href;

                my ($adjusted_physc, $debug_info_ref) = calculate_runq_modified_physc(
                    $vm_name, $assimilation_map_ref->{$vm_name}, $profile, \%pressure_flags, \%adaptive_thresholds
                );

                if (looks_like_number($adjusted_physc)) {
                    $assimilation_map_ref->{$vm_name}{CoreResults}{ProfileValues}{$p_name} = sprintf("%.4f", $adjusted_physc);
                }
            }

            # 8. Update Peak Value logic
            my $baseline_peak = $seasonal_debug_info{$vm_name}{$MANDATORY_PEAK_PROFILE_FOR_HINT}{'TrueBaseline'} // 0;
            my $predicted_peak = $seasonal_debug_info{$vm_name}{$MANDATORY_PEAK_PROFILE_FOR_HINT}{'PredictedPeak'} // 0;

            # Store the effective peak back into the map
            $assimilation_map_ref->{$vm_name}{CoreResults}{ProfileValues}{$PEAK_PROFILE_NAME} =
                ($baseline_peak > $predicted_peak) ? $baseline_peak : $predicted_peak;
        }
        log_predictive_peak_rationale($open_log_files{$system_identifier}, $seasonality_config->{$apply_seasonality_event}, $analysis_context);
    } else {
        _phase("Applying modifiers and generating rationale");
        # --- FINAL POST-PROCESSING, HINT GENERATION, and RATIONALE LOGGING ---

        # --- Progress Bar Initialization ---
        my $total_vms = scalar @vm_order;
        my $total_prof = scalar @profiles;
        my $total_units_for_progress = $total_vms * $total_prof;
        my $done_units_for_progress = 0;
        my $vm_count_for_progress = 0;
        my $show_profile_progress_flag = ($verbose || -t STDERR);

        foreach my $vm_name (@vm_order) {
            $vm_count_for_progress++;

            # Get this VM's complete map entry.
            my $vm_map_ref = $assimilation_map_ref->{$vm_name};
            next unless (ref($vm_map_ref) eq 'HASH');
            $vm_map_ref->{Configuration}{vm_name} = $vm_name; # Self-reference for convenience

            # --- Phase 1: Generate Hints and Pressure Flags ---
            my ($hint_type_tier, $hint_pattern_shape, $pressure_bool, $pressure_detail_str, $pressure_rationale_text, $p99w1_has_abs_pressure, $p99w1_has_norm_pressure) =
                generate_sizing_hint(
                    $vm_map_ref,
                    $log_fh_for_system,
                    $adaptive_runq_saturation_thresh
                );

            # Store hint results in the map for use by other functions and the CSV writer.
            $vm_map_ref->{Hinting}{AutoTier} = $hint_type_tier;
            $vm_map_ref->{Hinting}{Pattern} = $hint_pattern_shape;
            $vm_map_ref->{Hinting}{Pressure} = $pressure_bool;
            $vm_map_ref->{Hinting}{PressureDetail} = $pressure_detail_str // 'N/A';
            $vm_map_ref->{Hinting}{P99W1_AbsRunQPressure} = $p99w1_has_abs_pressure;
            $vm_map_ref->{Hinting}{P99W1_NormRunQPressure} = $p99w1_has_norm_pressure;
            $vm_map_ref->{Hinting}{FinalTierForVM} = $vm_tier_overrides{$vm_name} // $hint_type_tier;
            $tier_override_for_csv{$vm_name} = $vm_tier_overrides{$vm_name} // "";

            # --- [NEW] Standardise Growth for Recency Decay ---
            # If this is a recency_decay run, we must ensure ALL profiles use the
            # growth adjustment from the stable "Source" profile (e.g. G3),
            # rather than their own potentially volatile/zero adjustment.
            my $seasonal_cfg = (defined $apply_seasonality_event && exists $seasonality_config->{$apply_seasonality_event})
                ? $seasonality_config->{$apply_seasonality_event}
                : {};
            if (($seasonal_cfg->{model} // '') eq 'recency_decay') {

                # 1. Determine Source Profile (Pattern/Tier Logic)
                #    Priority: UserOverride -> AutoHint -> Default 'G'
                my $user_tier = $vm_map_ref->{Hinting}{FinalTierForVM} // "";
                my $auto_tier = $vm_map_ref->{Hinting}{AutoTier} // "G";
                my $pattern_source = ($user_tier ne "") ? $user_tier : $auto_tier;

                my ($pattern_char) = ($pattern_source =~ /^([A-Z])/);
                $pattern_char //= 'G'; # Default to General

                # Map pattern to the standard Tier-3 planning profile
                my %pat_map = ('O'=>'O3-95W15', 'B'=>'B3-95W15', 'G'=>'G3-95W15', 'P'=>'G3-95W15');
                my $growth_source_profile = $pat_map{$pattern_char} // 'G3-95W15';

                # 2. Get the Standard Adjustment (from the Source Profile's growth)
                #    Growth adjustments were populated into {Growth}{adjustments} during assimilation.
                my $standard_growth_adj = $vm_map_ref->{Growth}{adjustments}{$growth_source_profile} // 0;

                # 3. Store metadata for Logging
                $vm_map_ref->{Growth}{StandardAdjustment} = {
                    Value  => $standard_growth_adj,
                    Source => $growth_source_profile
                };

                # 4. [CRITICAL] Pre-calculate CSVModifiers for recency_decay BEFORE the profile loop.
                #    The recency_decay model requires RunQ_Potential to be available, but CSVModifiers
                #    are normally populated in Phase 2 (after this block). We must calculate them here.
                my ($growth_source_profile_obj) = grep { $_->{name} eq $growth_source_profile } @profiles;

                if ($growth_source_profile_obj) {
                    my %pressure_flags_rd = (
                        abs_pressure  => $vm_map_ref->{Hinting}{P99W1_AbsRunQPressure}  // 0,
                        norm_pressure => $vm_map_ref->{Hinting}{P99W1_NormRunQPressure} // 0
                    );
                    my %adaptive_thresholds_rd = %$adaptive_thresholds_href;

                    my (undef, $debug_info_ref_rd) = calculate_runq_modified_physc(
                        $vm_name, $vm_map_ref, $growth_source_profile_obj,
                        \%pressure_flags_rd, \%adaptive_thresholds_rd
                    );

                    # Populate CSVModifiers early so the profile loop can use them
                    $vm_map_ref->{CSVModifiers}{RunQ_Tactical}  = $debug_info_ref_rd->{'FinalAdditive'};
                    $vm_map_ref->{CSVModifiers}{RunQ_Strategic} = $debug_info_ref_rd->{'RunQ_Strategic'};
                    $vm_map_ref->{CSVModifiers}{RunQ_Potential} = $debug_info_ref_rd->{'RunQ_Potential'};
                    $vm_map_ref->{CSVModifiers}{RunQ_Source}    = $growth_source_profile;
                }

                # 5. Apply to ALL profiles (Update CoreResults)
                #    This ensures the CSV and the Logger see the standardised value (Base + StandardGrowth + RunQ).

                foreach my $prof (@profiles) {
                    my $p_name = $prof->{name};

                    # EXEMPTION: Special Handling for the peak helper profile
                    if (defined $p_name && $p_name eq $MANDATORY_PEAK_PROFILE_FOR_HINT) {
                        print {$log_fh_for_system}
                          "  - The $MANDATORY_PEAK_PROFILE_FOR_HINT profile is a special case used for peak analysis.\n"
                          if ($log_fh_for_system);
                        print {$log_fh_for_system}
                          "  - NO growth or RunQ modifiers are applied to this value (measurement-only peak reference).\n"
                          if ($log_fh_for_system);
                        next;
                    }

                    # Get the raw base value (pre-growth) directly from the Growth stash
                    my $base_val = $vm_map_ref->{Growth}{base_values}{$p_name};

                    if (defined $base_val && looks_like_number($base_val)) {
                        # A. Start with Base + StandardGrowth
                        my $final_val = $base_val + $standard_growth_adj;

                        # B. Add RunQ Modifier (Latent Demand / Potential)
                        #    CRITICAL: For recency_decay, we use RunQ_Potential (raw queue physics)
                        #    rather than RunQ_Tactical (change-managed, gated increment).
                        #    Rationale: recency_decay is a pure operational forecast answering
                        #    "What is actual demand now?" - it should not be subject to
                        #    change management gates that constrain monthly resizing plans.
                        my $runq_mod = $vm_map_ref->{CSVModifiers}{RunQ_Potential} // 0;

                        $final_val += $runq_mod;

                        # Update the authoritative CoreResults map
                        $vm_map_ref->{CoreResults}{ProfileValues}{$p_name} = sprintf("%.4f", $final_val);
                    }
                }
            }

            # Log the detailed pressure rationale text once per VM.
            if ($log_fh_for_system) {
                print {$log_fh_for_system} "\n######################################################################\n";
                print {$log_fh_for_system} "# Rationale for VM: $vm_name\n";
                print {$log_fh_for_system} "######################################################################\n\n";
                print {$log_fh_for_system} $pressure_rationale_text . "\n" if ($pressure_rationale_text);
            }

            # --- Phase 2: Calculate Modifiers (ONCE) and Final Profile Values ---

            # First, determine the single source profile for RunQ modifiers for this VM
            # GUARD: For recency_decay models, CSVModifiers were already calculated in the
            # dedicated block above. Skip recalculation to avoid overwriting with values
            # computed from the already-modified ProfileValues.
            my $seasonal_cfg_p2 = (defined $apply_seasonality_event && exists $seasonality_config->{$apply_seasonality_event})
                ? $seasonality_config->{$apply_seasonality_event}
              : {};
            my $skip_phase2_modifiers = (($seasonal_cfg_p2->{model} // '') eq 'recency_decay');

            # Priority: User TIER > AutoTier > Fallback to 'G'.
            my $user_tier_override_runq = $vm_map_ref->{Hinting}{FinalTierForVM} // "";
            my $auto_tier_runq = $vm_map_ref->{Hinting}{AutoTier} // "G";

            my $pattern_source_runq = ($user_tier_override_runq ne "") ? $user_tier_override_runq : $auto_tier_runq;
            my ($hint_pattern) = ($pattern_source_runq =~ /^([A-Z])/);
            $hint_pattern //= 'G'; # Default to 'G' if regex fails

            my %pattern_to_profile_map = ('O'=>'O3-95W15', 'B'=>'B3-95W15', 'G'=>'G3-95W15', 'P'=>'G3-95W15');
            my $runq_source_profile_name = $pattern_to_profile_map{$hint_pattern} // 'G3-95W15';

            # Find the profile object for the source profile
            my ($runq_source_profile_obj) = grep { $_->{name} eq $runq_source_profile_name } @profiles;

            # Only recalculate CSVModifiers if this is NOT a recency_decay run
            # (recency_decay already populated these values correctly)
            if ($runq_source_profile_obj && !$skip_phase2_modifiers) {
                my %pressure_flags = ( abs_pressure => $p99w1_has_abs_pressure, norm_pressure => $p99w1_has_norm_pressure );
                my %adaptive_thresholds = %$adaptive_thresholds_href;

                # Calculate modifiers ONLY for the source profile
                my (undef, $debug_info_ref) = calculate_runq_modified_physc(
                    $vm_name, $vm_map_ref, $runq_source_profile_obj, \%pressure_flags, \%adaptive_thresholds
                );

                # Store the authoritative modifiers for the CSV report
                $vm_map_ref->{CSVModifiers}{RunQ_Tactical}  = $debug_info_ref->{'FinalAdditive'};
                $vm_map_ref->{CSVModifiers}{RunQ_Strategic} = $debug_info_ref->{'RunQ_Strategic'};
                $vm_map_ref->{CSVModifiers}{RunQ_Potential} = $debug_info_ref->{'RunQ_Potential'};
                $vm_map_ref->{CSVModifiers}{RunQ_Source}    = $runq_source_profile_name;
            }

            # Now, iterate through all profiles to size them and log rationale
            foreach my $profile (@profiles) {
                my $profile_name = $profile->{name};
                $done_units_for_progress++;

                # Progress bar
                if ($show_profile_progress_flag) {
                    my $perc_done = ($total_units_for_progress > 0) ? (100.0 * $done_units_for_progress / $total_units_for_progress) : 0;
                    my $profile_name_for_progress = $profile->{name};
                    printf STDERR "\r    • Processing VM %d/%d (%s), Profile %d/%d (%s) [%.1f%%]...",
                        $vm_count_for_progress,
                        $total_vms,
                        $vm_name,
                        ($done_units_for_progress % $total_prof) || $total_prof,
                        $total_prof,
                        $profile_name_for_progress,
                        $perc_done;
                }

                my $base_physc_for_log = $vm_map_ref->{CoreResults}{ProfileValues}{$profile_name};
                next unless (defined $base_physc_for_log && looks_like_number($base_physc_for_log));

                if ($profile_name eq $MANDATORY_PEAK_PROFILE_FOR_HINT) {
                    log_peak_profile_rationale($log_fh_for_system, $vm_map_ref, $profile, $base_physc_for_log);
                } else {
                    # Recalculate for logging, but do NOT store these values for the CSV
                    my %pressure_flags = ( abs_pressure => $p99w1_has_abs_pressure, norm_pressure => $p99w1_has_norm_pressure );
                    my %adaptive_thresholds = %$adaptive_thresholds_href;

                    my ($adjusted_physc, $debug_info_ref) = calculate_runq_modified_physc(
                        $vm_name, $vm_map_ref, $profile, \%pressure_flags, \%adaptive_thresholds
                    );

                    # CRITICAL: For recency_decay models, ProfileValues was ALREADY correctly calculated
                    # in the dedicated block above (base + growth + runq_potential). We must NOT
                    # overwrite it here, as calculate_runq_modified_physc would double-apply modifications.
                    my $seasonal_cfg = (defined $apply_seasonality_event && exists $seasonality_config->{$apply_seasonality_event})
                        ? $seasonality_config->{$apply_seasonality_event}
                        : {};
                    my $is_recency_decay_model = (($seasonal_cfg->{model} // '') eq 'recency_decay');

                    my $final_csv_value;
                    if ($is_recency_decay_model) {
                        # Use the pre-calculated value (do NOT overwrite)
                        $final_csv_value = $vm_map_ref->{CoreResults}{ProfileValues}{$profile_name};
                    } else {
                        # Standard path: store the newly calculated value
                        $final_csv_value = looks_like_number($adjusted_physc) ? sprintf("%.3f", $adjusted_physc) : "N/A";
                        $vm_map_ref->{CoreResults}{ProfileValues}{$profile_name} = $final_csv_value;
                    }

                    my $raw_states_aref_for_log = $vm_map_ref->{RawNfitStates} || [];

                    log_profile_rationale(
                        $log_fh_for_system, $vm_map_ref, $profile,
                        $base_physc_for_log, $final_csv_value, $debug_info_ref,
                        $raw_states_aref_for_log,
                        $adaptive_runq_saturation_thresh
                    );
                }
            }

       }

        # --- Progress Bar Finalization ---
        if ($show_profile_progress_flag) {
            printf STDERR "\r    • Processing complete. (100.0%%)%s\n", ' ' x 70;
        }
    }

    # -- Reporting and Reset Block --
    # This block executes after all profiles have been run for the current system.

    # -- Reporting and Reset Block --
    # This block executes after all profiles have been run for the current system.
    _phase("Results");
    my $is_recency_decay_run = 0;

    # --- FIX: Prioritise Seasonality Name in Filename ---
    my $report_type_for_filename = 'state-based';

    if (defined $apply_seasonality_event && $apply_seasonality_event ne '') {
        $report_type_for_filename = $apply_seasonality_event;

        # Check if it is specifically recency decay
        my $event_config = $seasonality_config->{$apply_seasonality_event} // {};
        if (($event_config->{model} // '') eq 'recency_decay') {
            $is_recency_decay_run = 1;
        }
    }
    elsif ($nfit_decay_over_states) {
        $report_type_for_filename = 'hybrid-state-decay';
    } elsif ($nfit_enable_windowed_decay) {
        $report_type_for_filename = 'windowed-decay';
    }

    if ($is_seasonal_run && !$is_multiplicative_forecast_run) {
        my $event_config = $seasonality_config->{$apply_seasonality_event} // {};
        $is_recency_decay_run = (($event_config->{model} // '') eq 'recency_decay');
    }

   if ($is_multiplicative_forecast_run) {
        # This is the new path for the multiplicative model, which now uses the standard reporter.
        if (@vm_order) {
            _write_standard_csv_report($assimilation_map_ref, $report_type_for_filename, $system_identifier, $unique_date_suffix, 1, 0, 0, $adaptive_runq_saturation_thresh, $adaptive_thresholds_href, undef);

            # If verbose mode is on, generate the extra audit trail files.
            if ($verbose) {
                print STDERR "  ◆ Audit Trail\n";
                # The baseline results are the final values *before* the forecast was applied.
                # The %results_table was overwritten, so we can't re-use it.
                # However, the baseline is available inside the seasonal_debug_info hash.
                my %baseline_data_for_verbose;
                foreach my $vm (keys %seasonal_debug_info) {
                    foreach my $prof (keys %{$seasonal_debug_info{$vm}}) {
                        $baseline_data_for_verbose{$vm}{$prof} = $seasonal_debug_info{$vm}{$prof}{baseline};
                    }
                }
                # Use $apply_seasonality_event as it's in scope here.
                write_seasonal_csv_output("current_baseline", $system_identifier, $apply_seasonality_event, $unique_date_suffix, \%baseline_data_for_verbose);
                write_seasonal_csv_output("historic_snapshot", $system_identifier, $apply_seasonality_event, $unique_date_suffix, $historic_data_for_csv_href);
            }
        }
    }
    elsif ($is_seasonal_run) {
        # This path is for all non-multiplicative seasonal models (recency_decay, predictive_peak).
        my $event_config = $seasonality_config->{$apply_seasonality_event} // {};
        my $model_type = $event_config->{model} // '';

        if ($model_type eq 'recency_decay' || $model_type eq 'predictive_peak') {
            if (@vm_order) {
                # For recency_decay, pass a true flag to add its specific columns.
                # For predictive_peak, pass a false flag to generate a standard report.
                my $is_recency_flag = ($model_type eq 'recency_decay') ? 1 : 0;
                my $is_predictive_flag = ($model_type eq 'predictive_peak') ? 1 : 0;
                _write_standard_csv_report($assimilation_map_ref, $apply_seasonality_event, $system_identifier, $unique_date_suffix, 0, $is_recency_flag, $is_predictive_flag, $adaptive_runq_saturation_thresh, $adaptive_thresholds_href, undef);

                # Store recency_decay forecast results to unified history
                if ($model_type eq 'recency_decay')  {
                    my %recency_decay_results_for_history;

                    # 1. Extract core results for storage
                    foreach my $vm (keys %{$assimilation_map_ref}) {
                        my $core_res = $assimilation_map_ref->{$vm}{CoreResults}{ProfileValues};
                        if (ref($core_res) eq 'HASH') {
                            $recency_decay_results_for_history{$vm} = { %$core_res };
                        }
                    }

                    # 2. [REFACTORED] Horizon Metadata Calculation
                    # Use the deterministic event period relative to the analysis end date.
                    # This replaces the risky "scan for projection_days" and "localtime" logic.
                    my $horizon_meta = undef;

                    # Use the authoritative effective end date from the main scope
                    my $anchor_date = $effective_end_date_obj;

                    if (defined $anchor_date) {
                        # Identify the target event relative to the analysis anchor
                        my ($evt_start, $evt_end) = determine_event_period($event_config, $anchor_date, 'forecast');

                        if ($evt_start && $evt_end) {
                            my $days_diff = int(($evt_end->epoch - $anchor_date->epoch) / 86400);
                            $days_diff = 0 if $days_diff < 0;

                            $horizon_meta = {
                                target_date => $evt_end->ymd,
                                days        => $days_diff
                            };
                        }
                    }

                    # Note: We intentionally leave $horizon_meta as undef if the event period
                    # cannot be determined, rather than falling back to localtime() (Wall Clock).

                    _store_model_forecast_to_history(
                        $current_cache_path,
                        $apply_seasonality_event,
                        'recency_decay',
                        \%recency_decay_results_for_history,
                        $event_config,
                        $horizon_meta,
                        $analysis_context
                    );
                }
            } else {
                print STDERR "  [INFO] No VM forecast results were produced for seasonal event '$apply_seasonality_event' for model $model_type\n";
            }
        }
    } else {
        # This is the standard, non-seasonal run path. It now generates one report
        # per system before proceeding to the next.
        if (@vm_order) {
            _write_standard_csv_report($assimilation_map_ref, $report_type_for_filename, $system_identifier, $unique_date_suffix, 0, 0, 0, $adaptive_runq_saturation_thresh, $adaptive_thresholds_href, undef);
        }
    }

    # CRITICAL: Reset global data structures before processing the next system.
    @vm_order = ();
    %vm_seen = ();
    %results_table = ();
    %per_profile_runq_metrics = ();
    %primary_runq_metrics_captured_for_vm = ();
    %per_profile_nfit_raw_results = ();
    %seasonal_debug_info = ();

    # Report the duration for the completed system's analysis
    my $system_analysis_duration = time() - $system_analysis_start_time;
    print STDERR "  ✓ Processed system $system_identifier (elapsed " . format_duration($system_analysis_duration) . ")\n";

} # End foreach system

# --- Collect unique serials that were part of the output ---
my %serials_in_output_map;
if (%vm_config_data && @vm_order) { # Ensure vm_config_data was loaded and there are VMs to process
    foreach my $vm_name_in_order (@vm_order) {
        if (exists $vm_config_data{$vm_name_in_order} &&
            defined $vm_config_data{$vm_name_in_order}{serial} &&
            $vm_config_data{$vm_name_in_order}{serial} ne '') {
            $serials_in_output_map{$vm_config_data{$vm_name_in_order}{serial}} = 1;
        }
    }
}
my @sorted_unique_serials_list = sort keys %serials_in_output_map;
my $excel_row_num_counter = 1; # Excel rows are 1-based; header is row 1, so first data row is 2.

# --- Final Report Generation ---
# This block now only handles the default, non-seasonal run.
# All seasonal models now handle their own output from within the main system loop.
print STDERR "  ◆ Generated outputs\n";

# --- Script Footer ---
my $final_message = '';
if (@generated_files) {
    # Use List::Util::uniqstr if available, otherwise a simple hash works.
    my %seen;
    my @unique_files = grep { !$seen{$_}++ } @generated_files;

    $final_message .= "    ↳  CSV reports\n";
    foreach my $file (@unique_files) {
        $final_message .= "      ↳  $file\n";
    }
} elsif ($update_history_flag) {
    $final_message .= "    ↳  History\n      ↳  Monthly history updated\n";
} else {
    $final_message .= "    ↳  CSV reports\n      ↳  No output files were generated\n";
}
$final_message .= "    ↳  Rationale\n      ↳  $log_file_path_for_run\n";
print STDERR "$final_message\n";

# --- Report final script duration summary to STDERR ---
my $PROFILE_SCRIPT_END_TIME_EPOCH = time();
my $PROFILE_SCRIPT_DURATION = $PROFILE_SCRIPT_END_TIME_EPOCH - $PROFILE_SCRIPT_START_TIME_EPOCH;
print STDERR "  ⧗ Total execution time: " . format_duration($PROFILE_SCRIPT_DURATION) . "\n";

# --- Finalise and close all rationale log files ---
my $final_end_time_str = localtime($PROFILE_SCRIPT_END_TIME_EPOCH)->strftime("%Y-%m-%d %H:%M:%S %Z");
foreach my $system_id (keys %open_log_files) {
    my $log_fh = $open_log_files{$system_id};
    if ($log_fh) {
        print {$log_fh} "\n----------------------------------------------------------------------\n";
        print {$log_fh} "Analysis for System '$system_id' completed at: " . $final_end_time_str . "\n";
        # Note: A per-system duration would require storing start times in a hash as well.
        # This is sufficient to close the log with a final timestamp.
        print {$log_fh} "======================================================================\n";
        close $log_fh;
    }
}

exit 0;

# ==============================================================================
# Subroutines
# ==============================================================================

sub _phase {
    my ($msg) = @_;
    return unless ($verbose || -t STDERR);
    printf STDERR "\n▶ %s\n", $msg;
}

sub _task {
    my ($msg) = @_;
    return unless ($verbose || -t STDERR);
    printf STDERR "  • %s\n", $msg;
}

sub _subtask {
    my ($msg) = @_;
    return unless ($verbose || -t STDERR);
    printf STDERR "    • %s\n", $msg;
}

sub _step {
    my ($msg) = @_;
    return unless ($verbose || -t STDERR);
    printf STDERR "\n  [+] %s\n", $msg;
}

# --- get_excel_col_name ---
# Converts a 1-based column index to an Excel column name (e.g., 1 -> A, 27 -> AA).
sub get_excel_col_name {
    my ($idx) = @_;
    my $name = '';
    die "Column index must be positive" if (!defined $idx || $idx <= 0);
    while ($idx > 0) {
        my $mod = ($idx - 1) % 26;
        $name = chr(65 + $mod) . $name;
        $idx = int(($idx - $mod - 1) / 26); # Corrected logic for 1-based index progression
    }
    return $name;
}

# --- generate_nfit_ent_formula ---
# Generates the dynamic Excel formula for the "NFIT_ENT_UserFormula" column.
sub generate_nfit_ent_formula {
    my ($excel_row_num, $num_profiles, $column_offset) = @_;

	$column_offset //= 0;

    # First profile column is M (13th column).
    # Peak (L) is the 12th column. Profiles start after Peak.
    my $first_profile_excel_col_letter = get_excel_col_name(12 + 1);
    my $last_profile_excel_col_letter = get_excel_col_name(12 + $num_profiles);

    # The fixed array string for the MATCH function, as provided by the user.
    my $tier_match_array_str_for_formula = '{"P","G1","G2","G3","G4","O1","O2","O3","O4","B1","B2","B3","B4"}';

    # Dynamic column index for 'NFIT - Ent'.
    # 13 fixed leading columns (A-M) + num_profiles columns + 1 (for "NFIT - Ent" itself).
    my $entitlement_column_index = 13 + $num_profiles + 1 + $column_offset;

    # Using "A:AZ" as the VLOOKUP range as requested for stability.
    my $vlookup_range_for_peer_ent = "A:AZ";

    my $formula_body = sprintf(
        'IF(ISNUMBER(SEARCH("PowerHA Standby", I%d)),VLOOKUP(VLOOKUP(A%d, PowerHA!A:B, 2, FALSE),%s, %d, FALSE) * $L$258, CEILING(INDEX(%s%d:%s%d, MATCH(B%d, %s, 0)), 0.05))',
        $excel_row_num,                            # For I%d (SystemType)
        $excel_row_num,                            # For A%d (VM Name for inner VLOOKUP)
        $vlookup_range_for_peer_ent,               # Range for outer VLOOKUP (e.g., A:AZ)
        $entitlement_column_index,                 # Dynamic column index for Current_ENT of the peer
        $first_profile_excel_col_letter, $excel_row_num, # For M%d (start of profile data range)
        $last_profile_excel_col_letter,  $excel_row_num, # For e.g. Y%d (end of profile data range)
        $excel_row_num,                            # For C%d (Hint column, containing the tier string like "G3")
        $tier_match_array_str_for_formula          # For {"P","G1",...} array
    );
    return "=" . $formula_body; # Excel formulas start with "="
}

# --- print_csv_footer ---
# Prints the summary footer section with labels and Excel formulas.
# Make sure Time::Piece is used if not already at the top of your script for strftime
# use Time::Piece; # Already in the full script you provided.
# use List::Util qw(sum min max); # Already in the full script.
# Ensure get_excel_col_name and quote_csv are defined elsewhere or within this sub's scope.
# It accepts an offset to correctly calculate column letters when extra
# columns (like SeasonalMultiplier) are present in the report.
sub print_csv_footer {
    my ($fh, $last_data_row, $nmon_physc_file, $num_profiles, $sorted_unique_serials_list_ref, $col_offset) = @_;
    $col_offset //= 0; # Default to 0 if not provided

    my @sorted_unique_serials = @$sorted_unique_serials_list_ref;
    my $count_of_unique_serials = scalar(@sorted_unique_serials);
    my $loop_count_for_serials = ($count_of_unique_serials == 0) ? 1 : $count_of_unique_serials;

    # --- Calculate dynamic column letters based on script's output structure ---
    my $col_serial_letter = get_excel_col_name(8);
    my $col_system_type_letter = get_excel_col_name(9);

    # Apply the offset to all subsequent column calculations
    my $idx_current_ent = 12 + $num_profiles + 1 + $col_offset;
    my $col_current_ent_letter = get_excel_col_name($idx_current_ent);
    my $idx_nfit_ent_user_formula = 12 + $num_profiles + 2 + $col_offset;
    my $col_nfit_ent_user_formula_letter = get_excel_col_name($idx_nfit_ent_user_formula);

    # --- Get NMON physc data file modification timestamp ---
    my $file_timestamp_str = "N/A";
    if (defined $nmon_physc_file && -f $nmon_physc_file) {
        my $mtime_epoch = (stat($nmon_physc_file))[9];
        if (defined $mtime_epoch) {
            $file_timestamp_str = localtime($mtime_epoch)->strftime("%Y-%m-%d %H:%M:%S");
        } else {
            $file_timestamp_str = "Timestamp N/A (stat fetch failed for $nmon_physc_file)";
        }
    } else {
        $file_timestamp_str = "Timestamp N/A (File not found or not provided)";
    }

    # --- Define starting row for footer elements ---
    print $fh "\n"; # Blank line after main data
    my $footer_start_row = $last_data_row + 2;

    my $row_data_age = $footer_start_row;
    my $row_timestamp = $footer_start_row + 1;
    my $row_as_is_nfit_labels = $footer_start_row + 3;
    my $row_ent_col_headers = $footer_start_row + 4;
    my $row_unique_serials_start = $footer_start_row + 5;

    my $empty = "";
    my @csv_row;

    # --- Row 1 of Footer: Data Age ---
    my $col_letter_data_age_sum_current_incl = get_excel_col_name(23 + $col_offset);
    my $col_letter_data_age_sum_nfit_incl = get_excel_col_name(24 + $col_offset);
    my $col_letter_data_age_delta_incl = get_excel_col_name(25 + $col_offset);
    my $col_letter_data_age_perc_incl = get_excel_col_name(26 + $col_offset);

    my $formula_sum_current_ent_incl_sby = sprintf("=SUM(%s2:%s%d)", $col_current_ent_letter, $col_current_ent_letter, $last_data_row);
    my $formula_sum_nfit_ent_incl_sby = sprintf("=SUM(%s2:%s%d)", $col_nfit_ent_user_formula_letter, $col_nfit_ent_user_formula_letter, $last_data_row);
    my $formula_delta_incl_sby = sprintf("=%s%d-%s%d", $col_letter_data_age_sum_nfit_incl, $row_data_age, $col_letter_data_age_sum_current_incl, $row_data_age);
    my $formula_perc_incl_sby = sprintf("=IFERROR(%s%d/%s%d,\"\")", $col_letter_data_age_delta_incl, $row_data_age, $col_letter_data_age_sum_current_incl, $row_data_age);

    @csv_row = ($empty) x (22 + $col_offset);
    $csv_row[0] = "Data Age";
    $csv_row[21 + $col_offset] = "Incl. SBY";
    push @csv_row, $formula_sum_current_ent_incl_sby, $formula_sum_nfit_ent_incl_sby, $formula_delta_incl_sby, $formula_perc_incl_sby;
    print $fh join(",", map { quote_csv($_) } @csv_row) . "\n";

    # --- Row 2 of Footer: Timestamp ---
    my $col_letter_ts_sum_current_excl = $col_letter_data_age_sum_current_incl;
    my $col_letter_ts_sum_nfit_excl    = $col_letter_data_age_sum_nfit_incl;
    my $col_letter_ts_delta_excl       = $col_letter_data_age_delta_incl;
    my $col_letter_ts_perc_excl        = $col_letter_data_age_perc_incl;

    my $formula_sum_current_ent_excl_sby = sprintf("=SUMIFS(%s\$2:%s\$%d, %s\$2:%s\$%d, \"<>*PowerHA Standby*\")", $col_current_ent_letter, $col_current_ent_letter, $last_data_row, $col_system_type_letter, $col_system_type_letter, $last_data_row);
    my $formula_sum_nfit_ent_excl_sby = sprintf("=SUMIFS(%s\$2:%s\$%d, %s\$2:%s\$%d, \"<>*PowerHA Standby*\")", $col_nfit_ent_user_formula_letter, $col_nfit_ent_user_formula_letter, $last_data_row, $col_system_type_letter, $col_system_type_letter, $last_data_row);
    my $formula_delta_excl_sby = sprintf("=%s%d-%s%d", $col_letter_ts_sum_nfit_excl, $row_timestamp, $col_letter_ts_sum_current_excl, $row_timestamp);
    my $formula_perc_excl_sby = sprintf("=IFERROR(%s%d/%s%d,\"\")", $col_letter_ts_delta_excl, $row_timestamp, $col_letter_ts_sum_current_excl, $row_timestamp);

    @csv_row = ($empty) x (22 + $col_offset);
    $csv_row[0] = $file_timestamp_str;
    $csv_row[21 + $col_offset] = "Excl. SBY";
    push @csv_row, $formula_sum_current_ent_excl_sby, $formula_sum_nfit_ent_excl_sby, $formula_delta_excl_sby, $formula_perc_excl_sby;
    print $fh join(",", map { quote_csv($_) } @csv_row) . "\n";

    print $fh "\n";

    # --- Row: AS-IS NFIT Labels ---
    @csv_row = ($empty) x (23 + $col_offset);
    $csv_row[17 + $col_offset] = "AS-IS";
    $csv_row[18 + $col_offset] = "NFIT";
    $csv_row[20 + $col_offset] = "AS-IS";
    $csv_row[21 + $col_offset] = "NFIT";
    print $fh join(",", map { quote_csv($_) } @csv_row) . "\n";

    # --- Row: ENT Column Headers and other labels ---
    @csv_row = ($empty) x (22 + $col_offset);
    $csv_row[0] = "ENT"; $csv_row[1] = "ENT-NOVIO"; $csv_row[2] = "ENT-HA"; $csv_row[3] = "ENT-NFIT";
    $csv_row[4] = "NFIT-ENT-NO-VIO"; $csv_row[5] = "NFIT-ENT-NO-POWERHA-STANDBY";
    $csv_row[6] = "NFIT-ENT-NO-POWERHA-SBY-NO-VIO"; $csv_row[7] = "NFIT-ENT-POWERHA-SBY-AS-IS";
    $csv_row[8] = "NFIT-ENT-POWERHA-SBY-AS-IS-NOVIO";
    $csv_row[12] = "PowerHA SBY% TGT"; $csv_row[13] = "0.25";

    my $largest_frame_formula_as_is = sprintf("=LET(sys,%s\$2:%s\$%d,type,%s\$2:%s\$%d,ent,%s\$2:%s\$%d,rows,FILTER(HSTACK(sys,ent),NOT(type=\"VIO Server\")),uniqSys,UNIQUE(INDEX(rows,,1)),sums,BYROW(uniqSys,LAMBDA(s,SUM(FILTER(INDEX(rows,,2),INDEX(rows,,1)=s)))),XLOOKUP(MAX(sums),sums,uniqSys))", $col_serial_letter, $col_serial_letter, $last_data_row, $col_system_type_letter, $col_system_type_letter, $last_data_row, $col_current_ent_letter, $col_current_ent_letter, $last_data_row);
    my $largest_frame_formula_nfit = sprintf("=LET(sys,%s\$2:%s\$%d,type,%s\$2:%s\$%d,ent,%s\$2:%s\$%d,rows,FILTER(HSTACK(sys,ent),NOT(type=\"VIO Server\")),uniqSys,UNIQUE(INDEX(rows,,1)),sums,BYROW(uniqSys,LAMBDA(s,SUM(FILTER(INDEX(rows,,2),INDEX(rows,,1)=s)))),XLOOKUP(MAX(sums),sums,uniqSys))", $col_serial_letter, $col_serial_letter, $last_data_row, $col_system_type_letter, $col_system_type_letter, $last_data_row, $col_nfit_ent_user_formula_letter, $col_nfit_ent_user_formula_letter, $last_data_row);
    my $largest_powerha_formula_as_is = sprintf("=LET(sys,%s\$2:%s\$%d,type,%s\$2:%s\$%d,ent,%s\$2:%s\$%d,pharows,FILTER(HSTACK(sys,ent),ISNUMBER(SEARCH(\"PowerHA Primary\",type))),uniqSysPHA,UNIQUE(INDEX(pharows,,1)),sumsPHA,BYROW(uniqSysPHA,LAMBDA(s,SUM(FILTER(INDEX(pharows,,2),INDEX(pharows,,1)=s)))),XLOOKUP(MAX(sumsPHA),sumsPHA,uniqSysPHA,\"\"))", $col_serial_letter, $col_serial_letter, $last_data_row, $col_system_type_letter, $col_system_type_letter, $last_data_row, $col_current_ent_letter, $col_current_ent_letter, $last_data_row);
    my $largest_powerha_formula_nfit = sprintf("=LET(sys,%s\$2:%s\$%d,type,%s\$2:%s\$%d,ent,%s\$2:%s\$%d,pharows,FILTER(HSTACK(sys,ent),ISNUMBER(SEARCH(\"PowerHA Primary\",type))),uniqSysPHA,UNIQUE(INDEX(pharows,,1)),sumsPHA,BYROW(uniqSysPHA,LAMBDA(s,SUM(FILTER(INDEX(pharows,,2),INDEX(pharows,,1)=s)))),XLOOKUP(MAX(sumsPHA),sumsPHA,uniqSysPHA,\"\"))", $col_serial_letter, $col_serial_letter, $last_data_row, $col_system_type_letter, $col_system_type_letter, $last_data_row, $col_nfit_ent_user_formula_letter, $col_nfit_ent_user_formula_letter, $last_data_row);

    $csv_row[16 + $col_offset] = "Largest Frame";
    $csv_row[17 + $col_offset] = $largest_frame_formula_as_is;
    $csv_row[18 + $col_offset] = $largest_frame_formula_nfit;
    $csv_row[19 + $col_offset] = "Largest PowerHA";
    $csv_row[20 + $col_offset] = $largest_powerha_formula_as_is;
    $csv_row[21 + $col_offset] = $largest_powerha_formula_nfit;
    print $fh join(",", map { quote_csv($_) } @csv_row) . "\n";

    # --- Per-Serial Summary Rows ---
    for (my $i = 0; $i < $loop_count_for_serials; $i++) {
        my $current_formula_row = $row_unique_serials_start + $i;
        my @csv_row_serial_summary;

        if ($i == 0) {
            my $formula_unique_serials = sprintf("=UNIQUE(%s\$2:%s\$%d)", $col_serial_letter, $col_serial_letter, $last_data_row);
            push @csv_row_serial_summary, $formula_unique_serials;
        } else {
            push @csv_row_serial_summary, $empty;
        }

        # Formulas for columns B-J
        push @csv_row_serial_summary, sprintf("=IFERROR(SUMIFS(\$%s\$2:\$%s\$%d, \$%s\$2:\$%s\$%d, A%d),\"\")", $col_current_ent_letter, $col_current_ent_letter, $last_data_row, $col_serial_letter, $col_serial_letter, $last_data_row, $current_formula_row);
        push @csv_row_serial_summary, sprintf("=IFERROR(SUMIFS(\$%s\$2:\$%s\$%d, \$%s\$2:\$%s\$%d, A%d, \$%s\$2:\$%s\$%d, \"<>VIO Server\"),\"\")", $col_current_ent_letter, $col_current_ent_letter, $last_data_row, $col_serial_letter, $col_serial_letter, $last_data_row, $current_formula_row, $col_system_type_letter, $col_system_type_letter, $last_data_row);
        push @csv_row_serial_summary, sprintf("=IFERROR(SUMIFS(\$%s\$2:\$%s\$%d, \$%s\$2:\$%s\$%d, A%d, \$%s\$2:\$%s\$%d, \"*PowerHA Primary*\"),\"\")", $col_current_ent_letter, $col_current_ent_letter, $last_data_row, $col_serial_letter, $col_serial_letter, $last_data_row, $current_formula_row, $col_system_type_letter, $col_system_type_letter, $last_data_row);
        push @csv_row_serial_summary, sprintf("=IFERROR(SUMIFS(\$%s\$2:\$%s\$%d, \$%s\$2:\$%s\$%d, A%d),\"\")", $col_nfit_ent_user_formula_letter, $col_nfit_ent_user_formula_letter, $last_data_row, $col_serial_letter, $col_serial_letter, $last_data_row, $current_formula_row);
        push @csv_row_serial_summary, sprintf("=IFERROR(SUMIFS(\$%s\$2:\$%s\$%d, \$%s\$2:\$%s\$%d, A%d, \$%s\$2:\$%s\$%d, \"<>VIO Server\"),\"\")", $col_nfit_ent_user_formula_letter, $col_nfit_ent_user_formula_letter, $last_data_row, $col_serial_letter, $col_serial_letter, $last_data_row, $current_formula_row, $col_system_type_letter, $col_system_type_letter, $last_data_row);
        push @csv_row_serial_summary, sprintf("=IFERROR(SUMIFS(\$%s\$2:\$%s\$%d, \$%s\$2:\$%s\$%d, A%d, \$%s\$2:\$%s\$%d, \"<>*PowerHA Standby*\"),\"\")", $col_nfit_ent_user_formula_letter, $col_nfit_ent_user_formula_letter, $last_data_row, $col_serial_letter, $col_serial_letter, $last_data_row, $current_formula_row, $col_system_type_letter, $col_system_type_letter, $last_data_row);
        push @csv_row_serial_summary, sprintf("=IFERROR(SUMIFS(\$%s\$2:\$%s\$%d, \$%s\$2:\$%s\$%d, A%d, \$%s\$2:\$%s\$%d, \"<>*PowerHA Standby*\", \$%s\$2:\$%s\$%d, \"<>VIO Server\"),\"\")", $col_nfit_ent_user_formula_letter, $col_nfit_ent_user_formula_letter, $last_data_row, $col_serial_letter, $col_serial_letter, $last_data_row, $current_formula_row, $col_system_type_letter, $col_system_type_letter, $last_data_row, $col_system_type_letter, $col_system_type_letter, $last_data_row);
        push @csv_row_serial_summary, sprintf("=IFERROR(SUMPRODUCT((\$%s\$2:\$%s\$%d=A%d)*IF(ISNUMBER(SEARCH(\"PowerHA Standby\",\$%s\$2:\$%s\$%d)),\$%s\$2:\$%s\$%d,\$%s\$2:\$%s\$%d)),\"\")", $col_serial_letter, $col_serial_letter, $last_data_row, $current_formula_row, $col_system_type_letter, $col_system_type_letter, $last_data_row, $col_current_ent_letter, $col_current_ent_letter, $last_data_row, $col_nfit_ent_user_formula_letter, $col_nfit_ent_user_formula_letter, $last_data_row);
        push @csv_row_serial_summary, sprintf("=IFERROR(SUMPRODUCT((\$%s\$2:\$%s\$%d=A%d)*(\$%s\$2:\$%s\$%d<>\"VIO Server\")*IF(ISNUMBER(SEARCH(\"PowerHA Standby\",\$%s\$2:\$%s\$%d)),\$%s\$2:\$%s\$%d,\$%s\$2:\$%s\$%d)),\"\")", $col_serial_letter, $col_serial_letter, $last_data_row, $current_formula_row, $col_system_type_letter, $col_system_type_letter, $last_data_row, $col_system_type_letter, $col_system_type_letter, $last_data_row, $col_current_ent_letter, $col_current_ent_letter, $last_data_row, $col_nfit_ent_user_formula_letter, $col_nfit_ent_user_formula_letter, $last_data_row);

        if ($i == 0) {
            my $num_main_formulas = scalar(@csv_row_serial_summary);
            my $padding_needed = (16 + $col_offset) - $num_main_formulas;
            push @csv_row_serial_summary, ($empty) x $padding_needed if $padding_needed > 0;

            my $col_R_header_cell = get_excel_col_name(18 + $col_offset) . $row_ent_col_headers;
            my $col_S_header_cell = get_excel_col_name(19 + $col_offset) . $row_ent_col_headers;
            my $col_U_header_cell = get_excel_col_name(21 + $col_offset) . $row_ent_col_headers;

            push @csv_row_serial_summary, "Largest Frame ENT (Excl. VIO)";
            push @csv_row_serial_summary, sprintf("=IFERROR(SUMIFS(\$%s\$2:\$%s\$%d, \$%s\$2:\$%s\$%d, %s, \$%s\$2:\$%s\$%d, \"<>VIO Server\"),\"\")", $col_current_ent_letter, $col_current_ent_letter, $last_data_row, $col_serial_letter, $col_serial_letter, $last_data_row, $col_R_header_cell, $col_system_type_letter, $col_system_type_letter, $last_data_row);
            push @csv_row_serial_summary, sprintf("=IFERROR(SUMIFS(\$%s\$2:\$%s\$%d, \$%s\$2:\$%s\$%d, %s, \$%s\$2:\$%s\$%d, \"<>VIO Server\"),\"\")", $col_nfit_ent_user_formula_letter, $col_nfit_ent_user_formula_letter, $last_data_row, $col_serial_letter, $col_serial_letter, $last_data_row, $col_S_header_cell, $col_system_type_letter, $col_system_type_letter, $last_data_row);
            push @csv_row_serial_summary, "Largest PowerHA ENT";
            push @csv_row_serial_summary, sprintf("=IFERROR(SUMIFS(\$%s\$2:\$%s\$%d, \$%s\$2:\$%s\$%d, %s, \$%s\$2:\$%s\$%d, \"*PowerHA Primary*\"),\"\")", $col_current_ent_letter, $col_current_ent_letter, $last_data_row, $col_serial_letter, $col_serial_letter, $last_data_row, $col_U_header_cell, $col_system_type_letter, $col_system_type_letter, $last_data_row);
            push @csv_row_serial_summary, sprintf("=IFERROR(SUMIFS(\$%s\$2:\$%s\$%d, \$%s\$2:\$%s\$%d, %s, \$%s\$2:\$%s\$%d, \"*PowerHA Primary*\"),\"\")", $col_nfit_ent_user_formula_letter, $col_nfit_ent_user_formula_letter, $last_data_row, $col_serial_letter, $col_serial_letter, $last_data_row, $col_U_header_cell, $col_system_type_letter, $col_system_type_letter, $last_data_row);
        }
        print $fh join(",", map { quote_csv($_) } @csv_row_serial_summary) . "\n";
    }

    # --- Rows after per-serial summary (Frame Evac, etc.) ---
    my $actual_row_unique_serials_end = $row_unique_serials_start + $loop_count_for_serials - 1;
    my $row_after_serials_block = $row_unique_serials_start + $loop_count_for_serials;

    my $cell_largest_frame_asis_val = get_excel_col_name(18 + $col_offset) . $row_ent_col_headers;
    my $cell_largest_frame_nfit_val = get_excel_col_name(19 + $col_offset) . $row_ent_col_headers;
    my $cell_largest_pha_asis_val   = get_excel_col_name(21 + $col_offset) . $row_ent_col_headers;
    my $cell_largest_pha_nfit_val   = get_excel_col_name(22 + $col_offset) . $row_ent_col_headers;

    # Row: Frame Evac - Max Required
    @csv_row = ($empty) x (22 + $col_offset);
    $csv_row[16 + $col_offset] = "Frame Evac - Max Required";
    $csv_row[17 + $col_offset] = sprintf("=MAX(%s,%s)", $cell_largest_frame_asis_val, $cell_largest_pha_asis_val);
    $csv_row[21 + $col_offset] = sprintf("=MAX(%s,%s)", $cell_largest_frame_nfit_val, $cell_largest_pha_nfit_val);
    print $fh join(",", map { quote_csv($_) } @csv_row) . "\n";

    # Row: Frame Evac - Required per frame
    my $current_print_row_for_evac_max = $row_after_serials_block;
    my $cell_max_req_as_is_val = get_excel_col_name(18 + $col_offset) . $current_print_row_for_evac_max;
    my $cell_max_req_nfit_val  = get_excel_col_name(22 + $col_offset) . $current_print_row_for_evac_max;

    @csv_row = ($empty) x (22 + $col_offset);
    $csv_row[16 + $col_offset] = "Frame Evac - Required per frame";
    $csv_row[17 + $col_offset] = sprintf("=IFERROR(%s/COUNTA(UNIQUE(\$%s\$2:\$%s\$%d)),\"N/A\")", $cell_max_req_as_is_val, $col_serial_letter, $col_serial_letter, $last_data_row);
    $csv_row[21 + $col_offset] = sprintf("=IFERROR(%s/COUNTA(UNIQUE(\$%s\$2:\$%s\$%d)),\"N/A\")", $cell_max_req_nfit_val, $col_serial_letter, $col_serial_letter, $last_data_row);
    print $fh join(",", map { quote_csv($_) } @csv_row) . "\n";

    # --- Total Row for Per-Serial Summary ---
    @csv_row = ();
    push @csv_row, "Total";
    for my $col_idx (2..10) {
        my $col_letter = get_excel_col_name($col_idx);
        if ($count_of_unique_serials > 0) {
            push @csv_row, sprintf("=SUM(%s%d:%s%d)", $col_letter, $row_unique_serials_start, $col_letter, $actual_row_unique_serials_end);
        } else {
            push @csv_row, "0";
        }
    }
    my $current_cols = scalar(@csv_row);
    push @csv_row, ($empty) x ((22 + $col_offset) - $current_cols) if (22 + $col_offset) > $current_cols;
    print $fh join(",", map { quote_csv($_) } @csv_row) . "\n";
}


# ==============================================================================
# Subroutine to format nfit flags for display
# ==============================================================================
sub format_nfit_flags_for_display {
    my ($profile_name, $profile_specific_flags, $runq_perc_flags, $runq_behavior) = @_;
    my @output_lines;

    my $temp_profile_flags = $profile_specific_flags; # Work on a copy

    my @core_fit_parts;
    my @decay_parts;
    my @growth_parts;
    my @other_parts; # For flags not specifically categorized

    # Helper sub-subroutine to extract and remove a flag pattern
    # Arguments:
    #   1. Regex for the flag and its potential value (e.g., qr/-p\s+[^\s]+/)
    #   2. Array reference to store the extracted flag string
    #   3. Scalar reference to the string of flags to be processed (will be modified)
    sub _extract_flag {
        my ($flag_regex, $parts_array_ref, $flags_string_ref) = @_;
        if ($$flags_string_ref =~ s/($flag_regex)//) {
            my $extracted_part = $1;
            $extracted_part =~ s/^\s+|\s+$//g; # Trim whitespace
            push @$parts_array_ref, $extracted_part if $extracted_part;
        }
    }

    # --- Core Fit Parameters ---
    _extract_flag(qr/--percentile\s+(?:\"[^\"]*\"|\'[^\']*\'|[^\s\"]+)|-p\s+(?:\"[^\"]*\"|\'[^\']*\'|[^\s\"]+)/, \@core_fit_parts, \$temp_profile_flags);
    _extract_flag(qr/--process-window-size\s+(?:\"[^\"]*\"|\'[^\']*\'|[^\s\"]+)|-w\s+(?:\"[^\"]*\"|\'[^\']*\'|[^\s\"]+)/, \@core_fit_parts, \$temp_profile_flags);
    _extract_flag(qr/--filter-above-perc\s+(?:\"[^\"]*\"|\'[^\']*\'|[^\s\"]+)/, \@core_fit_parts, \$temp_profile_flags);
    # Add more related flags here if needed, e.g.:
    # _extract_flag(qr/--filter-metric\s+[^\s]+/, \@core_fit_parts, \$temp_profile_flags);
    # _extract_flag(qr/--filter-limit\s+[^\s]+/, \@core_fit_parts, \$temp_profile_flags);

    # --- Decay Options ---
    _extract_flag(qr/--decay\s+(?:\"[^\"]*\"|\'[^\']*\'|[^\s\"]+)/, \@decay_parts, \$temp_profile_flags);
    _extract_flag(qr/--runq-decay\s+(?:\"[^\"]*\"|\'[^\']*\'|[^\s\"]+)/, \@decay_parts, \$temp_profile_flags);
    # Add more related flags here, e.g.:
    # _extract_flag(qr/--ema-period\s+[^\s]+/, \@decay_parts, \$temp_profile_flags);
    # _extract_flag(qr/--sma-period\s+[^\s]+/, \@decay_parts, \$temp_profile_flags);

    # --- Growth Prediction ---
    _extract_flag(qr/--enable-growth-prediction\b/, \@growth_parts, \$temp_profile_flags); # \b for word boundary
    _extract_flag(qr/--max-growth-inflation-percent\s+(?:\"[^\"]*\"|\'[^\']*\'|[^\s\"]+)/, \@growth_parts, \$temp_profile_flags);
    _extract_flag(qr/--growth-period-days\s+(?:\"[^\"]*\"|\'[^\']*\'|[^\s\"]+)/, \@growth_parts, \$temp_profile_flags);

    if (@core_fit_parts) {
        push @output_lines, "  Core         : " . join(" ", @core_fit_parts);
    }
    if (@decay_parts) {
        push @output_lines, "  Decay        : " . join(" ", @decay_parts);
    }
    if (@growth_parts) {
        push @output_lines, "  Growth       : " . join(" ", @growth_parts);
    }

    # --- RunQ Percentiles (these are from the already processed $runq_perc_flags) ---
    my $trimmed_runq_perc_flags = $runq_perc_flags;
    $trimmed_runq_perc_flags =~ s/^\s+|\s+$//g; # Trim
    if ($trimmed_runq_perc_flags) {
        push @output_lines, "  RunQ Percs   : " . $trimmed_runq_perc_flags;
    }

    # --- Other/Remaining Profile Flags ---
    # Any flags left in $temp_profile_flags are considered "Other"
    $temp_profile_flags =~ s/^\s+|\s+$//g; # Trim remaining
    my @remaining_flags = split(/\s+/, $temp_profile_flags); # Split remaining by space
    @remaining_flags = grep { $_ ne "" } @remaining_flags; # Filter out empty strings
    if (@remaining_flags) {
        # Reconstruct to handle flags that might have been split from their values if not perfectly matched above
        # This simplistic split might not be perfect if un-extracted flags had quoted spaces.
        # For robust handling of complex "Other" flags, more sophisticated parsing of $temp_profile_flags would be needed.
        # However, ideally, most common flags are explicitly extracted above.
        push @output_lines, "  Other Args   : " . join(" ", @remaining_flags);
    }

    # --- RunQ Behavior ---
    # This comes directly from the profile config, not from the flag strings
    if (defined $runq_behavior && $runq_behavior ne 'default') {
        push @output_lines, "  RunQBehavior : $runq_behavior";
    }

    return join("\n", @output_lines);
}

# --- parse_profile_name_for_log ---
# Parses common nfit-profile profile name patterns for a more descriptive log output.
# Adheres to Allman style and includes comments.
sub parse_profile_name_for_log
{
    my ($profile_name_str) = @_;

    my $description = $profile_name_str; # Default to original name if no pattern matches
    my @parts;

    # Regex to capture common patterns like O3-95W15, P-99W1, G2-BatchSpecial etc.
    # This regex looks for: TypeChar [TierNum] - Percentile W WindowNum [SuffixLetters]
    if ($profile_name_str =~ /^([OBGP])(?:-?(\d+))?-?(\d{2,3})(?:W(\d+))?([A-Z]*)?$/i)
    {
        my $type_char   = uc($1);
        my $tier_num    = $2; # Optional
        my $perc_val    = $3;
        my $win_val     = $4; # Optional
        my $suffix_char = $5; # Optional

        my $type_desc = "Unknown Type"; # Default for safety
        if ($type_char eq 'O')
        {
            $type_desc = "Online";
        }
        elsif ($type_char eq 'B')
        {
            $type_desc = "Batch";
        }
        elsif ($type_char eq 'G')
        {
            $type_desc = "General";
        }
        elsif ($type_char eq 'P')
        {
            $type_desc = "Peak";
        }

        if (defined $tier_num && $tier_num ne "")
        {
            $type_desc .= " (Tier $tier_num)";
        }
        push @parts, $type_desc;

        if (defined $perc_val)
        {
            push @parts, "$perc_val" . "th Percentile";
        }
        if (defined $win_val)
        {
            push @parts, "$win_val-minute Window";
        }
        if (defined $suffix_char && $suffix_char ne "")
        {
            push @parts, "Variant '$suffix_char'";
        }

        $description = join(", ", @parts);
    }
    elsif (lc($profile_name_str) eq "peak") # Handle specific "Peak" profile name
    {
        $description = "Absolute Peak Value";
    }
    # Add more 'elsif' blocks here for other distinct profile naming conventions if needed.

    return "$profile_name_str ($description)";
}

# ==============================================================================
# SUBROUTINE: log_peak_profile_rationale
# PURPOSE:    Logs a simplified, explicit rationale for the P-99W1 profile,
#             clarifying that it is an unmodified, pure measurement.
# ==============================================================================
sub log_peak_profile_rationale {
    my ($fh, $vm_map_ref, $profile_ref, $base_physc) = @_;
    return unless $fh;

    my $vm_name = $vm_map_ref->{Configuration}{vm_name};
    my $profile_desc = parse_profile_name_for_log($profile_ref->{name});

    print {$fh} "\n======================================================================\n";
    printf {$fh} "VM Name                                      : %s\n", $vm_name;
    printf {$fh} "Profile Processed                            : %s\n", $profile_desc;
    print {$fh} "----------------------------------------------------------------------\n";
    print {$fh} "Measurement Type: Pure Peak (Statistical Ceiling)\n\n";
    print {$fh} "  - The $MANDATORY_PEAK_PROFILE_FOR_HINT profile is a measurement-only statistical ceiling.\n";
    print {$fh} "  - It represents the smoothed (1-min SMA) 99.75th percentile of the\n";
    print {$fh} "    unfiltered historical data.\n";
    print {$fh} "  - NO growth, RunQ, or forecasting modifiers are applied to this value.\n\n";
    printf {$fh} "  Final Unmodified Value                       : %s cores\n", ($base_physc // 'N/A');
    print {$fh} "======================================================================\n\n";
}

# --- log_profile_rationale ---
# Logs the detailed rationale for how a profile's PhysC value was adjusted.
# Incorporates a summary-first approach and clearer narrative for planners.
# Adheres to Allman style and includes comments.
sub log_profile_rationale
{
    # This function is now refactored to read from the assimilation map.
    my ($fh, $vm_map_ref, $profile_ref, $base_physc_for_log, $final_csv_value_for_profile, $debug_info_ref, $raw_nfit_states_aref, $adaptive_runq_saturation_thresh) = @_;

    # Declare and initialise variables for STD rationale block
    my $eff_p_base_numeric = (defined $base_physc_for_log && looks_like_number($base_physc_for_log)) ? ($base_physc_for_log + 0) : undef;

    # Ensure script doesn't die if log handle isn't valid
    return unless $fh;

    # --- Unpack all required values from the map and arguments ---
    my $vm_name = $vm_map_ref->{Configuration}{vm_name};
    my $profile_obj = $profile_ref;
    my $base_physc_for_profile = $base_physc_for_log;
    my $calc_debug_info_ref = $debug_info_ref;

    # Source from map's Configuration block
    my $cfg = $vm_map_ref->{Configuration};
    my $smt_val = $cfg->{smt};
    my $entitlement_val = $cfg->{entitlement};
    my $lpar_max_cpu_cfg_val_from_config = $cfg->{max_cpu};
    my $curr_ent_numeric = (defined $entitlement_val && looks_like_number($entitlement_val)) ? ($entitlement_val + 0) : undef;

    # Source from map's RunQMetrics block
    my $runq = $vm_map_ref->{RunQMetrics};
    my $runq_metrics_source_profile_name_for_this_calc = $runq->{SourceProfile};
    my $normP50_for_this_calc = $runq->{'NormRunQ_P50'};
    my $normP90_for_this_calc = $runq->{'NormRunQ_P90'};
    my $abs_runq_value_used_for_calc = $calc_debug_info_ref->{AbsRunQValueUsedForCalc};

    # Source other values
    my $profile_rq_behavior = $profile_ref->{runq_behavior};


    # --- Determine if a seasonal run occurred to select the correct logging path ---
    my $event_config = defined($apply_seasonality_event) ? ($seasonality_config->{$apply_seasonality_event} // {}) : {};

    # --- Get profile name and metric key from the profile object ---
	my $model_type = $event_config->{model} // '';
	my $profile_being_adjusted = $profile_obj->{name}; # Get name from the object
	my $profile_desc = parse_profile_name_for_log($profile_being_adjusted);
	my $profile_physc_perc_val_num;
	if (defined $profile_obj->{flags} && $profile_obj->{flags} =~ /(?:-p|--percentile)\s+([0-9.]+)/) {
		$profile_physc_perc_val_num = $1 + 0;
	}
	my $p_metric_key = "P" . clean_perc_label($profile_physc_perc_val_num // $DEFAULT_PERCENTILE);

    # --- PATH A: Multiplicative Seasonal Model has its own log format ---
    if ($model_type eq 'multiplicative_seasonal' && exists $seasonal_debug_info{$vm_name}{$profile_being_adjusted}) {
        my $s_data = $seasonal_debug_info{$vm_name}{$profile_being_adjusted};
        print {$fh} "\n======================================================================\n";
        printf {$fh} "VM Name                                : %s\n", $vm_name;
        printf {$fh} "Profile Processed                      : %s\n", $profile_desc;
        print {$fh} "----------------------------------------------------------------------\n";
        print {$fh} "  CPU Forecasting Model: Multiplicative Seasonal Forecast\n\n";
        printf {$fh} "  - Current Baseline Value      : %.4f cores\n", $s_data->{baseline};
        printf {$fh} "  - Historical Multiplier       : %.4f\n", $s_data->{multiplier};
        printf {$fh} "  - Volatility Buffer           : %.4f\n", $s_data->{volatility};
        print {$fh} "  - Calculation                 : Baseline * Multiplier * Volatility\n";
        printf {$fh} "  Final Forecasted Value       : %.4f cores\n", $s_data->{forecast};
        print {$fh} "======================================================================\n\n";
        return;
    }

    # --- PATH B: Standard Rationale (now also used by recency_decay model) ---
    my $na = 'N/A'; # Consistent N/A string for display
    my $abs_runq_key_reported_in_log = $calc_debug_info_ref->{AbsRunQKeyUsed} // 'AbsRunQ_P90 (default)';

    # --- Top Summary Block ---
    my $profile_description_log = parse_profile_name_for_log($profile_being_adjusted);

    # Use the unrounded final value from debug_info for precise change calculation
    my $final_recommendation_unrounded_str = $calc_debug_info_ref->{'FinalAdjustedPhysC'} // $na;
    my $final_recommendation_unrounded_num = ($final_recommendation_unrounded_str ne $na && $final_recommendation_unrounded_str =~ /^-?[0-9.]+$/)
    ? ($final_recommendation_unrounded_str + 0) : undef;

    my $base_physc_val_num = looks_like_number($base_physc_for_profile) ? $base_physc_for_profile + 0 : undef;

    my $net_change_str = $na;
    if (defined $base_physc_val_num && defined $final_recommendation_unrounded_num) {
        my $delta = $final_recommendation_unrounded_num - $base_physc_val_num;
        my $perc_change_str = (abs($base_physc_val_num) > $FLOAT_EPSILON) ? sprintf(" (Change: %s%.1f%%)", ($delta >=0 ? "+" : ""), ($delta / $base_physc_val_num) * 100) : "";
        $net_change_str = sprintf("%s%.4f cores%s", ($delta >=0 ? "+" : ""), abs($delta), $perc_change_str);
    }

    print {$fh} "\n======================================================================\n";
    printf {$fh} "VM Name                                      : %s\n", $vm_name;
    printf {$fh} "Profile Processed                            : %s\n", $profile_description_log;
    print {$fh} "----------------------------------------------------------------------\n";
    if ($model_type eq 'recency_decay') {
		# --- PATH B: Log the Recency Decay Rationale (Standardised) ---
		my $profile_desc = parse_profile_name_for_log($profile_being_adjusted);

        print {$fh} "  CPU Forecasting Model: Recency-Anchored Decay (Seasonal: '$apply_seasonality_event')\n\n";

		# EXEMPTION: Special Handling for Peak Helper (measurement-only profiles)
		# NOTE: The peak helper (Peak_P-99W1) is intercepted BEFORE reaching this function
		# (see dispatch at log_peak_profile_rationale). If a peak helper profile somehow
		# reaches here, emit its simplified rationale and return.
		if ($profile_being_adjusted eq $MANDATORY_PEAK_PROFILE_FOR_HINT) {
			print {$fh} "  - The $MANDATORY_PEAK_PROFILE_FOR_HINT profile is a measurement-only statistical ceiling.\n";
			print {$fh} "  - NO growth, RunQ, or forecasting modifiers are applied to this value.\n\n";
			printf {$fh} "  - Final nfit Value (Unrounded)    : %s cores\n", ($vm_map_ref->{CoreResults}{ProfileValues}{$profile_being_adjusted} // "N/A");
		}
		else {
			# 1. Retrieve Standardised Growth Info
			#    We rely on the data stored in the main loop: $vm_map_ref->{Growth}{StandardAdjustment}
			my $growth_info   = $vm_map_ref->{Growth}{StandardAdjustment} || {};
			my $growth_adj    = $growth_info->{Value} // 0;
			my $growth_source = $growth_info->{Source}; # Retrieve the source name (e.g., G3-95W15)

			# 2. Retrieve Base Value (Pre-Growth) from storage
			my $base_val_unrounded = $vm_map_ref->{Growth}{base_values}{$profile_being_adjusted};

			# Fallback for Base Value if not explicitly stored
			if (!defined $base_val_unrounded) {
				 $base_val_unrounded = (looks_like_number($base_physc_for_profile))
					? $base_physc_for_profile - $growth_adj
					: "N/A";
			}

            # 3. Retrieve RunQ Modifier (Potential - ungated queue physics)
			#    For recency_decay, we use RunQ_Potential which represents the raw
			#    queue-implied demand without change management gates.
			my $runq_mod = $vm_map_ref->{CSVModifiers}{RunQ_Potential} // 0;
			my $runq_label = "RunQ Modifier (Potential)";

			# --- Output ---
			print {$fh} "  - This model solves the 'Start-of-Month' problem and includes nfit's\n";
			print {$fh} "    standard growth prediction.\n\n";
			printf {$fh} "  - Analysis Reference Date         : %s\n", ($nfit_analysis_reference_date_str // "N/A");

			printf {$fh} "  - Base Value (Recency-Anchored)   : %.4f cores\n", $base_val_unrounded if (looks_like_number($base_val_unrounded));

			# Print Growth (Standardised)
			printf {$fh} "  - Growth Adjustment               : %+.4f cores (Standardised Source: %s)\n", $growth_adj, ($growth_source // "N/A");

			# Print RunQ (Signed - Positive or Negative)
			if ($runq_mod != 0) {
				 printf {$fh} "  - %-32s: %+.4f cores\n", $runq_label, $runq_mod;
			}

			print {$fh} "  --------------------------------------------------------------------\n";
			printf {$fh} "  - Final nfit Value (Unrounded)    : %s cores\n", ($vm_map_ref->{CoreResults}{ProfileValues}{$profile_being_adjusted} // "N/A");
		}

		print {$fh} "======================================================================\n\n";
		return;
	}

    # --- Section A: nfit Raw State Analysis ---
    # CRITICAL: Track pre-growth and post-growth values separately for audit transparency
    # base_physc_for_log now contains the growth-inclusive value (FinalValue from nfit).
    # Retrieve the true pre-growth BaseValue from the Growth.base_values storage.
    my $pre_growth_base_physc = $vm_map_ref->{Growth}{base_values}{$profile_being_adjusted};
    # Fallback: If base_values not available, compute from current value minus growth adj
    if (!defined $pre_growth_base_physc || !looks_like_number($pre_growth_base_physc)) {
        my $nfit_growth_adj = $vm_map_ref->{Growth}{adjustments}{$profile_being_adjusted} // 0;
        $pre_growth_base_physc = (looks_like_number($base_physc_for_log) ? $base_physc_for_log : 0) - $nfit_growth_adj;
    }
    my $nfit_growth_adj = $vm_map_ref->{Growth}{adjustments}{$profile_being_adjusted} // 0;
    my $post_growth_base_physc = (looks_like_number($pre_growth_base_physc) ? $pre_growth_base_physc : 0) + $nfit_growth_adj;

    if (ref($raw_nfit_states_aref) eq 'ARRAY' && @$raw_nfit_states_aref) {
        print {$fh} "Section A: nfit Raw State Analysis & Base Value Calculation\n";
        printf {$fh} "  - nfit reported the following configuration states for this profile:\n";

        my $first_result = $raw_nfit_states_aref->[0];
        my $is_aggregated = ($first_result->{analysisType} || '') =~ /aggregated/;

        if ($is_aggregated) {
            # Use the already-parsed configuration and base value from the assimilation map.
            my $config = $vm_map_ref->{Configuration} || {};
            my $metric_val = $pre_growth_base_physc; # This is the correct pre-growth base value.

            # NOTE: In aggregated mode we do not have per-state VCPU values here; report the
            # current/most-recent known VCPU config explicitly (do not imply it was aggregated).
            my $vcpu_curr =
                (defined $config->{virtual_cpus} && looks_like_number($config->{virtual_cpus})) ? ($config->{virtual_cpus} + 0) :
                (defined $cfg->{virtual_cpus}    && looks_like_number($cfg->{virtual_cpus}))    ? ($cfg->{virtual_cpus}    + 0) :
                undef;
            my $vcpu_curr_str = defined $vcpu_curr ? sprintf("%d", $vcpu_curr) : $na;

            printf {$fh} "    - %-39s: Ent=%.2f, MaxCPU=%.2f, SMT=%d, BaseValue=%.4f, VCPUs=%s\n",
                "Effective Config (aggregated)",
                $config->{entitlement} // 0,
                $config->{max_cpu} // 0,
                $config->{smt} // 0,
                (defined($metric_val) && looks_like_number($metric_val)) ? $metric_val : 0,
                $vcpu_curr_str;

			my $state_count = $first_result->{state}{stateCount} // 1;
            printf {$fh} "    - Aggregation Method                     : Time-weighted decay model applied across %d configuration states.\n", $state_count;

       } else {
            # Standard logging for non-aggregated results
            foreach my $state_res (@$raw_nfit_states_aref) {
                my $state_id_str = $state_res->{state}{id} // 'N/A';
                my $config = $state_res->{metadata}{configuration} || {};
                my $metric_val = $state_res->{metrics}{physc}{$p_metric_key};

                printf {$fh} "    - %-26s: Ent=%.2f, MaxCPU=%.2f, SMT=%d, %s=%s\n",
                    $state_id_str,
                    $config->{entitlement} // 0,
                    $config->{maxCpu} // 0,
                    $config->{smt} // 0,
                    $p_metric_key,
                    defined($metric_val) ? sprintf("%.4f", $metric_val) : $na;
            }
            if (@$raw_nfit_states_aref > 1) {
                printf {$fh} "    - Aggregation Method                      : Simple Average of %d states was used.\n", scalar(@$raw_nfit_states_aref);
            } else {
                printf {$fh} "    - Aggregation Method                      : Direct value from a single state was used.\n";
            }
        }
    }

    printf {$fh} "Initial Base PhysC for Profile               : %s cores (Aggregated value from nfit)\n",
        (defined($pre_growth_base_physc) && looks_like_number($pre_growth_base_physc))
            ? sprintf("%.4f", $pre_growth_base_physc) : $na;
    printf {$fh} "Final nfit-profile Recommendation            : %s cores (Unrounded: %s)\n",
        ($final_csv_value_for_profile // $na), $final_recommendation_unrounded_str;
    printf {$fh} "Net Adjustment by nfit-profile               : %s\n", $net_change_str;
    print {$fh} "======================================================================\n";

    # --- Section A.1: Comprehensive Growth Adjustment Rationale ---
    # Retrieve growth rationale from the assimilation map (harvested during map building)
    my $gd = $vm_map_ref->{GrowthRationaleByProfile}{$profile_being_adjusted} || {};

    # Display growth section if adjustment is non-zero OR if rationale exists
    if (abs($nfit_growth_adj) > $FLOAT_EPSILON || (ref($gd) eq 'HASH' && scalar keys %$gd > 0)) {
        print {$fh} "Section A.1: nfit GrowthAdj (Theil-Sen Robust Trend)\n";
        printf {$fh} "  - nfit GrowthAdj Applied                   : %s%.4f cores\n",
            ($nfit_growth_adj >= 0 ? "+" : ""), $nfit_growth_adj;

        # Helper for consistent boolean/skipped formatting
        my $format_check_result = sub {
            my $val = shift;
            return "Skipped" if (!defined $val || $val eq 'Skipped');
            return $val ? "Passed" : "Failed";
        };

        # --- Print Adaptive Projection Metadata (if available) ---
        if (defined $gd->{projection_days}) {
            print {$fh} "  - Rationale for GrowthAdj (from nfit):\n";
            printf {$fh} "    - Projection Horizon    : %d days (%s)\n",
                $gd->{projection_days}, $gd->{projection_days_source} // 'unknown';
            printf {$fh} "    - Analysis Window       : %d days\n",
                $gd->{analysis_days} if defined $gd->{analysis_days};
            printf {$fh} "    - Sampling Interval     : %d minutes (average)\n",
                $gd->{avg_sampling_interval_mins} if defined $gd->{avg_sampling_interval_mins};

            # Calculate and display extrapolation ratio for adaptive projections
            if (($gd->{projection_days_source} // '') eq 'adaptive' &&
                defined $gd->{analysis_days} && $gd->{analysis_days} > 0) {
                my $ratio = sprintf("%.2f", $gd->{projection_days} / $gd->{analysis_days});
                printf {$fh} "    - Extrapolation Ratio   : %s:1 (projection:analysis)\n", $ratio;
            }
        }

        # --- Detailed Rationale for Robust Trend Analysis (if growth was calculated) ---
        print {$fh} "    - Rationale for GrowthAdj (Theil-Sen):\n";

        # Check if we have Theil-Sen specific fields (indicates v6.25+ cache)
        my $has_theil_sen_data = (defined $gd->{sen_slope} || defined $gd->{mk_variant});

        if ($has_theil_sen_data) {
            # NEW FORMAT: Comprehensive Theil-Sen rationale
            printf {$fh} "    1. Method         : %s (trend analysis on %d %s samples)\n",
                $gd->{method_used} // 'Theil-Sen',
                $gd->{sample_points} // ($gd->{num_hist_periods} // 0),
                $gd->{aggregation_basis} // 'daily';

            printf {$fh} "    2. Volatility Check : CV = %.4f (Limit: < %.2f) [Result: %s]\n",
                $gd->{volatility_cv} // ($gd->{stats_cv} // 0),
                $GROWTH_MAX_CV_THRESHOLD,
                $format_check_result->($gd->{volatility_check_passed} // $gd->{cv_check_passed});

            printf {$fh} "    3. Trend Analysis   : Theil-Sen Estimator (Sen's Slope)\n";
            printf {$fh} "       - Slope          : %+.6f cores/day\n", $gd->{sen_slope} // 0;
            printf {$fh} "       - Trend          : %s\n", $gd->{sen_trend} // 'none';

            printf {$fh} "    4. Significance     : Mann-Kendall Test (Variant: %s)\n",
                $gd->{mk_variant} // 'standard';
            printf {$fh} "       - p-value        : %.4f (Significant if < 0.05)\n",
                $gd->{sen_p_value} // 1.0;
            printf {$fh} "       - Kendall's Tau  : %.4f\n", $gd->{sen_tau} // 0;

            # Print result or skip reason
            if ($gd->{skip_reason}) {
                printf {$fh} "    5. Result         : Growth skipped. Reason: %s\n",
                    $gd->{skip_reason};
            } elsif (defined $gd->{growth_adj}) {
                my $capping_msg = (defined $gd->{was_capped} && $gd->{was_capped} eq '1')
                    ? sprintf(" (CAPPED from %.4f by %.0f%% limit)",
                              $gd->{original_growth_adj} // $gd->{growth_adj},
                              $gd->{cap_percent_applied} // $DEFAULT_MAX_GROWTH_INFLATION_PERCENT)
                    : "";
                printf {$fh} "    5. Result         : Trend is significant. GrowthAdj (%.4f) applied.%s\n",
                    $gd->{growth_adj}, $capping_msg;
            }

            # Print OLS comparison
            printf {$fh} "    6. OLS Comparison : Slope = %+.6f, R-squared = %.4f\n",
                $gd->{ols_slope} // 0, $gd->{ols_r2} // 0;

            # --- Hamed-Rao Correction Diagnostics (if present) ---
            printf {$fh} "    7. Mann-Kendall Test Details\n";
            if (defined $gd->{mk_variant} && $gd->{mk_variant} eq 'hamed-rao') {
                print {$fh} "    - Hamed-Rao Correction Diagnostics:\n";
                printf {$fh} "      - Adjustment Factor : %.4f\n",
                    $gd->{mk_adjustment_factor} // 1;
                printf {$fh} "      - Effective N       : %.0f (Reduced from %d due to autocorrelation)\n",
                    $gd->{mk_effective_n} // ($gd->{sample_points} // 0),
                    $gd->{sample_points} // 0;
                printf {$fh} "      - Lag-1 Autocorr    : %.4f\n",
                    $gd->{lag1_autocorr} // 0;
            }

            # Display MK variant and routing information
                if (my $mk_variant = $gd->{mk_variant}) {
                    if ($mk_variant eq 'standard_mk_trend_dominant') {
                        my $tau = "\x{03C4}";
                        my $ge  = "\x{2265}";
                        print {$fh} "    - Test Variant       : Standard MK (strong trend detected, |$tau| $ge 0.4)\n";
                        if (my $reason = $gd->{hamed_rao_skip_reason}) {
                            print {$fh} "    - Note               : $reason\n";
                        }
                    } elsif ($mk_variant eq 'standard_mk_via_hamed_rao_fallback') {
                        print {$fh} "    - Test Variant       : Standard MK (Hamed-Rao fallback)\n";
                        if (my $reason = $gd->{hamed_rao_fallback_reason}) {
                            print {$fh} "    - Fallback Reason    : $reason\n";
                        }
                        if (my $af = $gd->{hamed_rao_adjustment_factor}) {
                            print {$fh} "    - Attempted Adj Factor: $af\n";
                        }
                    } elsif ($mk_variant eq 'hamed-rao') {
                        # Already displayed above in existing section
                    } elsif ($mk_variant eq 'standard') {
                        print {$fh} "    - Test Variant       : Standard Mann-Kendall\n";
                    }
                }

                # Display autocorrelation max lag when available
                if (my $K = $gd->{hamed_rao_autocorr_max_lag}) {
                    print {$fh} "    - Max Lag (K)        : $K\n";
                }

        } elsif (defined $gd->{slope_check_passed} && $gd->{slope_check_passed} eq '1') {
            # LEGACY FORMAT: Old OLS-based rationale (pre-v6.25 cache)
            my $proj_val_str = looks_like_number($gd->{projected_val})
                ? sprintf("%.4f", $gd->{projected_val}) : "N/A";
            printf {$fh} "        4. Projection                    : Trend projected to %s cores over %d days.\n",
                $proj_val_str, $DEFAULT_GROWTH_PROJECTION_DAYS;

            my $inflation_str = looks_like_number($gd->{inflation_perc})
                ? sprintf("%.2f", $gd->{inflation_perc}) : "N/A";
            my $capping_msg = (defined $gd->{was_capped} && $gd->{was_capped} eq '1')
                ? "CAPPED" : "not capped";
            printf {$fh} "        5. Inflation                         : Calculated inflation is %s%%. This was %s to the max of %d%%.\n",
                $inflation_str, $capping_msg, $DEFAULT_MAX_GROWTH_INFLATION_PERCENT;
        } else {
            # No growth calculated or insufficient data
            print {$fh} "    - Rationale details not available (no growth calculated).\n";
        }

        # Subtotal after growth adjustment
        printf {$fh} "  - Subtotal (Growth-Adjusted Base)            : %.4f cores\n",
            $post_growth_base_physc;
        print {$fh} "======================================================================\n";
    }
    print {$fh} "\n";

    # --- Section B: Key Inputs & Configuration ---

    print {$fh} "Section B: Key Inputs & Configuration for Modifier Logic\n";
    printf {$fh} "  1. Key RunQ Metrics (source: %s, state: %s)\n", $runq_metrics_source_profile_name_for_this_calc, "Most Recent";
    printf {$fh} "     - AbsRunQ for Upsizing (%s)    : %s threads\n", ($abs_runq_key_reported_in_log // $na), (defined $abs_runq_value_used_for_calc && looks_like_number($abs_runq_value_used_for_calc) ? sprintf("%.2f", $abs_runq_value_used_for_calc) : ($abs_runq_value_used_for_calc // $na));
    printf {$fh} "     - NormRunQ P25                          : %s\n", (looks_like_number($calc_debug_info_ref->{'NormRunQ_P25_Val'}) ? sprintf("%.4f", $calc_debug_info_ref->{'NormRunQ_P25_Val'}) : $na);
    printf {$fh} "     - NormRunQ P50                          : %.4f\n", ($normP50_for_this_calc // ($calc_debug_info_ref->{'NormRunQ_P50_Val'} // $na) );
    printf {$fh} "     - NormRunQ P75                          : %.4f\n", ($calc_debug_info_ref->{'NormRunQ_P75_Val'} // $na);
    printf {$fh} "     - NormRunQ P90                          : %.4f\n", ($normP90_for_this_calc // $na);

    my $iqrc_val_for_log_A_sec = looks_like_number($calc_debug_info_ref->{'NormRunQ_IQRC_Val'}) ? sprintf("%.2f", $calc_debug_info_ref->{'NormRunQ_IQRC_Val'}) : $na;
    printf {$fh} "     - NormRunQ IQRC (Volatility)            : %s", $iqrc_val_for_log_A_sec;
    my $iqrc_interpretation_log_A_sec = $na;
    if ($iqrc_val_for_log_A_sec ne $na && $iqrc_val_for_log_A_sec =~ /^-?[0-9.]+$/)
    {
        my $iqrc_num_A_sec = $iqrc_val_for_log_A_sec + 0;
        if    ($iqrc_num_A_sec < 0.3)  { $iqrc_interpretation_log_A_sec = "Very steady"; }
        elsif ($iqrc_num_A_sec <= 0.6) { $iqrc_interpretation_log_A_sec = "Moderate variability"; }
        elsif ($iqrc_num_A_sec <= 1.0) { $iqrc_interpretation_log_A_sec = "High variability"; }
        else                           { $iqrc_interpretation_log_A_sec = "Very bursty/erratic"; }
        printf {$fh} " (%s)\n", $iqrc_interpretation_log_A_sec;
    } else {
        print {$fh} "\n";
    }


    # ------------------------------------------------------------------
    # Additional diagnostics (DailySeries-based PhysC diagnostics + headroom)
    # ------------------------------------------------------------------
    my $physc_iqrc_log = looks_like_number($calc_debug_info_ref->{'PhysC_Daily_IQRC'}) ? sprintf("%.4f", $calc_debug_info_ref->{'PhysC_Daily_IQRC'}) : $na;
    my $physc_pae_log  = looks_like_number($calc_debug_info_ref->{'PhysC_Daily_PAE'})  ? sprintf("%.4f", $calc_debug_info_ref->{'PhysC_Daily_PAE'})  : $na;
    my $physc_aae_log  = looks_like_number($calc_debug_info_ref->{'PhysC_Daily_AAE'})  ? sprintf("%.4f", $calc_debug_info_ref->{'PhysC_Daily_AAE'})  : $na;
    printf {$fh} "     - PhysC Daily IQRC (Diagnostic)         : %s\n", $physc_iqrc_log;
    printf {$fh} "     - PhysC Daily PAE  (Diag: %%days > Ent)  : %s\n", $physc_pae_log;
    printf {$fh} "     - PhysC Daily AAE  (Diag: excess/total) : %s\n", $physc_aae_log;

    my $hg_t = looks_like_number($calc_debug_info_ref->{'Headroom_Guaranteed_Threads'}) ? sprintf("%.2f", $calc_debug_info_ref->{'Headroom_Guaranteed_Threads'}) : $na;
    my $hg_c = looks_like_number($calc_debug_info_ref->{'Headroom_Guaranteed_Cores'})   ? sprintf("%.4f", $calc_debug_info_ref->{'Headroom_Guaranteed_Cores'})   : $na;
    my $hr_t = looks_like_number($calc_debug_info_ref->{'Headroom_Reachable_Threads'})  ? sprintf("%.2f", $calc_debug_info_ref->{'Headroom_Reachable_Threads'})  : $na;
    my $hr_c = looks_like_number($calc_debug_info_ref->{'Headroom_Reachable_Cores'})    ? sprintf("%.4f", $calc_debug_info_ref->{'Headroom_Reachable_Cores'})    : $na;
    my $hqk  = $calc_debug_info_ref->{'Headroom_AbsRunQKey'} // $na;
    my $hqt  = looks_like_number($calc_debug_info_ref->{'Headroom_AbsRunQ_Threads'}) ? sprintf("%.2f", $calc_debug_info_ref->{'Headroom_AbsRunQ_Threads'}) : $na;
    my $hrc  = looks_like_number($calc_debug_info_ref->{'Headroom_EffectiveReachableCores'}) ? sprintf("%.2f", $calc_debug_info_ref->{'Headroom_EffectiveReachableCores'}) : $na;
    printf {$fh} "     - Headroom vs %s (AbsRunQ=%s thr)       : Guaranteed %s thr (%s cores), Reachable[%s cores] %s thr (%s cores)\n",
        $hqk, $hqt, $hg_t, $hg_c, $hrc, $hr_t, $hr_c;

    my $is_runq_pressure_C_log_sec = ($calc_debug_info_ref->{'IsRunQPressure'} // "False") eq "True";
    my $is_workload_pressure_C_log_sec = ($calc_debug_info_ref->{'IsWorkloadPressure'} // "False") eq "True";
    printf {$fh} "  2. VM Configuration & Profile Behavior\n";
    printf {$fh} "     - SMT                                   : %s\n", ($smt_val // $na);
	my $entitlement_display_A_log_sec = (defined $entitlement_val && looks_like_number($entitlement_val)) ? $entitlement_val : $na;
    printf {$fh} "     - Current Entitlement                   : %s cores\n", $entitlement_display_A_log_sec;
    my $lpar_max_cpu_display_A_log_sec = ($lpar_max_cpu_cfg_val_from_config > 0) ? sprintf("%.2f", $lpar_max_cpu_cfg_val_from_config) : $na;
    printf {$fh} "     - LPAR MaxCPU                           : %s cores\n", $lpar_max_cpu_display_A_log_sec;
    printf {$fh} "     - Profile RunQ Behaviour                : %s\n", ($profile_rq_behavior // $na);
    printf {$fh} "  3. Pressure Assessment Summary\n";

    # Determine per-profile pressure flags (these match the override gate logic)
    my $has_runq_pressure = (($calc_debug_info_ref->{'IsRunQPressure'} // "False") eq "True");
    my $has_workload_pressure = (($calc_debug_info_ref->{'IsWorkloadPressure'} // "False") eq "True");
    my $combined_pressure = ($has_runq_pressure || $has_workload_pressure);

    # Source labelling: report the actual key used in calculate_runq_modified_physc
    my $abs_runq_key_used = $calc_debug_info_ref->{AbsRunQKeyUsed} // 'AbsRunQ';
    my $abs_runq_source_str = "$runq_metrics_source_profile_name_for_this_calc $abs_runq_key_used";

    # Correctly retrieve the rationale string from the debug info hash
    my $pressure_basis_str = $calc_debug_info_ref->{'PressureBasisRationale'} // "MaxCPU";
    my $lpar_pressure_reason;

    if ($has_runq_pressure) {
        $lpar_pressure_reason = sprintf("Ratio %.2f > %.2f (Pressure detected)", ($calc_debug_info_ref->{'RunQPressure_P90_Val'} // 0), $adaptive_runq_saturation_thresh);
    } else {
        $lpar_pressure_reason = sprintf("Ratio %.2f <= %.2f (No pressure detected)", ($calc_debug_info_ref->{'RunQPressure_P90_Val'} // 0), $adaptive_runq_saturation_thresh);
    }

    printf {$fh} "     - Overall LPAR RunQ Pressure            : %s (Source: %s)\n",
        ($combined_pressure ? "True" : "False"),
        $abs_runq_source_str;

    print {$fh}  "         Basis for Pressure Calc             :\n";
    if (defined $pressure_basis_str && length($pressure_basis_str)) {
        # PressureBasisRationale is a multi-line block (often starts with a leading newline).
        my $basis = $pressure_basis_str;
        $basis =~ s/^\n+//;  # avoid an extra blank line after the header
        for my $ln (split /\n/, $basis, -1) {
            # Keep the block's own indentation; anchor it under this section.
            print {$fh} "         $ln\n";
        }
    } else {
        print {$fh} "         N/A\n";
    }

    printf {$fh} "         Pressure Asessment Status           : %s\n", $lpar_pressure_reason;
    # Enhanced line for Normalised Workload Pressure
    my $norm_runq_source_str = "$runq_metrics_source_profile_name_for_this_calc";
    printf {$fh} "     - Normalised Workload Pressure          : %s (Source: %s; Reason: %s)\n\n",
        ($has_workload_pressure ? "Abnormal" : "OK"),
        $norm_runq_source_str,
        ($calc_debug_info_ref->{'WorkloadPressureReason'} // "N/A");

    if (defined $calc_debug_info_ref->{'ReasonForNoModification'} && $calc_debug_info_ref->{'ReasonForNoModification'} ne '')
    {
        printf {$fh} "CPU Modification Path Skipped: %s\n", $calc_debug_info_ref->{'ReasonForNoModification'};
    }
    else
    {
        # --- Section C: CPU Downsizing (Efficiency Assessment) ---
        print {$fh} "Section C: CPU Downsizing (Efficiency Assessment)\n";
        my $downsizing_reason_B_log = $calc_debug_info_ref->{'DownsizingReason'} // "Not calculated or N/A."; # Use renamed key
        my $downsizing_factor_B_log = $calc_debug_info_ref->{'DownsizingFactor'} // "1.00"; # Use renamed key
		my $physc_after_downsizing_B_log = $calc_debug_info_ref->{'DownsizedPhysC'} // $na;

        if ($downsizing_reason_B_log =~ /Single-Thread Dominant \(STD\) Workload Pattern Detected/) {
            # --- Path 1: STD Pattern was detected ---
            printf {$fh} "  - Overall Status                           : Tactical Downsizing Skipped (Entitlement Floor Guard)\n";
            printf {$fh} "  - Guardrail Rationale                      : Base PhysC (%.4f) > Entitlement (%.2f)\n", ($eff_p_base_numeric // 0), ($curr_ent_numeric // 0);
            printf {$fh} "  - Heuristic Override                       : High-Confidence STD workload pattern was detected.\n";
            printf {$fh} "      - Confidence Checks                    : %s\n", ($calc_debug_info_ref->{'STDConfidenceChecks'} // 'N/A');
            print  {$fh} "  - Strategic Signal Calculation (RunQ_Strategic):\n";
            my $runq_uncapped_val = $calc_debug_info_ref->{'RunQ_Strategic'} // 'N/A';
            my $potential_downsizing = (looks_like_number($calc_debug_info_ref->{'EffActualReductionCores'}) ? $calc_debug_info_ref->{'EffActualReductionCores'} : 0);
            printf {$fh} "      - Potential Downsizing                 : %.4f cores\n", $potential_downsizing;
            printf {$fh} "      - Dampening Tier                       : %s\n", ($calc_debug_info_ref->{'STDDampeningTier'} // 'N/A');
            my $final_dampening = $calc_debug_info_ref->{'STDFinalDampeningFactor'} // 0;
            printf {$fh} "      - Final Dampening Factor Applied       : %.0f%%\n", ($final_dampening * 100);
            printf {$fh} "      - Final RunQ_Strategic Value           : %.4f cores\n", (looks_like_number($runq_uncapped_val) ? $runq_uncapped_val : 0);

        } elsif ($downsizing_reason_B_log =~ /workload does not match STD pattern/) {
            # --- Path 2: Bursting, but STD Pattern was NOT detected ---
            my $band = $calc_debug_info_ref->{'DownsizeGuardBand'} // 'RED';
            printf {$fh} "  - Overall Status                           : Tactical Downsizing Skipped (%s band guardrail)\n", $band;
            printf {$fh} "  - Guardrail Rationale                      : Base PhysC (%.4f) > Entitlement (%.2f)\n", ($eff_p_base_numeric // 0), ($curr_ent_numeric // 0);
            printf {$fh} "  - Guardrail Decision                       : AAE/PAE/IQRC banding veto (workload does not match STD pattern).\n";
            printf {$fh} "      - Inputs                               : AAE=%s, PAE=%s, NormRunQ_IQRC=%s\n",
                ($calc_debug_info_ref->{'PhysC_Daily_AAE'} // 'N/A'),
                ($calc_debug_info_ref->{'PhysC_Daily_PAE'} // 'N/A'),
                (defined $calc_debug_info_ref->{'NormRunQ_IQRC_Val'} ? $calc_debug_info_ref->{'NormRunQ_IQRC_Val'} : 'N/A');
            printf {$fh} "      - Confidence Checks                    : %s\n", ($calc_debug_info_ref->{'STDConfidenceChecks'} // 'N/A');

        } else {
            # --- Path 3: Standard logging for all other downsizing scenarios ---
            printf {$fh} "  - Overall Status                           : %s\n", $downsizing_reason_B_log;
        }

        printf {$fh} "  - Final Downsizing Factor                  : %s\n", $downsizing_factor_B_log;

        # rVCPU is stored per profile name in CoreResults
        my $rv = $vm_map_ref->{CoreResults}{Profile_rVCPU}{$profile_being_adjusted};
        my $re = $vm_map_ref->{CoreResults}{Profile_rVCPU_Explain}{$profile_being_adjusted} // {};

        # Determine binding constraint for plain-language verdict
        my $rv_n   = (defined $rv && looks_like_number($rv)) ? ($rv + 0) : undef;
        my $re_raw = (defined $re->{Raw} && looks_like_number($re->{Raw})) ? ($re->{Raw} + 0) : undef;
        my $re_min = (defined $re->{Min} && looks_like_number($re->{Min})) ? ($re->{Min} + 0) : undef;
        my $re_max = (defined $re->{Max} && looks_like_number($re->{Max})) ? ($re->{Max} + 0) : undef;
        my $re_flr = (defined $re->{P99W1Floor} && looks_like_number($re->{P99W1Floor})) ? ($re->{P99W1Floor} + 0) : undef;
        # Account for even-rounding: final may be 1 above the true binding value
        my $rv_pre_even = (defined $rv_n && $VCPU_ROUND_UP_TO_EVEN && ($rv_n % 2) == 0) ? ($rv_n - 1) : $rv_n;

        my $governed_by = 'N/A';
        my @lower = ();
        my @higher = ();
        if (defined $rv_n) {
            # Identify which constraint(s) equal the final (or final-1 for even-rounding)
            my $env_binds  = (defined $re_min && ($rv_n == $re_min || (defined $rv_pre_even && $rv_pre_even == $re_min)));
            my $flr_binds  = (defined $re_flr && ($rv_n == $re_flr || (defined $rv_pre_even && $rv_pre_even == $re_flr)));
            my $raw_binds  = (defined $re_raw && ($rv_n == $re_raw || (defined $rv_pre_even && $rv_pre_even == $re_raw)));
            my $max_binds  = (defined $re_max && ($rv_n == $re_max || (defined $rv_pre_even && $rv_pre_even == $re_max)));

            # Compute the uncapped max-of-constraints (for cap-aware messaging)
            my $uncapped = undef;
            for my $v ($re_raw, $re_flr, $re_min) {
                next unless defined $v;
                $uncapped = $v if (!defined $uncapped || $v > $uncapped);
            }
            my $capped_by_max = (defined $re_max && defined $uncapped && $uncapped > $re_max && $max_binds) ? 1 : 0;

            if ($capped_by_max) {
                $governed_by = sprintf("VP limits max (%s)", $re_max // $na);
            } elsif ($env_binds && $flr_binds) {
                $governed_by = sprintf("Minimum VPs from rENT_P (%s) tied with peak floor (%s)", $re_min // $na, $re_flr // $na);
            } elsif ($env_binds) {
                $governed_by = sprintf("Minimum VPs from rENT_P (%s)", $re_min // $na);
            } elsif ($flr_binds) {
                $governed_by = sprintf("Peak floor (%s)", $re_flr // $na);
            } elsif ($raw_binds) {
                $governed_by = sprintf("RunQ dispatch sizing (%s)", $re_raw // $na);
            } else {
                $governed_by = sprintf("Minimum VPs from rENT_P (%s)", $re_min // $na);  # Safe default
            }

            # Collect context: constraints lower than the chosen rVCPU, and constraints that were capped
            if ($capped_by_max) {
                push @lower, sprintf("peak floor (%s)", $re_flr // $na)
                    if (defined $re_flr && $re_flr < ($rv_pre_even // $rv_n));
                push @lower, sprintf("Minimum VPs from rENT_P (%s)", $re_min // $na)
                    if (defined $re_min && $re_min < ($rv_pre_even // $rv_n));
                push @higher, sprintf("RunQ dispatch sizing (%s) exceeded the cap (%s)", $re_raw // $na, $re_max // $na)
                    if (defined $re_raw && $re_raw > ($rv_pre_even // $rv_n));
            } else {
                push @lower, sprintf("peak floor (%s)", $re_flr // $na)
                    if (defined $re_flr && !$flr_binds);
                push @lower, sprintf("Minimum VPs from rENT_P (%s)", $re_min // $na)
                    if (defined $re_min && !$env_binds);
                push @lower, sprintf("RunQ dispatch sizing (%s)", $re_raw // $na)
                    if (defined $re_raw && !$raw_binds);
            }
        }
        my $secondary_txt = do {
            my @parts = ();
            if (@lower) {
                push @parts, join(', ', @lower) . (@lower == 1 ? ' was lower.' : ' were lower.');
            }
            if (@higher) {
                push @parts, join(', ', @higher) . '.';
            }
            @parts ? join(' ', @parts) : '';
        };

        printf {$fh} "  - rVCPU (per-profile)                      : %s virtual processors\n",
            (defined $rv ? $rv : 'N/A');
        printf {$fh} "    Governed by: %s.%s\n",
            $governed_by,
            ($secondary_txt ? " $secondary_txt" : '');

        # Credibility-adjusted peak line (only when adjustment was applied)
        my $re_cred = (defined $re->{P99W1CredRatio} && looks_like_number($re->{P99W1CredRatio})) ? ($re->{P99W1CredRatio} + 0) : 1.00;
        if ($re_cred < 1.00 && defined $re->{P99W1RawPeak} && defined $re->{P99W1AdjPeak}) {
            # Recompute effective credibility for display (matches computation path)
            my $re_cred_eff = $re_cred + $VCPU_PEAK_CRED_BIAS;
            $re_cred_eff = $VCPU_PEAK_CRED_MIN if ($re_cred_eff < $VCPU_PEAK_CRED_MIN);
            $re_cred_eff = 1.00 if ($re_cred_eff > 1.00);
            my $re_cred_suffix = (defined $re->{P99W1CredRatioSuffix}) ? $re->{P99W1CredRatioSuffix} : '';
            printf {$fh} "    Credible peak: %s cores (PhysC %s x %.3f effective credibility; corroboration ratio %.3f%s).\n",
                $re->{P99W1AdjPeak},
                $re->{P99W1RawPeak},
                $re_cred_eff,
                $re_cred,
                $re_cred_suffix;

            # Diagnostic hint when credibility discount is material (>30% of peak unexplained by RunQ)
            if ($re_cred < 0.70 && looks_like_number($re->{P99W1RawPeak}) && looks_like_number($re->{P99W1AdjPeak})) {
                my $discount_cores = sprintf("%.2f", $re->{P99W1RawPeak} - $re->{P99W1AdjPeak});
                my $discount_pct   = sprintf("%.0f", (1.0 - ($re->{P99W1AdjPeak} / $re->{P99W1RawPeak})) * 100);
                printf {$fh} "    PLANNER NOTE: Low RunQ corroboration - approximately %s cores (%s%%) of the P99W1 peak\n", $discount_cores, $discount_pct;
                printf {$fh} "                  is not backed by run queue evidence and may reflect scheduler effects (including over-provisioning/folding)\n";
                printf {$fh} "                  or short-lived bursts not visible in the RunQ indicators.\n";
                printf {$fh} "                  Review the LPAR's current VP count (%s VPs) relative to observed workload concurrency.\n",
                    (defined $cfg->{virtual_cpus} && looks_like_number($cfg->{virtual_cpus})) ? sprintf("%d VPs", $cfg->{virtual_cpus}) : 'unknown';
            }
        }

        printf {$fh} "    Detail: raw=%s; env=%s..%s; tier=%s/f=%s; P99W1_floor=%s @ factor=%s (headroom=%s; peak=%s adj=%s cred=%s); RunQ=%s=%s\n",
            ($re->{Raw} // 'N/A'),
            ($re->{Min} // 'N/A'),
            ($re->{Max} // 'N/A'),
            ($re->{TierNum} // 'N/A'),
            ($re->{TierFactor} // 'N/A'),
            ($re->{P99W1Floor} // 'N/A'),
            ($re->{P99W1FactorEff} // 'N/A'),
            ($re->{P99W1Mult} // 'N/A'),
            ($re->{P99W1RawPeak} // 'N/A'),
            ($re->{P99W1AdjPeak} // 'N/A'),
            ($re->{P99W1CredRatio} // 'N/A'),
            ($re->{RunQKeyUsed} // 'N/A'),
            ($re->{RunQHiThreads} // 'N/A');

        printf {$fh} "    Downsize Guard Summary: %s\n", ($calc_debug_info_ref->{DownsizeGuardSummary} // 'N/A');

        ## --- END: Enhanced Section C Rationale Logging ---

        # Conditionally print detailed analytical breakdown for downsizing
        if ($downsizing_reason_B_log =~ /^Analytical/ &&
            defined $calc_debug_info_ref->{'EffPEfficientTarget'} && # Internal keys can remain Eff...
            defined $calc_debug_info_ref->{'EffCondNormP50Met'} &&
            defined $calc_debug_info_ref->{'EffCondVolatilityMet'})
        {
            printf {$fh} "  - Detailed Analytical Path for Downsizing:\n";
            printf {$fh} "     a. Initial Condition Checks for Downsizing Path:\n";
            my $eff_cond_norm_p50_met_str_B = $calc_debug_info_ref->{'EffCondNormP50Met'} ? "YES (Low P50)" : "NO (P50 not low enough)";
            printf {$fh} "        - NormRunQ P50                       : %-5s (Condition: < %.2f for consideration? %s)\n",
            ((defined $normP50_for_this_calc) ? sprintf("%.2f", $normP50_for_this_calc) : $na),
            $NORM_P50_THRESHOLD_FOR_EFFICIENCY_CONSIDERATION,
            $eff_cond_norm_p50_met_str_B;

            my $eff_cond_volatility_met_str_B = $calc_debug_info_ref->{'EffCondVolatilityMet'} ? "YES (Not excessively volatile)" : "NO (Too volatile)";
            printf {$fh} "        - Workload Volatility                : %-5s (NormP90 %.2f / NormP50 %.2f. Condition: < %.2f to proceed? %s)\n",
            ($calc_debug_info_ref->{'EffVolatilityRatio'} // $na),
            ($normP90_for_this_calc ne $na ? ($normP90_for_this_calc+0):0), # Ensure numeric for sprintf
            ($normP50_for_this_calc ne $na ? ($normP50_for_this_calc+0):0),
            $VOLATILITY_CAUTION_THRESHOLD,
            $eff_cond_volatility_met_str_B;
            print {$fh} "\n";

            printf {$fh} "     b. Calculating Raw Efficient PhysC Target (Theoretical Minimum if RunQ was at Target Norm):\n";
            printf {$fh} "        - Base PhysC for Profile             : %s cores\n", ($calc_debug_info_ref->{'EffPBase'} // $na);
            printf {$fh} "        - AbsRunQ Metric Used                : %s (value: %.2f threads)\n", $abs_runq_key_reported_in_log, ($abs_runq_value_used_for_calc // $na);
            printf {$fh} "        - SMT Value                          : %s\n", ($calc_debug_info_ref->{'EffSMTValue'} // $na);
            my $smt_txt = defined $calc_debug_info_ref->{'EffSMTValue'}
                          ? $calc_debug_info_ref->{'EffSMTValue'} : $na;
            my $tgt_val = $calc_debug_info_ref->{'EffTargetNormRunQ'};
            my $tgt_txt = (defined $tgt_val && looks_like_number($tgt_val))
                          ? sprintf('%.2f', $tgt_val) : $na;
            printf {$fh} "        - Target NormRunQ for SMT%-11s : %s (internal heuristic for optimal queue/LCPU)\n", $smt_txt, $tgt_txt;
            printf {$fh} "        - Raw Efficient PhysC Target         : %s / (SMT * Target NormRunQ)\n", $abs_runq_key_reported_in_log;
            printf {$fh} "                                               %s / (%s * %.2f) = %s cores\n",
            ($abs_runq_value_used_for_calc // $na),
            ($calc_debug_info_ref->{'EffSMTValue'} // $na),
            ($calc_debug_info_ref->{'EffTargetNormRunQ'} // $na),
            ($calc_debug_info_ref->{'EffPEfficientTargetRaw'} // $na);
            print {$fh} "\n";

            printf {$fh} "     c. Blending Raw Target with Observed Base PhysC (Applying Confidence):\n";
            printf {$fh} "        - Blending Weights                   : %.0f%% Base PhysC / %.0f%% Raw Target\n",
            defined $calc_debug_info_ref->{'EffBlendWeightBase'} ? (($calc_debug_info_ref->{'EffBlendWeightBase'} // 0) * 100) : 0,
            defined $calc_debug_info_ref->{'EffBlendWeightTarget'} ? (($calc_debug_info_ref->{'EffBlendWeightTarget'} // 0) * 100) : 0;
            printf {$fh} "        - Blending Rationale                 : %s\n", ($calc_debug_info_ref->{'EffBlendReason'} // $na);
            printf {$fh} "        - Blended Efficient Target           : (Base PhysC * Weight) + (Raw Target * Weight)\n";
            printf {$fh} "                                               (%s * %.2f) + (%s * %.2f) = %s cores\n",
            ($calc_debug_info_ref->{'EffPBase'} // $na),
            ($calc_debug_info_ref->{'EffBlendWeightBase'} // 0.0),
            ($calc_debug_info_ref->{'EffPEfficientTargetRaw'} // $na),
            ($calc_debug_info_ref->{'EffBlendWeightTarget'} // 0.0),
            ($calc_debug_info_ref->{'EffPEfficientTarget'} // $na);
            print {$fh} "\n";

            printf {$fh} "     d. Determining Potential CPU Downsizing (Based on Blended Target):\n";
            my $eff_comp_base_vs_target_met_str_B = defined($calc_debug_info_ref->{'EffComparisonBaseVsTargetMet'})
            ? ($calc_debug_info_ref->{'EffComparisonBaseVsTargetMet'} ? "YES" : "NO") : $na;
            printf {$fh} "        - Comparison                         : Base PhysC (%s) > Blended Efficient Target (%s)? %s\n",
            ($calc_debug_info_ref->{'EffPBase'} // $na),
            ($calc_debug_info_ref->{'EffPEfficientTarget'} // $na),
            $eff_comp_base_vs_target_met_str_B;

            if (defined $calc_debug_info_ref->{'EffComparisonBaseVsTargetMet'} && $calc_debug_info_ref->{'EffComparisonBaseVsTargetMet'})
            {
                printf {$fh} "        - Potential CPU Downsize             : %s - %s = %s cores\n",
                ($calc_debug_info_ref->{'EffPBase'} // $na),
                ($calc_debug_info_ref->{'EffPEfficientTarget'} // $na),
                ($calc_debug_info_ref->{'EffPotentialReduction'} // $na);
                printf {$fh} "        - Max Downsize Cap %%                 : %.1f%% (Reason: %s)\n",
                ($calc_debug_info_ref->{'EffMaxAllowableReductionPerc'} eq $na ? ($MAX_EFFICIENCY_REDUCTION_PERCENTAGE*100) : ($calc_debug_info_ref->{'EffMaxAllowableReductionPerc'} +0) ),
                ($calc_debug_info_ref->{'EffReductionCapReason'} // $na);
                printf {$fh} "        - Max Allowable Downsize             : %s cores (Base PhysC * Max Downsize Cap %%)\n",
                ($calc_debug_info_ref->{'EffMaxAllowableReductionCores'} // $na);
                printf {$fh} "        - Actual CPU Downsized By            : %s cores (min of Potential and Max Allowable)\n",
                ($calc_debug_info_ref->{'EffActualReductionCores'} // $na);
            }
            else
            {
                printf {$fh} "        - No potential for downsizing based on Blended Target, or reduction was zero.\n";
            }
            print {$fh} "\n";

            printf {$fh} "     e. Final Downsizing Factor Calculation:\n";
            printf {$fh} "        - Calculated Factor                  : (Base PhysC - Actual Reduction) / Base PhysC\n";
            my $eff_p_base_val_for_div_B = ($calc_debug_info_ref->{'EffPBase'} ne $na && ($calc_debug_info_ref->{'EffPBase'} + 0) != 0)
            ? ($calc_debug_info_ref->{'EffPBase'} + 0) : 1.0;
            my $eff_p_base_display_for_div_B = ($eff_p_base_val_for_div_B == 1.0 && ($calc_debug_info_ref->{'EffPBase'} eq $na || ($calc_debug_info_ref->{'EffPBase'} + 0) == 0))
            ? "$eff_p_base_val_for_div_B (adj for display)" : ($calc_debug_info_ref->{'EffPBase'} // $na);
            printf {$fh} "                                               (%s - %s) / %s = %s\n",
            ($calc_debug_info_ref->{'EffPBase'} // $na),
            ($calc_debug_info_ref->{'EffActualReductionCores'} // "0.0000"),
            $eff_p_base_display_for_div_B,
            ($calc_debug_info_ref->{'EffCalculatedFactor'} // $na);
        }
        printf {$fh} "  => PhysC after Downsizing                  : %s cores\n\n", $physc_after_downsizing_B_log;

        # --- Section D: CPU Upsizing (Additive CPU) ---
        print {$fh} "Section D: CPU Upsizing (Additive CPU)\n";
        my $apply_additive_C_log_sec = $is_runq_pressure_C_log_sec || $is_workload_pressure_C_log_sec;

		printf {$fh} "  - Additive Logic Triggered                 : %s\n", ($apply_additive_C_log_sec ? "Yes" : "No");

        # --- Tier-Aware Scaling (if applied) ---
        if (exists $calc_debug_info_ref->{'TierScalingFactor'} &&
            $calc_debug_info_ref->{'TierScalingFactor'} ne '1.00') {

            printf {$fh} "  - Tier Scaling Applied                     : Tier %s (Factor: %sx)\n",
                   ($calc_debug_info_ref->{'TierNumber'} // 'N/A'),
                   ($calc_debug_info_ref->{'TierScalingFactor'} // '1.00');
            printf {$fh} "      Before Tier Scaling                    : %.4f cores\n",
                   (looks_like_number($calc_debug_info_ref->{'AdditiveCPU_PreTierScaling'})
                    ? $calc_debug_info_ref->{'AdditiveCPU_PreTierScaling'} : 0);
            printf {$fh} "      After Tier Scaling                     : %.4f cores\n",
                   (looks_like_number($calc_debug_info_ref->{'AdditiveCPU_PostTierScaling'})
                    ? $calc_debug_info_ref->{'AdditiveCPU_PostTierScaling'} : 0);
            printf {$fh} "      Rationale                              : %s\n",
                   ($calc_debug_info_ref->{'TierScalingRationale'} // 'N/A');
        }

        if ($apply_additive_C_log_sec)
        {
            printf {$fh} "    Details of Upsizing Calculation:\n";
            printf {$fh} "    - Base for Upsizing                      : %s cores (PhysC after Downsizing)\n", $physc_after_downsizing_B_log;
            printf {$fh} "    - Effective LCPUs at Base                : %s threads\n", ($calc_debug_info_ref->{'EffectiveLCPUsAtBase'} // $na);
            printf {$fh} "    - Excess Threads Calculated              : %s threads\n", ($calc_debug_info_ref->{'ExcessThreads'} // $na);
            printf {$fh} "    - Raw Additive CPU                       : %s cores (excess threads / SMT)\n", ($calc_debug_info_ref->{'RawAdditive'} // $na);
            printf {$fh} "    - Entitlement-Based Cap                  : %s cores (Max Additive Cap: %s)\n",
            (defined $calc_debug_info_ref->{'HotThreadWLDampenedAdditiveFrom'} && $calc_debug_info_ref->{'HotThreadWLDampeningApplied'} eq "True"
                ? $calc_debug_info_ref->{'HotThreadWLDampenedAdditiveFrom'}
                : ($calc_debug_info_ref->{'CappedRawAdditive'} // $na) # Show original value before HTW if HTW was applied
            ), ($calc_debug_info_ref->{'MaxAdditiveCap'} // $na);

            # Log enhanced burst and small entitlement handling details
            printf {$fh} "  Pressure Basis Rationale                   : %s\n", ($calc_debug_info_ref->{'PressureBasisRationale'} // "N/A");
            printf {$fh} "  Burst Allowance Used                       : %s\n", ($calc_debug_info_ref->{'BurstAllowanceUsed'} // "N/A");
            printf {$fh} "  Small Entitlement Handler Active           : %s\n", ($calc_debug_info_ref->{'SmallEntitlementHandler'} // "No");
            printf {$fh} "  Effective LCPUs for Pressure Calc          : %s\n", ($calc_debug_info_ref->{'EffectiveLCPUsForPressure'} // "N/A");

            if (defined $calc_debug_info_ref->{'HotThreadWLDampeningApplied'} && $calc_debug_info_ref->{'HotThreadWLDampeningApplied'} eq "True")
            {
                printf {$fh} "    - Hot Thread Dampening                : Applied\n";
                printf {$fh} "        Conditions Summary                : %s\n", ($calc_debug_info_ref->{'HotThreadWLConditionsString'} // $na);
                printf {$fh} "        Dynamic Dampen Factor             : %s\n", ($calc_debug_info_ref->{'HotThreadWLDynamicFactor'} // $na);
                printf {$fh} "        Additive (Before HTW)             : %s cores -> (After HTW): %s cores\n",
                ($calc_debug_info_ref->{'HotThreadWLDampenedAdditiveFrom'} // $na),
                ($calc_debug_info_ref->{'HotThreadWLDampenedAdditiveTo'} // $na);
            }
            elsif (defined $calc_debug_info_ref->{'HotThreadWLDampeningApplied'}) # Checked but not applied
            {
                printf {$fh} "    - Hot Thread Dampening                   : Not Applied (Details: %s)\n", ($calc_debug_info_ref->{'HotThreadWLConditionsString'} // "Conditions not met");
            }

            printf {$fh} "    - Volatility Factor Applied              : %s (Reason: %s)\n", ($calc_debug_info_ref->{'VoltFactor'} // $na), ($calc_debug_info_ref->{'VoltFactorReason'} // $na);
            printf {$fh} "    - Pool Factor Applied                    : %s\n", ($calc_debug_info_ref->{'PoolFactor'} // $na);
            if (defined $calc_debug_info_ref->{'DBWDampeningApplied'} && $calc_debug_info_ref->{'DBWDampeningApplied'} ne 'False') {
            printf {$fh} "    - Dispatch-Bound (DBW) Anomaly Dampening : %s\n", $calc_debug_info_ref->{'DBWDampeningApplied'};
            }

            if (defined $calc_debug_info_ref->{'AdditiveSafetyCapApplied'} && $calc_debug_info_ref->{'AdditiveSafetyCapApplied'} =~ /^True/)
            {
                printf {$fh} "    - Final Additive Safety Cap              : %s\n", $calc_debug_info_ref->{'AdditiveSafetyCapApplied'};
            }
        }
		printf {$fh} "  => Final Additive CPU                      : %s cores\n", ($calc_debug_info_ref->{'FinalAdditive'} // "0.0000");

        # Tactical vs Strategic diagnostic (Fix 3)
        if ($calc_debug_info_ref->{'TacticalExceedsStrategic'} && $calc_debug_info_ref->{'TacticalExceedsStrategicNote'}) {
            printf {$fh} "  => PLANNER NOTE: %s\n", $calc_debug_info_ref->{'TacticalExceedsStrategicNote'};
        }

		printf {$fh} "  => PhysC after Upsizing                    : %s cores\n\n", ($calc_debug_info_ref->{'PreMaxCpuCapRec'} // $na);


        # --- Section E: Maximum CPU Sizing Sanity Checks ---
        print {$fh} "Section E: Maximum CPU Sizing Sanity Checks\n";
        printf {$fh} "  - Recommendation before Max Sanity Check   : %s cores\n", ($calc_debug_info_ref->{'PreMaxCpuCapRec'} // $na);
        printf {$fh} "  - LPAR MaxCPU (from VM config)             : %s cores\n", $lpar_max_cpu_display_A_log_sec;
        printf {$fh} "  - Entitlement (for forecast multiplier)    : %s cores\n", $entitlement_display_A_log_sec;
        printf {$fh} "  - Forecast Multiplier Used                 : %s\n", ($calc_debug_info_ref->{'ForecastMultiplier'} // $na);
        printf {$fh} "  - Effective MaxCPU Sanity Limit            : %s cores\n", ($calc_debug_info_ref->{'EffectiveMaxCPUCap'} // $na);
        printf {$fh} "  - Limited by MaxCPU Sanity Check?          : %s\n", ($calc_debug_info_ref->{'CappedByMaxCPU'} // $na);
        printf {$fh} "  => Recommendation after Max Sanity Check   : %s cores\n\n", ($final_recommendation_unrounded_str // $na);
    } # End else for ReasonForNoModification (main calculation block)

    # --- Section F: VCPU Recommendation (Dispatch Capacity) ---
    print {$fh} "Section F: VCPU Recommendation (Dispatch Capacity)\n";

    my $curr_vcpu_log = (defined $cfg->{virtual_cpus} && looks_like_number($cfg->{virtual_cpus})) ? sprintf("%d", $cfg->{virtual_cpus}) : $na;
    my $maxcpu_log    = (defined $cfg->{max_cpu} && looks_like_number($cfg->{max_cpu})) ? sprintf("%.2f", $cfg->{max_cpu}) : $na;

    my $rvcpu_final_log = (defined $vm_map_ref->{CoreResults}{Profile_rVCPU}{$profile_being_adjusted} && $vm_map_ref->{CoreResults}{Profile_rVCPU}{$profile_being_adjusted} ne "")
        ? $vm_map_ref->{CoreResults}{Profile_rVCPU}{$profile_being_adjusted}
        : ($calc_debug_info_ref->{'rVCPU_Final'} // $na);

    printf {$fh} "  - Current VCPUs / MaxCPU                   : %s / %s\n", $curr_vcpu_log, $maxcpu_log;

# Determine binding constraint for plain-language verdict
    my $sf_rv_n  = looks_like_number($rvcpu_final_log) ? ($rvcpu_final_log + 0) : undef;
    my $sf_raw   = (defined $calc_debug_info_ref->{'rVCPU_Raw'} && looks_like_number($calc_debug_info_ref->{'rVCPU_Raw'}))
                 ? ($calc_debug_info_ref->{'rVCPU_Raw'} + 0) : undef;
    my $sf_min   = (defined $calc_debug_info_ref->{'rVCPU_Min'} && looks_like_number($calc_debug_info_ref->{'rVCPU_Min'}))
                 ? ($calc_debug_info_ref->{'rVCPU_Min'} + 0) : undef;
    my $sf_flr   = (defined $calc_debug_info_ref->{'rVCPU_P99W1Floor'} && looks_like_number($calc_debug_info_ref->{'rVCPU_P99W1Floor'}))
                 ? ($calc_debug_info_ref->{'rVCPU_P99W1Floor'} + 0) : undef;
    my $sf_pre_even = (defined $sf_rv_n && $VCPU_ROUND_UP_TO_EVEN && ($sf_rv_n % 2) == 0) ? ($sf_rv_n - 1) : $sf_rv_n;

    my $sf_governed = 'N/A';
    my @sf_lower = ();
    my @sf_higher = ();
    if (defined $sf_rv_n) {
        my $sf_max = (defined $calc_debug_info_ref->{'rVCPU_Max'} && looks_like_number($calc_debug_info_ref->{'rVCPU_Max'}))
                   ? ($calc_debug_info_ref->{'rVCPU_Max'} + 0) : undef;

        my $sf_env_binds = (defined $sf_min && ($sf_rv_n == $sf_min || (defined $sf_pre_even && $sf_pre_even == $sf_min)));
        my $sf_flr_binds = (defined $sf_flr && ($sf_rv_n == $sf_flr || (defined $sf_pre_even && $sf_pre_even == $sf_flr)));
        my $sf_raw_binds = (defined $sf_raw && ($sf_rv_n == $sf_raw || (defined $sf_pre_even && $sf_pre_even == $sf_raw)));
        my $sf_max_binds = (defined $sf_max && ($sf_rv_n == $sf_max || (defined $sf_pre_even && $sf_pre_even == $sf_max)));

        my $sf_uncapped = undef;
        for my $v ($sf_raw, $sf_flr, $sf_min) {
            next unless defined $v;
            $sf_uncapped = $v if (!defined $sf_uncapped || $v > $sf_uncapped);
        }
        my $sf_capped_by_max = (defined $sf_max && defined $sf_uncapped && $sf_uncapped > $sf_max && $sf_max_binds) ? 1 : 0;

        if ($sf_capped_by_max) {
            $sf_governed = sprintf("VP limits max (%s)", $sf_max // $na);
        } elsif ($sf_env_binds && $sf_flr_binds) {
            $sf_governed = sprintf("Minimum VPs from rENT_P (%s) tied with peak floor (%s)", $sf_min // $na, $sf_flr // $na);
        } elsif ($sf_env_binds) {
            $sf_governed = sprintf("Minimum VPs from rENT_P (%s)", $sf_min // $na);
        } elsif ($sf_flr_binds) {
            $sf_governed = sprintf("Peak floor (%s)", $sf_flr // $na);
        } elsif ($sf_raw_binds) {
            $sf_governed = sprintf("RunQ dispatch sizing (%s)", $sf_raw // $na);
        } else {
            $sf_governed = sprintf("Minimum VPs from rENT_P (%s)", $sf_min // $na);
        }

        if ($sf_capped_by_max) {
            push @sf_lower, sprintf("Peak floor (%s)", $sf_flr // $na)
                if (defined $sf_flr && $sf_flr < ($sf_pre_even // $sf_rv_n));
            push @sf_lower, sprintf("Minimum VPs from rENT_P (%s)", $sf_min // $na)
                if (defined $sf_min && $sf_min < ($sf_pre_even // $sf_rv_n));
            push @sf_higher, sprintf("RunQ dispatch sizing (%s) exceeded the cap (%s)", $sf_raw // $na, $sf_max // $na)
                if (defined $sf_raw && defined $sf_max && $sf_raw > ($sf_pre_even // $sf_rv_n));
        } else {
            push @sf_lower, sprintf("Peak floor (%s)", $sf_flr // $na)
                if (defined $sf_flr && !$sf_flr_binds);
            push @sf_lower, sprintf("Minimum VPs from rENT_P (%s)", $sf_min // $na)
                if (defined $sf_min && !$sf_env_binds);
            push @sf_lower, sprintf("RunQ dispatch sizing (%s)", $sf_raw // $na)
                if (defined $sf_raw && !$sf_raw_binds);
        }
    }
    my $sf_secondary_txt = do {
        my @parts = ();
        if (@sf_lower) {
            push @parts, join(', ', @sf_lower) . (@sf_lower == 1 ? ' was lower.' : ' were lower.');
        }
        if (@sf_higher) {
            push @parts, join(', ', @sf_higher) . '.';
        }
        @parts ? join(' ', @parts) : '';
    };

    printf {$fh} "  - Recommended VCPUs (rVCPU)                : %s virtual processors\n",
        ($rvcpu_final_log // $na);
    printf {$fh} "    Governed by: %s.%s\n",
        $sf_governed,
        ($sf_secondary_txt ? " $sf_secondary_txt" : '');

    # rVCPU is stored per profile name in CoreResults
    my $rv = $vm_map_ref->{CoreResults}{Profile_rVCPU}{$profile_being_adjusted};
    my $re = $vm_map_ref->{CoreResults}{Profile_rVCPU_Explain}{$profile_being_adjusted} // {};

    # Practical audit trail: explain each width constraint in a capacity-planner-friendly way
    # (kept concise; detailed engineer-level fields remain in the "Detail:" line below).
    my $smt_used = (defined $cfg->{smt} && looks_like_number($cfg->{smt})) ? ($cfg->{smt} + 0) : undef;
    my $runq_thr = (defined $re->{RunQHiThreads} && looks_like_number($re->{RunQHiThreads})) ? ($re->{RunQHiThreads} + 0) : undef;
    my $raw_vp   = (defined $re->{Raw} && looks_like_number($re->{Raw})) ? ($re->{Raw} + 0) : undef;
    my $env_min  = (defined $re->{Min} && looks_like_number($re->{Min})) ? ($re->{Min} + 0) : undef;
    my $env_max  = (defined $re->{Max} && looks_like_number($re->{Max})) ? ($re->{Max} + 0) : undef;
    my $ent_cores          = (defined $re->{EntCores} && looks_like_number($re->{EntCores})) ? ($re->{EntCores} + 0) : undef;
    my $ent_cores_planning = (defined $re->{EntCoresPlanning} && looks_like_number($re->{EntCoresPlanning})) ? ($re->{EntCoresPlanning} + 0) : undef;
    my $env_exceeds_op     = (defined $re->{EnvelopeExceedsOperational} && $re->{EnvelopeExceedsOperational}) ? 1 : 0;

    my $floor_vp = (defined $re->{P99W1Floor} && looks_like_number($re->{P99W1Floor})) ? ($re->{P99W1Floor} + 0) : undef;
    my $tier_n   = (defined $re->{TierNum} && looks_like_number($re->{TierNum})) ? ($re->{TierNum} + 0) : undef;
    my $tier_f   = (defined $re->{TierFactor} && looks_like_number($re->{TierFactor})) ? ($re->{TierFactor} + 0) : undef;

    my $p_raw    = (defined $re->{P99W1RawPeak} && looks_like_number($re->{P99W1RawPeak})) ? ($re->{P99W1RawPeak} + 0) : undef;
    my $p_adj    = (defined $re->{P99W1AdjPeak} && looks_like_number($re->{P99W1AdjPeak})) ? ($re->{P99W1AdjPeak} + 0) : undef;
    my $p_cred   = (defined $re->{P99W1CredRatio} && looks_like_number($re->{P99W1CredRatio})) ? ($re->{P99W1CredRatio} + 0) : undef;
    my $p_fact   = (defined $re->{P99W1FactorEff} && looks_like_number($re->{P99W1FactorEff})) ? ($re->{P99W1FactorEff} + 0) : undef;

    # Dispatch reserve (extract early so it's in scope for final decision logging)
    my $disp_res_vp     = (defined $re->{DispatchReserveVP} && looks_like_number($re->{DispatchReserveVP})) ? ($re->{DispatchReserveVP} + 0) : 0;
    my $disp_res_reason = $re->{DispatchReserveReason} // '';
    my $disp_res_pre    = (defined $re->{DispatchReservePreFinal} && looks_like_number($re->{DispatchReservePreFinal})) ? ($re->{DispatchReservePreFinal} + 0) : undef;

    # Only print the breakdown when we have the key parts (it should be present for modern runs)
    if (defined $raw_vp || defined $floor_vp || defined $env_min) {
        print {$fh} "    Constraint breakdown:\n";

        # 1) RunQ dispatch sizing
        if (defined $runq_thr && defined $smt_used && $smt_used > 0) {
            my $runq_cores = $runq_thr / $smt_used;
            my $tier_f_eff = (defined $tier_f && looks_like_number($tier_f)) ? ($tier_f + 0) : 1.00;
            my $tgt_eff    = (defined $RUNQ_TARGET_HEADROOM_FACTOR && looks_like_number($RUNQ_TARGET_HEADROOM_FACTOR)) ? ($RUNQ_TARGET_HEADROOM_FACTOR + 0) : 1.00;
            printf {$fh} "      * RunQ dispatch sizing: %s = %.2f threads (SMT-%d) -> %.2f core-equivalents; target utilisation %.2f and tier headroom %.2f -> requires %s VPs (rVCPU, even-rounded)\n",
                ($re->{RunQKeyUsed} // 'AbsRunQ'),
                $runq_thr,
                $smt_used,
                $runq_cores,
                $tgt_eff,
                $tier_f_eff,
                (defined $raw_vp ? sprintf("%d", $raw_vp) : 'N/A');
        } else {
            printf {$fh} "      * RunQ dispatch sizing: %s -> width %s VPs (even-rounded)\n",
                ($re->{RunQKeyUsed} // 'AbsRunQ'),
                (defined $raw_vp ? sprintf("%d", $raw_vp) : 'N/A');
        }

        # 2) Peak floor (with effective credibility, not raw cred_ratio)
        if (defined $p_raw && defined $p_adj && defined $p_cred && defined $p_fact) {
            # Recompute effective credibility from stored raw ratio (matches computation path)
            my $p_cred_eff = $p_cred + $VCPU_PEAK_CRED_BIAS;
            $p_cred_eff = $VCPU_PEAK_CRED_MIN if ($p_cred_eff < $VCPU_PEAK_CRED_MIN);
            $p_cred_eff = 1.00 if ($p_cred_eff > 1.00);
            printf {$fh} "      * Peak floor: corroborated peak %.2f cores (x%.3f credibility) -> %.2f cores; headroom x%.3f -> floor %s VPs (even-rounded)\n",
                $p_raw, $p_cred_eff, $p_adj, $p_fact,
                (defined $floor_vp ? sprintf("%d", $floor_vp) : 'N/A');
        } elsif (defined $floor_vp) {
            printf {$fh} "      * Peak floor: floor %d VPs (even-rounded)\n", $floor_vp;
        } else {
            print {$fh} "      * Peak floor: N/A\n";
        }

        # 3) VP limits (VP:ENT ratio band, from planning rENT)
        if (defined $env_min && defined $env_max) {
            if ($env_exceeds_op && defined $ent_cores_planning && defined $ent_cores) {
                # Show both planning and operational values when they differ
                printf {$fh} "      * VP limits (from planning rENT_P=%.2f cores, pre MaxCPU sanity): min %d VPs, max %d VPs (even-rounded)\n",
                    $ent_cores_planning, $env_min, $env_max;
                printf {$fh} "        NOTE: Planning envelope derived from workload demand (%.2f cores), which exceeds\n",
                    $ent_cores_planning;
                printf {$fh} "              operational rENT_P (%.2f cores, post MaxCPU sanity). Raise MaxCPU to realise full envelope.\n",
                    $ent_cores;
            } elsif (defined $ent_cores_planning) {
                printf {$fh} "      * VP limits (from rENT_P=%.2f cores): min %d VPs, max %d VPs (even-rounded)\n",
                    $ent_cores_planning, $env_min, $env_max;
            } elsif (defined $ent_cores) {
                printf {$fh} "      * VP limits (from rENT_P=%.2f cores): min %d VPs, max %d VPs (even-rounded)\n",
                    $ent_cores, $env_min, $env_max;
            } else {
                printf {$fh} "      * VP limits: min %d VPs, max %d VPs (even-rounded)\n", $env_min, $env_max;
            }

            # 4) Dispatch reserve (conditional, Layer 5)
            if ($disp_res_vp > 0 && $disp_res_reason ne '') {
                printf {$fh} "      * Dispatch reserve: +%d VPs applied (pre-reserve: %s VPs)\n",
                    $disp_res_vp, (defined $disp_res_pre ? $disp_res_pre : 'N/A');
                printf {$fh} "        %s\n", $disp_res_reason;
            }
        } elsif (defined $env_min) {
            if (defined $ent_cores_planning) {
                printf {$fh} "      * VP limits (from rENT_P=%.2f cores): min %d VPs (even-rounded)\n",
                    $ent_cores_planning, $env_min;
            } elsif (defined $ent_cores) {
                printf {$fh} "      * VP limits (from rENT_P=%.2f cores): min %d VPs (even-rounded)\n",
                    $ent_cores, $env_min;
            } else {
                printf {$fh} "      * VP limits: min %d VPs (even-rounded)\n", $env_min;
            }
        } else {
            print {$fh} "      * VP limits: N/A\n";
        }

        # Final decision (max-of-constraints)
        if (defined $rv && looks_like_number($rv) && defined $raw_vp && defined $floor_vp && defined $env_min) {
            my $uncapped = $raw_vp;
            $uncapped = $floor_vp if ($floor_vp > $uncapped);
            $uncapped = $env_min  if ($env_min  > $uncapped);

            my $reserve_str = '';
            if ($disp_res_vp > 0) {
                $reserve_str = sprintf(" + reserve %d", $disp_res_vp);
            }

            if (defined $env_max && ($uncapped + $disp_res_vp) > $env_max && $rv == $env_max) {
                printf {$fh} "    Final rVCPU = min(VP max %d, max(%d, %d, %d)=%d%s) => %d\n",
                    $env_max, $raw_vp, $floor_vp, $env_min, $uncapped, $reserve_str, $rv;
            } elsif ($disp_res_vp > 0) {
                printf {$fh} "    Final rVCPU = max(%d, %d, %d) = %d%s => %d%s\n",
                    $raw_vp, $floor_vp, $env_min, $uncapped, $reserve_str, $rv,
                    (defined $env_max) ? sprintf(" (<= VP max %d)", $env_max) : "";

            } else {
                printf {$fh} "    Final rVCPU = max(%d, %d, %d) = %d%s\n",
                    $raw_vp, $floor_vp, $env_min, $rv,
                    (defined $env_max) ? sprintf(" (<= VP max %d)", $env_max) : "";
            }
        }
    }

    # Credibility-adjusted peak line (only when adjustment was applied)
    my $sf_cred = (defined $calc_debug_info_ref->{'rVCPU_P99W1CredRatio'} && looks_like_number($calc_debug_info_ref->{'rVCPU_P99W1CredRatio'}))
                ? ($calc_debug_info_ref->{'rVCPU_P99W1CredRatio'} + 0) : 1.00;
    if ($sf_cred < 1.00 && defined $calc_debug_info_ref->{'rVCPU_P99W1RawPeak'} && defined $calc_debug_info_ref->{'rVCPU_P99W1AdjPeak'}) {
        # Recompute effective credibility for display (matches computation path)
        my $sf_cred_eff = $sf_cred + $VCPU_PEAK_CRED_BIAS;
        $sf_cred_eff = $VCPU_PEAK_CRED_MIN if ($sf_cred_eff < $VCPU_PEAK_CRED_MIN);
        $sf_cred_eff = 1.00 if ($sf_cred_eff > 1.00);
        printf {$fh} "    Credible peak: %s cores (PhysC %s x %.3f peak credibility; RunQ corroboration ratio %.3f).\n",
            $calc_debug_info_ref->{'rVCPU_P99W1AdjPeak'},
            $calc_debug_info_ref->{'rVCPU_P99W1RawPeak'},
            $sf_cred_eff,
            $sf_cred;

        # Diagnostic hint when credibility discount is material (>30% of peak unexplained by RunQ)
        if ($sf_cred < 0.70
            && looks_like_number($calc_debug_info_ref->{'rVCPU_P99W1RawPeak'})
            && looks_like_number($calc_debug_info_ref->{'rVCPU_P99W1AdjPeak'})) {
            my $sf_disc_cores = sprintf("%.2f", $calc_debug_info_ref->{'rVCPU_P99W1RawPeak'} - $calc_debug_info_ref->{'rVCPU_P99W1AdjPeak'});
            my $sf_disc_pct   = sprintf("%.0f", (1.0 - ($calc_debug_info_ref->{'rVCPU_P99W1AdjPeak'} / $calc_debug_info_ref->{'rVCPU_P99W1RawPeak'})) * 100);
            printf {$fh} "    PLANNER NOTE: Low RunQ corroboration - approximately %s cores (%s%%) of the P99W1 peak\n", $sf_disc_cores, $sf_disc_pct;
            printf {$fh} "                  is not backed by run queue evidence and may reflect scheduler effects (including over-provisioning/folding)\n";
            printf {$fh} "                  or short-lived bursts not visible in the RunQ indicators.\n";
            printf {$fh} "                  Review the LPAR's current VP count (%s) relative to workload concurrency.\n",
                (defined $cfg->{virtual_cpus} && looks_like_number($cfg->{virtual_cpus})) ? sprintf("%d VPs", $cfg->{virtual_cpus}) : 'unknown';
        }
    }

    # Micro-partition RunQ envelope override PLANNER NOTE
    my $runq_override_flag = $calc_debug_info_ref->{'rVCPU_RunQEnvelopeOverride'} // "No";
    if ($runq_override_flag eq "Yes") {
        my $runq_basis  = $calc_debug_info_ref->{'rVCPU_RunQEnvelopeBasis'} // 'N/A';
        my $orig_min    = $calc_debug_info_ref->{'rVCPU_OrigMin'} // 'N/A';
        my $orig_max    = $calc_debug_info_ref->{'rVCPU_OrigMax'} // 'N/A';
        my $final_max   = $calc_debug_info_ref->{'rVCPU_Max'} // 'N/A';
        my $ent_cores   = $calc_debug_info_ref->{'rVCPU_EntCores'};
        my $final_rv    = $calc_debug_info_ref->{'rVCPU_Final'} // 'N/A';

        print  {$fh} "    PLANNER NOTE: Micro-partition RunQ override active.\n";
        printf {$fh} "      RunQ tail (%.2f cores) exceeds P-99W1 PhysC (%.2f cores) by %.1fx.\n",
            ($runq_basis ne 'N/A' && looks_like_number($runq_basis) ? $runq_basis : 0),
            (defined $calc_debug_info_ref->{'rVCPU_P99W1RawPeak'} && looks_like_number($calc_debug_info_ref->{'rVCPU_P99W1RawPeak'})
                ? $calc_debug_info_ref->{'rVCPU_P99W1RawPeak'} : 0),
            ($sf_cred > 0 ? $sf_cred : 1.0);
        printf {$fh} "      VP envelope (from rENT_P=%.2f cores): min %s, max %s\n",
            (defined $ent_cores && looks_like_number($ent_cores) ? $ent_cores : 0),
            $orig_min, $orig_max;
        printf {$fh} "      RunQ override applied: extended env_max to %s (RunQ dispatch sizing requires %s VPs)\n",
            $final_max, ($calc_debug_info_ref->{'rVCPU_Raw'} // 'N/A');
        if (defined $ent_cores && looks_like_number($ent_cores) && $ent_cores > 0
            && defined $final_rv && looks_like_number($final_rv) && $final_rv > 0) {
            printf {$fh} "      Resulting VP:ENT ratio (%.1fx) exceeds normal 2.0x maximum - justified by dispatch pressure evidence.\n",
                ($final_rv / $ent_cores);
        }
    }

    printf {$fh} "    Detail: raw=%s; env=%s..%s; tier=%s/f=%s; P99W1_floor=%s @ factor=%s (headroom=%s; peak=%s adj=%s cred=%s); RunQ=%s=%s%s\n\n",
        ($calc_debug_info_ref->{'rVCPU_Raw'} // $na),
        ($calc_debug_info_ref->{'rVCPU_Min'} // $na),
        ($calc_debug_info_ref->{'rVCPU_Max'} // $na),
        ($calc_debug_info_ref->{'rVCPU_TierNum'} // $na),
        ($calc_debug_info_ref->{'rVCPU_TierFactor'} // $na),
        ($calc_debug_info_ref->{'rVCPU_P99W1Floor'} // $na),
        ($calc_debug_info_ref->{'rVCPU_P99W1FactorEff'} // $na),
        ($calc_debug_info_ref->{'rVCPU_P99W1Mult'} // $na),
        ($calc_debug_info_ref->{'rVCPU_P99W1RawPeak'} // $na),
        ($calc_debug_info_ref->{'rVCPU_P99W1AdjPeak'} // $na),
        ($calc_debug_info_ref->{'rVCPU_P99W1CredRatio'} // $na),
        ($calc_debug_info_ref->{'rVCPU_RunQKeyUsed'} // $na),
        ($calc_debug_info_ref->{'rVCPU_RunQHiThreads'} // $na),
        (($calc_debug_info_ref->{'rVCPU_DispatchReserveVP'} // 0) > 0
            ? sprintf("; reserve=+%d", $calc_debug_info_ref->{'rVCPU_DispatchReserveVP'})
            : "");

    # --- Footer Block (Bottom Summary) ---
    print {$fh} "----------------------------------------------------------------------\n";
    printf {$fh} "Overall Summary for Profile: %s\n", $profile_being_adjusted;
    printf {$fh} "  - Initial Base PhysC (pre-growth)        : %.4f cores\n", $pre_growth_base_physc;
    printf {$fh} "  - nfit GrowthAdj (Theil-Sen)             : %s%.4f cores\n",
        ($nfit_growth_adj >= 0 ? "+" : ""), $nfit_growth_adj;
    printf {$fh} "  - nfit-profile Downsizing                : %s%.4f cores (Factor: %s)\n",
        (($calc_debug_info_ref->{'DownsizedPhysC'} // 0) - $post_growth_base_physc >= 0 ? "+" : ""),
        abs(($calc_debug_info_ref->{'DownsizedPhysC'} // 0) - $post_growth_base_physc),
        ($calc_debug_info_ref->{'DownsizingFactor'} // $na);

    my $downsizing_summary_reason_footer = $calc_debug_info_ref->{'DownsizingReason'} // "N/A";
    printf {$fh} "    - CPU Downsizing                         : Factor %s -> PhysC became %s cores. (Summary: %s)\n",
        ($calc_debug_info_ref->{'DownsizingFactor'} // $na),
        ($calc_debug_info_ref->{'DownsizedPhysC'} // $na),
        $downsizing_summary_reason_footer;

    my $additive_final_val_footer = $calc_debug_info_ref->{'FinalAdditive'} // "0.0000";
    my $additive_reason_summary_footer;

    if ($calc_debug_info_ref->{'IsRunQPressure'} eq "True" || $calc_debug_info_ref->{'IsWorkloadPressure'} eq "True") {
        $additive_reason_summary_footer = "Pressure detected";
        my @pressure_types_footer;
        if ($calc_debug_info_ref->{'IsRunQPressure'} eq "True") { push @pressure_types_footer, "Overall LPAR"; }
        if ($calc_debug_info_ref->{'IsWorkloadPressure'} eq "True") { push @pressure_types_footer, "Normalized Workload"; }
        if (@pressure_types_footer) { $additive_reason_summary_footer .= " (" . join(", ", @pressure_types_footer) . ")";}

        if (defined $calc_debug_info_ref->{'HotThreadWLDampeningApplied'} && $calc_debug_info_ref->{'HotThreadWLDampeningApplied'} eq "True")
        {
            $additive_reason_summary_footer .= "; HTW Dampened";
        }
        if (defined $calc_debug_info_ref->{'AdditiveSafetyCapApplied'} && $calc_debug_info_ref->{'AdditiveSafetyCapApplied'} =~ /^True/) {
            $additive_reason_summary_footer .= "; Safety Capped";
        }
        # Add note if additive ended up zero despite pressure
        if (abs(($additive_final_val_footer // 0)+0) < $FLOAT_EPSILON && ($calc_debug_info_ref->{'IsRunQPressure'} eq "True" || $calc_debug_info_ref->{'IsWorkloadPressure'} eq "True")) {
            $additive_reason_summary_footer .= "; Final Additive Zero (due to caps/dampening)";
        }
    }
    else {
        # This path is taken only when no pressure flags were set.
        if (defined $calc_debug_info_ref->{'ExcessThreads'} && $calc_debug_info_ref->{'ExcessThreads'} =~ /No excess threads/i) {
             $additive_reason_summary_footer = "No excess threads calculated for additive.";
         }
        else {
             $additive_reason_summary_footer = "No significant pressure detected.";
        }
    }
    printf {$fh} "    - CPU Upsizing (Additive)                : %s cores. (Reason: %s)\n",
        $additive_final_val_footer, $additive_reason_summary_footer;

    printf {$fh} "  - nfit-profile Upsizing (Additive)         : %s%.4f cores\n",
        ($additive_final_val_footer >= 0 ? "+" : ""), $additive_final_val_footer;

    my $lpar_cap_applied_summary_footer = "Not applied or N/A";
    if (defined $calc_debug_info_ref->{'CappedByMaxCPU'})
    {
        $lpar_cap_applied_summary_footer = $calc_debug_info_ref->{'CappedByMaxCPU'} eq "True"
        ? "Applied (Effective Limit: " . ($calc_debug_info_ref->{'EffectiveMaxCPUCap'} // $na) . ")"
        : "Not Applied (Effective Limit: " . ($calc_debug_info_ref->{'EffectiveMaxCPUCap'} // $na) . ")";
    }
    printf {$fh} "  - Max CPU Sanity Check                   : %s\n", $lpar_cap_applied_summary_footer;
    print {$fh} "  --------------------------------------------------------------------\n";
    printf {$fh} "  Final Recommended Value : %s cores (Unrounded: %s)\n",
        ($final_csv_value_for_profile // $na), # This is rounded for CSV
        $final_recommendation_unrounded_str;    # This is unrounded from calc
    print {$fh} "======================================================================\n\n";

}
# End of log_profile_rationale

# --- Helper subroutine to parse and collate percentile lists for nfit calls ---
# Takes an existing list of percentiles (as a comma-separated string) and an array
# of numeric percentiles that must be ensured to be present in the final list.
# Returns a sorted, unique array of percentile strings, formatted for nfit.
sub parse_and_collate_percentiles
{
    my ($existing_perc_list_str, @ensure_these_percs_numeric) = @_;
    my %final_percs_map; # Use a hash to store unique percentiles to avoid duplicates

    # Add percentiles from the existing string (e.g., from profile flags or global nfit-profile default)
    if (defined $existing_perc_list_str && $existing_perc_list_str ne '')
    {
        my @raw_list = split /,\s*/, $existing_perc_list_str;
        foreach my $p_str (@raw_list)
        {
            next if $p_str eq ''; # Skip empty strings that might result from split (e.g. "80,,90")

            # Validate if it looks like a percentile number, then format it consistently for nfit
            if ($p_str =~ /^[0-9.]+$/ && $p_str >= 0 && $p_str <= 100)
            {
                my $p_label = sprintf("%.2f", $p_str + 0); # Normalize format (e.g., "90.00")
                $p_label =~ s/\.?0+$//;                    # Clean trailing ".00" (e.g., "90")
                $p_label = "0" if $p_label eq "" && abs(($p_str+0) - 0) < 0.001; # Handle case of "0.00" -> "0"
                $final_percs_map{$p_label} = 1; # Add to hash (value 1 is arbitrary, key is what matters)
            }
            else
            {
                # If it's not a simple number, it might be an invalid value.
                # nfit will ultimately validate it. For now, include as is.
                # Alternatively, one could issue a warning here:
                # warn " [WARN] Non-standard percentile string '$p_str' found in list '$existing_perc_list_str'. Passing to nfit as is.\n";
                $final_percs_map{$p_str} = 1;
            }
        }
    }

    # Add percentiles that must be ensured (e.g., P90 for AbsRunQ, P50/P90 for NormRunQ)
    foreach my $p_num (@ensure_these_percs_numeric)
    {
        my $p_label = sprintf("%.2f", $p_num); # Format, e.g., 90 -> "90.00", 98.5 -> "98.50"
        $p_label =~ s/\.?0+$//;                # Clean to "90", "98.5"
        $p_label = "0" if $p_label eq "" && abs($p_num - 0) < 0.001; # "0.00" -> "0"
        $final_percs_map{$p_label} = 1; # Add/overwrite in hash to ensure it's present
    }

    # Return a numerically sorted list of unique percentile strings
    my @sorted_keys = sort {
        # Robust sort: treat as numbers if possible, otherwise string compare
        my $is_a_num = ($a =~ /^[0-9.]+$/); # Check if $a looks like a number
        my $is_b_num = ($b =~ /^[0-9.]+$/); # Check if $b looks like a number
        if ($is_a_num && $is_b_num) { return ($a+0) <=> ($b+0); } # Both are numbers, numeric sort
        elsif ($is_a_num) { return -1; } # Numbers come before non-numbers
        elsif ($is_b_num) { return 1;  } # Non-numbers come after numbers
        else { return $a cmp $b; }       # Both are non-numbers (e.g. invalid values), string compare
    } keys %final_percs_map; # Get unique keys from hash and sort them

    return @sorted_keys;
}

# --- quote_csv ---
# Ensures a string is properly quoted for CSV output, escaping internal double quotes.
sub quote_csv {
    my ($field) = @_;
    if (!defined $field) # Handle undefined fields as empty strings
    {
        $field = '';
    }
    $field =~ s/"/""/g; # Escape any double quotes within the field by doubling them
    return qq/"$field"/; # Enclose the entire field in double quotes
}

# --- load_profile_definitions ---
# Loads profile configurations from the specified INI-like file.
# Each section [Profile Name] defines a profile.
# Keys: nfit_flags (mandatory), runq_modifier_behavior (optional, default: 'default').
# This version correctly handles multi-line values for the 'nfit_flags' key.
sub load_profile_definitions {
    my ($filepath) = @_;
    my @loaded_profiles_list;
    my $current_profile = undef;
    my $last_key = '';

    open my $fh, '<:encoding(utf8)', $filepath or die "[ERROR] Cannot open profiles config file '$filepath': $!\n";

    while (my $line = <$fh>) {
        chomp $line;
        $line =~ s/\s*[#;].*//;    # Remove comments
        $line =~ s/^\s+|\s+$//g; # Trim whitespace
        next if $line eq '';

        if ($line =~ /^\s*\[\s*([^\]]+?)\s*\]\s*$/) { # New section
            my $section_name = $1;
            $current_profile = { name => $section_name, flags => '', runq_behavior => 'default', csv_output => 1 };
            push @loaded_profiles_list, $current_profile;
            $last_key = '';
        } elsif ($current_profile && $line =~ /^\s*([^=]+?)\s*=\s*(.*)$/) { # New key-value pair
            my $key = lc($1);
            my $value = $2;
            $key =~ s/^\s+|\s+$//g;
            $value =~ s/^\s+|\s+$//g;
            $last_key = $key;

            if ($key eq 'nfit_flags') {
                $current_profile->{flags} = $value;
            } elsif ($key eq 'runq_modifier_behavior') {
                $current_profile->{runq_behavior} = $value;
            } elsif ($key eq 'csv_output') {
                $current_profile->{csv_output} = ($value =~ /^(true|1|yes)$/i) ? 1 : 0;
            }
        } elsif ($current_profile && $last_key eq 'nfit_flags') { # Continuation of nfit_flags
            $current_profile->{flags} .= " $line";
        }
    }
    close $fh;

    # Final validation and cleanup
    my @valid_profiles;
    foreach my $p_ref (@loaded_profiles_list) {
        # Clean up any leading/trailing whitespace from concatenated flags
        $p_ref->{flags} =~ s/^\s+|\s+$//g if defined $p_ref->{flags};

        if (defined $p_ref->{flags} && $p_ref->{flags} ne '') {
            # (existing validation logic for mandatory P-99W1, etc.)
            push @valid_profiles, $p_ref;
        } else {
            warn " [WARN] Profile '[$p_ref->{name}]' in '$filepath' is missing 'nfit_flags'. Skipping.\n";
        }
    }
    return @valid_profiles;
}

# --- calculate_graduated_burst ---
# Returns graduated burst allowance based on entitlement size
# Small entitlements get higher burst allowance to reflect real-world burst capability
sub calculate_graduated_burst {
    my ($entitlement) = @_;

    if ($entitlement < 0.25) {
        return 0.50;  # 50% burst for micro-partitions
    } elsif ($entitlement < 0.50) {
        return 0.40;  # 40% burst for very small
    } elsif ($entitlement < 1.00) {
        return 0.30;  # 30% burst for small
    } else {
        return 0.25;  # 25% standard burst
    }
}

# --- calculate_structural_availability_factor ---
# For small entitlements, estimates the effective availability of vCPU threads
# based on entitlement size. Scales from 0.5 at Ent=0.1 to 0.95 at Ent=0.9
sub calculate_structural_availability_factor {
    my ($entitlement) = @_;

    # Ensure entitlement is in valid range
    return 0.5 if $entitlement <= 0.1;
    return 0.95 if $entitlement >= 0.9;

    # Linear scaling: 0.5 + (0.45 * entitlement)
    return 0.5 + (0.45 * $entitlement);
}

# --- apply_minimum_lcpu_floor ---
# Applies a minimum LCPU floor based on SMT level
# Prevents unrealistic near-zero LCPU calculations for micro-partitions
sub apply_minimum_lcpu_floor {
    my ($effective_lcpus, $smt_used) = @_;

    # Ensure at least 1 full hardware thread for micro-partitions
    # Scale slightly with SMT to be platform-aware
    my $min_floor = max(1, int($smt_used / 4));  # At SMT=8, floor=2; at SMT=4, floor=1

    return max($min_floor, $effective_lcpus);
}

# ==============================================================================
# SUBROUTINE: apply_tier_aware_upsizing_scaling
# PURPOSE:    Applies tier-number-aware scaling to upsizing (additive CPU)
#             adjustments. Higher tiers (1-2) receive more aggressive responses
#             to RunQ pressure due to lower latency tolerance. Tier 3 is the
#             baseline. Tier 4 receives slightly moderated adjustments.
#
# ARGUMENTS:
#   1. $additive_cpu (float): The calculated additive CPU before tier scaling
#   2. $final_tier (string): The tier identifier (e.g., "G3", "O1", "P")
#
# RETURNS:
#   - Hash containing:
#       scaled_value: The tier-adjusted additive CPU value
#       scaling_factor: The multiplier applied
#       tier_number: The extracted tier number
#       rationale: Human-readable explanation for logging
# ==============================================================================
sub apply_tier_aware_upsizing_scaling {
    my ($additive_cpu, $final_tier) = @_;

    # Default result (no scaling)
    my %result = (
        scaled_value   => $additive_cpu,
        scaling_factor => 1.00,
        tier_number    => 'N/A',
        rationale      => 'No tier-based scaling applied',
    );

    # Only scale positive values (upsizing adjustments)
    return \%result if (!defined $additive_cpu || $additive_cpu <= 0);

    # Extract tier number from tier identifier
    # Handles formats: G3, O1, B4, P (P is treated as tier 1)
    my $tier_num;
    if (defined $final_tier && $final_tier =~ /(\d)$/) {
        $tier_num = $1;
    } elsif (defined $final_tier && $final_tier eq 'P') {
        $tier_num = 1;  # P-tier is equivalent to tier 1
    } else {
        return \%result;  # Cannot determine tier, no scaling
    }

    # Tier-based scaling factors
    my %tier_upsizing_factors = (
        1 => 1.30,  # Tier 1: +30% aggressive response (low latency tolerance)
        2 => 1.15,  # Tier 2: +15% enhanced response
        3 => 1.00,  # Tier 3: Baseline STANDARD correction
        4 => 0.95,  # Tier 4: Slightly moderated (still substantial)
    );

    # Apply scaling if tier is valid
    if (exists $tier_upsizing_factors{$tier_num}) {
        my $factor = $tier_upsizing_factors{$tier_num};
        my $scaled = $additive_cpu * $factor;

        $result{scaled_value}   = $scaled;
        $result{scaling_factor} = $factor;
        $result{tier_number}    = $tier_num;

        # Generate detailed rationale
        if ($factor > 1.0) {
            my $increase_pct = ($factor - 1.0) * 100;
            $result{rationale} = sprintf(
                "Tier %d: Enhanced upsizing (+%.0f%%) for lower latency tolerance. Base: %.3f -> Scaled: %.3f cores",
                $tier_num, $increase_pct, $additive_cpu, $scaled
            );
        } elsif ($factor < 1.0) {
            my $decrease_pct = (1.0 - $factor) * 100;
            $result{rationale} = sprintf(
                "Tier %d: Moderated upsizing (-%.0f%%) for standard efficiency. Base: %.3f -> Scaled: %.3f cores",
                $tier_num, $decrease_pct, $additive_cpu, $scaled
            );
        } else {
            $result{rationale} = sprintf(
                "Tier %d: Standard baseline correction (no scaling). Value: %.3f cores",
                $tier_num, $scaled
            );
        }
    }

    return \%result;
}

# --- calculate_runq_modified_physc (with enhanced efficiency logic and detailed debug output) ---
# Calculates the final PhysC value for a profile after applying efficiency factors
# and RunQ-driven additive CPU adjustments.
# Takes the raw PhysC from nfit, RunQ metrics, SMT, entitlement, MaxCPU, etc.
# Returns the adjusted PhysC value and a hash of debug information for logging.
# --- calculate_runq_modified_physc (refactor: decoupled C & D, no early-returns, adds RunQ_Strategic) ---
#
sub calculate_runq_modified_physc
{
    my ($vm_name, $vm_map_ref, $profile_ref, $pressure_flags_href, $adaptive_thresholds_href) = @_;

    # --- Unpack all required values from the assimilation map and arguments ---
    my $profile_name = $profile_ref->{name};
    my $profile_runq_behavior_setting = $profile_ref->{runq_behavior} // 'default';

    # Core values from the map
    my $selected_tier_physc_value_str = $vm_map_ref->{CoreResults}{ProfileValues}{$profile_name};
    my $pure_p99w1_physc              = $vm_map_ref->{CoreResults}{ProfileValues}{$MANDATORY_PEAK_PROFILE_FOR_HINT};

    # Configuration from the map
    my $cfg_ref                 = $vm_map_ref->{Configuration};
    my $smt_used                = $cfg_ref->{smt};
    my $current_entitlement_str = $cfg_ref->{entitlement};
    my $max_cpu_config_str      = $cfg_ref->{max_cpu};
    my $vcpu_for_lpar_numeric   = $cfg_ref->{virtual_cpus};
    my $pool_cpu_for_lpar_numeric = $cfg_ref->{pool_cpu};
    my $is_in_non_default_pool  = (defined $cfg_ref->{pool_id} && $cfg_ref->{pool_id} != 0);

    # RunQ Metrics from the map
    my $runq_metrics_ref = $vm_map_ref->{RunQMetrics};
    my $norm_runq_p25_str = $runq_metrics_ref->{'NormRunQ_P25'};
    my $norm_runq_p50_str = $runq_metrics_ref->{'NormRunQ_P50'};
    my $norm_runq_p75_str = $runq_metrics_ref->{'NormRunQ_P75'};
    my $norm_runq_p90_str = $runq_metrics_ref->{'NormRunQ_P90'};

    # Determine which AbsRunQ percentile value to use based on profile behaviour.
    my $abs_runq_key_to_use = "AbsRunQ_P90"; # Default to fixed
    if ($runq_perc_behavior_mode eq 'match' && $profile_ref->{flags} =~ /-p\s+([0-9.]+)/) {
        $abs_runq_key_to_use = "AbsRunQ_P" . clean_perc_label($1);
    }
    my $abs_runq_p_value_str = $runq_metrics_ref->{$abs_runq_key_to_use};
    my $abs_runq_key_for_debug = $abs_runq_key_to_use;

    # Pressure flags and adaptive thresholds passed in from the main loop
    my $p99w1_overall_vm_has_abs_runq_pressure  = $pressure_flags_href->{abs_pressure};
    my $p99w1_overall_vm_has_norm_runq_pressure = $pressure_flags_href->{norm_pressure};
    my $adaptive_runq_saturation_thresh   = $adaptive_thresholds_href->{saturation};
    my $adaptive_target_norm_runq         = $adaptive_thresholds_href->{target};
    my $adaptive_max_efficiency_reduction = $adaptive_thresholds_href->{max_reduction};

    # The tier for all modifier calculations must come from the profile being processed,
    # not the global VM hint, to ensure logical consistency.
    my $final_tier = $profile_ref->{name};
    $final_tier =~ s/-.*//; # Extract the tier part (e.g., "G2-98W10" -> "G2")

    my %debug_info;
    my $na_str = "N/A";


    # Defensive normalisation: seasonal/bucket execution paths may call into
    # RunQ logic without a tier PhysC string populated.
    $selected_tier_physc_value_str = $na_str
        unless defined($selected_tier_physc_value_str) && length($selected_tier_physc_value_str);

    # --- Initialize all debug fields to sensible defaults or N/A ---
    $debug_info{'AbsRunQKeyUsed'} = $abs_runq_key_for_debug // 'N/A (key not provided)';
    $debug_info{'AbsRunQValueUsedForCalc'} = $abs_runq_p_value_str;
    $debug_info{'BasePhysC'} = $selected_tier_physc_value_str // $na_str;
    $profile_runq_behavior_setting //= 'default';

    my $base_physc = ($selected_tier_physc_value_str ne $na_str && $selected_tier_physc_value_str =~ /^-?[0-9.]+$/)
        ? ($selected_tier_physc_value_str + 0)
        : undef;

    # Efficiency related fields - meticulously initialized
    $debug_info{'DownsizingReason'} = "Efficiency calculation not initiated or skipped by initial guards.";
    $debug_info{'DownsizingFactor'} = "1.00"; # final sprintf form later
    $debug_info{'EffCondNormP50Met'} = undef;
    $debug_info{'EffCondVolatilityMet'} = undef;
    $debug_info{'EffVolatilityRatio'} = $na_str;
    $debug_info{'EffPBase'} = (defined $base_physc) ? sprintf("%.4f", $base_physc) : $na_str;
    $debug_info{'EffSMTValue'} = $smt_used // $na_str;
    $debug_info{'EffTargetNormRunQ'} = $na_str;
    $debug_info{'EffPEfficientTargetRaw'} = $na_str;
    $debug_info{'EffBlendReason'} = "Blending not applied or not applicable.";
    $debug_info{'EffBlendWeightBase'} = $na_str;
    $debug_info{'EffBlendWeightTarget'} = $na_str;
    $debug_info{'EffPEfficientTarget'} = $na_str;
    $debug_info{'EffComparisonBaseVsTargetMet'} = undef;
    $debug_info{'EffPotentialReduction'} = $na_str;
    $debug_info{'EffMaxAllowableReductionPerc'} = $adaptive_max_efficiency_reduction * 100;
    $debug_info{'EffReductionCapReason'} = "Default reduction cap applied.";
    $debug_info{'EffMaxAllowableReductionCores'} = $na_str;
    $debug_info{'EffActualReductionCores'} = $na_str;
    $debug_info{'EffCalculatedFactor'} = "1.0000";
    $debug_info{'EffFinalFactorApplied'} = "1.00";

    # Diagnostics (daily-series based; diagnostic approximation)
    $debug_info{'PhysC_Daily_PAE'}  = $na_str;
    $debug_info{'PhysC_Daily_AAE'}  = $na_str;
    $debug_info{'PhysC_Daily_IQRC'} = $na_str;

    # Headroom diagnostics (threads + cores)
    $debug_info{'Headroom_Guaranteed_Threads'} = $na_str;
    $debug_info{'Headroom_Guaranteed_Cores'}   = $na_str;
    $debug_info{'Headroom_Reachable_Threads'}  = $na_str;
    $debug_info{'Headroom_Reachable_Cores'}    = $na_str;
    $debug_info{'Headroom_AbsRunQKey'}         = $abs_runq_key_for_debug // $na_str;
    $debug_info{'Headroom_AbsRunQ_Threads'}    = $na_str;
    $debug_info{'Headroom_EffectiveReachableCores'} = $na_str;

    # General fields
    $debug_info{'DownsizedPhysC'} = defined($base_physc) ? sprintf("%.4f", $base_physc) : $na_str;
    $debug_info{'RunQ_Strategic'} = "0.0000";
    $debug_info{'RunQ_Potential'} = "0.0000";
    $debug_info{'RunQPressure_P90_Val'} = 0; $debug_info{'IsRunQPressure'} = "False";
    $debug_info{'IsWorkloadPressure'} = "False"; $debug_info{'WorkloadPressureReason'} = "Conditions not met or N/A inputs";
    $debug_info{'EffectiveLCPUsAtBase'} = $na_str; $debug_info{'ExcessThreads'} = $na_str;
    $debug_info{'RawAdditive'} = "0.0000"; $debug_info{'MaxAdditiveCap'} = "0.0000"; $debug_info{'CappedRawAdditive'} = "0.0000";
    $debug_info{'VoltFactorReason'} = "Default (no overriding condition met or additive not applied)";
    $debug_info{'VoltFactor'} = "1.00";
    $debug_info{'PoolFactor'} = "1.00";
    $debug_info{'FinalAdditive'} = "0.0000";
    $debug_info{'PreMaxCpuCapRec'} = $debug_info{'DownsizedPhysC'};
    $debug_info{'LPARMaxCPUConfig'} = ($max_cpu_config_str ne "" && $max_cpu_config_str =~ /^[0-9.]+$/ && ($max_cpu_config_str+0) > 0)
        ? ($max_cpu_config_str+0) : $na_str;
    $debug_info{'EntitlementForForecast'} = (defined $current_entitlement_str && $current_entitlement_str ne "" && $current_entitlement_str =~ /^-?[0-9.]+$/)
        ? ($current_entitlement_str + 0) : 0;
    $debug_info{'ForecastMultiplier'} = $na_str; $debug_info{'EffectiveMaxCPUCap'} = $na_str;
    $debug_info{'CappedByMaxCPU'} = $na_str;
    $debug_info{'FinalAdjustedPhysC'} = $debug_info{'BasePhysC'};
    $debug_info{'ReasonForNoModification'} = "";

    my $curr_ent_numeric = $debug_info{'EntitlementForForecast'};
    my $eff_p_base_numeric = undef;          # Pre-growth base for efficiency/downsizing logic
    my $growth_adj_numeric = 0.0;            # GrowthAdj associated with this profile
    my $pre_growth_base_physc_numeric = undef;

    # ------------------------------------------------------------------
    # Growth vs Downsizing ordering fix (production fix)
    # ------------------------------------------------------------------
    # We must avoid disqualifying downsizing because GrowthAdj inflated BasePhysC.
    # Therefore:
    #   1) Compute downsizing on PRE-GROWTH base (BaseValue)
    #   2) Apply GrowthAdj AFTER downsizing, then continue with additive logic
    #
    # NOTE: CoreResults.ProfileValues already holds the growth-inclusive FinalValue.
    #       We retrieve BaseValue & GrowthAdj from vm_map->{Growth}{...} to restore order.
    my $pre_base_from_map = _safe_dig($vm_map_ref, 'Growth', 'base_values', $profile_name);
    my $growth_adj_from_map = _safe_dig($vm_map_ref, 'Growth', 'adjustments', $profile_name);

    if (defined $growth_adj_from_map && looks_like_number($growth_adj_from_map)) {
        $growth_adj_numeric = $growth_adj_from_map + 0;
    }

    if (defined $pre_base_from_map && looks_like_number($pre_base_from_map)) {
        $pre_growth_base_physc_numeric = $pre_base_from_map + 0;
    } elsif (defined $base_physc) {
        # Fallback: if BaseValue missing, infer pre-growth base from growth-inclusive base_physc
        $pre_growth_base_physc_numeric = ($base_physc + 0) - $growth_adj_numeric;
    }

    # Final guard: if still undefined, fall back to growth-inclusive base
    $pre_growth_base_physc_numeric = $base_physc
        if (!defined $pre_growth_base_physc_numeric && defined $base_physc);

    $eff_p_base_numeric = defined($pre_growth_base_physc_numeric) ? ($pre_growth_base_physc_numeric + 0) : undef;

    $debug_info{'PreGrowthBasePhysC'}  = defined($pre_growth_base_physc_numeric) ? sprintf("%.4f", $pre_growth_base_physc_numeric) : $na_str;
    $debug_info{'GrowthAdjUsed'}       = defined($growth_adj_numeric) ? sprintf("%+.4f", $growth_adj_numeric) : $na_str;
    $debug_info{'PostGrowthBasePhysC'} = defined($base_physc) ? sprintf("%.4f", $base_physc) : $na_str;

    # EffPBase should reflect the PRE-GROWTH base for the downsizing/efficiency logic.
    $debug_info{'EffPBase'} = defined($eff_p_base_numeric) ? sprintf("%.4f", $eff_p_base_numeric) : $na_str;

    unless (defined $base_physc)
    {
        $debug_info{'ReasonForNoModification'} = "BasePhysC for profile not numeric or N/A";
        $debug_info{'FinalAdjustedPhysC'} = $selected_tier_physc_value_str // $na_str;
        return ($selected_tier_physc_value_str // $na_str, \%debug_info);
    }

    my $norm_p50_numeric = ($norm_runq_p50_str ne $na_str && $norm_runq_p50_str =~ /^-?[0-9.]+$/) ? ($norm_runq_p50_str + 0) : undef;
    my $norm_p90_numeric = ($norm_runq_p90_str ne $na_str && $norm_runq_p90_str =~ /^-?[0-9.]+$/) ? ($norm_runq_p90_str + 0) : undef;
    my $abs_runq_p_numeric  = ($abs_runq_p_value_str ne $na_str && $abs_runq_p_value_str  =~ /^-?[0-9.]+$/) ? ($abs_runq_p_value_str + 0)  : undef;
    $debug_info{'NormRunQ_P90_Val'} = (defined $norm_p90_numeric) ? sprintf("%.2f", $norm_p90_numeric) : $na_str;
    # ------------------------------------------------------------------
    # Harvest per-profile PhysC diagnostics (DailySeries-based approximation)
    # ------------------------------------------------------------------
    my $physc_diag = _safe_dig($vm_map_ref, 'Diagnostics', 'PhysC', $profile_name);
    if (ref($physc_diag) eq 'HASH') {
        if (defined $physc_diag->{PAE} && looks_like_number($physc_diag->{PAE})) {
            $debug_info{'PhysC_Daily_PAE'} = sprintf("%.4f", $physc_diag->{PAE} + 0);
        }
        if (defined $physc_diag->{AAE} && looks_like_number($physc_diag->{AAE})) {
            $debug_info{'PhysC_Daily_AAE'} = sprintf("%.4f", $physc_diag->{AAE} + 0);
        }
        if (defined $physc_diag->{IQRC} && looks_like_number($physc_diag->{IQRC})) {
            $debug_info{'PhysC_Daily_IQRC'} = sprintf("%.4f", $physc_diag->{IQRC} + 0);
        }
    }

    # ------------------------------------------------------------------
    # Headroom diagnostics (threads & cores)
    # ------------------------------------------------------------------
    if (defined $smt_used && $smt_used > 0 && defined $abs_runq_p_numeric) {
        my $guaranteed_threads = (defined $curr_ent_numeric && looks_like_number($curr_ent_numeric) && $curr_ent_numeric > 0)
                               ? ($smt_used * $curr_ent_numeric)
                               : 0.0;

        # "Reachable" cores is bounded by vCPU and pool CPU when uncapped.
        my $reachable_cores = 0.0;
        if (defined $cfg_ref->{is_capped} && $cfg_ref->{is_capped}) {
            $reachable_cores = $curr_ent_numeric;
        } else {
            if (defined $pool_cpu_for_lpar_numeric && looks_like_number($pool_cpu_for_lpar_numeric) && $pool_cpu_for_lpar_numeric > 0
                && defined $vcpu_for_lpar_numeric && looks_like_number($vcpu_for_lpar_numeric) && $vcpu_for_lpar_numeric > 0) {
                $reachable_cores = ($vcpu_for_lpar_numeric < $pool_cpu_for_lpar_numeric) ? $vcpu_for_lpar_numeric : $pool_cpu_for_lpar_numeric;
            } elsif (defined $vcpu_for_lpar_numeric && looks_like_number($vcpu_for_lpar_numeric) && $vcpu_for_lpar_numeric > 0) {
                $reachable_cores = $vcpu_for_lpar_numeric;
            } else {
                $reachable_cores = $curr_ent_numeric;
            }
        }
        $reachable_cores = 0.0 if (!defined $reachable_cores || $reachable_cores < 0);
        my $reachable_threads = $smt_used * $reachable_cores;

        my $headroom_g_threads = $guaranteed_threads - $abs_runq_p_numeric;
        my $headroom_r_threads = $reachable_threads  - $abs_runq_p_numeric;

        $debug_info{'Headroom_AbsRunQ_Threads'} = sprintf("%.2f", $abs_runq_p_numeric);
        $debug_info{'Headroom_EffectiveReachableCores'} = sprintf("%.2f", $reachable_cores);
        $debug_info{'Headroom_Guaranteed_Threads'} = sprintf("%.2f", $headroom_g_threads);
        $debug_info{'Headroom_Guaranteed_Cores'}   = sprintf("%.4f", ($smt_used > 0 ? ($headroom_g_threads / $smt_used) : 0.0));
        $debug_info{'Headroom_Reachable_Threads'}  = sprintf("%.2f", $headroom_r_threads);
        $debug_info{'Headroom_Reachable_Cores'}    = sprintf("%.4f", ($smt_used > 0 ? ($headroom_r_threads / $smt_used) : 0.0));
    }


    # Calculate Effective LCPUs for Pressure
    my ($effective_lcpus_for_pressure_calc, $pressure_basis_rationale);
    my $max_cpu_for_lpar_numeric = (defined $max_cpu_config_str && looks_like_number($max_cpu_config_str)) ? ($max_cpu_config_str + 0) : 0;
    my $base_burst_factor = 0.50;

    my $evr_num;              # EVR used for tiering (Ent floored to 1.0)
    my $evr_raw_num;          # Raw EVR (vCPU/Entitlement) for reporting
    my $evr_str = $na_str;    # Display EVR (raw)
    my $evr_tier_note = '';   # Optional note when Ent < 1.0 (to explain tiering EVR)

    my $evr_low = 1.25;   # “low headroom intent”
    my $evr_mod = 1.50;   # “moderate / burst-oriented”
    my $evr_hi  = 2.00;   # “vendor line / cap benefit”

    my $evr_tier_rationale = "Conservative/Standard (EVR < ${evr_low}x)";

    if (defined $curr_ent_numeric && $curr_ent_numeric > 0
        && defined $vcpu_for_lpar_numeric && $vcpu_for_lpar_numeric > 0) {

        # Keep the entitlement floor to avoid tiny-ent skew;
        # optionally also suppress EVR-based boosts for ent < 1.0.
        my $ent_for_evr = max($curr_ent_numeric, 1.0);

        # Raw EVR (for display): vCPU / Entitlement
        $evr_raw_num = $vcpu_for_lpar_numeric / $curr_ent_numeric;
        $evr_str     = sprintf("%.1f:1", $evr_raw_num);

        # Tiering EVR (for headroom policy): vCPU / max(Entitlement, 1.0)
        $evr_num = $vcpu_for_lpar_numeric / $ent_for_evr;

        # If Entitlement is fractional, explain why the tiering EVR differs from the raw EVR.
        if ($curr_ent_numeric < 1.0) {
            $evr_tier_note = sprintf(
                "         - EVR used for headroom tiering (Ent floor 1.0) : %s/%.1f = %.1fx\n",
                $vcpu_for_lpar_numeric, $ent_for_evr, $evr_num
            );
        }

        # Optional: cap benefit above 2.0 so EVR doesn't inflate allowance indefinitely.
        my $evr_effective = min($evr_num, $evr_hi);

        if ($evr_num >= $evr_hi) {
            # High EVR: classify it, but cap the headroom benefit.
            $base_burst_factor  = 1.00;   # capped benefit
            $evr_tier_rationale = sprintf("High (EVR %.1fx >= ${evr_hi}x; headroom benefit capped)", $evr_num);
        } elsif ($evr_effective >= $evr_mod) {
            $base_burst_factor  = 1.00;
            $evr_tier_rationale = sprintf("Moderate (EVR %.1fx >= ${evr_mod}x)", $evr_num);
        } elsif ($evr_effective >= $evr_low) {
            $base_burst_factor  = 0.75;
            $evr_tier_rationale = sprintf("Low headroom intent (EVR %.1fx >= ${evr_low}x)", $evr_num);
        } else {
            $base_burst_factor  = 0.50;
        }
    }
    my $pool_constraint_multiplier = 1.0;
    my $pool_constraint_rationale = "Default Pool (ID 0), no constraint applied";
    if ($is_in_non_default_pool && $pool_cpu_for_lpar_numeric > 0 && $curr_ent_numeric > 0) {
        my $vm_share_of_pool = $curr_ent_numeric / $pool_cpu_for_lpar_numeric;
        $pool_constraint_multiplier = 1.0 - (0.3 * min(1.0, $vm_share_of_pool * 2));
        $pool_constraint_multiplier = max(0.7, $pool_constraint_multiplier);
        $pool_constraint_rationale = sprintf("User Pool (Share %.1f%%), multiplier of %.2f applied", $vm_share_of_pool*100, $pool_constraint_multiplier);
    }
    my $final_burst_allowance = $base_burst_factor * $pool_constraint_multiplier;
    my $theoretical_lcpus = ($curr_ent_numeric * (1 + $final_burst_allowance)) * $smt_used;

    # handle potentially undefined P-99W1 values
    my $p99w1_physc_val_str = looks_like_number($pure_p99w1_physc) ? sprintf("%.3f", $pure_p99w1_physc) : $na_str;
    my $p99w1_physc_val = ($p99w1_physc_val_str ne $na_str && $p99w1_physc_val_str =~ /^-?[0-9.]+\z/)
        ? ($p99w1_physc_val_str + 0)
        : 0;
    my $capacity_headroom = ($max_cpu_for_lpar_numeric > 0) ? (1 - ($p99w1_physc_val / $max_cpu_for_lpar_numeric)) : 0;
    my $headroom_confidence_modifier = 1.0;
    my $headroom_rationale = "High Headroom (>30%), full confidence in observed peak";
    if ($capacity_headroom < 0.10) {
        $headroom_confidence_modifier = 0.75;
        $headroom_rationale = sprintf("Low Headroom (%.1f%%), low confidence in further bursting", $capacity_headroom * 100);
    } elsif ($capacity_headroom < 0.30) {
        $headroom_confidence_modifier = 0.90;
        $headroom_rationale = sprintf("Medium Headroom (%.1f%%), medium confidence", $capacity_headroom * 100);
    }
    my $confidence_adjusted_peak_lcpus = ($p99w1_physc_val * $smt_used) * $headroom_confidence_modifier;
    my $absolute_ceiling_lcpus = $max_cpu_for_lpar_numeric > 0 ? ($max_cpu_for_lpar_numeric * $smt_used) : ($vcpu_for_lpar_numeric * $smt_used);

    # Effective Dispatch Threads (Allowance):
    #   - Select the larger of theoretical vs observed capacity
    #   - Then cap that allowance at the maximum dispatch threads (MaxCPU * SMT)
    my $selected_allowance_lcpus = max($theoretical_lcpus, $confidence_adjusted_peak_lcpus);
    $effective_lcpus_for_pressure_calc = min($absolute_ceiling_lcpus, $selected_allowance_lcpus);

    # Pre-compute planner-friendly deltas for rationale output.
    my $gap_vs_observed = $effective_lcpus_for_pressure_calc - $confidence_adjusted_peak_lcpus;
    my $gap_vs_theoretical = $effective_lcpus_for_pressure_calc - $theoretical_lcpus;
    my $gap_vs_observed_pct = ($confidence_adjusted_peak_lcpus > 0)
        ? (($gap_vs_observed / $confidence_adjusted_peak_lcpus) * 100)
        : 0;
    my $gap_vs_theoretical_pct = ($theoretical_lcpus > 0)
        ? (($gap_vs_theoretical / $theoretical_lcpus) * 100)
        : 0;

    # Cap binding status: did the MaxCPU ceiling actually constrain the selected allowance?
    my $cap_is_binding = ($absolute_ceiling_lcpus + 0.0 < $selected_allowance_lcpus - 0.0001) ? 1 : 0;
    my $cap_status_str = $cap_is_binding ? 'binding' : 'not binding';

    # Allowance binding flags (rounded-output safe):
    my $binding_theoretical = (abs($gap_vs_theoretical) < 0.05) ? ' [binding]' : '';
    $pressure_basis_rationale = sprintf(
        "\n   1. Theoretical Dispatch Capacity\n".
        "      - vCPU-to-Entitlement Ratio (EVR)       : %s\n".
        "         - EVR (vCPU/Ent)                     : %s/%s = %.1fx\n".
        "%s".
        "      - Uncapped Headroom Factor (Base)       : %.0f%% (Reason: %s)\n".
        "      - Pool Headroom / Contention Factor     : %.2fx (Reason: %s)\n".
        "      => Theoretical Dispatch Threads         : %.2f Ent * (1 + %.2f Uncapped Headroom (Calculated)) * %d SMT = %.1f\n".
        "         - Uncapped Headroom (Calculated)     : %.0f%% (base %.0f%% x pool %.2f)\n".
        "   2. Observed Dispatch Capacity (Evidence-based)\n".
        "      - Observed Peak (P-99W1)                : %.2f cores (Headroom: %.1f%%)\n".
        "      - Confidence Modifier                   : %.2fx (Reason: %s)\n".
        "      => Observed Dispatch Threads (adjusted) : (%.2f Cores * %d SMT) * %.2f = %.1f\n".
        "   3. Effective Allowance and Decision\n".
        "      - Effective Dispatch Threads (Allowance) : %.1f\n".
        "         - Selected from                      : max(%.1f Theoretical, %.1f Observed) = %.1f\n".
        "         - Capped by                          : MaxCPU %.1f (%s)\n".
        "         - Gap vs Observed Allowance          : %+.1f threads (%+.1f%%)\n".
        "         - Gap vs Theoretical Allowance       : %+.1f threads (%+.1f%%)%s\n".
        "         - Method                             : min(%.1f Max LCPUs, max(%.1f Theoretical, %.1f Observed))",
        $evr_str,
        $vcpu_for_lpar_numeric, $curr_ent_numeric, (defined $evr_raw_num ? $evr_raw_num : 'N/A'),
        $evr_tier_note,
        $base_burst_factor*100, $evr_tier_rationale,
        $pool_constraint_multiplier, $pool_constraint_rationale,
        $curr_ent_numeric, $final_burst_allowance, $smt_used, $theoretical_lcpus,
        $base_burst_factor * 100 * $pool_constraint_multiplier, $base_burst_factor*100, $pool_constraint_multiplier,
        $p99w1_physc_val, $capacity_headroom*100,
        $headroom_confidence_modifier, $headroom_rationale,
        $p99w1_physc_val, $smt_used, $headroom_confidence_modifier, $confidence_adjusted_peak_lcpus,
        $effective_lcpus_for_pressure_calc,
        $theoretical_lcpus, $confidence_adjusted_peak_lcpus, $selected_allowance_lcpus,
        $absolute_ceiling_lcpus, $cap_status_str,
        $gap_vs_observed, $gap_vs_observed_pct,
        $gap_vs_theoretical, $gap_vs_theoretical_pct, $binding_theoretical,
        $absolute_ceiling_lcpus, $theoretical_lcpus, $confidence_adjusted_peak_lcpus
    );

    my $norm_p25_numeric = ($norm_runq_p25_str ne $na_str && $norm_runq_p25_str =~ /^-?[0-9.]+$/) ? ($norm_runq_p25_str + 0) : undef;
    my $norm_p75_numeric = ($norm_runq_p75_str ne $na_str && $norm_runq_p75_str =~ /^-?[0-9.]+$/) ? ($norm_runq_p75_str + 0) : undef;
    my $normrunq_iqrc_val = undef;
    if (defined $norm_p25_numeric && defined $norm_p50_numeric && defined $norm_p75_numeric)
    {
        my $p50_denominator_for_iqrc = (abs($norm_p50_numeric) > $MIN_P50_DENOMINATOR_FOR_VOLATILITY)
            ? $norm_p50_numeric
            : (($norm_p50_numeric >= 0) ? $MIN_P50_DENOMINATOR_FOR_VOLATILITY : -$MIN_P50_DENOMINATOR_FOR_VOLATILITY);
        if (abs($p50_denominator_for_iqrc) > $FLOAT_EPSILON)
        {
            $normrunq_iqrc_val = ($norm_p75_numeric - $norm_p25_numeric) / $p50_denominator_for_iqrc;
        }
        elsif (abs($norm_p75_numeric - $norm_p25_numeric) < $FLOAT_EPSILON) { $normrunq_iqrc_val = 0.0; }
        else { $normrunq_iqrc_val = 999.0; }
    }
    $debug_info{'NormRunQ_P25_Val'} = (defined $norm_p25_numeric) ? sprintf("%.2f", $norm_p25_numeric) : $na_str;
    $debug_info{'NormRunQ_P50_Val'} = (defined $norm_p50_numeric) ? sprintf("%.2f", $norm_p50_numeric) : $na_str;
    $debug_info{'NormRunQ_P75_Val'} = (defined $norm_p75_numeric) ? sprintf("%.2f", $norm_p75_numeric) : $na_str;
    $debug_info{'EffectiveLCPUsForPressure'} = sprintf("%.1f", $effective_lcpus_for_pressure_calc);
    $debug_info{'EffectiveLCPUsAtBase'} = sprintf("%.4f", $effective_lcpus_for_pressure_calc);
    my $runq_pressure_p_val = ($effective_lcpus_for_pressure_calc > 0) ? (($abs_runq_p_numeric // 0) / $effective_lcpus_for_pressure_calc) : 0;
    $debug_info{'NormRunQ_IQRC_Val'} = (defined $normrunq_iqrc_val) ? sprintf("%.3f", $normrunq_iqrc_val) : $na_str;
    $debug_info{'RunQPressure_P90_Val'} = sprintf("%.4f", $runq_pressure_p_val);
    $debug_info{'PressureBasisRationale'} = $pressure_basis_rationale;
    $debug_info{'BurstAllowanceUsed'} = sprintf("%.2f%% (EVR-based, Pool-adjusted)", $final_burst_allowance * 100);
    $debug_info{'SmallEntitlementHandlerActive'} = ($curr_ent_numeric < 1.0 && $max_cpu_for_lpar_numeric > $curr_ent_numeric) ? "Yes" : "No";

    # =========================
    # Section C: Strategic Efficiency / Downsizing (ALWAYS RUNS)
    # =========================
    my $efficiency_factor_numeric = 1.00;
    $debug_info{'DownsizingFactor'} = sprintf("%.2f", $efficiency_factor_numeric);
    my $base_adjusted_physc = (defined $eff_p_base_numeric ? $eff_p_base_numeric : 0) * $efficiency_factor_numeric;
    $debug_info{'DownsizedPhysC'} = sprintf("%.4f", $base_adjusted_physc);
    $debug_info{'DownsizingReason'} = "Default (No CPU downsizing applied or eligible).";
    if (defined $norm_p50_numeric && defined $norm_p90_numeric && defined $abs_runq_p_numeric && $smt_used > 0 && defined $eff_p_base_numeric)
    {
        $debug_info{'EffCondNormP50Met'} = ($norm_p50_numeric < $NORM_P50_THRESHOLD_FOR_EFFICIENCY_CONSIDERATION);
        my $effective_p50_for_volatility = ($norm_p50_numeric > 0.0001) ? max($norm_p50_numeric, $MIN_P50_DENOMINATOR_FOR_VOLATILITY) : $MIN_P50_DENOMINATOR_FOR_VOLATILITY;
        my $volatility_ratio = (defined $norm_p90_numeric && $effective_p50_for_volatility > 0.0001) ? ($norm_p90_numeric / $effective_p50_for_volatility) : 1.0;
        $debug_info{'EffVolatilityRatio'} = sprintf("%.2f", $volatility_ratio);
        $debug_info{'EffCondVolatilityMet'} = ($volatility_ratio < $VOLATILITY_CAUTION_THRESHOLD);
        if (!$debug_info{'EffCondVolatilityMet'}) {
            $debug_info{'DownsizingReason'} = sprintf("Skipped CPU Downsizing: Workload volatile (NormP90/P50 ratio %.2f >= %.2f). No analytical reduction.", $volatility_ratio, $VOLATILITY_CAUTION_THRESHOLD);
        }
        elsif (!$debug_info{'EffCondNormP50Met'}) {
            $debug_info{'DownsizingReason'} = sprintf("Skipped CPU Downsizing: NormRunQ P50 (%.2f) not below threshold (%.2f) for efficiency consideration. No analytical reduction.", $norm_p50_numeric, $NORM_P50_THRESHOLD_FOR_EFFICIENCY_CONSIDERATION);
        }
        else {
            my $target_norm_runq_eff_calc;
            my $adaptive_target_reason;
            my $tier_to_check = $final_tier;
            if (exists $custom_runq_targets{$tier_to_check}) {
                $target_norm_runq_eff_calc = $custom_runq_targets{$tier_to_check};
                $adaptive_target_reason = "User-defined target for tier '$tier_to_check' from nfit.profiles.cfg";
            } elsif (exists $DEFAULT_RUNQ_TARGETS{$tier_to_check}) {
                $target_norm_runq_eff_calc = $DEFAULT_RUNQ_TARGETS{$tier_to_check};
                $adaptive_target_reason = "Built-in default target for tier '$tier_to_check'";
            } else {
                my ($pattern) = ($final_tier =~ /^([OGBP])/);
                my $base_tier = $pattern // 'G';
                $target_norm_runq_eff_calc = $DEFAULT_RUNQ_TARGETS{$base_tier} // $DEFAULT_TARGET_NORM_RUNQ_FOR_EFFICIENCY_CALC;
                $adaptive_target_reason = "Fallback target for base pattern '$base_tier'";
            }
            $debug_info{'AdaptiveTargetNormRunQReason'} = "$adaptive_target_reason: ${target_norm_runq_eff_calc}";
            my $p_efficient_target_raw = ($smt_used * $target_norm_runq_eff_calc > 0.0001) ? ($abs_runq_p_numeric / ($smt_used * $target_norm_runq_eff_calc)) : $eff_p_base_numeric + 1;
            $debug_info{'EffPEfficientTargetRaw'} = sprintf("%.4f", $p_efficient_target_raw);
            $debug_info{'EffTargetNormRunQ'} = sprintf("%.2f", $target_norm_runq_eff_calc);
            my $base_physc_weight = $BLEND_WEIGHT_BASE_DEFAULT_LOW_P50;
            my $efficient_target_weight = 1.0 - $base_physc_weight;
            my $blending_details_str = sprintf("Default low P50 blend (%.0f%% Base / %.0f%% Target).", $base_physc_weight*100, $efficient_target_weight*100);
            if ($norm_p50_numeric < $NORM_P50_LOW_THRESH_FOR_BLEND1) {
                $base_physc_weight = $BLEND_WEIGHT_BASE_FOR_LOW_P50_1;
                $efficient_target_weight = 1.0 - $base_physc_weight;
                $blending_details_str = sprintf("NormP50 (%.2f) < %.2f, using more aggressive blend (%.0f%% Base / %.0f%% Target).", $norm_p50_numeric, $NORM_P50_LOW_THRESH_FOR_BLEND1, $base_physc_weight*100, $efficient_target_weight*100);
            } elsif ($norm_p50_numeric < $NORM_P50_MODERATE_THRESH_FOR_BLEND2) {
                $base_physc_weight = $BLEND_WEIGHT_BASE_FOR_LOW_P50_2;
                $efficient_target_weight = 1.0 - $base_physc_weight;
                $blending_details_str = sprintf("NormP50 (%.2f) < %.2f, using moderate blend (%.0f%% Base / %.0f%% Target).", $norm_p50_numeric, $NORM_P50_MODERATE_THRESH_FOR_BLEND2, $base_physc_weight*100, $efficient_target_weight*100);
            }
            $debug_info{'EffBlendWeightBase'}   = sprintf("%.2f", $base_physc_weight);
            $debug_info{'EffBlendWeightTarget'} = sprintf("%.2f", $efficient_target_weight);
            $debug_info{'EffBlendReason'}       = $blending_details_str;
            my $blended_efficient_target = ($eff_p_base_numeric * $base_physc_weight) + ($p_efficient_target_raw * $efficient_target_weight);
            $debug_info{'EffPEfficientTarget'}  = sprintf("%.4f", $blended_efficient_target);
            $debug_info{'EffComparisonBaseVsTargetMet'} = ($eff_p_base_numeric > $blended_efficient_target);
            if ($debug_info{'EffComparisonBaseVsTargetMet'})
            {
                my $potential_reduction_cores = $eff_p_base_numeric - $blended_efficient_target;
                $debug_info{'EffPotentialReduction'} = sprintf("%.4f", $potential_reduction_cores);
                $debug_info{'RunQ_Potential'} = sprintf("%.4f", -1 * $potential_reduction_cores);
                my $base_max_reduction_factor;
                my $size_aware_cap_reason;
                if ($eff_p_base_numeric < 1.0) {
                    $base_max_reduction_factor = 0.30;
                    $size_aware_cap_reason = sprintf("BasePhysC (%.2f) < 1.0, using %.0f%% cap", $eff_p_base_numeric, $base_max_reduction_factor * 100);
                } elsif ($eff_p_base_numeric < 4.0) {
                    $base_max_reduction_factor = 0.25;
                    $size_aware_cap_reason = sprintf("BasePhysC (%.2f) < 4.0, using %.0f%% cap", $eff_p_base_numeric, $base_max_reduction_factor * 100);
                } else {
                    $base_max_reduction_factor = 0.20;
                    $size_aware_cap_reason = sprintf("BasePhysC (%.2f) >= 4.0, using %.0f%% cap", $eff_p_base_numeric, $base_max_reduction_factor * 100);
                }
                my $current_max_reduction_perc_val = $base_max_reduction_factor;
                my $volatility_reason = "";
                if ($volatility_ratio > $VOLATILITY_MODERATE_HIGH_CAP_THRESH) {
                    $current_max_reduction_perc_val *= $REDUCTION_CAP_SCALE_FOR_MODERATE_HIGH_VOLATILITY;
                    $volatility_reason = sprintf(" (scaled by %.2fx for high volatility)", $REDUCTION_CAP_SCALE_FOR_MODERATE_HIGH_VOLATILITY);
                } elsif ($volatility_ratio > $VOLATILITY_MODERATE_MEDIUM_CAP_THRESH) {
                    $current_max_reduction_perc_val *= $REDUCTION_CAP_SCALE_FOR_MODERATE_MEDIUM_VOLATILITY;
                    $volatility_reason = sprintf(" (scaled by %.2fx for medium volatility)", $REDUCTION_CAP_SCALE_FOR_MODERATE_MEDIUM_VOLATILITY);
                } elsif ($volatility_ratio > $VOLATILITY_MODERATE_LOW_CAP_THRESH) {
                    $current_max_reduction_perc_val *= $REDUCTION_CAP_SCALE_FOR_MODERATE_VOLATILITY;
                    $volatility_reason = sprintf(" (scaled by %.2fx for moderate volatility)", $REDUCTION_CAP_SCALE_FOR_MODERATE_VOLATILITY);
                }
                $debug_info{'EffReductionCapReason'} = $size_aware_cap_reason . $volatility_reason;
                $debug_info{'EffMaxAllowableReductionPerc'} = $current_max_reduction_perc_val * 100;
                my $max_allowable_reduction_cores = $eff_p_base_numeric * $current_max_reduction_perc_val;
                $debug_info{'EffMaxAllowableReductionCores'} = sprintf("%.4f", $max_allowable_reduction_cores);
                my $actual_reduction_cores = min($potential_reduction_cores, $max_allowable_reduction_cores);
                $actual_reduction_cores = max(0, $actual_reduction_cores);
                $debug_info{'EffActualReductionCores'} = sprintf("%.4f", $actual_reduction_cores);
                $debug_info{'RunQ_Strategic'} = sprintf("%.4f", -1 * ($actual_reduction_cores+0));
                if ($actual_reduction_cores > 0.0001) {
                    my $new_physc_after_reduction = $eff_p_base_numeric - $actual_reduction_cores;
                    $efficiency_factor_numeric = ($eff_p_base_numeric > 0.0001) ? ($new_physc_after_reduction / $eff_p_base_numeric) : 1.00;
                    my $min_expected_eff_factor = 1 - $current_max_reduction_perc_val;
                    if ($efficiency_factor_numeric < ($min_expected_eff_factor - 0.001) ) { $efficiency_factor_numeric = $min_expected_eff_factor; }
                    $efficiency_factor_numeric = 1.00 if $efficiency_factor_numeric > 1.00;
                    $efficiency_factor_numeric = max(0, $efficiency_factor_numeric);
                    $debug_info{'EffCalculatedFactor'}   = sprintf("%.4f", $efficiency_factor_numeric);
                    $debug_info{'EffFinalFactorApplied'} = sprintf("%.2f",  $efficiency_factor_numeric);
                    $debug_info{'DownsizingReason'}      = sprintf("Analytical CPU Downsizing (using blended target & dynamic cap): Reduction of %.4f cores applied.", $actual_reduction_cores);
                } else {
                    $debug_info{'EffFinalFactorApplied'} = "1.00";
                    $debug_info{'EffCalculatedFactor'}   = "1.0000";
                    $debug_info{'DownsizingReason'} = sprintf("Analytical CPU Downsizing (using blended target & dynamic cap): Base_PhysC %.4f, Blended_Target %.4f. Calculated reduction (%.4f) negligible or zero. No adjustment from this path.", $eff_p_base_numeric, $blended_efficient_target, $actual_reduction_cores // 0.0);
                }
            } else {
                $debug_info{'EffFinalFactorApplied'} = "1.00";
                $debug_info{'EffCalculatedFactor'}   = "1.0000";
                $debug_info{'DownsizingReason'} = sprintf("Analytical CPU Downsizing: Base_PhysC %.4f not greater than Blended_Efficient_Target_PhysC %.4f. No reduction. Blending Reason: %s", $eff_p_base_numeric, $blended_efficient_target, $blending_details_str);
                $debug_info{'RunQ_Potential'} = "0.0000";
            }
        }
    } else {
        $debug_info{'DownsizingReason'} = "Key metrics (NormP50/P90, AbsRunQ) N/A for full analytical CPU downsizing check. No efficiency adjustment applied.";
    }

    my $skip_downsizing_reason;

    # Mission-critical peak-coverage policy: P-tier sizing never applies efficiency downsizing.
    # The workload must be able to service observed peaks without queueing, so any reduction
    # based on "efficiency" (spin/cache-warm overhead) is intentionally suppressed.
    if (defined $profile_name && $profile_name eq $MISSION_CRITICAL_PROFILE_NAME) {
        $skip_downsizing_reason = "P-tier profile: CPU downsizing permanently disabled.";
    }

    if (($p99w1_overall_vm_has_abs_runq_pressure && $p99w1_overall_vm_has_abs_runq_pressure) || ($p99w1_overall_vm_has_norm_runq_pressure && $p99w1_overall_vm_has_norm_runq_pressure)) {
        $skip_downsizing_reason = sprintf("VM's %s profile shows RunQ pressure.", $MANDATORY_PEAK_PROFILE_FOR_HINT);
    } elsif ($profile_runq_behavior_setting eq 'additive_only') {
       $skip_downsizing_reason = "Profile runq_behavior=additive_only.";
    } elsif (defined $curr_ent_numeric && $curr_ent_numeric > 0 && defined $eff_p_base_numeric && ($eff_p_base_numeric > $curr_ent_numeric)) {

        # --- Enhanced STD Pattern Detection with Multiple Confidence Boosters ---

        # 1. Dynamically determine the source profile for PhysC stability metrics.
        #    Priority: User TIER > AutoTier > Fallback to 'G'.
        my $user_tier_override_std = $vm_map_ref->{Hinting}{FinalTierForVM} // "";
        my $auto_tier_std = $vm_map_ref->{Hinting}{AutoTier} // "G";

        my $pattern_source_std = ($user_tier_override_std ne "") ? $user_tier_override_std : $auto_tier_std;
        my ($pattern) = ($pattern_source_std =~ /^([A-Z])/);
        $pattern //= 'G'; # Default to 'G' if regex fails

        my %pattern_to_profile_map = ('O' => 'O3-95W15', 'B' => 'B3-95W15', 'G' => 'G3-95W15', 'P' => 'G3-95W15');
        my $source_profile_base = $pattern_to_profile_map{$pattern} // 'G3-95W15';

        my $p50_profile_name = $source_profile_base; $p50_profile_name =~ s/-95W/-50W/;

        # 2. Look up PhysC P50 and P95 from the assimilation map's CoreResults.
        my $physc_p50 = $vm_map_ref->{CoreResults}{ProfileValues}{$p50_profile_name};
        my $physc_p95 = $vm_map_ref->{CoreResults}{ProfileValues}{$source_profile_base};

        # 3. Calculate PhysC stability using the P95/P50 ratio.
        my $physc_stability_ratio = (defined $physc_p50 && looks_like_number($physc_p50) && $physc_p50 > 0.01) ? ((looks_like_number($physc_p95) ? $physc_p95 : 0) / $physc_p50) : 999;
        my $is_physc_stable = ($physc_stability_ratio < (1.0 + $STD_PHYSC_STABILITY_THRESH));

        # 4. Core STD pattern detection (all 3 conditions must be met).
        my $is_std_pattern = (defined $norm_p90_numeric && $norm_p90_numeric < $STD_NORM_P90_THRESH) && (defined $normrunq_iqrc_val && $normrunq_iqrc_val < $STD_IQRC_THRESH) && $is_physc_stable;
        $debug_info{'STDConfidenceChecks'} = sprintf("NormP90<%.1f(%.2f), IQRC<%.1f(%.3f), PhysC_Stable(P95/P50)<%.2f(%.2f)", $STD_NORM_P90_THRESH, ($norm_p90_numeric // 0), $STD_IQRC_THRESH, ($normrunq_iqrc_val // 0), (1.0 + $STD_PHYSC_STABILITY_THRESH), $physc_stability_ratio);

        # ------------------------------------------------------------------
        # Downsizing guardrail banding (AAE / PAE / IQRC)
        # ------------------------------------------------------------------
        my $aae = (defined $debug_info{'PhysC_Daily_AAE'} && looks_like_number($debug_info{'PhysC_Daily_AAE'})) ? ($debug_info{'PhysC_Daily_AAE'} + 0) : 999;
        my $pae = (defined $debug_info{'PhysC_Daily_PAE'} && looks_like_number($debug_info{'PhysC_Daily_PAE'})) ? ($debug_info{'PhysC_Daily_PAE'} + 0) : 999;
        my $iqrc = (defined $normrunq_iqrc_val && looks_like_number($normrunq_iqrc_val)) ? ($normrunq_iqrc_val + 0) : 999;

        my $downsize_guard_band = "RED";

        # Treat missing metrics as "unknown" => conservative.
        my $has_metrics = ($aae != 999 && $pae != 999 && $iqrc != 999);

        if ($has_metrics) {
            # 0) Volatility veto (IQRC too high => RED regardless)
            if ($iqrc > $DOWNSIZE_GUARD_AMBER_MAX_IQRC) {
                $downsize_guard_band = "RED";
            }
            else {
                # 1) GREEN: essentially no above-entitlement behaviour and stable
                if ($aae <= 0.0
                    && $pae <= $DOWNSIZE_GUARD_GREEN_MAX_PAE
                    && $iqrc <= $DOWNSIZE_GUARD_GREEN_MAX_IQRC) {
                    $downsize_guard_band = "GREEN";
                }
                else {
                    # 2) 2D boundary (AAE x PAE) with IQRC already bounded to <= AMBER max

                    # Hard RED: magnitude too large (independent of frequency)
                    if ($aae > $DOWNSIZE_GUARD_RED_HARD_AAE) {
                        $downsize_guard_band = "RED";
                    }
                    else {
                        # Frequent-but-shallow AMBER: high PAE allowed if AAE is shallow
                        if ($pae > $DOWNSIZE_GUARD_RED_FREQUENT_PAE) {
                            if ($aae <= $DOWNSIZE_GUARD_AMBER_SHALLOW_AAE) {
                                $downsize_guard_band = "AMBER";
                            } else {
                                $downsize_guard_band = "RED";
                            }
                        }
                        else {
                            # Otherwise, fall back to your existing AMBER envelope
                            if ($aae <= $DOWNSIZE_GUARD_AMBER_MAX_AAE
                                && $pae <= $DOWNSIZE_GUARD_AMBER_MAX_PAE
                                && $iqrc <= $DOWNSIZE_GUARD_AMBER_MAX_IQRC) {
                                $downsize_guard_band = "AMBER";
                            } else {
                                $downsize_guard_band = "RED";
                            }
                        }
                    }
                }
            }
        }

        $debug_info{'DownsizeGuardBand'} = $downsize_guard_band;

        # Build a concise, grep-friendly guardrail summary for rationale consumers.
        # Note: This describes whether tactical downsizing is eligible, independent of tier policy.
        my $ds_aae_disp  = (defined $debug_info{'PhysC_Daily_AAE'} && looks_like_number($debug_info{'PhysC_Daily_AAE'})) ? sprintf("%.3f", $aae) : 'N/A';
        my $ds_pae_disp  = (defined $debug_info{'PhysC_Daily_PAE'} && looks_like_number($debug_info{'PhysC_Daily_PAE'})) ? sprintf("%.3f", $pae) : 'N/A';
        my $ds_iqrc_disp = (defined $normrunq_iqrc_val && looks_like_number($normrunq_iqrc_val)) ? sprintf("%.3f", $iqrc) : 'N/A';

        my @ds_reasons;
        if ($downsize_guard_band eq 'GREEN') {
            push @ds_reasons, sprintf("AAE<=0, PAE<=%.2f, IQRC<=%.2f",
                $DOWNSIZE_GUARD_GREEN_MAX_PAE, $DOWNSIZE_GUARD_GREEN_MAX_IQRC);
        } elsif ($downsize_guard_band eq 'AMBER') {
            push @ds_reasons, sprintf("AAE<=%.2f, PAE<=%.2f, IQRC<=%.2f, cap_scale=%.2f",
                $DOWNSIZE_GUARD_AMBER_MAX_AAE, $DOWNSIZE_GUARD_AMBER_MAX_PAE,
                $DOWNSIZE_GUARD_AMBER_MAX_IQRC, $DOWNSIZE_GUARD_AMBER_CAP_SCALE);
        } else {
            push @ds_reasons, sprintf("AAE=%s (max %.2f)",  $ds_aae_disp,  $DOWNSIZE_GUARD_AMBER_MAX_AAE)  if ($ds_aae_disp ne 'N/A'  && $aae  > $DOWNSIZE_GUARD_AMBER_MAX_AAE);
            push @ds_reasons, sprintf("PAE=%s (max %.2f)",  $ds_pae_disp,  $DOWNSIZE_GUARD_AMBER_MAX_PAE)  if ($ds_pae_disp ne 'N/A'  && $pae  > $DOWNSIZE_GUARD_AMBER_MAX_PAE);
            push @ds_reasons, sprintf("IQRC=%s (max %.2f)", $ds_iqrc_disp, $DOWNSIZE_GUARD_AMBER_MAX_IQRC) if ($ds_iqrc_disp ne 'N/A' && $iqrc > $DOWNSIZE_GUARD_AMBER_MAX_IQRC);
        }

        $debug_info{'DownsizeGuardSummary'} = sprintf(
            "Band=%s; AAE=%s; PAE=%s; IQRC=%s; STD=%s%s%s",
            $downsize_guard_band,
            $ds_aae_disp, $ds_pae_disp, $ds_iqrc_disp,
            ($is_std_pattern ? 'True' : 'False'),
            (@ds_reasons ? "; " : ""),
            (@ds_reasons ? join("; ", @ds_reasons) : "")
        );

        # If pattern matched: This is a high-confidence inefficient workload.
        if ($is_std_pattern) {
            my $actual_reduction_num = (defined $debug_info{'EffActualReductionCores'} && looks_like_number($debug_info{'EffActualReductionCores'})) ? ($debug_info{'EffActualReductionCores'} + 0) : 0;

            # 5. Impact-based dampening.
            my $spinning_thread_impact_ratio = 1.0 / max(1, $effective_lcpus_for_pressure_calc);
            my $graduated_dampening_factor;
            my $tier_rationale;

            if ($spinning_thread_impact_ratio >= 0.20) {
                # High impact
                $graduated_dampening_factor = 0.85;
                $tier_rationale = sprintf("High Impact (%.1f%% of VM capacity)", $spinning_thread_impact_ratio * 100);
            } elsif ($spinning_thread_impact_ratio >= 0.10) {
                # Medium impact
                $graduated_dampening_factor = 0.70;
                $tier_rationale = sprintf("Medium Impact (%.1f%% of VM capacity)", $spinning_thread_impact_ratio * 100);
            } else {
                $graduated_dampening_factor = 0.30;
                $tier_rationale = sprintf("Low Impact (%.1f%% of VM capacity)", $spinning_thread_impact_ratio * 100);
            }
            my $dampened_reduction = $actual_reduction_num * $graduated_dampening_factor;
            $debug_info{'RunQ_Strategic'} = sprintf("%.4f", -$dampened_reduction);
            $skip_downsizing_reason = "Single-Threaded Dominant (STD) Workload Pattern Detected. Tactical downsizing skipped; updating strategic RunQ_Strategic recommendation.";
            $debug_info{'STDDampeningTier'} = sprintf("%s, L_eff=%.1f", $tier_rationale, $effective_lcpus_for_pressure_calc);
            $debug_info{'STDFinalDampeningFactor'} = $graduated_dampening_factor;
        } else {
            # PATTERN NOT MATCHED: Use AAE/PAE/IQRC banding to decide whether tactical downsizing may proceed.
            my $band = $debug_info{'DownsizeGuardBand'} // "RED";

            if ($band eq 'RED') {
                # Conservative fallback: block tactical downsizing.
                my $pre_growth_str  = $debug_info{'PreGrowthBasePhysC'}  // 'N/A';
                my $post_growth_str = $debug_info{'PostGrowthBasePhysC'} // (defined $eff_p_base_numeric ? sprintf("%.4f",$eff_p_base_numeric) : 'N/A');
                $skip_downsizing_reason = sprintf(
                    "Downsizing blocked (RED band): Base PhysC (post-growth %.4f; pre-growth %s) > Entitlement (%.2f); AAE=%s, PAE=%s, IQRC=%s; workload does not match STD pattern.",
                    $post_growth_str, $pre_growth_str, $curr_ent_numeric,
                    ($debug_info{'PhysC_Daily_AAE'} // 'N/A'),
                    ($debug_info{'PhysC_Daily_PAE'} // 'N/A'),
                    (defined $normrunq_iqrc_val && looks_like_number($normrunq_iqrc_val) ? sprintf("%.3f", $normrunq_iqrc_val) : 'N/A')
                );
                $debug_info{'RunQ_Strategic'} = "0.0000";
            }
            elsif ($band eq 'AMBER') {
                # Allow downsizing, but dampen the analytical reduction for safety.
                my $actual_reduction_num = (defined $debug_info{'EffActualReductionCores'} && looks_like_number($debug_info{'EffActualReductionCores'}))
                    ? ($debug_info{'EffActualReductionCores'} + 0) : 0.0;

                if ($actual_reduction_num > 1e-6 && defined $eff_p_base_numeric && $eff_p_base_numeric > 1e-6) {
                    my $scaled_reduction = $actual_reduction_num * $DOWNSIZE_GUARD_AMBER_CAP_SCALE;
                    $scaled_reduction = max(0, $scaled_reduction);
                    $scaled_reduction = min($scaled_reduction, $actual_reduction_num);

                    $debug_info{'EffActualReductionCores'} = sprintf("%.4f", $scaled_reduction);
                    my $new_physc_after_reduction = $eff_p_base_numeric - $scaled_reduction;
                    $efficiency_factor_numeric = $new_physc_after_reduction / $eff_p_base_numeric;

                    $efficiency_factor_numeric = 1.00 if $efficiency_factor_numeric > 1.00;
                    $efficiency_factor_numeric = max(0, $efficiency_factor_numeric);

                    $debug_info{'RunQ_Potential'} = sprintf("%.4f", -$scaled_reduction);
                }

                # Do NOT set skip_downsizing_reason -> downsizing proceeds (with dampened factor if applicable).
            }
            else {
                # GREEN band: allow tactical downsizing to proceed normally.
                # Do NOT set skip_downsizing_reason.
            }
        }
    }
    if (defined $skip_downsizing_reason) {
        $base_adjusted_physc = $eff_p_base_numeric;
        $efficiency_factor_numeric = 1.0;
        $debug_info{'DownsizingReason'} = "Skipped Applying Downsizing: " . $skip_downsizing_reason;
    } else {
        $base_adjusted_physc = $eff_p_base_numeric * $efficiency_factor_numeric;
        if ($efficiency_factor_numeric < 1 - 1e-4) {
            $debug_info{'DownsizingReason'} = "Analytical CPU Downsizing applied.";
        }
    }

    # Apply GrowthAdj AFTER downsizing (ordering fix).
    # This prevents GrowthAdj from disqualifying downsizing gates whilst preserving
    # growth-inclusive values for subsequent additive logic and final recommendation.
    my $downsized_pre_growth = $base_adjusted_physc;
    if (defined $growth_adj_numeric && looks_like_number($growth_adj_numeric)) {
        $base_adjusted_physc = $downsized_pre_growth + $growth_adj_numeric;
    }
    $debug_info{'DownsizedPhysC_PreGrowth'} = sprintf("%.4f", $downsized_pre_growth);


    # Now, definitively set the final factor that was actually used.
    $debug_info{'DownsizingFactor'} = sprintf("%.2f", $efficiency_factor_numeric);
    $debug_info{'DownsizedPhysC'} = sprintf("%.4f", $base_adjusted_physc);

    # =========================
    # Section D: Tactical Additive CPU
    # =========================
    my $tier_scaling_factor = 1.00;
    # Default to Tier 3 (baseline, 1.00x scaling) if the tier cannot be determined.
    # This is a safe fallback that prevents uninitialised value warnings.
    my $tier_num_for_scaling = 3;
    my $is_runq_pressure = ($runq_pressure_p_val > $adaptive_runq_saturation_thresh);
    $debug_info{'IsRunQPressure'} = $is_runq_pressure ? "True" : "False";
    my $is_workload_pressure_calc = 0;
    my $workload_pressure_reason_str_calc = "Workload pressure conditions not met or inputs N/A.";
    my $min_absrunq_for_workload_pressure_check = $smt_used > 0 ? $smt_used : 1.0;
    if (defined $norm_p90_numeric) {
        if ($norm_p90_numeric > $WORKLOAD_PRESSURE_NORM_P90_TRIGGER_THRESHOLD) {
            if (defined $abs_runq_p_numeric && $abs_runq_p_numeric >= $min_absrunq_for_workload_pressure_check) {
                $is_workload_pressure_calc = 1;
                $workload_pressure_reason_str_calc = sprintf("NormRunQ P90 (%.2f) > threshold (%.2f) AND AbsRunQ (%s=%.2f) >= SMT-based min threshold (%.2f)", $norm_p90_numeric, $WORKLOAD_PRESSURE_NORM_P90_TRIGGER_THRESHOLD, $debug_info{AbsRunQKeyUsed}, $abs_runq_p_numeric, $min_absrunq_for_workload_pressure_check);
            } else {
                $workload_pressure_reason_str_calc = sprintf("NormRunQ P90 (%.2f) > threshold (%.2f), BUT AbsRunQ (%s=%.2f) < SMT-based min threshold (%.2f). Workload Pressure NOT flagged.", $norm_p90_numeric, $WORKLOAD_PRESSURE_NORM_P90_TRIGGER_THRESHOLD, $debug_info{AbsRunQKeyUsed}, $abs_runq_p_numeric // $na_str, $min_absrunq_for_workload_pressure_check);
            }
        } else {
            $workload_pressure_reason_str_calc = sprintf("NormRunQ P90 (%.2f) <= threshold (%.2f)", $norm_p90_numeric, $WORKLOAD_PRESSURE_NORM_P90_TRIGGER_THRESHOLD);
        }
    } else {
        $workload_pressure_reason_str_calc = "NormRunQ P90 N/A";
    }
    $debug_info{'IsWorkloadPressure'} = $is_workload_pressure_calc ? "True" : "False";
    $debug_info{'WorkloadPressureReason'} = $workload_pressure_reason_str_calc;


    # --- RunQ Design Philosophy ---
    # These three signals provide complementary views of CPU pressure at different
    # planning horizons, cleanly separating statistical measurement from business
    # policy. All values are ADDITIVE (delta cores), not absolute totals.
    #
    # RunQ_Potential: Pure Statistical Demand (Measurement)
    #   - Formula: excess_threads / SMT
    #   - No caps, no policy, no constraints applied
    #   - Answers: "What is the raw, unconstrained CPU demand that is not being met?"
    #   - Purpose: Baseline workload signal for trend analysis and anomaly detection
    #   - Time Horizon: Instantaneous snapshot of current pressure
    #
    # RunQ_Strategic: Architecturally Feasible Demand (Measurement)
    #   - Formula: min(RunQ_Potential, MaxCPU - BasePhysC)
    #   - Capped ONLY by available architectural headroom before hitting MaxCPU
    #   - No entitlement constraints, no tier policy, no volatility dampening
    #   - Answers: "What is the maximum demand that could be met within the VM's
    #              current architectural limits?"
    #   - Purpose: Identify VMs that are architecturally constrained (hitting MaxCPU)
    #   - Time Horizon: Quarterly planning cycle (3-6 months)
    #   - Diagnostic: If Tactical > Strategic, MaxCPU must be raised to implement
    #                 the recommendation
    #
    # RunQ_Tactical: Actionable Monthly Increment (Recommendation)
    #   - Formula: Raw demand passed through all safety gates and policy adjustments
    #   - Applies: EntCap (tier-aware), volatility factor, pool constraints,
    #              tier scaling, final adaptive safety cap
    #   - Answers: "What is the single, safe, incremental change I should make
    #              this month to begin addressing the pressure?"
    #   - Purpose: Safe, implementable monthly entitlement adjustment
    #   - Time Horizon: Monthly operational cycle (30 days)
    #
    # CRITICAL DESIGN PRINCIPLES:
    #   1. Potential and Strategic are MEASUREMENTS (no business policy)
    #   2. Tactical is a RECOMMENDATION (all business policy applied)
    #   3. Never contaminate Potential/Strategic with tier classifications or
    #      operational constraints (entitlement) - these are statistical signals
    #   4. All three signals are additive deltas for direct comparability
    #   5. The relationship Tactical â‰¤ Strategic â‰¤ Potential should hold under
    #      normal conditions (violations indicate architectural constraints)
    # =============================================================================

    my $apply_additive_logic = ($is_runq_pressure || $is_workload_pressure_calc);
    my $additive_cpu = 0.0;
    my $raw_additive_cpu = 0.0;
    my $max_additive_cap_sliding = 0.0;
    my $capped_raw_additive_val = 0.0;
    my $volatility_confidence_factor = 1.0;
    my $pool_confidence_factor = 1.0;
    my $excess_threads = 0;

    # The additive logic path is only entered if significant pressure has been detected.
    if ($apply_additive_logic && defined $smt_used && $smt_used > 0)
    {
        # Calculate how many threads are waiting above a tolerated ceiling.
        # The tolerance factor allows for a healthy, non-problematic queue before recommending upsizing.

        # --- Tier-Aware Tolerance Factor ---
        # Select the tolerance for calculating "excess threads" based on the VM's tier.
        # Performance-critical tiers have a much lower tolerance for queuing.
        my $tolerance_factor = $RUNQ_ADDITIVE_TOLERANCE_FACTOR; # Default to 1.80

        if ($final_tier eq 'P') {
            # Extreme Performance (e.g., VIO Servers) - virtually no tolerance for queuing.
            $tolerance_factor = 1.10;
        } elsif ($final_tier =~ /^O1$/) {
            # High Performance (e.g., Core OLTP DBs) - very low tolerance.
            $tolerance_factor = 1.25;
        } elsif ($final_tier =~ /^O2$|^G1$|^G2$/) {
            # Balanced Performance (e.g., Web Apps, General DBs) - moderate tolerance.
            $tolerance_factor = 1.50;
        }
        # Efficiency Tiers (G3, G4, B1-B4) will use the default 1.80.

        my $capacity_threshold_for_excess = $tolerance_factor * $effective_lcpus_for_pressure_calc;

        # Trigger remains tied to the tolerance ceiling (pressure boundary), but the additive magnitude
        # should aim for a safer target below that ceiling.
        #
        # - Tolerance ceiling: the maximum "acceptable" queue before action is required.
        # - Target ceiling:    the desired operating point (headroom), below the tolerance ceiling.
        my $capacity_threshold_target = $capacity_threshold_for_excess * $RUNQ_TARGET_HEADROOM_FACTOR;

        $excess_threads = (defined $abs_runq_p_numeric ? $abs_runq_p_numeric : 0) - $capacity_threshold_target;
        if ((defined $abs_runq_p_numeric ? $abs_runq_p_numeric : 0) > $capacity_threshold_for_excess && $excess_threads > 0)
        {
            # Convert the excess threads into a raw CPU core requirement. This is the unconstrained demand.
            $debug_info{'ExcessThreads'} = sprintf(
                "\n         1. Observed Run Queue (%s)          : %.2f threads\n".
                "         2. Tolerance Ceiling               : %.1f LCPUs * %.2f (Tolerance Factor) = %.2f threads\n".
                "         3. Target Ceiling (Headroom)        : %.2f * %.2f (Target Factor) = %.2f threads\n".
                "         => Excess Threads to Service (to target): %.2f - %.2f = %.2f",
                $debug_info{AbsRunQKeyUsed},
                ($abs_runq_p_numeric // 0.0),
                $effective_lcpus_for_pressure_calc, $tolerance_factor, $capacity_threshold_for_excess,
                $capacity_threshold_for_excess, $RUNQ_TARGET_HEADROOM_FACTOR, $capacity_threshold_target,
                ($abs_runq_p_numeric // 0.0), $capacity_threshold_target, $excess_threads
            );
            $raw_additive_cpu = $excess_threads / $smt_used if $smt_used > 0;

            # Determine the business policy scaling factor based on the VM's final tier.
            if (defined $final_tier && $final_tier =~ /(\d)$/) {
                $tier_num_for_scaling = $1;
            } elsif (defined $final_tier && $final_tier eq 'P') {
                $tier_num_for_scaling = 1;
            }

            if (defined $tier_num_for_scaling) {
                my %tier_upsizing_factors = (
                    1 => 1.30, 2 => 1.15, 3 => 1.00, 4 => 0.95
                );
                $tier_scaling_factor = $tier_upsizing_factors{$tier_num_for_scaling} // 1.00;
            }

            # --- Apply the Tier-Aware Entitlement-Based Cap (EntCap) ---
            # This is the first major safety gate. It limits the upsizing recommendation to a proportion
            # of the VM's current size, enforcing incremental change. Performance tiers get more aggressive caps.
            # IMPORTANT: This cap is applied ONLY to the TACTICAL path.
            my ($max_add_abs1, $max_add_perc2, $max_add_perc3, $max_add_perc_else);

            # Determine which tier group the VM falls into
            if ($final_tier =~ /^[PO]1$|^[PO]2$|^G1$|^G2$/) {
                # Performance Tiers (Aggressive Response)
                $max_add_abs1      = 2.0;  # Allow up to +2.0 cores for small critical VMs
                $max_add_perc2     = 1.50; # 150% of Ent
                $max_add_perc3     = 1.00; # 100% of Ent
                $max_add_perc_else = 0.75; # 75% of Ent
            } else {
                # Efficiency Tiers (Conservative, Original Logic)
                $max_add_abs1      = 1.0;  # 1.0 core
                $max_add_perc2     = 1.00; # 100% of Ent
                $max_add_perc3     = 0.75; # 75% of Ent
                $max_add_perc_else = 0.50; # 50% of Ent
            }

            if ($curr_ent_numeric < 1.0)      { $max_additive_cap_sliding = $max_add_abs1; }
            elsif ($curr_ent_numeric < 2.0)   { $max_additive_cap_sliding = $curr_ent_numeric * $max_add_perc2; }
            elsif ($curr_ent_numeric < 4.0)   { $max_additive_cap_sliding = $curr_ent_numeric * $max_add_perc3; }
            else                               { $max_additive_cap_sliding = $curr_ent_numeric * $max_add_perc_else; }
            if ($curr_ent_numeric == 0 && $max_additive_cap_sliding == 0) { $max_additive_cap_sliding = $max_add_abs1; }

            # The EntCap is applied to the raw demand to begin the TACTICAL calculation path.
            $capped_raw_additive_val = min($raw_additive_cpu, $max_additive_cap_sliding);

            # --- Heuristic Checks for Pathological Workloads (HTW, DBW, etc.) ---
            # These checks identify workloads where simply adding CPU is inefficient or incorrect.
            $debug_info{'HotThreadWLDampeningApplied'} = "False"; $debug_info{'HotThreadWLConditionsString'} = "N/A";
            $debug_info{'HotThreadWLDynamicFactor'} = "N/A"; $debug_info{'HotThreadWLDampenedAdditiveFrom'} = "N/A";
            $debug_info{'HotThreadWLDampenedAdditiveTo'} = "N/A";
            if ($capped_raw_additive_val > $FLOAT_EPSILON)
            {
                my @htw_conditions_met_details; my $htw_condition_count = 0;
                if ($is_workload_pressure_calc) { push @htw_conditions_met_details, "HighNormP90"; $htw_condition_count++; }
                my $cond2_underutilized = 0;
                if (defined $base_physc && $base_physc > $FLOAT_EPSILON) {
                    my $underutilized_vs_ent = (defined $curr_ent_numeric && $curr_ent_numeric > $FLOAT_EPSILON && $base_physc < ($curr_ent_numeric * $HOT_THREAD_WL_ENT_FACTOR));
                    my $underutilized_vs_maxcpu = (defined $max_cpu_for_lpar_numeric && $max_cpu_for_lpar_numeric > $FLOAT_EPSILON && $base_physc < ($max_cpu_for_lpar_numeric * $HOT_THREAD_WL_MAXCPU_FACTOR));
                    if ($underutilized_vs_ent || $underutilized_vs_maxcpu) {
                        $cond2_underutilized = 1;
                        my $detail_ent_str = $underutilized_vs_ent ? sprintf("BaseP(%.2f)<Ent(%.2f)*%.1f", $base_physc, $curr_ent_numeric // 0, $HOT_THREAD_WL_ENT_FACTOR) : "";
                        my $detail_max_str = $underutilized_vs_maxcpu ? sprintf("BaseP(%.2f)<MaxP(%.2f)*%.1f", $base_physc, $max_cpu_for_lpar_numeric // 0, $HOT_THREAD_WL_MAXCPU_FACTOR) : "";
                        push @htw_conditions_met_details, "UnderutilizedCap(" . join(" or ", grep { $_ ne "" } $detail_ent_str, $detail_max_str) . ")";
                        $htw_condition_count++;
                    }
                }
                if (defined $norm_p50_numeric && $norm_p50_numeric > $HOT_THREAD_WL_HIGH_NORM_P50_THRESHOLD) { push @htw_conditions_met_details, sprintf("HighNormP50(%.2f>%.1f)", $norm_p50_numeric, $HOT_THREAD_WL_HIGH_NORM_P50_THRESHOLD); $htw_condition_count++; }
                if (!$is_runq_pressure && defined $debug_info{'RunQPressure_P90_Val'} && $debug_info{'RunQPressure_P90_Val'} ne $na_str) { push @htw_conditions_met_details, sprintf("NoLPARRunQSat(AbsPVal:%.2f<%.1f)", ($debug_info{'RunQPressure_P90_Val'} + 0), $RUNQ_PRESSURE_P90_SATURATION_THRESHOLD); $htw_condition_count++; }
                if (defined $normrunq_iqrc_val && abs($normrunq_iqrc_val) > $HOT_THREAD_WL_IQRC_THRESHOLD) { push @htw_conditions_met_details, sprintf("HighIQRC(%.2f>%.1f)", $normrunq_iqrc_val, $HOT_THREAD_WL_IQRC_THRESHOLD); $htw_condition_count++; }
                my $hot_thread_wl_conditions_met_str = @htw_conditions_met_details ? join("; ", @htw_conditions_met_details) : "No specific conditions met";
                if ($htw_condition_count >= $HOT_THREAD_WL_DETECTION_MIN_CONDITIONS_MET)
                {
                    my $util_ratio = (defined $max_cpu_for_lpar_numeric && $max_cpu_for_lpar_numeric > $FLOAT_EPSILON && defined $base_physc) ? ($base_physc / $max_cpu_for_lpar_numeric) : 1.0;
                    my $util_damp_multiplier = max(0.1, min(1.0, $util_ratio));
                    my $iqrc_damp_multiplier = (defined $normrunq_iqrc_val) ? min(1.0, 1.0 / (1.0 + abs($normrunq_iqrc_val))) : 1.0;
                    my $base_physc_severity_multiplier = (defined $base_physc && $base_physc > $FLOAT_EPSILON) ? min(1.0, $base_physc / 1.0) : 0.1;
                    my $calculated_dynamic_damp_factor = $HOT_THREAD_WL_BASE_DAMPENING_FACTOR * $util_damp_multiplier * $iqrc_damp_multiplier * $base_physc_severity_multiplier;
                    my $final_dynamic_dampening_factor_calc = max($HOT_THREAD_WL_MIN_DYNAMIC_DAMPENING, min($HOT_THREAD_WL_MAX_DYNAMIC_DAMPENING, $calculated_dynamic_damp_factor));
                    my $original_additive_val = $capped_raw_additive_val;
                    $capped_raw_additive_val *= $final_dynamic_dampening_factor_calc;
                    $debug_info{'HotThreadWLDampeningApplied'} = "True";
                    $debug_info{'HotThreadWLConditionsString'} = $hot_thread_wl_conditions_met_str;
                    $debug_info{'HotThreadWLDynamicFactor'} = sprintf("%.4f (Base:%.2f UtilM:%.2f IqrcM:%.2f SevM:%.2f -> RawCalc:%.4f)", $final_dynamic_dampening_factor_calc, $HOT_THREAD_WL_BASE_DAMPENING_FACTOR, $util_damp_multiplier, $iqrc_damp_multiplier, $base_physc_severity_multiplier, $calculated_dynamic_damp_factor);
                    $debug_info{'HotThreadWLDampenedAdditiveFrom'} = sprintf("%.4f", $original_additive_val);
                    $debug_info{'HotThreadWLDampenedAdditiveTo'}   = sprintf("%.4f", $capped_raw_additive_val);
                    $debug_info{'CappedRawAdditive'} = sprintf("%.4f", $capped_raw_additive_val);
                } else {
                    $debug_info{'HotThreadWLDampeningApplied'} = "False";
                    $debug_info{'HotThreadWLConditionsString'} = sprintf("Conditions not met for HTW dampening (%d/5 met: %s).", $htw_condition_count, $hot_thread_wl_conditions_met_str);
                }
            }
            if ($is_runq_pressure) {
                $volatility_confidence_factor = $RUNQ_PRESSURE_SATURATION_CONFIDENCE_FACTOR;
                $debug_info{'VoltFactorReason'} = sprintf("RunQPressure Saturation (Factor set to %.2f)", $RUNQ_PRESSURE_SATURATION_CONFIDENCE_FACTOR);
            } elsif ($is_workload_pressure_calc && defined $norm_p50_numeric && defined $norm_p90_numeric && $norm_p90_numeric > 0.01) {
                my $volatility_ratio_for_factor = ($norm_p50_numeric > 0.01) ? ($norm_p90_numeric / $norm_p50_numeric) : 999;
                if ($volatility_ratio_for_factor < $VOLATILITY_SPIKY_THRESHOLD) { $volatility_confidence_factor = $VOLATILITY_SPIKY_FACTOR; }
                elsif ($volatility_ratio_for_factor < $VOLATILITY_MODERATE_THRESHOLD){ $volatility_confidence_factor = $VOLATILITY_MODERATE_FACTOR; }
                else { $volatility_confidence_factor = 1.0; }
                $debug_info{'VoltFactorReason'} = sprintf("Calculated (NormRQ P90/P50 ratio %.2f for WorkloadPressure -> Factor %.2f)", $volatility_ratio_for_factor, $volatility_confidence_factor);
            } else {
                $debug_info{'VoltFactorReason'} = "Additive logic applied, but conditions for specific Volatility Factor adjustment not met (e.g., WorkloadPressure False or P50/P90 N/A for ratio). Using default factor.";
            }
            $additive_cpu = $capped_raw_additive_val * $volatility_confidence_factor;
            if ($is_in_non_default_pool && $additive_cpu > 0) {
                $pool_confidence_factor = $POOL_CONSTRAINT_CONFIDENCE_FACTOR;
                $additive_cpu *= $pool_confidence_factor;
            }

            # --- FINAL SIGNAL ASSIGNMENT ---
            my $sanity_bound = (defined $max_cpu_for_lpar_numeric && $max_cpu_for_lpar_numeric > 0) ? ($max_cpu_for_lpar_numeric * 2.0) : 10.0;

            # 1. Potential is the pure, unscaled, raw demand.
            $debug_info{'RunQ_Potential'} = $raw_additive_cpu;

            # 2. RunQ_Strategic: Architecturally feasible demand, capped only by MaxCPU.
            # formula: min(RunQ_Potential, MaxCPU - BasePhysC)
            my $strategic_val = $raw_additive_cpu;
            if ($max_cpu_for_lpar_numeric > 0) {
                my $available_headroom = $max_cpu_for_lpar_numeric - $base_physc;
                $strategic_val = min($raw_additive_cpu, $available_headroom);
            }
            $debug_info{'RunQ_Strategic'} = $strategic_val;

            # 3. Check for pathological state (Signal Inversion)
            if ($raw_additive_cpu > $sanity_bound) {
                # Handle pathological state (Signal Inversion)
                $debug_info{'RunQ_Potential'} = "ANOMALY (XRQ)";
                $debug_info{'RunQ_Strategic'} = $max_cpu_for_lpar_numeric;
                $debug_info{'ReasonForSignalInversion'} = sprintf(
                    "Raw Additive CPU (%.2f) exceeded sanity bound (%.2f), indicating a probable system backlog or malfunction.",
                    $raw_additive_cpu, $sanity_bound
                );
            }

      } else {
            my $capacity_threshold_target = $capacity_threshold_for_excess * $RUNQ_TARGET_HEADROOM_FACTOR;
            $debug_info{'ExcessThreads'} = sprintf(
                "0.0000 (AbsRunQ %s was %.2f; target ceiling %.2f = tolerance %.2f * target %.2f; basis=%s)",
                $debug_info{AbsRunQKeyUsed}, ($abs_runq_p_numeric // $na_str),
                $capacity_threshold_target, $capacity_threshold_for_excess, $RUNQ_TARGET_HEADROOM_FACTOR,
                ($pressure_basis_rationale =~ /Burst-Tolerant/ ? "Entitlement+Burst" : "MaxCPU")
            );
            $raw_additive_cpu = 0.0;
            $capped_raw_additive_val = 0.0;
            $additive_cpu = 0.0;
            $debug_info{'VoltFactorReason'} = "No excess threads, so no additive CPU calculated.";
            $debug_info{'RunQ_Strategic'} = "0.0000";
            $debug_info{'RunQ_Potential'} = "0.0000";
        }
    } else {
        $debug_info{'ExcessThreads'} = "N/A (Additive logic not applied as no significant pressure detected or missing inputs)";
        $debug_info{'VoltFactorReason'} = "Additive logic not applied.";
    }
    $debug_info{'RawAdditive'}       = sprintf("%.4f", $raw_additive_cpu);
    $debug_info{'MaxAdditiveCap'}    = sprintf("%.4f", $max_additive_cap_sliding);
    $debug_info{'CappedRawAdditive'} = sprintf("%.4f", $capped_raw_additive_val);
    $debug_info{'VoltFactor'}        = sprintf("%.2f", $volatility_confidence_factor);
    $debug_info{'PoolFactor'}        = sprintf("%.2f", $pool_confidence_factor);
    my $dbw_dampening_applied = "False";
    my $cond1_low_util = (defined $base_physc && $base_physc < ($curr_ent_numeric * $DBW_LOW_UTIL_FACTOR));
    my $dsr = 0;
    if (defined $base_physc && $base_physc > 0.01 && defined $abs_runq_p_numeric) { $dsr = $abs_runq_p_numeric / ($base_physc * $smt_used); }
    my $cond2_high_dsr = ($dsr > $DBW_DSR_THRESHOLD);
    my $cond3_high_median_pressure = (defined $norm_p50_numeric && $norm_p50_numeric > $DBW_MEDIAN_PRESSURE_THRESHOLD);
    if ($cond1_low_util && $cond2_high_dsr && $cond3_high_median_pressure) {
        my $original_additive = $additive_cpu;
        $debug_info{'RunQ_Potential'} = "ANOMALY (DBW)";
        $additive_cpu *= 0.25;
        $dbw_dampening_applied = sprintf("True (75%% dampening applied).\n                                               Additive CPU reduced from %.4f to %.4f.\n                                               Conditions met: Low Util (BaseP %.2f < %.2f Ent * %.2f), High DSR (%.1f > %.1f), High Median Pressure (NormP50 %.2f > %.1f)", $original_additive, $additive_cpu, $base_physc, $curr_ent_numeric, $DBW_LOW_UTIL_FACTOR, $dsr, $DBW_DSR_THRESHOLD, $norm_p50_numeric, $DBW_MEDIAN_PRESSURE_THRESHOLD);
    }
    $debug_info{'DBWDampeningApplied'} = $dbw_dampening_applied;

    # This is the final and most important safety cap. It prevents recommendations from violating
    # the VM's hard limits (MaxCPU) while allowing for "Intelligent Headroom" based on pressure severity.
    # --- Adaptive Safety Hard-Cap with Intelligent Headroom ---
    my $original_additive_before_safety_cap = $additive_cpu;
    my $additive_safety_cap_applied_reason = "Not applied";

    # Step 1: Calculate Pressure Severity to determine Virtual Headroom
    my $pressure_ratio = ($effective_lcpus_for_pressure_calc > 0) ? (($excess_threads // 0) / $effective_lcpus_for_pressure_calc) : 0;
    my $virtual_headroom_factor = 0;
    if ($apply_additive_logic) { # Only apply headroom if there's pressure
        if ($pressure_ratio >= 1.0) {
            $virtual_headroom_factor = 0.50; # 50% for Severe Pressure
        } else {
            $virtual_headroom_factor = 0.25; # 25% for Significant Pressure
        }
    }
    my $virtual_max_cpu = $max_cpu_for_lpar_numeric * (1 + $virtual_headroom_factor);

    # Step 2: Calculate the Primary Dynamic Cap using Virtual Headroom
    # NOTE (Fix 3): Tactical should not be constrained by MaxCPU-derived headroom.
    # The EntCap provides a safe incremental limit based on entitlement alone.
    # The HeadCap is now computed for diagnostic/rationale purposes only.

    my $cap_from_ent = $curr_ent_numeric * 0.40;
    my $headroom = ($virtual_max_cpu > $curr_ent_numeric) ? ($virtual_max_cpu - $curr_ent_numeric) : 0;
    my $cap_from_headroom = $headroom * 0.50;

    # Tactical uses EntCap only (demand-oriented, not environment-constrained).
    # HeadCap is retained for rationale logging and Strategic comparison.
    my $primary_cap = $cap_from_ent;

    # Record whether HeadCap would have been binding (for rationale/diagnostics)
    my $headcap_would_bind = ($headroom > 0 && $cap_from_headroom < $cap_from_ent);
    my $headcap_constrained_value = ($headroom > 0) ? min($cap_from_ent, $cap_from_headroom) : $cap_from_ent;

    # The Pressure Gate moderates the cap for borderline cases.
    # If pressure is only moderate, we apply a small dampening to the cap.
    # For significant or severe pressure, we apply the full cap without dampening.
    my $pressure_gate_modifier = 1.0;
    if ($pressure_ratio < 0.5) {
        $pressure_gate_modifier = 0.75;
    }

    my $gated_cap = $primary_cap * $pressure_gate_modifier;
    my $relative_floor = 0.02 * max($curr_ent_numeric, 1.0);
    my $final_safety_cap_value = max($relative_floor, $gated_cap);
    $final_safety_cap_value = min($final_safety_cap_value, 2.0);

    # Build cap rationale (now EntCap-only for Tactical; HeadCap is diagnostic)
    my $cap_rationale;
    if ($headcap_would_bind) {
        $cap_rationale = sprintf("EntCap=%.2f * PresGate=%.2f -> %.4f (HeadCap=%.2f would be lower but Tactical is demand-oriented)",
            $cap_from_ent, $pressure_gate_modifier, $final_safety_cap_value, $cap_from_headroom);
    } else {
        $cap_rationale = sprintf("EntCap=%.2f * PresGate=%.2f -> %.4f", $cap_from_ent, $pressure_gate_modifier, $final_safety_cap_value);
    }

    if ($additive_cpu > $final_safety_cap_value) {
        $additive_cpu = $final_safety_cap_value;
        $additive_safety_cap_applied_reason = sprintf("True: Additive CPU hard-capped to %.4f (%s)", $additive_cpu, $cap_rationale);
    } else {
        $additive_safety_cap_applied_reason = sprintf("False (Additive %.4f within adaptive hard cap of %.4f from %s)", $original_additive_before_safety_cap, $final_safety_cap_value, $cap_rationale);
    }
    $debug_info{'AdditiveSafetyCapApplied'} = $additive_safety_cap_applied_reason;

    # --- Final Tier-Aware Scaling for the Tactical Recommendation ---
    # This is the last step, applying the business policy (tier importance) to the final, safety-vetted additive value.
    if ($tier_scaling_factor != 1.00 && $additive_cpu > 0) {
        my $pre_scaled_additive = $additive_cpu;
        $additive_cpu *= $tier_scaling_factor;

        # Reconstruct the rationale for logging
        my $increase_pct = ($tier_scaling_factor - 1.0) * 100;
        my $rationale_text = sprintf(
            "Tier %d: Enhanced upsizing (+%.0f%%) for lower latency tolerance. Base: %.3f -> Scaled: %.3f cores",
            $tier_num_for_scaling, $increase_pct, $pre_scaled_additive, $additive_cpu
        );

        $debug_info{'TierScalingFactor'}        = sprintf("%.2f", $tier_scaling_factor);
        $debug_info{'TierNumber'}               = $tier_num_for_scaling;
        $debug_info{'TierScalingRationale'}     = $rationale_text;
        $debug_info{'AdditiveCPU_PreTierScaling'}  = sprintf("%.4f", $pre_scaled_additive);
        $debug_info{'AdditiveCPU_PostTierScaling'} = sprintf("%.4f", $additive_cpu);
    }

    my $final_tactical_modifier = 0;
    if ($additive_cpu > $FLOAT_EPSILON) {
        $final_tactical_modifier = $additive_cpu;
    } elsif ($efficiency_factor_numeric < (1.0 - $FLOAT_EPSILON)) {
        my $actual_reduction = (defined $debug_info{'EffActualReductionCores'} && looks_like_number($debug_info{'EffActualReductionCores'})) ? ($debug_info{'EffActualReductionCores'} + 0) : 0;
        $final_tactical_modifier = -1 * $actual_reduction;
    }
    $debug_info{'RunQ_Tactical'} = $final_tactical_modifier;
    $debug_info{'FinalAdditive'} = sprintf("%.4f", $final_tactical_modifier);

    # ================================================================
    # Tactical vs Strategic Diagnostic (Fix 3)
    # ================================================================
    # When Tactical > Strategic, the safe incremental step exceeds what
    # is achievable under current MaxCPU. This is a signal that MaxCPU
    # must be raised before implementing the Tactical recommendation.
    # ================================================================
    my $strategic_val = (defined $debug_info{'RunQ_Strategic'} && looks_like_number($debug_info{'RunQ_Strategic'}))
                      ? ($debug_info{'RunQ_Strategic'} + 0) : 0;
    my $tactical_exceeds_strategic = ($final_tactical_modifier > 0 && $strategic_val >= 0 && $final_tactical_modifier > ($strategic_val + 0.005));
    $debug_info{'TacticalExceedsStrategic'} = $tactical_exceeds_strategic ? 1 : 0;
    if ($tactical_exceeds_strategic) {
        $debug_info{'TacticalExceedsStrategicNote'} = sprintf(
            "Tactical (%.4f cores) exceeds Strategic (%.4f cores). Raise MaxCPU before implementing Tactical.",
            $final_tactical_modifier, $strategic_val
        );
    }

    # =========================
    # Final Synthesis + MaxCPU
    # =========================
    my $calculated_demand = $base_adjusted_physc + $additive_cpu;

    # Preserve the workload-driven (planning) entitlement before any MaxCPU sanity capping.
    # This is the "what the workload needs" value, independent of current environment limits.
    my $rENT_P_planning = $calculated_demand;

    my $runq_modified_rec = $calculated_demand;
    $debug_info{'PreMaxCpuCapRec'} = sprintf("%.4f", $runq_modified_rec);
    my $forecast_multiplier_val = 1.25;
    if ($curr_ent_numeric < 0.5)    { $forecast_multiplier_val = 2.5; }
    elsif ($curr_ent_numeric < 1.0) { $forecast_multiplier_val = 2.0; }
    elsif ($curr_ent_numeric < 2.0) { $forecast_multiplier_val = 1.75; }
    elsif ($curr_ent_numeric < 4.0) { $forecast_multiplier_val = 1.5; }
    $debug_info{'ForecastMultiplier'} = $forecast_multiplier_val;
    my $effective_max_cpu_cap_val = ($max_cpu_for_lpar_numeric > 0) ? ($max_cpu_for_lpar_numeric * $forecast_multiplier_val) : undef;
    $debug_info{'EffectiveMaxCPUCap'} = defined($effective_max_cpu_cap_val) ? sprintf("%.4f", $effective_max_cpu_cap_val) : $na_str;
    if (defined $effective_max_cpu_cap_val && $calculated_demand > $effective_max_cpu_cap_val) {
        $runq_modified_rec = $effective_max_cpu_cap_val;
        $debug_info{'CappedByMaxCPU'} = "True";
    } else {
        $debug_info{'CappedByMaxCPU'} = (defined $effective_max_cpu_cap_val) ? "False" : "N/A (No LPAR MaxCPU for cap check or MaxCPU not exceeded)";
    }
    if ($runq_modified_rec < 0) { $runq_modified_rec = 0; }
    $debug_info{'FinalAdjustedPhysC'} = sprintf("%.4f", $runq_modified_rec);
    $debug_info{'rENT_P_Planning'} = sprintf("%.4f", $rENT_P_planning);

    # ------------------------------------------------------------------
    # rVCPU computation (stress-driven, slot-aware, rENT-bounded, peak-aware)
    #
    # IMPORTANT: This must be computed here (where per-profile AbsRunQ_Pxx is
    # already available as $abs_runq_p_numeric) so rationale logging can be
    # deterministic and consistent with other debug_info fields.
    # ------------------------------------------------------------------
    do {
        my $rvcpu_raw;
        my $rvcpu_min;
        my $rvcpu_max;
        my $rvcpu_final;
        my $p99w1_floor_vcpu;
        my $p99w1_factor_eff;
        my $p99w1_mult;
        my $cred_ratio = 1.00;
        my $p99w1_adjusted;

        # Dispatch reserve variables (Layer 5: uncertainty/reserve adjustments)
        my $dispatch_reserve_vp = 0;
        my $dispatch_reserve_reason = '';
        my $dispatch_reserve_pre_final;  # Store the pre-reserve rVCPU for rationale

        # Micro-partition RunQ envelope override tracking
        my $runq_envelope_override = 0;
        my $runq_envelope_basis = undef;
        my ($orig_rvcpu_min, $orig_rvcpu_max) = (undef, undef);

        # rENT_P (operational): the post-MaxCPU-sanity value used for the final entitlement output.
        my $rENT_P = (defined $runq_modified_rec && looks_like_number($runq_modified_rec)) ? ($runq_modified_rec + 0) : undef;

        # rENT_P_for_envelope (planning): the pre-MaxCPU-sanity value used for VP envelope derivation.
        # This ensures the VP envelope reflects workload demand, not current environment limits.
        # When MaxCPU sanity does not cap the recommendation, these are identical.
        my $rENT_P_for_envelope = (defined $rENT_P_planning && looks_like_number($rENT_P_planning) && $rENT_P_planning > 0)
                                ? ($rENT_P_planning + 0)
                                : $rENT_P;

        # Track whether the planning and operational values diverge (for rationale clarity).
        my $envelope_exceeds_operational = (defined $rENT_P_for_envelope && defined $rENT_P && $rENT_P_for_envelope > $rENT_P + 0.005);

        my $smt_for_vcpu = (defined $vm_map_ref->{Configuration}{smt} && looks_like_number($vm_map_ref->{Configuration}{smt}))
                         ? ($vm_map_ref->{Configuration}{smt} + 0)
                         : undef;

        # Use the per-profile AbsRunQ percentile already computed in this function.
        my $runq_hi_threads = (defined $abs_runq_p_numeric && looks_like_number($abs_runq_p_numeric)) ? ($abs_runq_p_numeric + 0) : undef;

        # Tier-aware headroom: extract tier number from $final_tier (already in scope).
        my $rvcpu_tier_num = 3;  # Default baseline
        if (defined $final_tier && $final_tier =~ /(\d)$/) {
            $rvcpu_tier_num = $1;
        } elsif (defined $final_tier && $final_tier eq 'P') {
            $rvcpu_tier_num = 1;
        }
        my $rvcpu_tier_factor = $VCPU_TIER_HEADROOM_FACTORS{$rvcpu_tier_num} // 1.00;

        # Interval-adaptive divisor for Gate 1 (dispatch sizing).
        # This is a divisor (target utilisation), not a multiplier.
        my $runq_width_divisor = $RUNQ_TARGET_HEADROOM_FACTOR;
        $runq_width_divisor = 0.80 if (!defined $runq_width_divisor || !looks_like_number($runq_width_divisor) || $runq_width_divisor <= 0);

        my $runq_width_divisor_base = $runq_width_divisor;
        my $runq_interval_minutes_snapped;

        if (defined $adaptive_thresholds_href && ref($adaptive_thresholds_href) eq 'HASH') {
            if (defined $adaptive_thresholds_href->{runq_width_divisor_base}
                && looks_like_number($adaptive_thresholds_href->{runq_width_divisor_base})
                && $adaptive_thresholds_href->{runq_width_divisor_base} > 0) {
                $runq_width_divisor_base = $adaptive_thresholds_href->{runq_width_divisor_base} + 0;
            }
            if (defined $adaptive_thresholds_href->{runq_width_divisor}
                && looks_like_number($adaptive_thresholds_href->{runq_width_divisor})
                && $adaptive_thresholds_href->{runq_width_divisor} > 0) {
                $runq_width_divisor = $adaptive_thresholds_href->{runq_width_divisor} + 0;
            }
            if (defined $adaptive_thresholds_href->{snapped_minutes}
                && looks_like_number($adaptive_thresholds_href->{snapped_minutes})
                && $adaptive_thresholds_href->{snapped_minutes} > 0) {
                $runq_interval_minutes_snapped = int($adaptive_thresholds_href->{snapped_minutes} + 0);
            }
        }

        # Hard safety clamps (never allow divide-by-zero or absurd divisors).
        my $MIN_RUNQ_WIDTH_DIVISOR = 0.50;
        my $MAX_RUNQ_WIDTH_DIVISOR = 1.00;
        $runq_width_divisor_base = 0.80 if (!defined $runq_width_divisor_base || !looks_like_number($runq_width_divisor_base) || $runq_width_divisor_base <= 0);
        $runq_width_divisor_base = $MIN_RUNQ_WIDTH_DIVISOR if ($runq_width_divisor_base < $MIN_RUNQ_WIDTH_DIVISOR);
        $runq_width_divisor_base = $MAX_RUNQ_WIDTH_DIVISOR if ($runq_width_divisor_base > $MAX_RUNQ_WIDTH_DIVISOR);

        $runq_width_divisor = $runq_width_divisor_base if (!defined $runq_width_divisor || !looks_like_number($runq_width_divisor) || $runq_width_divisor <= 0);
        $runq_width_divisor = $MIN_RUNQ_WIDTH_DIVISOR if ($runq_width_divisor < $MIN_RUNQ_WIDTH_DIVISOR);
        $runq_width_divisor = $MAX_RUNQ_WIDTH_DIVISOR if ($runq_width_divisor > $MAX_RUNQ_WIDTH_DIVISOR);

        # Interval credibility correction (for corroboration ratio only):
        # Coarser sampling dampens RunQ tails, which can falsely reduce RunQ↔PhysC corroboration.
        # Apply a *relative* correction ONLY to the RunQ-derived cores used in the credibility ratio:
        #   corr = base_divisor / adaptive_divisor
        # This is an identity at 60s (corr=1.00), never deflates RunQ credibility (corr>=1.00),
        # and is hard-clamped to avoid over-trusting RunQ tails.
        my $runq_cred_corr = 1.00;
        my $runq_cred_corr_suffix = '';
        do {
            my $corr = 1.00;
            if (defined $runq_width_divisor_base && defined $runq_width_divisor && $runq_width_divisor > 0) {
                $corr = $runq_width_divisor_base / $runq_width_divisor;
            }
            $corr = 1.00 if (!defined $corr || !looks_like_number($corr) || $corr <= 0);
            $corr = 1.00 if ($corr < 1.00);
            $corr = 1.50 if ($corr > 1.50);
            $runq_cred_corr = $corr;

            if ($runq_cred_corr > 1.0005) {
                $runq_cred_corr_suffix = sprintf(" [Interval Correction: x%.2f%s]",
                    $runq_cred_corr,
                    (defined $runq_interval_minutes_snapped ? sprintf(", %dm", $runq_interval_minutes_snapped) : "")
                );
            }
        };

        if (defined $runq_hi_threads && defined $smt_for_vcpu && $smt_for_vcpu > 0 && defined $rENT_P && $rENT_P > 0) {
            # Workload-driven width requirement (threads -> cores -> vCPUs), tier-adjusted.
            # Higher tiers demand more spare runnable capacity (lower acceptable queue utilisation).
            # NOTE: The divisor below is a target utilisation divisor (e.g., 0.80), not a multiplier.
            my $required_threads = ($runq_hi_threads / $runq_width_divisor) * $rvcpu_tier_factor;
            my $required_cores   = $required_threads / $smt_for_vcpu;
            $rvcpu_raw = int($required_cores);
            $rvcpu_raw++ if ($required_cores > $rvcpu_raw); # ceil

            # Vendor envelope from rENT_P_for_envelope (planning entitlement, pre-MaxCPU-sanity).
            # This ensures the VP envelope reflects workload demand, not current environment limits.
            my $min_v = $rENT_P_for_envelope * $VCPU_RATIO_MIN;
            my $max_v = $rENT_P_for_envelope * $VCPU_RATIO_MAX;
            $rvcpu_min = int($min_v); $rvcpu_min++ if ($min_v > $rvcpu_min);
            $rvcpu_max = int($max_v); $rvcpu_max++ if ($max_v > $rvcpu_max);

            # Enforce minimum VP floor (evenisation removed - odd VP counts now permitted)
            if (defined $rvcpu_min) {
                $rvcpu_min = 1 if ($rvcpu_min < 1);
            }
            if (defined $rvcpu_max) {
                $rvcpu_max = 1 if ($rvcpu_max < 1);
                $rvcpu_max = $rvcpu_min if (defined $rvcpu_min && $rvcpu_max < $rvcpu_min);
            }

            # Track original envelope for rationale (before any override)
            ($orig_rvcpu_min, $orig_rvcpu_max) = ($rvcpu_min, $rvcpu_max);

            # Clamp
            $rvcpu_final = $rvcpu_raw;
            $rvcpu_final = $rvcpu_min if (defined $rvcpu_min && $rvcpu_final < $rvcpu_min);
            $rvcpu_final = $rvcpu_max if (defined $rvcpu_max && $rvcpu_final > $rvcpu_max);

            # Low-entitlement sanity rules
            $rvcpu_final = $VCPU_MIN_ABS if (defined $rvcpu_final && $rvcpu_final < $VCPU_MIN_ABS);
            if (defined $rENT_P && $rENT_P >= 0.5 && $rENT_P < 1.0) {
                $rvcpu_final = $VCPU_MIN_FOR_ENT_GE_0_5_LT_1 if (defined $rvcpu_final && $rvcpu_final < $VCPU_MIN_FOR_ENT_GE_0_5_LT_1);
            }

            # Peak-aware floor from P-99W1 (mandatory helper profile)
            if (defined $pure_p99w1_physc && looks_like_number($pure_p99w1_physc) && $pure_p99w1_physc > 0) {
                # RunQ credibility adjustment: verify PhysC peak against RunQ evidence.
                # When RunQ-derived core demand is significantly below PhysC, the gap
                # likely reflects VP dispatch overhead rather than true workload demand.
                $p99w1_adjusted = $pure_p99w1_physc;

                # Credibility uses a tail-to-tail comparison: P99W1 PhysC vs AbsRunQ_P99W1.
                # The per-profile RunQ percentile (P90/P95/P98/P99) is correct for Gate 1
                # (dispatch sizing), but the credibility check asks whether the P99W1 PhysC
                # peak is corroborated, which requires a matching tail RunQ percentile.
                #
                # Prefer the peak-helper-aligned W1 RunQ tail for credibility, so we
                # compare like-for-like: P99W1 PhysC vs AbsRunQ_P99W1 (window=1).
                my $runq_p99w1_for_cred =
                    $vm_map_ref->{RunQMetrics}{AbsRunQ_P99W1} //
                    $vm_map_ref->{RunQMetrics}{AbsRunQ_P99};

                if (defined $runq_p99w1_for_cred && $runq_p99w1_for_cred > 0 && defined $smt_for_vcpu && $smt_for_vcpu > 0) {
                    my $runq_peak_cores_raw = $runq_p99w1_for_cred / $smt_for_vcpu;
                    my $runq_peak_cores = $runq_peak_cores_raw * $runq_cred_corr;
                    $cred_ratio = $runq_peak_cores / $pure_p99w1_physc;
                    my $credibility = $cred_ratio + $VCPU_PEAK_CRED_BIAS;
                    $credibility = $VCPU_PEAK_CRED_MIN if ($credibility < $VCPU_PEAK_CRED_MIN);
                    $credibility = 1.00 if ($credibility > 1.00);
                    $p99w1_adjusted = $pure_p99w1_physc * $credibility;
                }

                # Peak floor: PAE/AAE-driven headroom, tier-scaled.
                # Base is 1.00 (VCPUs must at minimum cover peak cores).
                # Headroom above that is proportional to empirical above-entitlement evidence,
                # amplified by tier policy. This avoids the multiplicative compounding of
                # base × dynamic_mult × tier that inflated recommendations for stressed tier-1 LPARs.
                my $pae_raw = (defined $debug_info{'PhysC_Daily_PAE'} && looks_like_number($debug_info{'PhysC_Daily_PAE'})) ? ($debug_info{'PhysC_Daily_PAE'} + 0) : 0.0;
                my $aae_raw = (defined $debug_info{'PhysC_Daily_AAE'} && looks_like_number($debug_info{'PhysC_Daily_AAE'})) ? ($debug_info{'PhysC_Daily_AAE'} + 0) : 0.0;

                # Each component contributes up to +15% of peak when its threshold is reached.
                # PAE: proportion of days above entitlement (chronic burst indicator)
                # AAE: aggregate excess above entitlement (burst magnitude indicator)
                my $pae_component = ($pae_raw > 0) ? (($pae_raw < 0.30 ? $pae_raw : 0.30) / 0.30) * 0.15 : 0.0;
                my $aae_component = ($aae_raw > 0) ? (($aae_raw < 0.10 ? $aae_raw : 0.10) / 0.10) * 0.15 : 0.0;

                my $headroom = ($pae_component + $aae_component) * $rvcpu_tier_factor;
                $p99w1_factor_eff = 1.00 + $headroom;
                $p99w1_mult = $headroom;  # For rationale: the headroom portion above 1.00

                my $floor_v = ($p99w1_adjusted + 0) * $p99w1_factor_eff;

                $p99w1_floor_vcpu = int($floor_v); $p99w1_floor_vcpu++ if ($floor_v > $p99w1_floor_vcpu);

                # Enforce minimum floor (evenisation removed)
                if (defined $p99w1_floor_vcpu) {
                    $p99w1_floor_vcpu = 1 if ($p99w1_floor_vcpu < 1);
                }

                if (defined $p99w1_floor_vcpu && defined $rvcpu_final && $p99w1_floor_vcpu > $rvcpu_final) {
                    $rvcpu_final = $p99w1_floor_vcpu;
                    $rvcpu_final = $rvcpu_max if (defined $rvcpu_max && defined $rvcpu_final && $rvcpu_final > $rvcpu_max);
                }
            }

            # Enforce minimum (evenisation removed)
            $rvcpu_final = $VCPU_MIN_ABS if (defined $rvcpu_final && $rvcpu_final < $VCPU_MIN_ABS);

            # ================================================================
            # LAYER 5a: Micro-partition RunQ Envelope Override
            # ================================================================
            # When rENT_P < threshold and RunQ strongly exceeds PhysC (cred_ratio > threshold),
            # and the RunQ-derived VP requirement exceeds the envelope max,
            # expand the envelope to accommodate the RunQ evidence.
            # This addresses scheduling disadvantage in tiny partitions.
            # ================================================================
            my $runq_peak_cores_for_override = undef;

            # Calculate RunQ-derived core requirement for comparison
            if (defined $runq_hi_threads && defined $smt_for_vcpu && $smt_for_vcpu > 0) {
                $runq_peak_cores_for_override = $runq_hi_threads / $smt_for_vcpu;
            }

            # Check DBW/XRQ anomaly state (do not apply override if pathological)
            my $is_dbw_anomaly = (defined $debug_info{'RunQ_Potential'} && $debug_info{'RunQ_Potential'} =~ /ANOMALY.*DBW/i);
            my $is_xrq_anomaly = (defined $debug_info{'RunQ_Potential'} && $debug_info{'RunQ_Potential'} =~ /ANOMALY.*XRQ/i);

            # Check pressure flags - use per-profile detection (not overall flags which can miss cases)
            my $has_pressure_for_override =
                (defined $debug_info{'IsRunQPressure'} && $debug_info{'IsRunQPressure'} eq "True") ||
                (defined $debug_info{'IsWorkloadPressure'} && $debug_info{'IsWorkloadPressure'} eq "True");

            if (   defined $rENT_P && $rENT_P > 0 && $rENT_P < $MICRO_PARTITION_RUNQ_OVERRIDE_ENT_THRESH
                && defined $cred_ratio && $cred_ratio > $MICRO_PARTITION_RUNQ_OVERRIDE_CRED_THRESH
                && defined $rvcpu_raw && defined $rvcpu_max && $rvcpu_raw > $rvcpu_max
                && !$is_dbw_anomaly && !$is_xrq_anomaly
                && $has_pressure_for_override
                && defined $runq_peak_cores_for_override) {

                $runq_envelope_override = 1;
                $runq_envelope_basis = $runq_peak_cores_for_override;

                # Extend env_max to permit the RunQ-derived VP requirement
                my $extended_max = $rvcpu_raw;
                $extended_max = $MICRO_PARTITION_RUNQ_MAX_VP_CAP if ($extended_max > $MICRO_PARTITION_RUNQ_MAX_VP_CAP);
                $rvcpu_max = $extended_max;

                # Re-apply clamp with new envelope
                $rvcpu_final = $rvcpu_raw;
                $rvcpu_final = $rvcpu_min if (defined $rvcpu_min && $rvcpu_final < $rvcpu_min);
                $rvcpu_final = $rvcpu_max if (defined $rvcpu_max && $rvcpu_final > $rvcpu_max);
            }

            # ================================================================
            # LAYER 5: Conditional Dispatch Reserve (Dynamic Scaling)
            # ================================================================
            # When uncertainty indicators are present and rVCPU would collapse
            # to the envelope minimum, add a scaled reserve to provide dispatch
            # margin. Reserve scales with risk level and respects env_max.
            # ================================================================
            $dispatch_reserve_pre_final = $rvcpu_final;

            RESERVE_CALC: {
                last RESERVE_CALC unless (defined $rvcpu_final && defined $rvcpu_min && defined $rvcpu_max);

                # Only trigger if final would otherwise equal or near the floor
                my $would_collapse = ($rvcpu_final <= $rvcpu_min);
                last RESERVE_CALC unless $would_collapse;

                # Uncertainty indicators
                my $has_growth = (defined $growth_adj_numeric && $growth_adj_numeric > 0.25);
                my $has_pressure = ($p99w1_overall_vm_has_abs_runq_pressure || $p99w1_overall_vm_has_norm_runq_pressure);
                my $has_low_corroboration = (defined $cred_ratio && $cred_ratio < 0.70);

                # Count risk triggers
                my $risk_count = 0;
                $risk_count++ if $has_growth;
                $risk_count++ if $has_pressure;
                $risk_count++ if $has_low_corroboration;

                last RESERVE_CALC unless ($risk_count > 0);

                # --- Dynamic Scaling Logic ---

                # 1. Define Base Percentage by Tier
                my %tier_base_pct = (1 => 0.10, 2 => 0.075, 3 => 0.05);
                my $target_pct = $tier_base_pct{$rvcpu_tier_num} // 0.05;

                # 2. Apply Risk Multiplier (2.0x boost for multiple risk indicators)
                my $risk_multiplier = ($risk_count >= 2) ? 2.0 : 1.0;
                $target_pct *= $risk_multiplier;

                # 3. Calculate Raw Reserve based on Planning Entitlement
                my $basis_cores = $rENT_P_for_envelope // $rENT_P // 0;
                my $raw_reserve = $basis_cores * $target_pct;

                # 4. Tiered Floor (always even to maintain VP parity policy)
                #    Tier 1/2: Critical workloads need minimum +4 VPs headroom
                #    Tier 3+:  Non-critical workloads need minimum +2 VPs
                my $floor_vps = ($rvcpu_tier_num <= 2) ? 4 : 2;

                # 5. Integer Rounding (Ceiling)
                my $calc_reserve = int($raw_reserve + 0.999);  # ceil

                # Apply floor
                $calc_reserve = $floor_vps if ($calc_reserve < $floor_vps);

                # 6. Evenisation (round UP to next even if odd)
                #$calc_reserve++ if ($calc_reserve % 2 == 1);

                # 7. Safe Clamping (respect env_max)
                my $available_headroom = $rvcpu_max - $rvcpu_final;
                my $was_clamped = 0;
                my $pre_clamp_reserve = $calc_reserve;

                $dispatch_reserve_vp = $calc_reserve;

                if ($dispatch_reserve_vp > $available_headroom) {
                    $dispatch_reserve_vp = $available_headroom;
                    $was_clamped = 1;

                    # Ensure non-negative
                    $dispatch_reserve_vp = 0 if ($dispatch_reserve_vp < 0);

                    # Re-evenise after clamp (round DOWN to stay within env_max)
                    #if ($dispatch_reserve_vp > 0 && ($dispatch_reserve_vp % 2 == 1)) {
                    #    $dispatch_reserve_vp--;
                    #}
                }

                # Build trigger list for rationale
                my @triggers;
                push @triggers, sprintf("GrowthAdj +%.2f", $growth_adj_numeric) if $has_growth;
                push @triggers, "Pressure" if $has_pressure;
                push @triggers, sprintf("LowCorroboration %.2f", $cred_ratio) if $has_low_corroboration;
                my $trigger_str = join(", ", @triggers);
                my $risk_label = ($risk_count >= 2) ? "HighRisk" : "Standard";

                # Build reason for rationale (including suppression cases)
                if ($dispatch_reserve_vp > 0) {
                    my $clamp_note = "";
                    if ($was_clamped) {
                        $clamp_note = sprintf(" [clamped from %d; headroom %d]", $pre_clamp_reserve, $available_headroom);
                    }
                    $dispatch_reserve_reason = sprintf(
                        "Dispatch reserve +%d VPs (Mode: %s; Scale: %.1f%% of %.2f cores; Floor: %d; Triggers: %s)%s",
                        $dispatch_reserve_vp, $risk_label, $target_pct * 100, $basis_cores, $floor_vps, $trigger_str, $clamp_note
                    );
                    $rvcpu_final += $dispatch_reserve_vp;
                } else {
                    # Reserve triggered but fully suppressed by env_max (diagnostic)
                    $dispatch_reserve_reason = sprintf(
                        "Dispatch reserve SUPPRESSED (would add %d VPs but env_max headroom is %d; Mode: %s; Triggers: %s)",
                        $pre_clamp_reserve, $available_headroom, $risk_label, $trigger_str
                    );
                }
            }

        }

        # Persist for CSV and rationale consumers
        $vm_map_ref->{CoreResults}{Profile_rVCPU}{$profile_name} = defined($rvcpu_final) ? $rvcpu_final : '';
        $vm_map_ref->{CoreResults}{Profile_rVCPU_Explain}{$profile_name} = {
            Raw            => $rvcpu_raw,
            Min            => $rvcpu_min,
            Max            => $rvcpu_max,
            EntCores       => (defined $rENT_P ? sprintf("%.2f", $rENT_P) : undef),
            EntCoresPlanning => (defined $rENT_P_for_envelope ? sprintf("%.2f", $rENT_P_for_envelope) : undef),
            EnvelopeExceedsOperational => ($envelope_exceeds_operational ? 1 : 0),
            DispatchReserveVP     => $dispatch_reserve_vp,
            DispatchReserveReason => $dispatch_reserve_reason,
            DispatchReservePreFinal => $dispatch_reserve_pre_final,
            TierNum        => $rvcpu_tier_num,
            TierFactor     => sprintf("%.2f", $rvcpu_tier_factor),
            RunQKeyUsed    => $abs_runq_key_for_debug,  # Must match the key actually selected for this profile's AbsRunQ
            RunQHiThreads  => (defined $runq_hi_threads ? sprintf("%.2f", $runq_hi_threads) : undef),
            RunQWidthDivisor      => sprintf("%.2f", $runq_width_divisor),
            RunQWidthDivisorBase  => sprintf("%.2f", $runq_width_divisor_base),
            RunQIntervalMinutes   => (defined $runq_interval_minutes_snapped ? $runq_interval_minutes_snapped : undef),
            P99W1Floor     => $p99w1_floor_vcpu,
            P99W1FactorEff => (defined $p99w1_factor_eff ? sprintf("%.3f", $p99w1_factor_eff) : undef),
            P99W1Mult      => (defined $p99w1_mult ? sprintf("%.3f", $p99w1_mult) : undef),
            P99W1CredRatio => sprintf("%.3f", $cred_ratio),
            P99W1CredCorr  => sprintf("%.2f", $runq_cred_corr),
            P99W1CredRatioSuffix => $runq_cred_corr_suffix,
            P99W1AdjPeak   => sprintf("%.2f", $p99w1_adjusted),
            P99W1RawPeak   => sprintf("%.2f", $pure_p99w1_physc),
            RunQEnvelopeOverride => $runq_envelope_override ? "Yes" : "No",
            RunQEnvelopeBasis    => defined $runq_envelope_basis ? sprintf("%.3f", $runq_envelope_basis) : undef,
            OrigMin              => $orig_rvcpu_min,
            OrigMax              => $orig_rvcpu_max,
        };

        # Mirror into debug_info
        $debug_info{'rVCPU_RunQKeyUsed'}    = $abs_runq_key_for_debug // 'N/A';
        $debug_info{'rVCPU_TierNum'}        = $rvcpu_tier_num;
        $debug_info{'rVCPU_TierFactor'}     = sprintf("%.2f", $rvcpu_tier_factor);
        $debug_info{'rVCPU_Raw'}            = defined($rvcpu_raw) ? $rvcpu_raw : 'N/A';
        $debug_info{'rVCPU_Min'}            = defined($rvcpu_min) ? $rvcpu_min : 'N/A';
        $debug_info{'rVCPU_Max'}            = defined($rvcpu_max) ? $rvcpu_max : 'N/A';
        $debug_info{'rVCPU_EntCores'}       = defined($rENT_P) ? sprintf("%.2f", $rENT_P) : 'N/A';
        $debug_info{'rVCPU_EntCoresPlanning'} = defined($rENT_P_for_envelope) ? sprintf("%.2f", $rENT_P_for_envelope) : 'N/A';
        $debug_info{'rVCPU_EnvelopeExceedsOp'} = $envelope_exceeds_operational ? 1 : 0;
        $debug_info{'rVCPU_DispatchReserveVP'} = $dispatch_reserve_vp;
        $debug_info{'rVCPU_DispatchReserveReason'} = $dispatch_reserve_reason;
        $debug_info{'rVCPU_DispatchReservePreFinal'} = defined($dispatch_reserve_pre_final) ? $dispatch_reserve_pre_final : 'N/A';
        $debug_info{'rVCPU_P99W1Floor'}     = defined($p99w1_floor_vcpu) ? $p99w1_floor_vcpu : 'N/A';
        $debug_info{'rVCPU_P99W1FactorEff'} = defined($p99w1_factor_eff) ? sprintf("%.3f",$p99w1_factor_eff) : 'N/A';
        $debug_info{'rVCPU_P99W1Mult'}      = defined($p99w1_mult) ? sprintf("%.3f",$p99w1_mult) : 'N/A';
        $debug_info{'rVCPU_P99W1CredRatio'} = sprintf("%.3f", $cred_ratio);
        $debug_info{'rVCPU_P99W1AdjPeak'}   = sprintf("%.2f", $p99w1_adjusted);
        $debug_info{'rVCPU_P99W1RawPeak'}   = sprintf("%.2f", $pure_p99w1_physc);
        $debug_info{'rVCPU_RunQHiThreads'}  = defined($runq_hi_threads) ? sprintf("%.2f",$runq_hi_threads) : 'N/A';
        $debug_info{'rVCPU_RunQWidthDivisor'}     = sprintf("%.2f", $runq_width_divisor);
        $debug_info{'rVCPU_RunQWidthDivisorBase'} = sprintf("%.2f", $runq_width_divisor_base);
        $debug_info{'rVCPU_RunQIntervalMinutes'}  = defined($runq_interval_minutes_snapped) ? $runq_interval_minutes_snapped : 'N/A';
        $debug_info{'rVCPU_P99W1CredCorr'}        = sprintf("%.2f", $runq_cred_corr);
        $debug_info{'rVCPU_P99W1CredRatioSuffix'} = $runq_cred_corr_suffix;
        $debug_info{'rVCPU_Final'}          = defined($rvcpu_final) ? $rvcpu_final : 'N/A';
        $debug_info{'rVCPU_RunQEnvelopeOverride'} = $runq_envelope_override ? "Yes" : "No";
        $debug_info{'rVCPU_RunQEnvelopeBasis'}    = defined $runq_envelope_basis ? sprintf("%.3f", $runq_envelope_basis) : 'N/A';
        $debug_info{'rVCPU_OrigMin'}              = defined $orig_rvcpu_min ? $orig_rvcpu_min : 'N/A';
        $debug_info{'rVCPU_OrigMax'}              = defined $orig_rvcpu_max ? $orig_rvcpu_max : 'N/A';
    };

    return ($runq_modified_rec, \%debug_info);
}

# --- generate_sizing_hint (Unified Global Pressure Detection with Logging Rationale) ---
# Generates a sizing tier hint, pattern, and overall pressure indication for a VM.
# Also returns a detailed rationale string for its pressure assessment, AND
# specific boolean flags for P-99W1's RunQ pressure conditions.
sub generate_sizing_hint
{
    # This function is now refactored to read all its inputs from the assimilation map.
    my ($vm_map_ref, $log_fh, $adaptive_saturation_thresh) = @_;

    # --- Unpack all required values from the assimilation map ---
    my $vm_name                  = $vm_map_ref->{Configuration}{vm_name}; # We will add vm_name to the map
    my $config_ref               = $vm_map_ref->{Configuration};
    my $results_ref              = $vm_map_ref->{CoreResults}{ProfileValues};
    my $runq_metrics_ref         = $vm_map_ref->{RunQMetrics};
    my $max_cpu_for_vm_numeric   = $config_ref->{max_cpu} // 0;
    my $smt_used_for_vm_numeric  = $config_ref->{smt} // $default_smt_arg;
    my $LOG_FH                   = $log_fh;

    my $na_str_hint = "N/A";
    my @global_pressure_rationale_lines;

    # ... (initial part of rationale logging: Inputs like MaxCPU, SMT etc. - as in previous version) ...
    push @global_pressure_rationale_lines, sprintf("  Input LPAR MaxCPU            : %.2f cores", $max_cpu_for_vm_numeric);
    push @global_pressure_rationale_lines, sprintf("  Input SMT for VM             : %d", $smt_used_for_vm_numeric);
    # Determine if small entitlement handler would be active
    my $vm_entitlement = $config_ref->{'entitlement'} // 0;
    my $is_small_ent_handler = ($vm_entitlement > 0 && $vm_entitlement < 1.0 &&
                                 $max_cpu_for_vm_numeric > $vm_entitlement) ? "Yes" : "No";
    my $burst_used = ($vm_entitlement < 1.0 && $is_small_ent_handler eq "Yes") ?
                     calculate_graduated_burst($vm_entitlement) : 0.25;

    push @global_pressure_rationale_lines, sprintf("  Small Entitlement Handler    : %s", $is_small_ent_handler);
    push @global_pressure_rationale_lines, sprintf("  Burst Allowance Factor       : %.0f%% (Ent=%.2f)",
                                                   $burst_used * 100, $vm_entitlement);

    # --- VIO Server Check ---
    my $is_vio_server = 0;
    if (defined $config_ref->{systemtype} && $config_ref->{systemtype} =~ /VIO Server/i) {
       $is_vio_server = 1;
    }

    # --- Profile Value Parsing (Pattern/Peakiness & P-99W1 PhysC) ---
    my $o3_val_str = $results_ref->{'O3-95W15'} // "0";
    my $o3_val_num = ($o3_val_str ne $na_str_hint && $o3_val_str =~ /^-?[0-9.]+\z/) ? ($o3_val_str + 0) : 0;
    my $b3_val_str = $results_ref->{'B3-95W15'} // "0";
    my $b3_val_num = ($b3_val_str ne $na_str_hint && $b3_val_str =~ /^-?[0-9.]+\z/) ? ($b3_val_str + 0) : 0;
    my $g3_val_str = $results_ref->{'G3-95W15'} // "0";
    my $g3_val_num = ($g3_val_str ne $na_str_hint && $g3_val_str =~ /^-?[0-9.]+\z/) ? ($g3_val_str + 0) : 0;
    my $p99w1_physc_val_str = looks_like_number($results_ref->{$MANDATORY_PEAK_PROFILE_FOR_HINT}) ? sprintf("%.3f", $results_ref->{$MANDATORY_PEAK_PROFILE_FOR_HINT}) : $na_str_hint;
    my $p99w1_physc_val_num = ($p99w1_physc_val_str ne $na_str_hint && $p99w1_physc_val_str =~ /^-?[0-9.]+\z/)
    ? ($p99w1_physc_val_str + 0)
    : 0;
    push @global_pressure_rationale_lines, sprintf("  Input %s PhysC Value     : %s (from nfit output for %s)", $MANDATORY_PEAK_PROFILE_FOR_HINT, $p99w1_physc_val_str, $MANDATORY_PEAK_PROFILE_FOR_HINT);

    # (Pattern and Peakiness logic - unchanged)
    my $suggested_pattern = "G";
    if ($b3_val_num > 0.01 && $o3_val_num > ($b3_val_num * $PATTERN_RATIO_THRESHOLD)) { $suggested_pattern = "O"; }
    elsif ($o3_val_num > 0.01 && $b3_val_num > ($o3_val_num * $PATTERN_RATIO_THRESHOLD)) { $suggested_pattern = "B"; }
    my $peakiness_ratio = ($g3_val_num > 0.001) ? ($p99w1_physc_val_num / $g3_val_num) : 0;
    my $shape_descriptor = "Steady";
    if ($peakiness_ratio >= $HIGH_PEAK_RATIO_THRESHOLD) { $shape_descriptor = "Very Peaky"; }
    elsif ($peakiness_ratio >= $LOW_PEAK_RATIO_THRESHOLD) { $shape_descriptor = "Moderately Peaky"; }


    # --- Unified Pressure Detection ---
    my $pressure_detected_maxcpu_limit = 0;
    # Specific P-99W1 RunQ pressure flags to be returned
    my $p99w1_has_absolute_runq_pressure = 0;
    my $p99w1_has_normalized_runq_pressure = 0;
    my @pressure_points;

    # Fetch P-99W1's specific RunQ metrics
    my $p99w1_abs_runq_p90_val_str = looks_like_number($runq_metrics_ref->{'AbsRunQ_P90'}) ? sprintf("%.3f", $runq_metrics_ref->{'AbsRunQ_P90'}) : $na_str_hint;
    my $p99w1_norm_runq_p90_val_str = looks_like_number($runq_metrics_ref->{'NormRunQ_P90'}) ? sprintf("%.3f", $runq_metrics_ref->{'NormRunQ_P90'}) : $na_str_hint;

    push @global_pressure_rationale_lines, sprintf("  Input %s AbsRunQ P90     : %s threads (from nfit output for %s)", $MANDATORY_PEAK_PROFILE_FOR_HINT, $p99w1_abs_runq_p90_val_str, $MANDATORY_PEAK_PROFILE_FOR_HINT);
    push @global_pressure_rationale_lines, sprintf("  Input %s NormRunQ P90    : %s (from nfit output for %s)", $MANDATORY_PEAK_PROFILE_FOR_HINT, $p99w1_norm_runq_p90_val_str, $MANDATORY_PEAK_PROFILE_FOR_HINT);
    push @global_pressure_rationale_lines, "";


    # 1. MaxCPU Limit Pressure
    # ... (logic as before, sets $pressure_detected_maxcpu_limit, logs to @global_pressure_rationale_lines) ...
    # ... (ensure push @pressure_points, "MaxCPU"; happens if $pressure_detected_maxcpu_limit = 1;) ...
    push @global_pressure_rationale_lines, sprintf("  1. MaxCPU Limit Pressure Check (%s PhysC vs LPAR MaxCPU):", $MANDATORY_PEAK_PROFILE_FOR_HINT);
    # ... (detailed logging lines for MaxCPU check)
    my $maxcpu_limit_calc_threshold = ($max_cpu_for_vm_numeric > 0) ? ($max_cpu_for_vm_numeric * $LIMIT_THRESHOLD_PERC) : 0;
    my $maxcpu_condition_met_str = "FALSE";
    if ($max_cpu_for_vm_numeric > 0 && $p99w1_physc_val_num >= $maxcpu_limit_calc_threshold)
    {
        $pressure_detected_maxcpu_limit = 1;
        $maxcpu_condition_met_str = "TRUE";
        push @pressure_points, "MaxCPU";
    }
    push @global_pressure_rationale_lines, sprintf("     - %s PhysC Value      : %.2f", $MANDATORY_PEAK_PROFILE_FOR_HINT, $p99w1_physc_val_num);
    push @global_pressure_rationale_lines, sprintf("     - LPAR MaxCPU             : %.2f", $max_cpu_for_vm_numeric);
    push @global_pressure_rationale_lines, sprintf("     - Threshold (>= %.0f%%)      : %.2f cores", $LIMIT_THRESHOLD_PERC * 100, $maxcpu_limit_calc_threshold);
    push @global_pressure_rationale_lines, sprintf("     - Condition Met           : (%.2f >= %.2f) -> %s", $p99w1_physc_val_num, $maxcpu_limit_calc_threshold, $maxcpu_condition_met_str);
    push @global_pressure_rationale_lines, sprintf("     - MaxCPU Pressure Flag    : %s", $maxcpu_condition_met_str);
    push @global_pressure_rationale_lines, "";


    # 2. Absolute RunQ Pressure (using P-99W1's AbsRunQ_P90)
    # ... (logic as before, sets $p99w1_has_absolute_runq_pressure, logs to @global_pressure_rationale_lines) ...
    # ... (ensure push @pressure_points, ... happens if $p99w1_has_absolute_runq_pressure = 1;) ...
    my $p99w1_abs_runq_p90_num = ($p99w1_abs_runq_p90_val_str ne $na_str_hint && $p99w1_abs_runq_p90_val_str =~ /^-?[0-9.]+$/) ? ($p99w1_abs_runq_p90_val_str + 0) : undef;
    my $calculated_abs_runq_pressure_ratio = 0;
    my $lpar_max_lcpu_capacity = ($max_cpu_for_vm_numeric > 0 && $smt_used_for_vm_numeric > 0) ? ($max_cpu_for_vm_numeric * $smt_used_for_vm_numeric) : 0;
    push @global_pressure_rationale_lines, sprintf("  2. Absolute RunQ Pressure Check (%s AbsRunQ P90 vs LPAR Capacity):", $MANDATORY_PEAK_PROFILE_FOR_HINT);
    # ... (detailed logging lines for AbsRunQ check)
    if (defined $p99w1_abs_runq_p90_num && $lpar_max_lcpu_capacity > 0)
    {
        $calculated_abs_runq_pressure_ratio = $p99w1_abs_runq_p90_num / $lpar_max_lcpu_capacity;
    }
    my $absrunq_cond_met_str = "FALSE";
    if ($calculated_abs_runq_pressure_ratio > $adaptive_saturation_thresh)
    {
        $p99w1_has_absolute_runq_pressure = 1; # Set the specific flag
        $absrunq_cond_met_str = "TRUE";
        push @pressure_points, sprintf("RunQAbs_%s(P90=%.2f)", $MANDATORY_PEAK_PROFILE_FOR_HINT, $calculated_abs_runq_pressure_ratio);
    }
    push @global_pressure_rationale_lines, sprintf("     - %s AbsRunQ P90      : %s threads", $MANDATORY_PEAK_PROFILE_FOR_HINT, $p99w1_abs_runq_p90_val_str);
    push @global_pressure_rationale_lines, sprintf("     - LPAR Max LCPU Capacity  : (%.2f MaxCPU * %d SMT) = %.2f threads", $max_cpu_for_vm_numeric, $smt_used_for_vm_numeric, $lpar_max_lcpu_capacity);
    push @global_pressure_rationale_lines, sprintf("     - Calculated Ratio        : %.4f", $calculated_abs_runq_pressure_ratio);
    push @global_pressure_rationale_lines, sprintf("     - Threshold               : > %.2f", $adaptive_saturation_thresh);
    push @global_pressure_rationale_lines, sprintf("     - Condition Met           : (%.4f > %.2f) -> %s", $calculated_abs_runq_pressure_ratio, $adaptive_saturation_thresh, $absrunq_cond_met_str);
    push @global_pressure_rationale_lines, sprintf("     - %s Specific Absolute RunQ Pressure Flag: %s", $MANDATORY_PEAK_PROFILE_FOR_HINT, $absrunq_cond_met_str);
    push @global_pressure_rationale_lines, "";


    # 3. Normalised Workload Pressure (using P-99W1's NormRunQ_P90)
    # ... (logic as before, sets $p99w1_has_normalized_runq_pressure, logs to @global_pressure_rationale_lines) ...
    # ... (ensure push @pressure_points, ... happens if $p99w1_has_normalized_runq_pressure = 1;) ...
    my $p99w1_norm_runq_p90_num = ($p99w1_norm_runq_p90_val_str ne $na_str_hint && $p99w1_norm_runq_p90_val_str =~ /^-?[0-9.]+$/) ? ($p99w1_norm_runq_p90_val_str + 0) : undef;
    my $min_abs_runq_for_norm_check = $smt_used_for_vm_numeric > 0 ? $smt_used_for_vm_numeric : 1.0;
    push @global_pressure_rationale_lines, sprintf("  3. Normalised Workload Pressure Check (%s NormRunQ P90):", $MANDATORY_PEAK_PROFILE_FOR_HINT);
    # ... (detailed logging lines for NormRunQ check)
    my $normrunq_cond1_met_str = (defined $p99w1_norm_runq_p90_num && $p99w1_norm_runq_p90_num > $WORKLOAD_PRESSURE_NORM_P90_TRIGGER_THRESHOLD) ? "TRUE" : "FALSE";
    my $normrunq_cond2_met_str = (defined $p99w1_abs_runq_p90_num && $p99w1_abs_runq_p90_num >= $min_abs_runq_for_norm_check) ? "TRUE" : "FALSE";
    my $normrunq_overall_cond_met_str = "FALSE";
    if ($normrunq_cond1_met_str eq "TRUE" && $normrunq_cond2_met_str eq "TRUE")
    {
        $p99w1_has_normalized_runq_pressure = 1; # Set the specific flag
        $normrunq_overall_cond_met_str = "TRUE";
        push @pressure_points, sprintf("RunQNorm_%s(P90=%.2f)", $MANDATORY_PEAK_PROFILE_FOR_HINT, defined $p99w1_norm_runq_p90_num ? $p99w1_norm_runq_p90_num : 0);
    }
    push @global_pressure_rationale_lines, sprintf("     - %s NormRunQ P90     : %s", $MANDATORY_PEAK_PROFILE_FOR_HINT, $p99w1_norm_runq_p90_val_str);
    push @global_pressure_rationale_lines, sprintf("     - Threshold               : > %.2f", $WORKLOAD_PRESSURE_NORM_P90_TRIGGER_THRESHOLD);
    push @global_pressure_rationale_lines, sprintf("     - Condition Met (Norm)    : (%s > %.2f) -> %s", $p99w1_norm_runq_p90_val_str, $WORKLOAD_PRESSURE_NORM_P90_TRIGGER_THRESHOLD, $normrunq_cond1_met_str);
    push @global_pressure_rationale_lines, sprintf("     - %s AbsRunQ P90      : %s (for magnitude check)", $MANDATORY_PEAK_PROFILE_FOR_HINT, $p99w1_abs_runq_p90_val_str);
    push @global_pressure_rationale_lines, sprintf("     - Min AbsRunQ Threshold   : %.2f (typically SMT)", $min_abs_runq_for_norm_check);
    push @global_pressure_rationale_lines, sprintf("     - Condition Met (Mag)     : (%s >= %.2f) -> %s", $p99w1_abs_runq_p90_val_str, $min_abs_runq_for_norm_check, $normrunq_cond2_met_str);
    push @global_pressure_rationale_lines, sprintf("     - %s Specific Normalised Workload Pressure Flag: %s", $MANDATORY_PEAK_PROFILE_FOR_HINT, $normrunq_overall_cond_met_str);
    push @global_pressure_rationale_lines, "";


    # --- Combine pressure flags & Pool Context ---
    my $overall_pressure_detected_for_csv = $pressure_detected_maxcpu_limit ||
    $p99w1_has_absolute_runq_pressure || # Use the specific P-99W1 flags here for overall CSV flag
    $p99w1_has_normalized_runq_pressure;
    # Pool context logic, using @pressure_points
    push @global_pressure_rationale_lines, "  4. Pool Context:";

    # Get Pool ID from nfit's dynamic data, and Pool Name from the optional config file
    my $pool_id_from_nmon = $config_ref->{pool_id} // 0;
    my $pool_name_from_config = $vm_config_data{$vm_name}{pool_name} // undef; # Still need the original config file for this

    # A non-default pool is one where the ID is not 0.
    my $is_non_default_pool = (looks_like_number($pool_id_from_nmon) && $pool_id_from_nmon != 0);

    # Pool ID is now directly from the JSON output of nfit
    push @global_pressure_rationale_lines, sprintf("     - Pool ID                 : %s", $pool_id_from_nmon);
    push @global_pressure_rationale_lines, sprintf("     - Pool Name (from config) : %s", $pool_name_from_config // "N/A");
    push @global_pressure_rationale_lines, sprintf("     - Is Non-Default Pool     : %s", $is_non_default_pool ? "TRUE" : "FALSE");

    if ($is_non_default_pool && $overall_pressure_detected_for_csv) {
        # If we have a pool name, use it. Otherwise, fall back to the ID.
        my $pool_identifier = defined($pool_name_from_config) && $pool_name_from_config ne '' ? $pool_name_from_config : "ID:$pool_id_from_nmon";
        push @pressure_points, "Pool($pool_identifier)";
    }
    push @global_pressure_rationale_lines, "";

    my $overall_pressure_detected_for_csv_str = $overall_pressure_detected_for_csv ? "TRUE" : "FALSE";
    my $pressure_detail_str = @pressure_points ? join(", ", @pressure_points) : "None";
    push @global_pressure_rationale_lines, sprintf("  5. Overall Global Hint Pressure Flag (for CSV): %s", $overall_pressure_detected_for_csv_str);
    push @global_pressure_rationale_lines, sprintf("  6. Final PressureDetail string for CSV     : \"%s\"", $pressure_detail_str);

    my $global_pressure_rationale_text = "Section G: Global Sizing Hint Pressure Assessment (source: generate_sizing_hint)\n" .
    join("\n", @global_pressure_rationale_lines);


    # --- Tiering logic (remains unchanged) ---
    my $initial_tier_range_str = "3/4";
    # --- Tiering logic (with added safety checks for numeric values) ---
    my $p99w1_val_for_tier = $results_ref->{'P-99W1'};
    my $g3_val_for_tier    = $results_ref->{'G3-95W15'};

    if (looks_like_number($p99w1_val_for_tier) && looks_like_number($g3_val_for_tier) && $g3_val_for_tier > 0) {
        if ($p99w1_val_for_tier > $g3_val_for_tier * $HIGH_PEAK_RATIO_THRESHOLD) {
            $shape_descriptor = "Very Peaky";
        }
        elsif ($p99w1_val_for_tier > $g3_val_for_tier * $LOW_PEAK_RATIO_THRESHOLD) {
            $shape_descriptor = "Moderately Peaky";
        }
    }

    if ($shape_descriptor eq "Very Peaky") { $initial_tier_range_str = "1/2"; }
    elsif ($shape_descriptor eq "Moderately Peaky") { $initial_tier_range_str = "2/3"; }
    my $adjusted_tier_str = $initial_tier_range_str;
    if ($overall_pressure_detected_for_csv)
    {
        if ($initial_tier_range_str eq "3/4") { $adjusted_tier_str = "3"; }
        elsif ($initial_tier_range_str eq "2/3") { $adjusted_tier_str = "2"; }
        elsif ($initial_tier_range_str eq "1/2") { $adjusted_tier_str = "1"; }
    }
    my $pattern_tier_string = $suggested_pattern . $adjusted_tier_str;

    if ($is_vio_server)
    {
        # For VIOs, we return the *actual* calculated pressure flags and details,
        # but override the final tier/pattern hint for safety and clarity.
        return (
            "P",    # Override Hint to "P" for Peak/Manual
            $shape_descriptor, # Override Pattern
            $overall_pressure_detected_for_csv, # Return ACTUAL calculated pressure
            $pressure_detail_str,               # Return ACTUAL pressure details
            $global_pressure_rationale_text,
            $p99w1_has_absolute_runq_pressure,
            $p99w1_has_normalized_runq_pressure
        );
    }
    else
    {
        # For non-VIOs, return the standard, calculated hint and pattern.
        return (
            $pattern_tier_string,
            $shape_descriptor,
            $overall_pressure_detected_for_csv,
            $pressure_detail_str,
            $global_pressure_rationale_text,
            $p99w1_has_absolute_runq_pressure,
            $p99w1_has_normalized_runq_pressure
        );
    }
}
# end of generate_sizing_hint

# --- parse_percentile_list_for_header ---
# This sub is used by the OLD global RunQ metric collection logic (which is now superseded
# by per-profile RunQ metrics). It might still be called if that logic path is hit,
# or could be refactored/removed if that path is fully deprecated.
# For now, keeping it as it might be used by initial population of $results_table{$vm_name}{$rq_metric_name}.
# It prepares percentile numbers for use as metric name suffixes.
sub parse_percentile_list_for_header
{
    my ($perc_str, $clean_zeros) = @_;
    $clean_zeros = 1 if !defined $clean_zeros; # Default to cleaning "X.00" to "X"
    my @percentiles_cleaned;
    if (defined $perc_str && $perc_str ne '')
    {
        my @raw_percentiles = split /,\s*/, $perc_str;
        foreach my $p (@raw_percentiles)
        {
            if ($p =~ /^[0-9]+(?:\.[0-9]+)?$/ && $p >= 0 && $p <= 100) # Validate numeric and range
            {
                my $p_label = $p;
                if ($clean_zeros)
                {
                    $p_label = sprintf("%.2f", $p); # Format to two decimal places
                    $p_label =~ s/\.?0+$//;         # Remove trailing ".00" or ".0"
                    $p_label = "0" if $p_label eq "" && ($p eq "0" || $p eq "0.00"); # Handle "0.00" -> "0"
                }
                push @percentiles_cleaned, $p_label;
            }
            else # Invalid percentile value
            {
                die "[ERROR] Invalid percentile value '$p' found in list '$perc_str'. Must be numeric between 0 and 100.\n";
            }
        }
    }
    return \@percentiles_cleaned; # Return reference to array of cleaned percentile labels
}

# --- ensure_percentiles_requested ---
# Checks if a list of required percentiles are present in a given percentile string.
# Potentially used for validating if nfit was asked to calculate necessary percentiles
# for the old global RunQ metric collection. May be less relevant with per-profile logic.
sub ensure_percentiles_requested
{
    my ($perc_list_str, @required_percs) = @_; # perc_list_str is comma-separated, required_percs are numbers
    return 1 unless defined $perc_list_str && $perc_list_str ne ''; # If no list provided, assume not applicable or handled elsewhere

    # Parse the provided list string into a map for easy lookup
    my $parsed_percs_ref = parse_percentile_list_for_header($perc_list_str, 0); # Get raw numbers, no zero cleaning for comparison
    my %present_map = map { $_ => 1 } @{$parsed_percs_ref};

    foreach my $req_p_num (@required_percs) # Iterate through numerically required percentiles
    {
        # Check if the numeric value (or its string representation) exists in the parsed list
        my $req_p_str = "$req_p_num"; # Simple string conversion
        my $req_p_str_formatted = sprintf("%.2f", $req_p_num); # e.g. 90.00
        my $req_p_str_cleaned = $req_p_str_formatted;
        $req_p_str_cleaned =~ s/\.?0+$//;
        $req_p_str_cleaned = "0" if $req_p_str_cleaned eq "" && abs($req_p_num -0) < 0.001;


        unless (exists $present_map{$req_p_str} ||
            exists $present_map{$req_p_str_formatted} ||
            exists $present_map{$req_p_str_cleaned} )
        {
            # Check common string representations due to potential formatting differences
            my $found = 0;
            foreach my $key (keys %present_map) {
                if (abs($key - $req_p_num) < 0.001) { # Floating point comparison
                    $found = 1;
                    last;
                }
            }
            return 0 unless $found; # Required percentile not found
        }
    }
    return 1; # All required percentiles found
}

# --- get_nfit_output_dp_from_flags ---
# Determines the number of decimal places nfit is expected to use for a profile's output,
# based on the rounding flags (-r or -u) passed to nfit for that profile.
# This helps nfit-profile format its *own* adjusted values consistently.
sub get_nfit_output_dp_from_flags
{
    my ($nfit_flags_str_for_this_run) = @_; # Combined global and profile-specific flags for nfit

    # Regex to find -r[=increment] or -u[=increment]
    # It captures the increment value if provided.
    if ($nfit_flags_str_for_this_run =~ /-r(?:=(\d*\.\d+))?|-u(?:=(\d*\.\d+))?/)
    {
        my $increment_val_str = $1 // $2; # $1 for -r=val, $2 for -u=val

        # If -r or -u is present but no increment value, nfit uses its default increment.
        if (!(defined $increment_val_str && $increment_val_str ne ""))
        {
            # nfit's default increment is $DEFAULT_ROUND_INCREMENT (from nfit, assumed here to be same as nfit-profile's)
            # For robustness, it's better if nfit-profile knows nfit's default or this is coordinated.
            # Using nfit-profile's default as a proxy.
            return get_decimal_places($DEFAULT_ROUND_INCREMENT);
        }
        else # Increment value was specified
        {
            return get_decimal_places($increment_val_str);
        }
    }
    # If no -r or -u flag, nfit typically outputs with more precision (e.g., 4 decimal places by default internally).
    # nfit version 2.28.0.4 defaults to 4 DP if no rounding.
    return 4;
}

# --- get_decimal_places ---
# Calculates the number of decimal places in a given number string.
sub get_decimal_places
{
    my ($number_str) = @_;
    # Handle scientific notation by converting to fixed point string first
    $number_str = sprintf("%.15f", $number_str) if ($number_str =~ /e/i);

    if ($number_str =~ /\.(\d+)$/) # If there's a decimal part
    {
        return length($1); # Length of the digits after decimal point
    }
    else # No decimal part
    {
        return 0;
    }
}

# Helper to format a percentile number into a clean string for metric keys.
sub clean_perc_label
{
    my ($p) = @_;
    my $label = sprintf("%.2f", $p);
    $label =~ s/\.?0+$//;
    $label = "0" if $label eq "" && abs($p-0)<0.001;
    return $label;
}

# --- parse_nfit_json_output ---
# Parses the multi-line JSON output from nfit. Each line is a distinct JSON object.
# Returns a hash where keys are VM names and values are arrays of the parsed JSON objects (as Perl hashes).
sub parse_nfit_json_output
{
    my ($raw_output) = @_;
    my %parsed_data;
    my $json_decoder = JSON->new->utf8;
    my @lines = split /\n/, $raw_output;

    foreach my $line (@lines)
    {
        next if $line =~ /^\s*$/; # Skip empty lines

        my $decoded_hash = eval { $json_decoder->decode($line) };
        if ($@ || !ref($decoded_hash) eq 'HASH') {
            warn " [WARN] Could not decode JSON line from nfit: $line. Error: $@";
            next;
        }

        my $vm_name = $decoded_hash->{vmName};
        if ($vm_name) {
            push @{$parsed_data{$vm_name}}, $decoded_hash;
        }

    }

    return \%parsed_data;
}

# ==============================================================================
# Subroutine to determine the start and end date of a seasonal event period.
# It reads the event's configuration and calculates the absolute date range
# for the analysis.
# It accepts a base_date_obj to correctly calculate periods for
# historical months during an --update-history run.
# ==============================================================================
sub determine_event_period {
    my ($event_config, $base_date_obj, $context) = @_;

    my $model_type = $event_config->{model} // '';

    # Context:
    # - forecast: compute the next (or last-completed for recency_decay) period relative to base date
    # - history : compute the period that belongs to the historical month represented by base date
    #
    # Back-compat behaviour:
    # - If caller passes a base_date but no context, assume HISTORY (current behaviour).
    # - If caller passes no base_date, assume FORECAST anchored to today.
    $context //= (defined $base_date_obj) ? 'history' : 'forecast';

    # Use the provided base date for historical calculations, or default to today for forecasts.
    my $base_date = (defined $base_date_obj)
        ? $base_date_obj->truncate(to => 'day')
        : gmtime()->truncate(to => 'day');

    # --- Path for events defined by fixed 'dates' ---
    if (defined $event_config->{dates}) {
        my @date_ranges = split /\s*,\s*/, $event_config->{dates};
        my ($next_event_start, $next_event_end);

        foreach my $range (@date_ranges) {
            if ($range =~ /(\d{4}-\d{2}-\d{2}):(\d{4}-\d{2}-\d{2})/) {
                my $start_obj = Time::Piece->strptime($1, '%Y-%m-%d')->truncate(to => 'day');
                my $end_obj   = Time::Piece->strptime($2, '%Y-%m-%d')->truncate(to => 'day');

                if ($context eq 'forecast') {
                    # FORECAST CONTEXT: Find the *next* event period relative to the current date.
                    if ($end_obj >= $base_date) {
                        if (!defined $next_event_start || $start_obj < $next_event_start) {
                            $next_event_start = $start_obj;
                            $next_event_end   = $end_obj;
                        }
                    }
                } else {
                    # HISTORY CONTEXT: Check if this specific fixed-date event falls
                    # within the historical month being processed. The base_date is the
                    # first day of that historical month.
                    my $month_end = Time::Piece->new($base_date->epoch)->add_months(1) - ONE_DAY;
                    if ($start_obj >= $base_date && $end_obj <= $month_end) {
                        return ($start_obj, $end_obj);
                    }
                }
            }
        }
        # In a forecast context, we return the closest future event found.
        return ($next_event_start, $next_event_end) if (defined $next_event_start && $context eq 'forecast');
    }
    # --- Path for events defined by a recurring 'period' ---
    elsif (defined $event_config->{period} && lc($event_config->{period}) eq 'monthly') {
        my $day_of_period = $event_config->{day_of_period} // -1;
        my $duration_days = $event_config->{duration_days} // 7;

        # FORECAST CONTEXT for 'recency_decay' model: Find the *last completed* peak.
        if ($model_type eq 'recency_decay' && $context eq 'forecast') {
            my ($current_month_start, $current_month_end) = _get_recurring_monthly_period($base_date, $day_of_period, $duration_days);
            if ($current_month_end > $base_date) {
                my $last_month_base = add_months($base_date, -1);
                return _get_recurring_monthly_period($last_month_base, $day_of_period, $duration_days);
            } else {
                return ($current_month_start, $current_month_end);
            }
        }
        # FORECAST CONTEXT for other models: Find the *next upcoming* peak.
        elsif ($context eq 'forecast') {
            my ($current_month_start, $current_month_end) = _get_recurring_monthly_period($base_date, $day_of_period, $duration_days);
            if ($current_month_end >= $base_date) {
                return ($current_month_start, $current_month_end);
            } else {
                my $next_month_base = add_months($base_date, 1);
                return _get_recurring_monthly_period($next_month_base, $day_of_period, $duration_days);
            }
        }
        # HISTORY CONTEXT for any recurring model: Calculate the period for the *specific historical month* provided.
        else {
            return _get_recurring_monthly_period($base_date, $day_of_period, $duration_days);
        }
    }

    # Fallback: return nothing if no valid period is found.
    return;
}

# ==============================================================================
# Subroutine to parse a simple INI-style configuration file.
# This replaces the need for the external Config::Tiny module.
#
# Takes:
# 1. The path to the configuration file.
#
# Returns:
# - A hash reference representing the parsed INI data.
# - Dies on file open error.
# ==============================================================================
sub parse_seasonality_config {
    my ($filepath) = @_;
    my %config_data;
    my $current_section = '';

    open my $fh, '<:encoding(utf8)', $filepath
        or die "[ERROR] Cannot open seasonality config file '$filepath': $!";

    while (my $line = <$fh>) {
        chomp $line;
        $line =~ s/^\s+|\s+$//g; # Trim whitespace
        $line =~ s/\s*[#;].*//;   # Remove comments

        next if $line eq ''; # Skip empty or comment-only lines

        if ($line =~ /^\s*\[\s*([^\]]+?)\s*\]\s*$/) {
            # This is a section header
            $current_section = $1;
        } elsif ($current_section ne '' && $line =~ /^\s*([^=]+?)\s*=\s*(.+)$/) {
            # This is a key-value pair within a section
            my $key = $1;
            my $value = $2;
            $key =~ s/^\s+|\s+$//g;
            $value =~ s/^\s+|\s+$//g;
            $config_data{$current_section}{$key} = $value;
        }
    }
    close $fh;

    # --- Deprecation Guard & Normalisation ---
    # 'allow_compounding' has been removed in favour of the more explicit
    # 'interaction_policy = exclusive|combined'. We do NOT support both keys,
    # as mixed semantics are error-prone and undermine auditability.
    foreach my $section (keys %config_data) {
        # 1. Deprecation Check (Hard Fail)
        if (exists $config_data{$section}{allow_compounding}) {
            die "[ERROR] Config deprecated: 'allow_compounding' is no longer supported (found in section '$section').\n" .
                "        Use 'interaction_policy = exclusive|combined'.\n";
        }

        # 2. Alias Resolution (concurrent_interaction -> interaction_policy)
        # If the user used the alias, map it to the primary key.
        if ($config_data{$section}{concurrent_interaction}) {
            $config_data{$section}{interaction_policy} //= $config_data{$section}{concurrent_interaction};
        }

        # 3. Centralised Defaulting
        # Ensure the key always exists so downstream code doesn't need '// exclusive'
        $config_data{$section}{interaction_policy} //= 'exclusive';

        # 4. Input Sanitisation
        # Handle case variations (Exclusive vs exclusive)
        $config_data{$section}{interaction_policy} = lc($config_data{$section}{interaction_policy});
        unless ($config_data{$section}{interaction_policy} =~ /^(exclusive|combined)$/) {
            warn "[WARN] Invalid interaction_policy in [$section]: '$config_data{$section}{interaction_policy}' (defaulting to 'exclusive')\n";
            $config_data{$section}{interaction_policy} = 'exclusive';
        }
    }

    # ----------------------------------------------------------------------
    # Resolve inheritance EARLY (centralised): Global defaults + per-event overrides
    #
    # Goal: downstream code should consume a fully-resolved event config
    # hash, so helpers do not need to “know” about inheritance rules.
    #
    # Supported early-resolution behaviours:
    #  - Global fallback for common keys (any key set in [Global])
    #  - vms/exclude_vms inheritance, with support for "vms = ALL"
    #  - pre-parsed VM scope filter stored in event config as _vm_scope_filter
    # ----------------------------------------------------------------------
    my %resolved;

    # Preserve Global section as-is (raw), but also treat it as defaults.
    $resolved{Global} = { %{ $config_data{Global} // {} } };

    # Inheritable keys are simply any keys present in [Global]; per-event overrides win.
    # We additionally support explicit vm-scope semantics (vms/exclude_vms), including
    # "vms = ALL" to override a Global include restriction.
    foreach my $section (sort keys %config_data) {
        next if $section =~ /^(?:Global|Adaptive)$/i;

        my $event_raw  = $config_data{$section} // {};
        my $global_raw = $config_data{Global}   // {};

        # Start with Global defaults, then overlay event overrides.
        my %merged = (%$global_raw, %$event_raw);

        # Ensure event name is always available to downstream code.
        $merged{_eventName} = $section;

        # --- VM Scope Resolution (Phase 3.3) ---
        # Resolve inheritance and reserved keyword handling here.
        my $vms_str = exists $event_raw->{vms}
            ? $event_raw->{vms}
            : ($global_raw->{vms} // '');

        my $exclude_vms_str = exists $event_raw->{exclude_vms}
            ? $event_raw->{exclude_vms}
            : ($global_raw->{exclude_vms} // '');

        $vms_str //= '';
        $exclude_vms_str //= '';

        # "vms = ALL" explicitly means "include all VMs" and overrides any Global include list.
        if ($vms_str =~ /^\s*all\s*$/i) {
            $vms_str = '';
        }

        my $vm_scope_filter = _build_vm_scope_filter_from_strings(
            $vms_str,
            $exclude_vms_str,
            exists $event_raw->{vms}        ? 'event'  : 'global',
            exists $event_raw->{exclude_vms}? 'event'  : 'global',
        );

        # Store resolved strings for audit/logging and the parsed filter for fast use.
        $merged{vms}            = $vms_str;
        $merged{exclude_vms}    = $exclude_vms_str;
        $merged{_vm_scope_filter} = $vm_scope_filter;  # may be undef

        $resolved{$section} = \%merged;
    }

    return \%resolved;

}

# ------------------------------------------------------------------------------
# Helper: build a VM scope filter structure from resolved strings.
# Centralised so parse_seasonality_config can resolve inheritance early.
# Returns undef if no filters apply.
# ------------------------------------------------------------------------------
sub _build_vm_scope_filter_from_strings {
    my ($vms_str, $exclude_vms_str, $include_source, $exclude_source) = @_;

    $vms_str //= '';
    $exclude_vms_str //= '';

    return undef unless ($vms_str =~ /\S/ || $exclude_vms_str =~ /\S/);

    my %result = (
        include => undef,
        exclude => undef,
        include_source => $include_source,
        exclude_source => $exclude_source,
    );

    # Parse 'vms' (include list)
    if ($vms_str =~ /\S/) {
        my %include_lookup;
        foreach my $vm_raw (split /,/, $vms_str) {
            my $vm = $vm_raw;
            $vm =~ s/^\s+//;
            $vm =~ s/\s+$//;
            next unless length($vm);
            $include_lookup{$vm} = 1;
        }
        $result{include} = \%include_lookup if %include_lookup;
    }

    # Parse 'exclude_vms' (exclude list)
    if ($exclude_vms_str =~ /\S/) {
        my %exclude_lookup;
        foreach my $vm_raw (split /,/, $exclude_vms_str) {
            my $vm = $vm_raw;
            $vm =~ s/^\s+//;
            $vm =~ s/\s+$//;
            next unless length($vm);
            $exclude_lookup{$vm} = 1;
        }
        $result{exclude} = \%exclude_lookup if %exclude_lookup;
    }

    return undef unless ($result{include} || $result{exclude});
    return \%result;
}

# ==============================================================================
# SUBROUTINE: calculate_multiplicative_forecast (NEW ROBUST DESIGN)
# PURPOSE:    Calculates a forecast using the multiplicative seasonal model.
#             This version implements a robust, multi-stage forecast. It
#             intelligently determines the most appropriate recent baseline and
#             applies a recency-weighted seasonal multiplier derived from
#             historical, saturation-corrected peak data. It also includes
#             logic for volatility, residual peak forecasting, and concurrent event interaction.
# RETURNS:
#   - A hash reference containing the final forecasted results.
#   - A hash reference containing historical data for verbose reporting.
# ==============================================================================
sub calculate_multiplicative_forecast {
    my ($system_cache_dir, $system_identifier, $event_name, $event_config, $full_seasonality_config, $initial_results_table_href, $exclusions_href, $asof_start_obj, $asof_end_obj, $precomputed_baselines_href) = @_;

    # This subroutine implements a robust, multi-stage forecast for multiplicative seasonal events.
    # It intelligently determines the most appropriate recent baseline and applies a recency-weighted
    # seasonal multiplier derived from historical, saturation-corrected peak data.

    print STDERR "    ⧉ Executing Multiplicative Seasonal Forecast for event $event_name on system $system_identifier\n";

    # --- Configuration Constants ---
    # The number of days of stable, non-peak activity to use for the baseline.
    my $baseline_days_config = $event_config->{baseline_period_days} // 16;
    # A baseline must have at least this many clean days to be considered statistically valid.
    my $MIN_BASELINE_DAYS = 7;
    # Placeholder for future enhancement: A trend adjustment factor could be applied here.
    my $trend_adjustment_factor = 1.0;

    # --- Step 1: Get Key Dates and Read Unified History ---
    my $data_cache_file = File::Spec->catfile($system_cache_dir, '.nfit.cache.data');
    my ($cache_start_date_obj, $cache_end_date_obj) = _get_cache_date_range($data_cache_file);

    my $analysis_start_obj = (defined $asof_start_obj && ref($asof_start_obj) eq 'Time::Piece')
        ? $asof_start_obj->truncate(to => 'day')
        : $cache_start_date_obj->truncate(to => 'day');

    my $analysis_end_obj = (defined $asof_end_obj && ref($asof_end_obj) eq 'Time::Piece')
        ? $asof_end_obj->truncate(to => 'day')
        : $cache_end_date_obj->truncate(to => 'day');

    unless ($analysis_start_obj && $analysis_end_obj) {
        warn "  [WARN] Could not determine analysis date range. Forecasting cannot proceed.\n";
        return ({}, {});
    }

    my $unified_history = read_unified_history($system_cache_dir);
    my @event_history_initial;
    my ($most_recent_peak_snapshot, $most_recent_peak_end_obj);

    # Find all historical snapshots for this specific event from the unified history.
    foreach my $month_key (sort { $b cmp $a } keys %$unified_history) {
        my $month_data = $unified_history->{$month_key};
        if (exists $month_data->{SeasonalEventSnapshots}{$event_name}) {
            my $snapshot = $month_data->{SeasonalEventSnapshots}{$event_name};
            $snapshot->{_month_key} = $month_key;
            $snapshot->{eventName} //= $event_name;  # Ensure eventName is set for history lookup

            my $include_snapshot = 0;

            if ($snapshot->{periodEndDate}) {
                my $snap_end_obj;
                eval {
                    $snap_end_obj = Time::Piece->strptime($snapshot->{periodEndDate}, '%Y-%m-%d')->truncate(to => 'day');
                };
                if (!$@ && $snap_end_obj && $snap_end_obj <= $analysis_end_obj) {
                    $include_snapshot = 1;
                }
            } else {
                # Fallback: compare month key to analysis_end month (conservative)
                my $analysis_month_key = $analysis_end_obj->strftime('%Y-%m');
                $include_snapshot = 1 if ($month_key le $analysis_month_key);
            }

            push @event_history_initial, $snapshot if $include_snapshot;
        }
    }

    # Populate most_recent_peak from the first (newest) snapshot in the specified analysis time-frame if available
    if (@event_history_initial) {
        $most_recent_peak_snapshot = $event_history_initial[0];
        if ($most_recent_peak_snapshot->{periodEndDate}) {
            eval {
                $most_recent_peak_end_obj = Time::Piece->strptime(
                    $most_recent_peak_snapshot->{periodEndDate}, '%Y-%m-%d'
                );
            };
            if ($@) {
                warn "  [WARN] Could not parse most recent peak period end-date '$most_recent_peak_snapshot->{periodEndDate}': $@\n";
                $most_recent_peak_end_obj = undef;
            }
        }
    }

    # --- Step 2: Two-Tiered Baseline Strategy ---
    my %current_baseline_results;
    my $baseline_source_log = "N/A";
    my @event_history_final = @event_history_initial; # Start with all available history.

    # Phase 6: Use pre-computed history-based baseline if provided by orchestrator.
    # When present, this replaces the legacy L1-based Path A/B/C below.
    if ($precomputed_baselines_href && ref($precomputed_baselines_href) eq 'HASH' && %$precomputed_baselines_href) {
        %current_baseline_results = %$precomputed_baselines_href;
        $baseline_source_log = "History-based (canonical window)";
        print STDERR "  Φ Multiplicative Seasonal Forecast [Baseline]: history-based (pre-computed)\n";
    } else {

    # --- Path A (Ideal): Attempt to calculate a "Post-Peak" baseline ---
    # The ideal baseline is the N days of data ending on the last day of the cache.
    my $post_peak_baseline_start_obj = $analysis_end_obj->truncate(to => 'day') - (($baseline_days_config - 1) * 86400);

    # Check if this ideal baseline period is contaminated by the most recent historical peak.
    my $is_contaminated = 0;
    if ($most_recent_peak_end_obj) {
        $is_contaminated = ($post_peak_baseline_start_obj <= $most_recent_peak_end_obj);
    }
    # Also check if the clean period is long enough to be statistically valid.
    my $clean_days_available = $is_contaminated ?
    ($analysis_end_obj->epoch - $most_recent_peak_end_obj->epoch) / 86400 - 1 :
    $baseline_days_config;

    if (!$is_contaminated && $clean_days_available >= $MIN_BASELINE_DAYS) {
        $baseline_source_log = "Post-Peak (calculated from " . $post_peak_baseline_start_obj->date . " to " . $analysis_end_obj->date . ")";
        print STDERR "  Φ Multiplicative Seasonal Forecast [Baseline window]: post-peak (" . $post_peak_baseline_start_obj->date . " ↔ " . $analysis_end_obj->date . ")\n";

        my $start_str = $post_peak_baseline_start_obj->strftime('%Y-%m-%d');
        my $end_str   = $analysis_end_obj->strftime('%Y-%m-%d');

        my $profile_count = scalar(@profiles);
        my $profile_num = 0;
        foreach my $profile (@profiles) {
            $profile_num++;
            my $profile_name = $profile->{name};
            print STDERR "\r  • Analysing post-peak baseline $profile_num/$profile_count: $profile_name";
            # This is a contextual baseline, so is_generic is 0.
            my ($nfit_cmd, $manifest_obj) = _build_nfit_baseline_command($profile->{flags}, $start_str, $end_str, $system_cache_dir, 0, 0, undef, $profile->{name}, $exclusions_href);
            my $nfit_output = '';
            my $stderr_arg = ">&=" . fileno(STDERR);
            my $pid_nfit = open3(undef, my $stdout_nfit, $stderr_arg, $nfit_cmd);
            while (my $line = <$stdout_nfit>) { $nfit_output .= $line; }
            waitpid($pid_nfit, 0);
            next if $?; # Skip if nfit command failed

            my $parsed = parse_nfit_json_output($nfit_output);
            my $p_key  = "P" . clean_perc_label(($profile->{flags} =~ /-p\s+([0-9.]+)/) ? $1 : $DEFAULT_PERCENTILE);

            foreach my $vm (keys %$parsed) {
                my @vals = map {
                    my $physc = $_->{metrics}{physc} // {};
                    my $blk   = (ref($physc) eq 'HASH') ? ($physc->{$profile_name} // $physc) : {};
                    (ref($blk) eq 'HASH') ? $blk->{$p_key} : undef;
                } @{$parsed->{$vm}};
                my @valid_vals = grep { defined $_ && looks_like_number($_) } @vals;
                if (@valid_vals) {
                    $current_baseline_results{$vm}{$profile->{name}} = sum0(@valid_vals) / scalar(@valid_vals);
                }
            }
        }
        print STDERR "\r  • Analysis of post-peak baseline is complete.\t\t\t\t\n";
    }
    # --- Path B (Fallback): Re-use the "Pre-Peak" baseline from the most recent snapshot ---
    # periodEndDate: end of the Peak
    #
    elsif ($most_recent_peak_snapshot) {
        my $b_start = $most_recent_peak_snapshot->{baselinePeriod}{startDate};
        my $b_end   = $most_recent_peak_snapshot->{baselinePeriod}{endDate};

        if ($b_start && $b_end) {
             $baseline_source_log = "Pre-Peak ($b_start - $b_end)";
        } else {
             # Legacy Fallback
             $baseline_source_log = "Pre-Peak (ending " . $most_recent_peak_snapshot->{periodEndDate} . ")";
        }

        if ($is_contaminated) {
            print STDERR "  Φ Multiplicative Seasonal Forecast [Baseline window]: pre-peak (post-peak disqualified: contaminated)\n";
        } elsif ($clean_days_available < $MIN_BASELINE_DAYS) {
            print STDERR "  Φ Multiplicative Seasonal Forecast [Baseline window]: pre-peak (post-peak disqualified: too short)\n";
        } else {
            print STDERR "  Φ Multiplicative Seasonal Forecast [Baseline window]: pre-peak (post-peak disqualified)\n";
        }

        %current_baseline_results = %{$most_recent_peak_snapshot->{results}{HistoricBaseline} || {}};
        # If we re-use this baseline, we must exclude its corresponding peak from the multiplier calculation.
        shift @event_history_final;
    }

    } # end Phase 6: else (L1 legacy path)

    # --- Path C (Cold Start Recovery): Calculate baseline from pre-event window ---
    # This path activates when:
    #   1. Path A produced no results (post-peak too short/contaminated), AND
    #   2. Path B produced no results (snapshot has no valid baseline data)
    # For weekly events, this is the expected path since post-peak windows are too short.
    #
    unless (%current_baseline_results) {
        if ($analysis_end_obj) {
            print STDERR "  Φ Multiplicative Seasonal Forecast [Baseline strategy]: Cold Start (baseline_period_days)\n";

            if ($analysis_start_obj) {
                # Calculate baseline window relative to anchor date, respecting baseline_period_days
                # Baseline ends the day before the event (exclude event day)
                my $baseline_end_obj = $analysis_end_obj - ONE_DAY;

                # Baseline starts baseline_period_days before the end
                my $baseline_start_obj = $baseline_end_obj - (($baseline_days_config - 1) * ONE_DAY);

                # Clamp to cache start if baseline would extend before available data
                if ($baseline_start_obj->epoch < $analysis_start_obj->epoch) {
                    $baseline_start_obj = $analysis_start_obj;
                }

                my $start_str = $baseline_start_obj->strftime('%Y-%m-%d');
                my $end_str   = $baseline_end_obj->strftime('%Y-%m-%d');
                $baseline_source_log = "Pre-Event Baseline ($start_str to $end_str) [Cold Start, ${baseline_days_config}d config]";

                print STDERR "    ⧗ Cold-start baseline window: $start_str → $end_str\n";

                foreach my $profile (@profiles) {
                    my $profile_name = $profile->{name};
                    # Use is_generic_baseline=1 to strip time filters that might exclude data
                    my ($nfit_cmd, $manifest_obj) = _build_nfit_baseline_command(
                        $profile->{flags}, $start_str, $end_str, $system_cache_dir,
                        0,    # enable_clipping_detection
                        1,    # HistoricBaseline: contextual baseline (keeps time filters, matching the logic for peak; ensure "apples to apples" comparison)
                        undef, # allow_growth_prediction
                        $profile_name,
                        $exclusions_href
                    );

                    my $nfit_output = '';
                    my $stderr_arg = ">&=" . fileno(STDERR);
                    my $pid_nfit = open3(undef, my $stdout_nfit, $stderr_arg, $nfit_cmd);
                    while (my $line = <$stdout_nfit>) { $nfit_output .= $line; }
                    waitpid($pid_nfit, 0);
                    if ($?) {
                        print STDERR "    [DEBUG] nFit baseline command failed (exit $?) for profile $profile_name\n";
                        print STDERR "    [DEBUG] Window: $start_str → $end_str\n";
                        next;
                    }
                    if (!$nfit_output || $nfit_output !~ /\S/) {
                        print STDERR "    [DEBUG] nFit returned empty output for profile $profile_name\n";
                        next;
                    }

                    my $parsed = parse_nfit_json_output($nfit_output);
                    my $p_key = "P" . clean_perc_label(
                        ($profile->{flags} =~ /-p\s+([0-9.]+)/) ? $1 : $DEFAULT_PERCENTILE
                    );

                    foreach my $vm (keys %$parsed) {
                        my @vals = map {
                            my $physc = $_->{metrics}{physc} // {};
                            my $blk   = (ref($physc) eq 'HASH') ? ($physc->{$profile_name} // $physc) : {};
                            (ref($blk) eq 'HASH') ? $blk->{$p_key} : undef;
                        } @{$parsed->{$vm}};
                        my @valid_vals = grep { defined $_ && looks_like_number($_) } @vals;
                        if (@valid_vals) {
                            $current_baseline_results{$vm}{$profile_name} = sum0(@valid_vals) / scalar(@valid_vals);
                        }
                    }
                }

                if (%current_baseline_results) {
                    print STDERR "  ✓ Multiplicative Seasonal Forecast: Calculated Cold Start baseline for " . scalar(keys %current_baseline_results) . " VM(s)\n";
                }
            }
        }
    }

    unless (%current_baseline_results) {
        warn "  [WARN] Could not determine a valid baseline. Forecasting cannot proceed.\n";
        return ({}, {});
    }

    # --- Step 3: Detect Concurrent Events for Interaction (Overlap) ---
    my $today_for_recency = (defined $asof_end_obj && ref($asof_end_obj) eq 'Time::Piece') ? $asof_end_obj->truncate(to => 'day') : gmtime()->truncate(to => 'day');
    my ($next_event_start, $next_event_end) = determine_event_period($event_config, $today_for_recency, 'forecast');

    my $forecast_date_obj = (defined $next_event_start && ref($next_event_start) eq 'Time::Piece') ? $next_event_start : $today_for_recency;

    # Interaction policy controls whether this event combines with other concurrent events.
    # Values: exclusive (default) | combined
    my $primary_interaction_policy = lc($event_config->{interaction_policy} // 'exclusive');
    unless ($primary_interaction_policy =~ /^(exclusive|combined)$/) {
        warn "  [WARN] Invalid interaction_policy '$primary_interaction_policy' for event '$event_name' (defaulting to 'exclusive')\n";
        $primary_interaction_policy = 'exclusive';
    }

    # The current event being processed is ALWAYS the primary event.
    my @active_events;
    my $current_event_cfg_copy = { %$event_config, _eventName => $event_name };
    push @active_events, $current_event_cfg_copy;

    # Detect other concurrent events using STRICT overlap detection.
    # This checks for actual interval intersection with the analysis window,
    # NOT horizon-expanded discovery ranges.
    my @detected_concurrent = _detect_concurrent_events_strict(
        $analysis_start_obj,
        $analysis_end_obj,
        $event_name,
        $full_seasonality_config
    );

    my @overlap_event_names;
    my @ignored_overlap_names;

    foreach my $detected (@detected_concurrent) {
        my $detected_name = ($detected->{_eventName} // '');
        next if ($detected_name eq '' || $detected_name eq $event_name);
        push @overlap_event_names, $detected_name;

        if ($primary_interaction_policy eq 'combined') {
            my $detected_policy = lc($detected->{interaction_policy} // 'exclusive');
            if ($detected_policy eq 'combined') {
                push @active_events, $detected;
            } else {
                push @ignored_overlap_names, $detected_name;
            }
        } else {
            push @ignored_overlap_names, $detected_name;
        }
    }

    # --- Step 4: Calculate Recency-Weighted Multiplier & Final Forecast ---
    my %final_forecasts;
    my %historic_data_for_csv; # For verbose reporting

    if (!@active_events) {
        print STDERR "  Φ Multiplicative Seasonal Forecast [strategy]: no active events; baseline-only forecast selected\n";
        # If no events are active, the forecast is simply the baseline.
        foreach my $vm_name (keys %current_baseline_results) {
            foreach my $profile (@profiles) {
                $final_forecasts{$vm_name}{$profile->{name}} = $current_baseline_results{$vm_name}{$profile->{name}};
            }
        }
        return (\%final_forecasts, \%historic_data_for_csv);
    }

    my $is_interaction_run = (scalar(@active_events) > 1);
    my @dampening_factors;

    my $event_count = scalar(@active_events);
    my $event_list  = join(", ", map { $_->{_eventName} } @active_events);

    print STDERR "  ◆ Active seasonal events: $event_count ($event_list)\n";

    # Observability: report overlap discovery even when interaction is exclusive.
    if (@overlap_event_names) {
        my $n = scalar(@overlap_event_names);
        my $names = join(", ", @overlap_event_names);

        # Did overlap actually affect this run?
        # (You already know whether combined interaction applied: $primary_interaction_policy and $is_interaction_run)
        my $effect =
            ($primary_interaction_policy eq 'combined' && $is_interaction_run) ? "effect: applied"
          : ($primary_interaction_policy eq 'combined' && !$is_interaction_run) ? "effect: none"
          : "effect: ignored (exclusive)";

        print STDERR "  ◆ Concurrent Event overlap: $n ($effect)\n";

        if (($verbose // 0) >= 1) {
            print STDERR "    ↳  Overlapping Events: $names\n";
        }
    }

    # Interaction: combined means we will multiply in other events' multipliers (with optional dampening).
    if ($primary_interaction_policy eq 'combined') {
        if ($is_interaction_run) {
            my @dampening_candidates = grep { defined $_ && looks_like_number($_) } map { $_->{interaction_dampening_factor} } @active_events;
            my $dampening_to_apply = @dampening_candidates ? (sort { $a <=> $b } @dampening_candidates)[0] : undef;

            print STDERR "  [INFO] Concurrent Event Interaction: " . scalar(@active_events) . " events active ($event_list)\n";
            if (defined $dampening_to_apply) {
                print STDERR "         → Multipliers will be combined with dampening factor $dampening_to_apply\n";
            } else {
                print STDERR "         → Multipliers will be combined (no dampening configured)\n";
            }
        } else {
            print STDERR "  Φ Concurrent Event Interaction policy: combined (no combined participants)\n";
            if (@overlap_event_names) {
                print STDERR "         → Overlap present but no combined participants: " . join(", ", @overlap_event_names) . "\n";
            }
        }
    } else {
        print STDERR "  Φ Concurrent Event Interaction policy: exclusive (no multiplier combination)\n";
        if (@ignored_overlap_names) {
            print STDERR "         → Ignoring concurrent event(s): " . join(", ", @ignored_overlap_names) . "\n";
        }
    }

    # Main processing loop: iterate through each VM that has a valid baseline.
    foreach my $vm_name (sort keys %current_baseline_results) {
        my $vm_forecasts = {};
        my %vm_historic_multipliers;
        my %vm_historic_residuals;

        # Gather historical data for all active events for this VM.
        foreach my $active_event_cfg (@active_events) {
            my $active_event_name = $active_event_cfg->{_eventName};

            my @history_for_this_event;
            foreach my $hist_event (@event_history_final) {
                if ($hist_event->{eventName} eq $active_event_name) {
                    push @history_for_this_event, $hist_event;
                }
            }

            # Only proceed to calculate multipliers if history exists for this event.
            if (@history_for_this_event) {
                if (defined $active_event_cfg->{interaction_dampening_factor}) {
                    push @dampening_factors, $active_event_cfg->{interaction_dampening_factor};
                }
                foreach my $profile (@profiles) {
                    my $p_name = $profile->{name};

                    my @multipliers;
                    my @residuals;
                    foreach my $hist_event (@history_for_this_event) {
                        my $hist_results = $hist_event->{results};
                        my $hist_peak = $hist_results->{HistoricPeak}{$vm_name}{$p_name};
                        if (defined $hist_results->{ClippingInfo}{$vm_name}{$p_name}{unclippedPeakEstimate} && looks_like_number($hist_results->{ClippingInfo}{$vm_name}{$p_name}{unclippedPeakEstimate})) {
                            $hist_peak = $hist_results->{ClippingInfo}{$vm_name}{$p_name}{unclippedPeakEstimate};
                        }
                        my $hist_base = $hist_results->{HistoricBaseline}{$vm_name}{$p_name};
                        my $hist_residual = $hist_results->{PeakResidual}{$vm_name}{$p_name};

                        if (defined $hist_peak && defined $hist_base && $hist_base > 0.001) {
                            push @multipliers, { value => $hist_peak / $hist_base, date => Time::Piece->strptime($hist_event->{periodEndDate}, '%Y-%m-%d') };
                        }
                        if (defined $hist_residual && looks_like_number($hist_residual)) {
                            push @residuals, { value => $hist_residual, date => Time::Piece->strptime($hist_event->{periodEndDate}, '%Y-%m-%d') };
                        }
                    }
                    next unless @multipliers;

                    my $event_multiplier = calculate_recency_weighted_average(\@multipliers, $today_for_recency, 365);
                    $vm_historic_multipliers{$p_name}{$active_event_name} = { 'multiplier' => $event_multiplier, 'history' => \@multipliers };
                    $vm_historic_residuals{$p_name}{$active_event_name} = \@residuals;

                }
            }
        }

        # Now, synthesize the final forecast for each profile.
        foreach my $profile (@profiles) {
            my $p_name = $profile->{name};

            # EXEMPTION: Skip measurement profiles
            # P-99W1 is an empirical measurement (99.75th percentile of smoothed data),
            # not a sizing recommendation. Applying multipliers would be statistically
            # invalid and could produce physical impossibilities.
            # Note: "Peak" is not a profile - it's PeakValue in CoreResults, populated
            # separately from the raw peak tracker.
            if ($p_name eq $MANDATORY_PEAK_PROFILE_FOR_HINT) {
                next;
            }

            my $current_baseline_val = $current_baseline_results{$vm_name}{$p_name};

            next unless (defined $current_baseline_val && looks_like_number($current_baseline_val));

            # Bootstrap run: use multiplier = 1.0 if no history exists (automatic for new events)
            # No multipliers found for this profile - could be:
            # 1. True bootstrap (no snapshots at all) - _bootstrap_active is set
            # 2. Snapshots exist but have no valid peak/baseline data (prior bootstrap run)
            # In either case, proceed with baseline-only forecast (multiplier = 1.0)
            my $is_bootstrap_run = 0;
            unless (exists $vm_historic_multipliers{$p_name}) {
                # No multipliers found - proceed with baseline-only forecast
                $is_bootstrap_run = 1;
            }

            my $final_multiplier = 1.0;
            unless ($is_bootstrap_run) {
                foreach my $event_data (values %{$vm_historic_multipliers{$p_name}}) {
                    $final_multiplier *= $event_data->{'multiplier'};
                }
            }

            if ($is_interaction_run && @dampening_factors) {
                my $dampening_to_apply = (sort { $a <=> $b } @dampening_factors)[0];
                $final_multiplier = 1 + (($final_multiplier - 1) * $dampening_to_apply);
            }

            my $volatility_buffer = 1.0;
            my $primary_event_cfg = (grep { $_->{_eventName} eq $event_name } @active_events)[0] || $active_events[0];
            if (($primary_event_cfg->{volatility_adjustment} // 0) == 1 && exists $vm_historic_multipliers{$p_name}{$primary_event_cfg->{_eventName}}) {
                my $primary_event_history_ref = $vm_historic_multipliers{$p_name}{$primary_event_cfg->{_eventName}}{'history'};
                my $confidence = $primary_event_cfg->{seasonal_confidence_level} // '0.95';
                $volatility_buffer = calculate_volatility_buffer($primary_event_history_ref, $confidence);
            }

            my $forecasted_residual = 0;
            my @all_residuals_for_profile;
            if (exists $vm_historic_residuals{$p_name}) {
                foreach my $event_name (keys %{$vm_historic_residuals{$p_name}}) {
                    push @all_residuals_for_profile, @{$vm_historic_residuals{$p_name}{$event_name}};
                }
            }
            if (@all_residuals_for_profile) {
                $forecasted_residual = calculate_recency_weighted_average(\@all_residuals_for_profile, $today_for_recency, 365) // 0;
            }

            my $trend_adjusted_baseline = $current_baseline_val * $trend_adjustment_factor;
            my $primary_forecast = $trend_adjusted_baseline * $final_multiplier * $volatility_buffer;
            my $combined_forecast = $primary_forecast + $forecasted_residual;
            my $amplification = $event_config->{peak_amplification_factor} // 1.0;
            my $final_recommendation = $combined_forecast * $amplification;

            $vm_forecasts->{$p_name} = $final_recommendation;

            # Create a comma-separated list of active event names
            my $active_events_str = join(", ", map { $_->{_eventName} } @active_events);

            # Extract observed peak from initial results (before forecast overwrites it)
            my $observed_peak_val = undef;
            if ($initial_results_table_href && exists $initial_results_table_href->{$vm_name}{CoreResults}{ProfileValues}{$p_name}) {
                $observed_peak_val = $initial_results_table_href->{$vm_name}{CoreResults}{ProfileValues}{$p_name};
            }

            $seasonal_debug_info{$vm_name}{$p_name} = {
                historical_multipliers => $vm_historic_multipliers{$p_name}{$event_name}{'history'},
                baseline             => $current_baseline_val,
                baseline_source      => $baseline_source_log,
                trend_factor         => $trend_adjustment_factor,
                multiplier           => $final_multiplier,
                volatility           => $volatility_buffer,
                forecasted_residual  => $forecasted_residual,
                amplification_factor => $amplification,
                forecast             => $final_recommendation,
                observed_peak        => $observed_peak_val,  # Store for history
                OutlierWarning       => _get_warning_for_vm_forecast($vm_name, \%outlier_warnings),
                active_events        => $active_events_str,
                is_bootstrap_run     => $is_bootstrap_run,
            };
        }
        $final_forecasts{$vm_name} = $vm_forecasts;
    }

    # Populate data for the verbose historic_snapshot.csv file.
    if ($most_recent_peak_snapshot) {
        foreach my $vm_name (keys %{$most_recent_peak_snapshot->{results}{HistoricPeak}}) {
            $historic_data_for_csv{$vm_name}{'HistoricPeak'} = $most_recent_peak_snapshot->{results}{HistoricPeak}{$vm_name} || {};
            $historic_data_for_csv{$vm_name}{'HistoricBaseline'} = $most_recent_peak_snapshot->{results}{HistoricBaseline}{$vm_name} || {};
        }
    }

    # Return both the forecasts and the historical data for verbose reporting.
    return (\%final_forecasts, \%historic_data_for_csv);
}

# ==============================================================================
# Helper subroutine to write the multi-file CSV output for seasonal forecasts.
# ==============================================================================
# This version safely enhances the seasonal forecast output:
# - It adds a new, rich format ONLY for the 'final_forecast.csv'.
# - The logic for 'current_baseline' and 'historic_snapshot' is UNCHANGED,
#   ensuring no regressions.
# - It is fully commented and robust.
# ==============================================================================
sub write_seasonal_csv_output {
    my ($type, $system_id, $event_name, $timestamp, $data_href) = @_;

    # Prevent creation of empty files
    if (!defined $data_href || !%{$data_href} || !scalar(keys %{$data_href})) {
        print STDERR "  [INFO] No data available for seasonal output type '$type'; file not generated\n";
        return;
    }

    # Sanitise identifiers for use in filenames.
    my $s_id_safe = $system_id;
    my $e_name_safe = $event_name;
    $s_id_safe =~ s/[^a-zA-Z0-9_.-]//g;
    $e_name_safe =~ s/[^a-zA-Z0-9_.-]//g;

    my $filename = File::Spec->catfile($output_dir, "nfit-profile.$s_id_safe.$e_name_safe.$type.$timestamp.csv");
    # Store filename for final notification
    push @generated_files, $filename;

    print STDERR "    ↳  $filename\n";

    open my $fh, '>', $filename or do {
        warn " [WARN] Could not open output file '$filename': $!";
        return;
    };

    # --- Check if this is the rich forecast report ---
    if ($type eq 'final_forecast') {
        # --- PATH A: Generate the new, rich final_forecast.csv ---

        # Build the enhanced header with all standard columns plus the new one.
        my @header = (
            "VM", "TIER", "Hint", "Pattern", "Pressure", "PressureDetail", "SMT",
            "Serial", "SystemType", "Pool Name", "Pool ID", $PEAK_PROFILE_NAME
        );
        push @header, (map { $_->{name} } @csv_visible_profiles);
        # Add the new SeasonalMultiplier column before the entitlement columns.
        push @header, ("SeasonalMultiplier", "Current - ENT");
        print $fh join(",", map { quote_csv($_) } @header) . "\n";

        foreach my $vm_name (sort keys %$data_href) {
            my $vm_data = $data_href->{$vm_name};
            my $report_data = $vm_data->{_report_data} || {};
            my $cfg_csv = $vm_config_data{$vm_name};

            # Gather standard fields for the row
            my $smt_out = $cfg_csv->{smt} // $default_smt_arg;
            my ($serial_out, $systype_out, $poolname_out, $poolid_out, $ent_out) = ('', '', '', '', '');
            if (defined $cfg_csv) {
                $serial_out = $cfg_csv->{serial} // ''; $systype_out = $cfg_csv->{systemtype} // '';
                $poolname_out = $cfg_csv->{pool_name} // ''; $poolid_out = $cfg_csv->{pool_id} // '';
                $ent_out = $cfg_csv->{entitlement} // '';
            }

            my @row = (
                $vm_name, "", $report_data->{hint} // "", $report_data->{pattern} // "",
                $report_data->{pressure} // "", $report_data->{pressure_detail} // "", $smt_out,
                $serial_out, $systype_out, $poolname_out, $poolid_out, "" # Peak is not applicable for a forecast
            );

            my $seasonal_multiplier_for_row = "N/A";

            # --- Targeted Lookup for Seasonal Multiplier ---
            # 1. Determine Pattern/Tier (Respecting User Override)
            # We can use the global %vm_tier_overrides since it's populated at startup
            my $user_tier_override = $vm_tier_overrides{$vm_name} // "";
            my $hint_pattern_str = ($user_tier_override ne "") ? $user_tier_override : ($report_data->{pattern} // "G");
            my ($hint_pattern) = ($hint_pattern_str =~ /^([A-Z])/);
            $hint_pattern //= 'G';

            # 2. Map to standard profile
            my %pattern_to_profile_map = ('O'=>'O3-95W15', 'B'=>'B3-95W15', 'G'=>'G3-95W15', 'P'=>'G3-95W15');
            my $target_profile_name = $pattern_to_profile_map{$hint_pattern} // 'G3-95W15';

            # 3. Fetch multiplier from that specific profile
            if (exists $report_data->{seasonal_debug}{$target_profile_name}) {
                $seasonal_multiplier_for_row = sprintf("%.2f", $report_data->{seasonal_debug}{$target_profile_name}{multiplier});
            } elsif (exists $report_data->{seasonal_debug}{'G3-95W15'}) {
                # Fallback
                 $seasonal_multiplier_for_row = sprintf("%.2f", $report_data->{seasonal_debug}{'G3-95W15'}{multiplier});
            }

            foreach my $profile (@csv_visible_profiles) {
                my $p_name = $profile->{name};
                my $value = $vm_data->{$p_name} // '';
                push @row, (looks_like_number($value) ? sprintf("%.4f", $value) : $value);
            }

            # Add the new column value and the final entitlement value
            push @row, $seasonal_multiplier_for_row;
            push @row, (looks_like_number($ent_out) ? sprintf("%.2f", $ent_out) : $ent_out);

            print $fh join(",", map { quote_csv($_) } @row) . "\n";
        }

    } else {
        # --- PATH B: Original logic for other file types (UNCHANGED) ---

        # --- Build header ---
        my @header;
        if ($type eq 'historic_snapshot') {
            # Specialised header for the detailed historic report.
            @header = ("VM", "MetricType");
        } else {
            # Standard header for 'current_baseline' and 'final_forecast'
            @header = (
                "VM", "TIER", "Hint", "Pattern", "Pressure", "PressureDetail", "SMT",
                "Serial", "SystemType", "Pool Name", "Pool ID", $PEAK_PROFILE_NAME
            );
        }
        push @header, map { $_->{name} } @csv_visible_profiles;

        # --- Write data rows ---
        foreach my $vm_name (sort keys %$data_href) {
            if ($type eq 'historic_snapshot') {
                # --- Special handling for the historic snapshot file ---
                my $hist_data = $data_href->{$vm_name} || {};
                my $peak_data = $hist_data->{HistoricPeak} || {};
                my $base_data = $hist_data->{HistoricBaseline} || {};
                my @peak_row = ($vm_name, "HistoricPeak");
                foreach my $profile (@csv_visible_profiles) {
                    my $value = $peak_data->{$profile->{name}} // '';
                    push @peak_row, (looks_like_number($value) ? sprintf("%.4f", $value) : $value);
                }
                print $fh join(",", map { quote_csv($_) } @peak_row) . "\n";
                my @base_row = ($vm_name, "HistoricBaseline");
                foreach my $profile (@csv_visible_profiles) {
                    my $value = $base_data->{$profile->{name}} // '';
                    push @base_row, (looks_like_number($value) ? sprintf("%.4f", $value) : $value);
                }
                print $fh join(",", map { quote_csv($_) } @base_row) . "\n";
            } else {
                # --- Standard handling for 'current_baseline' file ---
                my @row = ($vm_name);
                my $vm_data = $data_href->{$vm_name};
                foreach my $profile (@csv_visible_profiles) {
                    my $value = $vm_data->{$profile->{name}} // '';
                    push @row, (looks_like_number($value) ? sprintf("%.4f", $value) : $value);
                }
            }
        }

        # Add the new SeasonalMultiplier column before the entitlement columns.
        push @header, ("SeasonalMultiplier", "Current - ENT");
        if ($add_excel_formulas) {
            push @header, ("NFIT - ENT", "NETT", "NETT%");
        }
        print $fh join(",", map { quote_csv($_) } @header) . "\n";

        my $excel_row_num_counter = 1;
        foreach my $vm_name (sort keys %$data_href) {
            my $vm_data = $data_href->{$vm_name};
            my $report_data = $vm_data->{_report_data} || {};
            my $cfg_csv = $vm_config_data{$vm_name};

            # Gather standard fields for the row
            my $smt_out = $cfg_csv->{smt} // $default_smt_arg;
            my ($serial_out, $systype_out, $poolname_out, $poolid_out, $ent_out) = ('', '', '', '', '');
            if (defined $cfg_csv) {
                $serial_out = $cfg_csv->{serial} // ''; $systype_out = $cfg_csv->{systemtype} // '';
                $poolname_out = $cfg_csv->{pool_name} // ''; $poolid_out = $cfg_csv->{pool_id} // '';
                $ent_out = $cfg_csv->{entitlement} // '';
            }

            $excel_row_num_counter++;
            my @row = (
                $vm_name, "", $report_data->{hint} // "", $report_data->{pattern} // "",
                $report_data->{pressure} // "", $report_data->{pressure_detail} // "", $smt_out,
                $serial_out, $systype_out, $poolname_out, $poolid_out, "" # Peak is not applicable for a forecast
            );

            my $seasonal_multiplier_for_row = "N/A";
            foreach my $profile (@csv_visible_profiles) {
                my $vm_data = $data_href->{$vm_name};
                my $p_name = $profile->{name};
                my $value = $vm_data->{$p_name} // '';
                push @row, (looks_like_number($value) ? sprintf("%.4f", $value) : $value);

                # Use the multiplier from the first available profile for the summary column
                if ($seasonal_multiplier_for_row eq 'N/A' && exists $report_data->{seasonal_debug}{$p_name}) {
                    $seasonal_multiplier_for_row = sprintf("%.2f", $report_data->{seasonal_debug}{$p_name}{multiplier});
                }
            }
        }
    }
    close $fh;
}

# ==============================================================================
# Subroutine to calculate a recency-weighted average.
# This is a key statistical function used by the multiplicative model.
# ==============================================================================
sub calculate_recency_weighted_average {
    my ($data_points_aref, $reference_date_obj, $half_life_days) = @_;

    my $sum_weighted_values = 0;
    my $sum_weights = 0;
    my $lambda = log(2) / $half_life_days;

    foreach my $dp (@$data_points_aref) {
        my $value = $dp->{value};
        my $date_obj = $dp->{date};

        my $days_diff = ($reference_date_obj->epoch - $date_obj->epoch) / ONE_DAY;
        $days_diff = 0 if $days_diff < 0;

        my $weight = exp(-$lambda * $days_diff);
        $sum_weighted_values += $value * $weight;
        $sum_weights += $weight;
    }

    if ($sum_weights > 1e-9) {
        return $sum_weighted_values / $sum_weights;
    } else {
        # Fallback: if all weights are zero (e.g., very old data), return a simple average.
        my @values = map { $_->{value} } @$data_points_aref;
        return @values ? (sum0(@values) / scalar(@values)) : undef;
    }
}

# ==============================================================================
# Subroutine to calculate a statistical volatility buffer.
# This increases the forecast based on the standard deviation of historical
# seasonal multipliers to account for year-over-year variance.
# ==============================================================================
sub calculate_volatility_buffer {
    my ($multipliers_aref, $confidence_level) = @_;

    my @values = map { $_->{value} } @$multipliers_aref;

    # Standard deviation requires at least 2 data points.
    return 1.0 if scalar(@values) < 2;

    # --- Calculate Mean and Standard Deviation ---
    my $sum = sum0(@values);
    my $mean = $sum / scalar(@values);
    return 1.0 if $mean == 0; # Avoid division by zero if mean is zero.

    my $sum_sq_diff = 0;
    foreach my $val (@values) {
        $sum_sq_diff += ($val - $mean)**2;
    }
    my $std_dev = sqrt($sum_sq_diff / (scalar(@values) - 1));

    # --- Z-score for common confidence levels ---
    # This lookup table provides the one-sided Z-score for a given confidence level.
    my %z_scores = (
        '0.90' => 1.645,
        '0.95' => 1.960,
        '0.98' => 2.326,
        '0.99' => 2.576,
    );
    my $z_score = $z_scores{$confidence_level} // 1.960; # Default to 95%

    # The buffer is 1 + (a fraction of the coefficient of variation).
    # This adds a percentage uplift proportional to the historical volatility.
    my $volatility_buffer = 1.0 + ($z_score * ($std_dev / $mean));

    # Sanity check: don't let the buffer be less than 1 (non-reducing).
    return $volatility_buffer > 1.0 ? $volatility_buffer : 1.0;
}

# ==============================================================================
# Subroutine to detect all active seasonal events for a given date.
# Returns:
# - An array of event configuration hashes, sorted by priority (desc).
# Notes:
# - CORRECTED to handle both fixed-date and recurring-period definitions.
# ==============================================================================
sub detect_active_events {
    my ($analysis_date_obj, $seasonality_config_href) = @_;

    my @active_events;
    return @active_events unless (defined $analysis_date_obj && ref($analysis_date_obj) eq 'Time::Piece');

    # *** FIX: Use UTC for consistent comparisons ***
    my $analysis_date_utc = gmtime($analysis_date_obj->epoch)->truncate(to => 'day');

    foreach my $event_name (keys %{$seasonality_config_href}) {
        # SAFETY: Skip config metadata sections.
        # We explicitly skip 'Global' and 'Adaptive' because they are
        # Attribute Containers (parents), not Schedulable Events (children).
        # These are not events and have no dates.
        next if $event_name =~ /^(?:Global|Adaptive)$/i;

        my $event_config = $seasonality_config_href->{$event_name};

        # Determine the period for this event relative to the analysis date.
        my ($start_obj, $end_obj) = determine_event_period($event_config, $analysis_date_utc);

        # --- Discovery Horizon Logic ---
        # Hierarchy: Event Config -> Global Config -> Default (30)
        my $lookback_days = $event_config->{seasonal_lookback_days}
                            // $seasonality_config_href->{Global}{seasonal_lookback_days}
                            // 30;
        my $lookahead_days = $event_config->{seasonal_lookahead_days}
                             // $seasonality_config_href->{Global}{seasonal_lookahead_days}
                             // 30;

        my $is_active = 0;

        if (defined $event_config->{dates}) {
            my @date_ranges = split /\s*,\s*/, $event_config->{dates};
            foreach my $range (@date_ranges) {
                $range =~ s/^\s+|\s+$//g;  # Trim whitespace

                if ($range =~ /^(\d{4}-\d{2}-\d{2}):(\d{4}-\d{2}-\d{2})$/) {
                    # Date range format: YYYY-MM-DD:YYYY-MM-DD
                    my $start_obj = Time::Piece->strptime($1, '%Y-%m-%d')->truncate(to => 'day');
                    my $end_obj   = Time::Piece->strptime($2, '%Y-%m-%d')->truncate(to => 'day');

                    # Apply Horizon:
                    # Active if: Analysis Date >= (Start - LookAhead) AND Analysis Date <= (End + LookBack)
                    my $horizon_start = $start_obj - ($lookahead_days * ONE_DAY);
                    my $horizon_end   = $end_obj + ($lookback_days * ONE_DAY);

                    if ($analysis_date_utc >= $horizon_start && $analysis_date_utc <= $horizon_end) {
                        $is_active = 1;
                        last;
                    }
                }
                elsif ($range =~ /^(\d{4}-\d{2}-\d{2})$/) {
                    # Single date format: YYYY-MM-DD (treat as single-day event)
                    my $date_obj = Time::Piece->strptime($1, '%Y-%m-%d')->truncate(to => 'day');

                    # Apply Horizon:
                    # Active if: Analysis Date >= (Date - LookAhead) AND Analysis Date <= (Date + LookBack)
                    my $horizon_start = $date_obj - ($lookahead_days * ONE_DAY);
                    my $horizon_end   = $date_obj + ($lookback_days * ONE_DAY);

                    if ($analysis_date_utc >= $horizon_start && $analysis_date_utc <= $horizon_end) {
                        $is_active = 1;
                        last;
                    }
                }
            }
        }
        elsif (defined $event_config->{period} && lc($event_config->{period}) eq 'monthly') {
            # For recurring events, determine_event_period returns the period relative to the analysis date.
            # However, with a horizon, we might be "outside" the standard period but "inside" the horizon.
            # So we need to check if the *nearest* occurrence is within the horizon.

            # Simplified approach: Use the returned period from determine_event_period.
            # If determine_event_period returns a period, it means it found one relative to the date.
            # But determine_event_period might be strict?
            # Let's check determine_event_period implementation.
            # It seems to find the "current or next" period.

            # Actually, the user requirement implies we should check if the *event itself* is within the horizon.
            # If determine_event_period returns ($start, $end), we check if analysis_date is within [Start-LookAhead, End+LookBack].

            if (defined $start_obj && defined $end_obj) {
                my $horizon_start = $start_obj - ($lookahead_days * ONE_DAY);
                my $horizon_end   = $end_obj + ($lookback_days * ONE_DAY);

                if ($analysis_date_utc >= $horizon_start && $analysis_date_utc <= $horizon_end) {
                    $is_active = 1;
                }
            }
        }

        if ($is_active) {
            $event_config->{_eventName} = $event_name;
            push @active_events, $event_config;
        }
    }

    return sort { ($b->{priority} // 0) <=> ($a->{priority} // 0) } @active_events;
}

# Find the end of detect_active_events (around line 5940, after the closing brace)
# and insert this new subroutine:

# ------------------------------------------------------------------------------
# SUBROUTINE: _detect_concurrent_events_strict
# PURPOSE:    Detect events that ACTUALLY overlap with a specific analysis window.
#             Unlike detect_active_events(), this does NOT apply horizon expansion.
#             Used for accurate concurrent event overlap reporting.
# ARGUMENTS:
#   $window_start_obj  - Time::Piece object for analysis window start
#   $window_end_obj    - Time::Piece object for analysis window end
#   $primary_event_name - Name of the primary event (excluded from results)
#   $seasonality_config_href - Full seasonality config hash
# RETURNS:    List of event config hashrefs that overlap the window
# ------------------------------------------------------------------------------
sub _detect_concurrent_events_strict {
    my ($window_start_obj, $window_end_obj, $primary_event_name, $seasonality_config_href) = @_;

    my @overlapping_events;
    return @overlapping_events unless (
        defined $window_start_obj && ref($window_start_obj) eq 'Time::Piece' &&
        defined $window_end_obj   && ref($window_end_obj)   eq 'Time::Piece'
    );

    # Normalise to UTC day boundaries for consistent comparison
    my $win_start_epoch = gmtime($window_start_obj->epoch)->truncate(to => 'day')->epoch;
    my $win_end_epoch   = gmtime($window_end_obj->epoch)->truncate(to => 'day')->epoch;

    foreach my $event_name (keys %{$seasonality_config_href}) {
        # Skip metadata sections and the primary event itself
        next if $event_name =~ /^(?:Global|Adaptive)$/i;
        next if $event_name eq $primary_event_name;

        my $event_config = $seasonality_config_href->{$event_name};
        my $duration_days = $event_config->{duration_days} // 1;

        # For explicit dates, check each date/range for overlap
        if (defined $event_config->{dates}) {
            my @date_ranges = split /\s*,\s*/, $event_config->{dates};
            foreach my $range (@date_ranges) {
                $range =~ s/^\s+|\s+$//g;

                my ($evt_start_epoch, $evt_end_epoch);

                if ($range =~ /^(\d{4}-\d{2}-\d{2}):(\d{4}-\d{2}-\d{2})$/) {
                    # Date range: YYYY-MM-DD:YYYY-MM-DD
                    my $s = eval { Time::Piece->strptime($1, '%Y-%m-%d')->truncate(to => 'day') };
                    my $e = eval { Time::Piece->strptime($2, '%Y-%m-%d')->truncate(to => 'day') };
                    next unless ($s && $e);
                    $evt_start_epoch = $s->epoch;
                    $evt_end_epoch   = $e->epoch;
                }
                elsif ($range =~ /^(\d{4}-\d{2}-\d{2})$/) {
                    # Single date: apply duration_days
                    my $d = eval { Time::Piece->strptime($1, '%Y-%m-%d')->truncate(to => 'day') };
                    next unless $d;
                    $evt_start_epoch = $d->epoch;
                    $evt_end_epoch   = $d->epoch + (($duration_days - 1) * 86400);
                }
                else {
                    next;
                }

                # Check for interval overlap: NOT (A ends before B starts OR A starts after B ends)
                if (!($evt_end_epoch < $win_start_epoch || $evt_start_epoch > $win_end_epoch)) {
                    my $cfg_copy = { %$event_config, _eventName => $event_name };
                    push @overlapping_events, $cfg_copy;
                    last;  # One overlap is sufficient to include this event
                }
            }
        }
        # For recurring events (monthly/weekly), determine the occurrence relative to the window
        elsif (defined $event_config->{period}) {
            # Use the window midpoint to find the relevant occurrence
            my $midpoint_epoch = int(($win_start_epoch + $win_end_epoch) / 2);
            my $midpoint_obj = gmtime($midpoint_epoch)->truncate(to => 'day');

            my ($evt_start_obj, $evt_end_obj) = determine_event_period($event_config, $midpoint_obj);
            next unless (defined $evt_start_obj && defined $evt_end_obj);

            my $evt_start_epoch = $evt_start_obj->epoch;
            my $evt_end_epoch   = $evt_end_obj->epoch;

            # Check for interval overlap
            if (!($evt_end_epoch < $win_start_epoch || $evt_start_epoch > $win_end_epoch)) {
                my $cfg_copy = { %$event_config, _eventName => $event_name };
                push @overlapping_events, $cfg_copy;
            }
        }
    }

    return @overlapping_events;
}

# ==============================================================================
# Helper function to validate data line format
# ==============================================================================
sub is_valid_data_line {
    my ($line) = @_;
    # Simply check if line starts with your timestamp format
    return $line && $line =~ /^\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2},/;
}

# ==============================================================================
# Efficiently gets the start and end timestamps from a large cache file
# by reading only the first and last data lines.
# ==============================================================================
sub _get_cache_date_range {
    my ($data_cache_file) = @_;
    return (undef, undef) unless (-f $data_cache_file && -s $data_cache_file);

    open my $fh, '<', $data_cache_file or die "Could not open $data_cache_file: $!";
    my ($start_ts_str, $end_ts_str);

    # Get first data line (line 2) - validate it's a proper data line
    my $header = <$fh>;  # Skip header
    while (my $line = <$fh>) {
        chomp $line;
        if ($line && is_valid_data_line($line)) {
            ($start_ts_str) = split ',', $line, 2;
            last;
        }
    }

    # Get last line by reading backwards line by line
    my $file_size = -s $data_cache_file;
    my $pos = $file_size;
    my $line_buffer = '';
    my $last_valid_line = '';

    # Read backwards in small chunks to build lines
    while ($pos > 0 && !$last_valid_line) {
        my $chunk_size = ($pos < 1024) ? $pos : 1024;
        $pos -= $chunk_size;

        seek $fh, $pos, 0;
        my $chunk;
        read $fh, $chunk, $chunk_size;

        # Prepend to our buffer
        $line_buffer = $chunk . $line_buffer;

        # Process complete lines from the end
        my @lines = split /\n/, $line_buffer;

        # If we're not at the start, the first line might be partial
        if ($pos > 0) {
            $line_buffer = shift @lines;  # Keep partial line for next iteration
        } else {
            $line_buffer = '';
        }

        # Check lines from end to start
        for my $line (reverse @lines) {
            if ($line && is_valid_data_line($line)) {
                $last_valid_line = $line;
                last;
            }
        }
    }

    if ($last_valid_line) {
        ($end_ts_str) = split ',', $last_valid_line, 2;
    }

    close $fh;

    # Parse timestamps
    my ($start_obj, $end_obj);
    eval { $start_obj = Time::Piece->strptime($start_ts_str, "%Y-%m-%d %H:%M:%S") if $start_ts_str; };
    if ($@) {
        warn " [WARN] Could not parse start timestamp '$start_ts_str' from cache.";
        return (undef, undef);
    }
    eval { $end_obj = Time::Piece->strptime($end_ts_str, "%Y-%m-%d %H:%M:%S") if $end_ts_str; };
    if ($@) {
        warn " [WARN] Could not parse end timestamp '$end_ts_str' from cache.";
        return (undef, undef);
    }

    return ($start_obj, $end_obj);
}

sub _get_warning_for_vm_forecast {
    my ($vm_name, $warnings_href) = @_;
    # This finds the most recent warning for a given VM, as the hash is keyed by snapshot date.
    foreach my $key (sort { $b cmp $a } keys %$warnings_href) {
        if (exists $warnings_href->{$key}{$vm_name}) {
            return $warnings_href->{$key}{$vm_name};
        }
    }
    return undef;
}

sub raw_init_vm_bucket {
    my ($store_hr, $vm) = @_;
    $store_hr->{$vm} //= {};
    die "raw_init_vm_bucket: VM bucket not HASH for $vm"
        unless ref($store_hr->{$vm}) eq 'HASH';
    return $store_hr->{$vm};
}

sub raw_set_profile_rows {
    my ($store_hr, $vm, $profile, $rows_aref) = @_;
    my $bucket = raw_init_vm_bucket($store_hr, $vm);
    $bucket->{$profile} = (ref($rows_aref) eq 'ARRAY') ? $rows_aref : [];
}

sub raw_get_profile_rows {
    my ($store_hr, $vm, $profile) = @_;
    return [] unless ref($store_hr) eq 'HASH';
    my $b = $store_hr->{$vm};
    return [] unless ref($b) eq 'HASH';
    my $a = $b->{$profile};
    return (ref($a) eq 'ARRAY') ? $a : [];
}

sub raw_get_last_state {
    my ($store_hr, $vm, $profile) = @_;
    my $rows = raw_get_profile_rows($store_hr, $vm, $profile);
    return @$rows ? $rows->[-1] : undef;
}

sub _safe_dig {
    my ($href, @path) = @_;
    my $cur = $href;
    for my $k (@path) {
        return undef unless ref($cur) eq 'HASH' && exists $cur->{$k};
        $cur = $cur->{$k};
    }
    return $cur;
}

# ==============================================================================
# SUBROUTINE: map_growth_confidence
# PURPOSE:    Maps a statistical p-value to a human-readable confidence level
#             for the CSV report and rationale log.
# ARGUMENTS:
#   1. $p_value (numeric): The Mann-Kendall p-value (or undef)
#   2. $method_used (string): The method string ('sen_slope', 'none', etc.)
# RETURNS:
#   - Confidence string: 'high', 'medium', 'low', or 'n/a'
# ==============================================================================
sub map_growth_confidence {
    my ($p_value, $method_used) = @_;

    # Default to 'n/a' (Not Applicable) if no growth was calculated or no p-value
    return 'n/a' if (!defined $method_used ||
                     $method_used eq 'none' ||
                     $method_used eq 'sen_slope_not_significant' ||
                     $method_used eq 'sen_slope_too_small' ||
                     !defined $p_value);

    # A p-value of 0 means the trend was extremely significant (p < 0.0001)
    if ($p_value == 0) {
        return 'high';
    }
    # Map based on standard statistical thresholds
    if ($p_value < 0.01) {
        return 'high';      # Very strong evidence (p < 1%)
    } elsif ($p_value < 0.05) {
        return 'medium';    # Conventional significance (p < 5%)
    } else {
        return 'low';       # Weak or no evidence (p >= 5%)
    }
}

# Rounds a value up to the next even number
sub _round_up_to_even
{
    my ($n) = @_;
    return undef if (!defined $n);
    $n = int($n);
    $n = 1 if ($n < 1);
    $n++ if (($n % 2) == 1);
    return $n;
}

# ==============================================================================
# SUBROUTINE: _write_standard_csv_report (Refactored for Assimilation Map)
# PURPOSE:    Encapsulates the entire CSV generation process. It now reads all
#             its data from the final, fully populated assimilation map.
# ==============================================================================
sub _write_standard_csv_report {
    my ($assimilation_map_ref, $report_type, $system_id, $file_timestamp, $is_multiplicative_run_flag, $is_recency_decay_run_flag, $is_predictive_peak_run_flag, $adaptive_runq_saturation_thresh, $adaptive_thresholds_href, $anchor_bucket) = @_;

    # Ensure we have data to process before creating a file.
    # The @vm_order array is still used to control the iteration order.
    return unless (@vm_order && ref($assimilation_map_ref) eq 'HASH');

    my $system_id_for_filename = $system_id || 'standard';
    $system_id_for_filename =~ s/[^a-zA-Z0-9_.-]//g;
    my $report_type_for_filename = $report_type;
    $report_type_for_filename =~ s/[^a-zA-Z0-9_.-]//g;

    my $output_filename = File::Spec->catfile($output_dir, "nfit-profile.$system_id_for_filename.$report_type_for_filename.$file_timestamp.csv");
    push @generated_files, $output_filename;

    open my $out_fh, '>', $output_filename or die "FATAL: Cannot open output file '$output_filename': $!";

    my @csv_visible_profiles = grep { $_->{csv_output} } @profiles;

    my @header_for_this_report = @output_header_cols_csv;
    my $formula_col_offset = 0;

    # Seasonal anchor month provenance (multiplicative seasonal runs).
    # This column is always present for seasonal runs; it is blank unless an anchor bucket (YYYY-MM) is provided.
    my $seasonal_anchor_month_val = (defined $anchor_bucket && $anchor_bucket =~ /^\d{4}-\d{2}$/) ? $anchor_bucket : "";

    my ($ent_idx) = grep { $header_for_this_report[$_] eq 'Current - ENT' } 0..$#header_for_this_report;
    $ent_idx //= scalar(@header_for_this_report);
    if ($is_multiplicative_run_flag) {
        my @new_cols = ("SeasonalEvents", "SeasonalAnchorMonth", "T3_SeasonalMultiplier", "Baseline_P99", "BaselineSource", "ForecastModel");

        # Remove all standard Growth columns (Theil-Sen) for this model (as this will perform a seasonal forecast instead)
        @header_for_this_report = grep {
            !/^(ProjectionDays|GrowthMethod|GrowthConfidence|GrowthTrend|GrowthSignificance)$/
        } @header_for_this_report;

        # Inject seasonal columns
        ($ent_idx) = grep { $header_for_this_report[$_] eq 'Current - ENT' } 0..$#header_for_this_report;
        $ent_idx //= scalar(@header_for_this_report); # Fallback
        splice @header_for_this_report, $ent_idx, 0, @new_cols;
        $formula_col_offset = scalar(@new_cols);
    } elsif ($is_recency_decay_run_flag || $nfit_enable_windowed_decay || $nfit_decay_over_states) {
        # Add GrowthAdj (from G3) and GrowthAdj_Source
        my @new_cols = ("GrowthAdj", "GrowthAdj_Min", "GrowthAdj_Max", "GrowthAdj_Source", "ProjectionDays", "GrowthMethod", "GrowthConfidence", "GrowthTrend", "GrowthSignificance");
        splice @header_for_this_report, $ent_idx, 0, @new_cols;
        $formula_col_offset = scalar(@new_cols);
    } elsif ($is_predictive_peak_run_flag) {
        # Remove growth/runq columns for this model
        @header_for_this_report = grep {
            !/^(ProjectionDays|GrowthMethod|GrowthConfidence|GrowthTrend|GrowthSignificance|GrowthAdj.*)$/
        } @header_for_this_report;

        # Find ent_idx again after grep
        ($ent_idx) = grep { $header_for_this_report[$_] eq 'Current - ENT' } 0..$#header_for_this_report;
        $ent_idx //= scalar(@header_for_this_report); # Fallback

        my @new_cols = ("ActiveEvents", "P99_PredictedDelta", "Baseline_P99", "BaselineSource", "ForecastModel");

        splice @header_for_this_report, $ent_idx, 0, @new_cols;
        $formula_col_offset = scalar(@new_cols);
    }

    print {$out_fh} join(",", map { quote_csv($_) } @header_for_this_report) . "\n";

    my $excel_row_num_counter = 1;

    foreach my $vm_name (@vm_order) {
        $excel_row_num_counter++;
        my @data_row_csv;

        # Get the complete, final data for this VM from the assimilation map.
        my $vm_map_ref = $assimilation_map_ref->{$vm_name};
        my $cfg = $vm_map_ref->{Configuration} //= {};
        # Ensure vm_name is always present for hinting/lookup logic (especially with --decay-over-states).
        # In some aggregated-state paths, the Configuration hash is synthetic and may omit vm_name,
        # even though the outer loop key ($vm_name) is authoritative.
        $cfg->{vm_name} //= $vm_name;

        my $core = $vm_map_ref->{CoreResults};
        my $hinting = $vm_map_ref->{Hinting};
        my $modifiers = $vm_map_ref->{CSVModifiers};

        # --- START: Hint-Aware Rationale Selection for CSV ---
        # Call generate_sizing_hint here, *once* per VM, to get the
        # authoritative hint. This is now safe, as the assimilation map is fully built.
        # We use a safe check for $vm_name in %vm_config_data.
        my ($hint_type_tier, $hint_pattern_shape, $pressure_bool, $pressure_detail_str);

        if (exists $vm_config_data{$vm_name}) {
            # Ensure vm_name is present for hinting in all code paths (e.g. --decay-over-states).
            $vm_map_ref->{Configuration} //= {};
            $vm_map_ref->{Configuration}{vm_name} //= $vm_name;
            ($hint_type_tier, $hint_pattern_shape, $pressure_bool, $pressure_detail_str) =
                generate_sizing_hint($vm_map_ref, undef, $adaptive_runq_saturation_thresh);
        } else {
            # VM is not in config file; fall back to safe defaults
            ($hint_type_tier, $hint_pattern_shape, $pressure_bool, $pressure_detail_str) = ('G3', 'G', 0, 'ConfigMissing');
        }

        ## Deterministic fallback chain for profile selection for growth and for the RunQ modifier columns

        # Map the hint pattern (O, B, G) to the correct T3 planning profile
        my $pattern_source = (defined $hint_type_tier && $hint_type_tier ne "") ? $hint_type_tier : "G3";
        my $hint_pattern = ($pattern_source =~ /^([A-Z])/) ? $1 : 'G';
        my %pattern_to_profile_map = ('O'=>'O3-95W15', 'B'=>'B3-95W15', 'G'=>'G3-95W15', 'P'=>'G3-95W15');

        my $source_profile_name = $pattern_to_profile_map{$hint_pattern} // 'G3-95W15';

        # --- Populate CSV row directly from the map ---
        push @data_row_csv, $vm_name, ($tier_override_for_csv{$vm_name} // "");
        # Use the hints previously generated
        push @data_row_csv, $hint_type_tier, $hint_pattern_shape;
        push @data_row_csv, ($pressure_bool ? "True" : "False"), $pressure_detail_str // "";
        push @data_row_csv, $cfg->{smt} // "";
        push @data_row_csv, $cfg->{serial_number} // "", $vm_config_data{$vm_name}{systemtype} // ""; # systemtype from old config is richer
        push @data_row_csv, $vm_config_data{$vm_name}{pool_name} // "", $cfg->{pool_id} // "";

        # This now correctly fetches the single, authoritative values for the VM (selecting the same profile name as $source_profile_name)
        push @data_row_csv, (defined $modifiers->{RunQ_Tactical} ? sprintf("%+.3f", $modifiers->{RunQ_Tactical}) : "0.000");
        push @data_row_csv, (defined $modifiers->{RunQ_Strategic} ? sprintf("%+.3f", $modifiers->{RunQ_Strategic}) : "0.000");
        push @data_row_csv, (defined $modifiers->{RunQ_Potential} && looks_like_number($modifiers->{RunQ_Potential}) ? sprintf("%+.3f", $modifiers->{RunQ_Potential}) : ($modifiers->{RunQ_Potential} // "0.000"));
        push @data_row_csv, $modifiers->{RunQ_Source} // "";

        # Push the Peak value, ensuring 3-digit precision
        my $peak_val = $core->{PeakValue};
        push @data_row_csv, (defined $peak_val && looks_like_number($peak_val)) ? sprintf("%.3f", $peak_val) : ($peak_val // "");

        # Push interleaved profile value + rVCPU (except for mandatory peak helper profile)
        my $smt_used = (defined $cfg->{smt} && looks_like_number($cfg->{smt})) ? ($cfg->{smt} + 0) : $DEFAULT_SMT;
        my $max_rvcpu_seen = undef;

        foreach my $profile (@csv_visible_profiles) {
            my $pname = $profile->{name};

            # Profile value (rENT_P; already growth-aware)
            my $val = $core->{ProfileValues}{$pname};
            my $formatted_val = (defined $val && looks_like_number($val)) ? sprintf("%.3f", $val) : ($val // "");
            push @data_row_csv, $formatted_val;

            # rVCPU column for all visible profiles except mandatory peak helper profile (P99W1)
            if (defined $pname && $pname ne $MANDATORY_PEAK_PROFILE_FOR_HINT) {
                my $rvcpu_out = "";

                # IMPORTANT: Prefer the authoritative, engine-computed rVCPU.
                # The CSV writer must not re-derive rVCPU from a different RunQ percentile
                # (this causes disagreements like P-99W1_rVCPU=6 in CSV vs 10 in rationale).
                if (defined $core->{Profile_rVCPU}
                    && ref($core->{Profile_rVCPU}) eq 'HASH'
                    && defined $core->{Profile_rVCPU}{$pname}
                    && looks_like_number($core->{Profile_rVCPU}{$pname})) {
                    $rvcpu_out = $core->{Profile_rVCPU}{$pname} + 0;
                    $max_rvcpu_seen = $rvcpu_out if (!defined $max_rvcpu_seen || $rvcpu_out > $max_rvcpu_seen);
                    push @data_row_csv, $rvcpu_out;
                    next;
                }

                my $rENT_P = (defined $val && looks_like_number($val)) ? ($val + 0) : undef;

                # Derive percentile from profile name (e.g. G3-95W15 => 95; G1-99W5 => 99)
                my $perc = undef;
                if ($pname =~ /^[A-Z]\d-([0-9]{2,3})W/i) {
                    $perc = $1 + 0;
                } elsif ($pname =~ /^[A-Z]\d-([0-9]{2,3})/i) {
                    $perc = $1 + 0;
                }
                $perc = 90 if (!defined $perc);

                # RunQ_hi_threads selection for rVCPU:
                # Prefer the SAME AbsRunQ key/value that the modifier logic selected for upsizing
                # (especially important for --runq-perc-behavior match). Fall back to profile-derived
                # AbsRunQ_P{perc} only if the selected values are not available.
                my $runq_key = sprintf("AbsRunQ_P%d", $perc);
                my $runq_hi_threads = undef;

                my $pdbg = undef;
                if (defined $vm_map_ref->{CoreResults}
                    && defined $vm_map_ref->{CoreResults}{ProfileDebugInfo}
                    && ref($vm_map_ref->{CoreResults}{ProfileDebugInfo}) eq 'HASH'
                    && defined $vm_map_ref->{CoreResults}{ProfileDebugInfo}{$pname}
                    && ref($vm_map_ref->{CoreResults}{ProfileDebugInfo}{$pname}) eq 'HASH'
                ) {
                    $pdbg = $vm_map_ref->{CoreResults}{ProfileDebugInfo}{$pname};
                }

                if (defined $pdbg
                    && defined $pdbg->{AbsRunQKeyUsed}
                    && defined $pdbg->{AbsRunQValueUsedForCalc}
                    && looks_like_number($pdbg->{AbsRunQValueUsedForCalc})
                ) {
                    $runq_key = $pdbg->{AbsRunQKeyUsed};
                    $runq_hi_threads = $pdbg->{AbsRunQValueUsedForCalc} + 0;
                } elsif (defined $vm_map_ref->{RunQMetrics} && ref($vm_map_ref->{RunQMetrics}) eq 'HASH') {
                    my $rm = $vm_map_ref->{RunQMetrics};
                    if (defined $rm->{$runq_key} && looks_like_number($rm->{$runq_key})) {
                        $runq_hi_threads = $rm->{$runq_key} + 0;
                    } elsif (defined $rm->{'AbsRunQ_P90'} && looks_like_number($rm->{'AbsRunQ_P90'})) {
                        $runq_key = 'AbsRunQ_P90';
                        $runq_hi_threads = $rm->{'AbsRunQ_P90'} + 0;
                    }
                }

                # Tier-aware headroom: extract tier from profile name (e.g., "G3-50W15" -> "G3" -> tier 3).
                my $csv_tier_str = $pname;
                $csv_tier_str =~ s/-.*//;
                my $rvcpu_tier_num = 3;  # Default baseline
                if ($csv_tier_str =~ /(\d)$/) {
                    $rvcpu_tier_num = $1;
                } elsif ($csv_tier_str eq 'P') {
                    $rvcpu_tier_num = 1;
                }

                my $rvcpu_tier_factor = $VCPU_TIER_HEADROOM_FACTORS{$rvcpu_tier_num} // 1.00;

                if (defined $runq_hi_threads && defined $rENT_P && $rENT_P > 0 && defined $smt_used && $smt_used > 0) {
                    # rVCPU explain vars must be in this scope (used later for CSV + rationale persistence)
                    my $p99w1_floor_vcpu = undef;
                    my $p99w1_factor_eff = undef;
                    my $p99w1_mult = undef;
                    my $cred_ratio = 1.00;
                    my $p99w1_adjusted;
                    my $runq_cred_corr = 1.00;
                    my $runq_cred_corr_suffix = '';

                    # Interval-adaptive divisor (dispatch sizing) for this fallback recomputation.
                    my $runq_width_divisor = $RUNQ_TARGET_HEADROOM_FACTOR;
                    $runq_width_divisor = 0.80 if (!defined $runq_width_divisor || !looks_like_number($runq_width_divisor) || $runq_width_divisor <= 0);
                    my $runq_width_divisor_base = $runq_width_divisor;
                    my $snapped_minutes;

                    if (defined $adaptive_thresholds_href && ref($adaptive_thresholds_href) eq 'HASH') {
                        if (defined $adaptive_thresholds_href->{runq_width_divisor_base}
                            && looks_like_number($adaptive_thresholds_href->{runq_width_divisor_base})
                            && $adaptive_thresholds_href->{runq_width_divisor_base} > 0) {
                            $runq_width_divisor_base = $adaptive_thresholds_href->{runq_width_divisor_base} + 0;
                        }
                        if (defined $adaptive_thresholds_href->{runq_width_divisor}
                            && looks_like_number($adaptive_thresholds_href->{runq_width_divisor})
                            && $adaptive_thresholds_href->{runq_width_divisor} > 0) {
                            $runq_width_divisor = $adaptive_thresholds_href->{runq_width_divisor} + 0;
                        }
                        if (defined $adaptive_thresholds_href->{snapped_minutes}
                            && looks_like_number($adaptive_thresholds_href->{snapped_minutes})
                            && $adaptive_thresholds_href->{snapped_minutes} > 0) {
                            $snapped_minutes = int($adaptive_thresholds_href->{snapped_minutes} + 0);
                        }
                    }

                    # Hard clamps (same safety intent as main path).
                    my $MIN_DIV = 0.50;
                    my $MAX_DIV = 1.00;
                    $runq_width_divisor_base = 0.80 if (!defined $runq_width_divisor_base || !looks_like_number($runq_width_divisor_base) || $runq_width_divisor_base <= 0);
                    $runq_width_divisor_base = $MIN_DIV if ($runq_width_divisor_base < $MIN_DIV);
                    $runq_width_divisor_base = $MAX_DIV if ($runq_width_divisor_base > $MAX_DIV);
                    $runq_width_divisor = $runq_width_divisor_base if (!defined $runq_width_divisor || !looks_like_number($runq_width_divisor) || $runq_width_divisor <= 0);
                    $runq_width_divisor = $MIN_DIV if ($runq_width_divisor < $MIN_DIV);
                    $runq_width_divisor = $MAX_DIV if ($runq_width_divisor > $MAX_DIV);

                    # Relative interval correction for credibility ratio only (identity at 60s).
                    my $corr = $runq_width_divisor_base / $runq_width_divisor;
                    $corr = 1.00 if (!defined $corr || !looks_like_number($corr) || $corr <= 0);
                    $corr = 1.00 if ($corr < 1.00);
                    $corr = 1.50 if ($corr > 1.50);
                    $runq_cred_corr = $corr;
                    if ($runq_cred_corr > 1.0005) {
                        $runq_cred_corr_suffix = sprintf(" [Interval Correction: x%.2f%s]", $runq_cred_corr,
                            (defined $snapped_minutes ? sprintf(", %dm", $snapped_minutes) : "")
                        );
                    }

                    # Workload-driven width requirement (threads -> cores -> vCPUs), tier-adjusted.
                    my $required_threads = ($runq_hi_threads / $runq_width_divisor) * $rvcpu_tier_factor;
                    my $required_cores   = $required_threads / $smt_used;
                    my $rvcpu_raw = int($required_cores);
                    $rvcpu_raw++ if ($required_cores > $rvcpu_raw); # ceil

                    # Vendor-safe envelope from rENT_P (growth-aware)
                    my $min_v = $rENT_P * $VCPU_RATIO_MIN;
                    my $max_v = $rENT_P * $VCPU_RATIO_MAX;
                    my $rvcpu_min = int($min_v); $rvcpu_min++ if ($min_v > $rvcpu_min);
                    my $rvcpu_max = int($max_v); $rvcpu_max++ if ($max_v > $rvcpu_max);

                    # Policy: rVCPU must always be even. Enforce this at the envelope level so
                    # that envelope bounds cannot block evenisation (e.g., avoid env=2..3).
                    #if (defined $rvcpu_min) {
                    #    $rvcpu_min = 2 if ($rvcpu_min < 2);
                    #    $rvcpu_min++ if (($rvcpu_min % 2) == 1);
                    #}
                    #if (defined $rvcpu_max) {
                    #    $rvcpu_max = 2 if ($rvcpu_max < 2);
                    #    $rvcpu_max++ if (($rvcpu_max % 2) == 1);
                    #    $rvcpu_max = $rvcpu_min if (defined $rvcpu_min && $rvcpu_max < $rvcpu_min);
                    #}

                    # Enforce entitlement-aware VP floor for micro-partitions
                    if (defined $rvcpu_min) {
                        $rvcpu_min = 1 if ($rvcpu_min < 1);
                    }
                    if (defined $rvcpu_max) {
                        $rvcpu_max = 1 if ($rvcpu_max < 1);
                        $rvcpu_max = $rvcpu_min if (defined $rvcpu_min && $rvcpu_max < $rvcpu_min);
                    }

                    my $rvcpu_final = $rvcpu_raw;
                    $rvcpu_final = $rvcpu_min if ($rvcpu_final < $rvcpu_min);
                    $rvcpu_final = $rvcpu_max if ($rvcpu_final > $rvcpu_max);

                    # Low-entitlement sanity / practical minimums
                    $rvcpu_final = $VCPU_MIN_ABS if ($rvcpu_final < $VCPU_MIN_ABS);
                    if ($rENT_P >= 0.5 && $rENT_P < 1.0 && $rvcpu_final < $VCPU_MIN_FOR_ENT_GE_0_5_LT_1) {
                        $rvcpu_final = $VCPU_MIN_FOR_ENT_GE_0_5_LT_1;
                    }

                    # Peak floor: PAE/AAE-driven headroom, tier-scaled.
                    # Base is 1.00 (VCPUs must at minimum cover peak cores).
                    # Headroom proportional to empirical above-entitlement evidence,
                    # amplified by tier policy. Avoids multiplicative compounding.
                    my $p99w1_cores = $core->{ProfileValues}{$MANDATORY_PEAK_PROFILE_FOR_HINT};
                    if (defined $p99w1_cores && looks_like_number($p99w1_cores) && $p99w1_cores > 0) {

                        # RunQ credibility adjustment
                        $p99w1_adjusted = $p99w1_cores;
                        $cred_ratio = 1.00;
                        # Credibility uses a tail-to-tail comparison: P99W1 PhysC vs AbsRunQ_P99W1.
                        # Prefer the peak-helper-aligned W1 RunQ tail for credibility, so we
                        # compare like-for-like: P99W1 PhysC vs AbsRunQ_P99W1 (window=1).
                        my $runq_p99w1_for_cred =
                            $vm_map_ref->{RunQMetrics}{AbsRunQ_P99W1} //
                            $vm_map_ref->{RunQMetrics}{AbsRunQ_P99};

                        if (defined $runq_p99w1_for_cred && $runq_p99w1_for_cred > 0 && defined $smt_used && $smt_used > 0) {
                            my $runq_peak_cores_raw = $runq_p99w1_for_cred / $smt_used;
                            my $runq_peak_cores = $runq_peak_cores_raw * $runq_cred_corr;
                            $cred_ratio = $runq_peak_cores / $p99w1_cores;
                            my $credibility = $cred_ratio + $VCPU_PEAK_CRED_BIAS;
                            $credibility = $VCPU_PEAK_CRED_MIN if ($credibility < $VCPU_PEAK_CRED_MIN);
                            $credibility = 1.00 if ($credibility > 1.00);
                            $p99w1_adjusted = $p99w1_cores * $credibility;
                        }

                        my $pae_raw = 0.0;
                        my $aae_raw = 0.0;
                        my $calc_debug_info_ref = $vm_map_ref->{CoreResults}{ProfileDebugInfo}{$pname} // {};
                        $pae_raw = (defined $calc_debug_info_ref->{'PhysC_Daily_PAE'} && looks_like_number($calc_debug_info_ref->{'PhysC_Daily_PAE'}))
                                 ? ($calc_debug_info_ref->{'PhysC_Daily_PAE'} + 0) : 0.0;
                        $aae_raw = (defined $calc_debug_info_ref->{'PhysC_Daily_AAE'} && looks_like_number($calc_debug_info_ref->{'PhysC_Daily_AAE'}))
                                 ? ($calc_debug_info_ref->{'PhysC_Daily_AAE'} + 0) : 0.0;

                        my $pae_component = ($pae_raw > 0) ? (($pae_raw < 0.30 ? $pae_raw : 0.30) / 0.30) * 0.15 : 0.0;
                        my $aae_component = ($aae_raw > 0) ? (($aae_raw < 0.10 ? $aae_raw : 0.10) / 0.10) * 0.15 : 0.0;

                        my $headroom = ($pae_component + $aae_component) * $rvcpu_tier_factor;
                        $p99w1_factor_eff = 1.00 + $headroom;
                        $p99w1_mult = $headroom;  # Rationale: the headroom portion above 1.00

                        my $floor_v = ($p99w1_adjusted + 0) * $p99w1_factor_eff;

                        $p99w1_floor_vcpu = int($floor_v); $p99w1_floor_vcpu++ if ($floor_v > $p99w1_floor_vcpu); # ceil
                        # Policy: peak-floor width must be even as well (matches AIX VP folding guidance).
                        if (defined $p99w1_floor_vcpu) {
                            $p99w1_floor_vcpu = 2 if ($p99w1_floor_vcpu < 2);
                            $p99w1_floor_vcpu++ if (($p99w1_floor_vcpu % 2) == 1);
                        }

                        if ($p99w1_floor_vcpu > $rvcpu_final) {
                            $rvcpu_final = $p99w1_floor_vcpu;
                            $rvcpu_final = $rvcpu_max if (defined $rvcpu_max && $rvcpu_final > $rvcpu_max);
                        }
                    }

                    # Enforce minimum (evenisation removed)
                    $rvcpu_final = $VCPU_MIN_ABS if (defined $rvcpu_final && $rvcpu_final < $VCPU_MIN_ABS);

                    $rvcpu_out = $rvcpu_final;
                    $max_rvcpu_seen = $rvcpu_final if (!defined $max_rvcpu_seen || $rvcpu_final > $max_rvcpu_seen);

                    # Persist rVCPU + explain so rationale can print an audit trail
                    $vm_map_ref->{CoreResults}{Profile_rVCPU}{$pname} = $rvcpu_final;
                    $vm_map_ref->{CoreResults}{Profile_rVCPU_Explain}{$pname} = {
                        Raw               => $rvcpu_raw,
                        Min               => $rvcpu_min,
                        Max               => $rvcpu_max,
                        EntCores          => (defined $rENT_P ? sprintf("%.2f", $rENT_P) : undef),
                        # Planning envelope and dispatch reserve: not computed in this path
                        # (standard CSV report uses pre-computed values from main modifier chain)
                        EntCoresPlanning           => undef,
                        EnvelopeExceedsOperational => 0,
                        DispatchReserveVP          => 0,
                        DispatchReserveReason      => '',
                        DispatchReservePreFinal    => undef,
                        TierNum           => $rvcpu_tier_num,
                        TierFactor        => sprintf("%.2f", $rvcpu_tier_factor),
                        RunQKeyUsed       => $runq_key,
                        RunQHiThreads     => (defined $runq_hi_threads ? sprintf("%.2f", $runq_hi_threads) : undef),
                        P99W1Floor        => $p99w1_floor_vcpu,
                        P99W1FactorEff    => (defined $p99w1_factor_eff ? sprintf("%.3f", $p99w1_factor_eff) : undef),
                        P99W1Mult         => (defined $p99w1_mult ? sprintf("%.3f", $p99w1_mult) : undef),
                        P99W1CredRatio    => sprintf("%.3f", $cred_ratio),
                        P99W1CredCorr   => sprintf("%.2f", $runq_cred_corr),
                        P99W1CredRatioSuffix => $runq_cred_corr_suffix,
                        P99W1AdjPeak      => sprintf("%.2f", $p99w1_adjusted),
                        P99W1RawPeak      => sprintf("%.2f", $p99w1_cores),
                        # Micro-partition override: not computed in CSV fallback path
                        RunQEnvelopeOverride => "No",
                        RunQEnvelopeBasis    => undef,
                        OrigMin              => undef,
                        OrigMax              => undef,
                    };

                    # ALSO persist into per-profile debug info used by rationale (prevents stale labels such as AbsRunQ_P95)
                    if (!defined $vm_map_ref->{CoreResults}{ProfileDebugInfo}{$pname} || ref($vm_map_ref->{CoreResults}{ProfileDebugInfo}{$pname}) ne 'HASH') {
                        $vm_map_ref->{CoreResults}{ProfileDebugInfo}{$pname} = {};
                    }
                    my $di = $vm_map_ref->{CoreResults}{ProfileDebugInfo}{$pname};
                    $di->{'rVCPU_RunQKeyUsed'}     = defined $runq_key ? $runq_key : "N/A";
                    $di->{'rVCPU_TierNum'}          = $rvcpu_tier_num;
                    $di->{'rVCPU_TierFactor'}       = sprintf("%.2f", $rvcpu_tier_factor);
                    $di->{'rVCPU_RunQHiThreads'}   = defined $runq_hi_threads ? sprintf("%.2f", $runq_hi_threads) : "N/A";
                    $di->{'rVCPU_Raw'}             = defined $rvcpu_raw ? $rvcpu_raw : "N/A";
                    $di->{'rVCPU_Min'}             = defined $rvcpu_min ? $rvcpu_min : "N/A";
                    $di->{'rVCPU_Max'}             = defined $rvcpu_max ? $rvcpu_max : "N/A";
                    $di->{'rVCPU_P99W1Floor'}       = defined $p99w1_floor_vcpu ? $p99w1_floor_vcpu : "N/A";
                    $di->{'rVCPU_P99W1FactorEff'}   = defined $p99w1_factor_eff ? sprintf("%.3f", $p99w1_factor_eff) : "N/A";
                    $di->{'rVCPU_P99W1Mult'}        = defined $p99w1_mult ? sprintf("%.3f", $p99w1_mult) : "N/A";
                    $di->{'rVCPU_P99W1CredRatio'}   = sprintf("%.3f", $cred_ratio);
                    $di->{'rVCPU_P99W1AdjPeak'}     = sprintf("%.2f", $p99w1_adjusted);
                    $di->{'rVCPU_P99W1RawPeak'}     = sprintf("%.2f", $p99w1_cores);
                    $di->{'rVCPU_Final'}            = defined $rvcpu_final ? $rvcpu_final : "N/A";
                }

                push @data_row_csv, $rvcpu_out;
            }
        }

        # Model-specific columns
        # Important Notes:
        # - A seasonal forecast should not ignore current structural deficits.
        # - Seasonal Forecasts Recommendations = (Baseline x SeasonalMultiplier) + RunQ_Tactical
        if ($is_multiplicative_run_flag) {
            # --- Targeted Lookup for Seasonal Multiplier ---
            # 1. Determine the VM's Pattern/Tier (Respecting User Override)
            my $user_tier_override = $tier_override_for_csv{$vm_name} // "";
            my $pattern_source = ($user_tier_override ne "") ? $user_tier_override : ($hinting->{AutoTier} // "G");
            my ($hint_pattern) = ($pattern_source =~ /^([A-Z])/);
            $hint_pattern //= 'G';

            # 2. Map to standard profile
            my %pattern_to_profile_map = ('O'=>'O3-95W15', 'B'=>'B3-95W15', 'G'=>'G3-95W15', 'P'=>'G3-95W15');
            my $multiplier_source_profile = $pattern_to_profile_map{$hint_pattern} // 'G3-95W15';

            # 3. Fetch multiplier from that specific profile
            my $s_data = $seasonal_debug_info{$vm_name}{$multiplier_source_profile} || {};

            # Fallback if specific profile missing (though unlikely if logic holds)
            if (!keys %$s_data) {
                 $s_data = $seasonal_debug_info{$vm_name}{'G3-95W15'} || {};
            }

            # ActiveEvents (now called SeasonalEvents) (from the specific profile, or generally for the VM if stored globally, but here per-profile)
            push @data_row_csv, ($s_data->{active_events} // "None");

            # SeasonalAnchorMonth (stable seasonal column; blank unless anchor_bucket YYYY-MM is provided).
            push @data_row_csv, $seasonal_anchor_month_val;

            # SeasonalMultiplier
            push @data_row_csv, (defined $s_data->{multiplier} ? sprintf("%.2f", $s_data->{multiplier}) : "N/A");

            # Baseline_PhysC
            push @data_row_csv, (defined $s_data->{baseline} ? sprintf("%.3f", $s_data->{baseline}) : "N/A");

            # BaselineSource
            push @data_row_csv, ($s_data->{baseline_source} // "N/A");

            # ForecastModel (New Column)
            push @data_row_csv, "Seasonal Multiplicative";

        } elsif ($is_recency_decay_run_flag || $nfit_enable_windowed_decay || $nfit_decay_over_states) {

            # 1. Determine the source profile for growth rationale.
            #    Priority: User TIER > AutoTier > Fallback to 'G'.
            my $user_tier_override_csv = $tier_override_for_csv{$vm_name} // "";
            my $auto_tier_csv = $hinting->{AutoTier} // "G";

            my $pattern_source_csv = ($user_tier_override_csv ne "") ? $user_tier_override_csv : $auto_tier_csv;
            my ($pattern_csv) = ($pattern_source_csv =~ /^([A-Z])/);
            $pattern_csv //= 'G'; # Default to 'G' if regex fails

            # 1. Map the pattern to the corresponding 95th percentile (T3) profile.
            my %pattern_to_profile_map_csv = ('O' => 'O3-95W15', 'B' => 'B3-95W15', 'G' => 'G3-95W15', 'P' => 'G3-95W15');
            my $GROWTH_RATIONALE_SOURCE_PROFILE = $pattern_to_profile_map_csv{$pattern_csv} // 'G3-95W15';

            # Growth Modifier Column Logic with Fallback
            # 2. Attempt to get growth rationale from primary source profile
            my $gr = _safe_dig($vm_map_ref, 'GrowthRationaleByProfile', $GROWTH_RATIONALE_SOURCE_PROFILE) || {};

            # 2a. Check if the primary profile had a computational failure
            my $is_computational_failure = 0;
            if (exists $gr->{skip_reason} && defined $gr->{skip_reason}) {
                # Check for insufficient data
                $is_computational_failure = 1 if ($gr->{skip_reason} =~ /Insufficient daily data points/i);
            } elsif (exists $gr->{hamed_rao_adjustment_factor} &&
                     defined $gr->{hamed_rao_adjustment_factor} &&
                     $gr->{hamed_rao_adjustment_factor} <= 0) {
                # Check for Hamed-Rao corruption
                $is_computational_failure = 1;
            } elsif (exists $gr->{method_used} &&
                     $gr->{method_used} ne 'none' &&
                     !defined $gr->{sen_slope}) {
                # Calculation attempted but critical metrics missing
                $is_computational_failure = 1;
            }

            # 2b. If computational failure and not already using G3-95W15, fallback to G3-95W15
            my $fallback_applied = 0;
            if ($is_computational_failure && $GROWTH_RATIONALE_SOURCE_PROFILE ne 'G3-95W15') {
                if ($verbose) {
                    warn sprintf(
                        "[INFO] Growth calculation failed for %s on VM %s (reason: %s). " .
                        "Falling back to G3-95W15.\n",
                        $GROWTH_RATIONALE_SOURCE_PROFILE,
                        $vm_name,
                        $gr->{skip_reason} // 'computational error'
                    );
                }

                # Attempt fallback to G3-95W15
                my $fallback_gr = _safe_dig($vm_map_ref, 'GrowthRationaleByProfile', 'G3-95W15') || {};

                # Only use fallback if it succeeded
                my $fallback_failed = 0;
                if (exists $fallback_gr->{skip_reason} && $fallback_gr->{skip_reason} =~ /Insufficient daily data points/i) {
                    $fallback_failed = 1;
                } elsif (exists $fallback_gr->{hamed_rao_adjustment_factor} &&
                         defined $fallback_gr->{hamed_rao_adjustment_factor} &&
                         $fallback_gr->{hamed_rao_adjustment_factor} <= 0) {
                    $fallback_failed = 1;
                }

                if (!$fallback_failed) {
                    # Fallback succeeded
                    $gr = $fallback_gr;
                    $GROWTH_RATIONALE_SOURCE_PROFILE = 'G3-95W15';
                    $fallback_applied = 1;
                } else {
                    # Both failed - keep original failure for transparency
                    if ($verbose) {
                        warn sprintf(
                            "WARNING: Fallback to G3-95W15 also failed for VM %s. " .
                            "No growth adjustment will be applied.\n",
                            $vm_name
                        );
                    }
                }
            }

            # 3. Get the specific GrowthAdj for this source profile (for the 'GrowthAdj' column)
            my $source_profile_growth_adj = _safe_dig($vm_map_ref, 'Growth', 'adjustments', $GROWTH_RATIONALE_SOURCE_PROFILE) // 0;

            # 4. Get the VM-wide Min/Max adjustments (from all profiles)
            my $growth_min = $vm_map_ref->{Growth}{min_adj} // 0;
            my $growth_max = $vm_map_ref->{Growth}{max_adj} // 0;

            # 5. Format and push the new columns
            my $source_adj_str = sprintf("%.3f", $source_profile_growth_adj);
            my $min_str = sprintf("%.3f", $growth_min);
            my $max_str = sprintf("%.3f", $growth_max);

            push @data_row_csv, $source_adj_str, $min_str, $max_str, $GROWTH_RATIONALE_SOURCE_PROFILE;

            # 6. Populate all other rationale fields from the source profile's rationale
            #    (This logic is now sourced from $gr, not $source_profile_name)
            my $proj_days = $gr->{projection_days};
            if (defined $proj_days && looks_like_number($proj_days)) {
                push @data_row_csv, int($proj_days);  # Integer format
            } else {
                push @data_row_csv, 'n/a';  # No growth calculation performed
            }

            # GrowthMethod
            my $method = $gr->{method_used} // 'none';
            push @data_row_csv, $method;

            # GrowthConfidence
            my $p_value = $gr->{sen_p_value};
            push @data_row_csv, map_growth_confidence($p_value, $method);

            # GrowthTrend
            # Ensure 'trend' is 'none' if method wasn't a significant trend
            my $trend = $gr->{sen_trend} // 'none';
            if ($method eq 'sen_slope_not_significant' || $method eq 'sen_slope_too_small' || $method eq 'none') {
                $trend = 'none';
            }
            push @data_row_csv, $trend;

            # GrowthSignificance (the p-value itself)
            if (defined $p_value && $method ne 'none') {
                # Format p-value to 4 decimal places, handling p=0
                push @data_row_csv, ($p_value == 0) ? "0.0000" : sprintf("%.4f", $p_value);
            } else {
                push @data_row_csv, 'n/a';
            }
            # --- END: Growth Trend Columns ---

        } elsif ($is_predictive_peak_run_flag) {
            my $s_data = $seasonal_debug_info{$vm_name}{'P-99W1'} || {};

            # 1. ActiveEvents
            push @data_row_csv, ($s_data->{active_events} // $apply_seasonality_event);

            # 2. PredictedDelta (Net Growth)
            # Formula: PredictedPeak - TrueBaseline
            my $delta_val = "N/A";
            if (defined $s_data->{PredictedPeak} && defined $s_data->{TrueBaseline} && looks_like_number($s_data->{PredictedPeak}) && looks_like_number($s_data->{TrueBaseline})) {
                $delta_val = sprintf("%+.3f", $s_data->{PredictedPeak} - $s_data->{TrueBaseline});
            }
            push @data_row_csv, $delta_val;

            # 3. Baseline_P99 (Mapped from TrueBaseline)
            push @data_row_csv, (defined $s_data->{TrueBaseline} ? sprintf("%.3f", $s_data->{TrueBaseline}) : "N/A");

            # 4. BaselineSource
            push @data_row_csv, ($s_data->{baseline_source} // "Generic Non-Peak");

            # 5. ForecastModel (Includes source info)
            my $source_raw = $s_data->{FinalSource} // 'Unknown';
            my $source_display = $source_raw;

            # Restore the user-friendly mapping for the report
            if ($source_raw eq 'PeakPrediction') {
                $source_display = 'PredictedPeak';
            }
            elsif ($source_raw eq 'TrueBaseline') {
                $source_display = 'BaselineIsHigher';
            }

            push @data_row_csv, "Predictive ($source_display)";
        }

        # Entitlement and formula columns
        my $ent_out = $cfg->{entitlement};
        my $current_ent_display = (looks_like_number($ent_out)) ? sprintf("%.2f", $ent_out) : ($ent_out // "");
        push @data_row_csv, $current_ent_display; # Always add Current - ENT

        # Current - VCPU (adjacent to Current - ENT)
        my $vcpu_out = $cfg->{virtual_cpus};
        my $current_vcpu_display = (defined $vcpu_out && looks_like_number($vcpu_out)) ? int($vcpu_out) : ($vcpu_out // "");
        push @data_row_csv, $current_vcpu_display;

        # NFIT - ENT (blank unless Excel formulas are enabled; see branch below)
        push @data_row_csv, "";

        # VCPU_Constraints (binding-only status for the standard planning profile)
        my $maxcpu_out = $cfg->{max_cpu};
        my $maxcpu_num = (defined $maxcpu_out && looks_like_number($maxcpu_out)) ? ($maxcpu_out + 0) : undef;

        # Determine planning profile from Tier by looking up the Tier key in @csv_visible_profiles
        # - Tier key is typically: 'O1', 'G3', 'P' (blank allowed)
        # - If Tier is blank, use 'G3'
        # - Build a lookup table from the visible profile names (so we always pick a visible profile)
        # - If lookup fails, fall back to the capacity planner default 'G3-95W15'
        my $user_tier = $vm_tier_overrides{$vm_name} // "";
        my $tier_key  = ($user_tier ne "") ? $user_tier : ($hint_type_tier // "");
        $tier_key =~ s/\s+//g if defined $tier_key;
        $tier_key = ($tier_key ne "") ? uc($tier_key) : 'G3';

        my %tier_to_profile;
        foreach my $p (@csv_visible_profiles) {
            my $pn = $p->{name};
            next unless defined $pn && $pn ne "";
            if ($pn eq 'P-99W1') {
                $tier_to_profile{'P'} = $pn;
                next;
            }
            if ($pn =~ /^([A-Z]\d)\-/) {
                $tier_to_profile{$1} //= $pn;
            }
        }

        my $planning_profile_name = $tier_to_profile{$tier_key} // 'G3-95W15';

        # If the derived planning profile isn't present in this run, fall back to G3-95W15 (when available).
        if (!defined $core->{Profile_rVCPU}{$planning_profile_name}) {
            $planning_profile_name = 'G3-95W15' if (defined $core->{Profile_rVCPU}{'G3-95W15'});
        }

        my $planning_rv = $core->{Profile_rVCPU}{$planning_profile_name};
        my $explain_ref = $core->{Profile_rVCPU_Explain}{$planning_profile_name} // {};

        my $raw_vp  = (defined $explain_ref->{Raw} && looks_like_number($explain_ref->{Raw})) ? ($explain_ref->{Raw} + 0) : undef;
        my $env_min = (defined $explain_ref->{Min} && looks_like_number($explain_ref->{Min})) ? int($explain_ref->{Min}) : undef;
        my $env_max = (defined $explain_ref->{Max} && looks_like_number($explain_ref->{Max})) ? int($explain_ref->{Max}) : undef;

        # Emit a single, planner-friendly status only when a hard constraint is binding.
        my $vcpu_constraints_str = "";
        if (defined $planning_rv && looks_like_number($planning_rv)) {
            my $rv = int($planning_rv + 0);

            if (defined $maxcpu_num && $maxcpu_num > 0 && $rv > $maxcpu_num) {
                $vcpu_constraints_str = "BlockedByMaxCPU";
            } elsif (defined $env_max && defined $raw_vp && $raw_vp > $env_max && $rv == $env_max) {
                $vcpu_constraints_str = "GovernedByVPMax";
            } elsif (defined $env_min && defined $raw_vp && $raw_vp < $env_min && $rv == $env_min) {
                $vcpu_constraints_str = "GovernedByVPMin";
            }

            # Add RunQ envelope override indicator if active
            my $runq_override = $explain_ref->{RunQEnvelopeOverride} // "No";
            if ($runq_override eq "Yes") {
                my $override_tag = "MicroPartitionRunQOverride";
                if ($vcpu_constraints_str ne "") {
                    $vcpu_constraints_str .= "; $override_tag";
                } else {
                    $vcpu_constraints_str = $override_tag;
                }
            }
        }
        # Annotate with the planning profile used (Tier-mapped, else G3 fallback)
        if (defined $vcpu_constraints_str && $vcpu_constraints_str ne ""
            && defined $planning_profile_name && $planning_profile_name ne "") {
            $vcpu_constraints_str .= sprintf(" [%s]", $planning_profile_name);
        }
        push @data_row_csv, $vcpu_constraints_str;

        if ($add_excel_formulas) {
            my $nfit_ent_formula_str = generate_nfit_ent_formula($excel_row_num_counter, scalar(@csv_visible_profiles), $formula_col_offset);
            # Column lookups by label (robust to inserted columns)
            my ($idx_nfit_ent) = grep { $header_for_this_report[$_] eq 'NFIT - ENT' } 0..$#header_for_this_report;
            my ($idx_curr_ent) = grep { $header_for_this_report[$_] eq 'Current - ENT' } 0..$#header_for_this_report;
            my $col_nfit_ent_letter = defined($idx_nfit_ent) ? get_excel_col_name(($idx_nfit_ent + 1) + $formula_col_offset) : get_excel_col_name(scalar(@output_header_cols_csv) - 2 + $formula_col_offset);
            my $col_curr_ent_letter = defined($idx_curr_ent) ? get_excel_col_name(($idx_curr_ent + 1) + $formula_col_offset) : get_excel_col_name(scalar(@output_header_cols_csv) - 3 + $formula_col_offset);
            my $nett_user_formula_str = sprintf("=(%s%d-%s%d)", $col_nfit_ent_letter, $excel_row_num_counter, $col_curr_ent_letter, $excel_row_num_counter);
            my $nett_perc_user_formula_str = sprintf("=IFERROR((%s%d-%s%d)/%s%d,\"\")", $col_nfit_ent_letter, $excel_row_num_counter, $col_curr_ent_letter, $excel_row_num_counter, $col_curr_ent_letter, $excel_row_num_counter);
            $data_row_csv[$idx_nfit_ent] = $nfit_ent_formula_str if (defined $idx_nfit_ent);
            push @data_row_csv, $nett_user_formula_str, $nett_perc_user_formula_str;
        }

         print {$out_fh} join(",", map { quote_csv($_) } @data_row_csv) . "\n";
    }

    if ($add_excel_formulas) {
        my %serials_map;
        foreach my $vm_name (@vm_order) {
            my $serial = $assimilation_map_ref->{$vm_name}{Configuration}{serial_number};
            $serials_map{$serial} = 1 if (defined $serial && $serial ne '');
        }
        my @sorted_serials = sort keys %serials_map;
        print_csv_footer($out_fh, $excel_row_num_counter, $physc_data_file, scalar(@csv_visible_profiles), \@sorted_serials, $formula_col_offset);
    }

    close $out_fh;
}

# ==============================================================================
# Subroutine to format a duration in seconds into a human-readable string.
# ==============================================================================
sub format_duration {
    my ($seconds) = @_;
    if ($seconds >= 3600) {
        my $hours = int($seconds / 3600);
        my $minutes = int(($seconds % 3600) / 60);
        return sprintf("%dh %dm", $hours, $minutes);
    }
    return sprintf("%.2fs", $seconds) if $seconds < 60;
    my $minutes = int($seconds / 60);
    my $remaining_seconds = $seconds % 60;
    return sprintf("%dm %.2fs", $minutes, $remaining_seconds);
}

# Helper function to add months to a Time::Piece object
sub add_months {
    my ($time_obj, $months) = @_;

    my $year = $time_obj->year;
    my $month = $time_obj->mon + $months;
    my $day = $time_obj->mday;

    # Handle year rollover
    while ($month > 12) {
        $month -= 12;
        $year++;
    }
    while ($month < 1) {
        $month += 12;
        $year--;
    }

    # Handle day overflow (e.g., Jan 31 + 1 month should be Feb 28/29)
    my $days_in_month = days_in_month($year, $month);
    if ($day > $days_in_month) {
        $day = $days_in_month;
    }

    return Time::Piece->strptime(sprintf('%04d-%02d-%02d', $year, $month, $day), '%Y-%m-%d');
}

# Helper function to get days in a month
sub days_in_month {
    my ($year, $month) = @_;
    my @days = (31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31);

    # Check for leap year
    if ($month == 2 && (($year % 4 == 0 && $year % 100 != 0) || $year % 400 == 0)) {
        return 29;
    }

    return $days[$month - 1];
}

# ==============================================================================
# Calculates the start and end dates for a recurring monthly event based on a
# given reference date. It correctly handles month-end boundaries and invalid
# day-of-month configurations.
# ==============================================================================
sub _get_recurring_monthly_period {
    my ($base_date_obj, $day_of_period, $duration_days) = @_;

    my $end_date_obj;
    if ($day_of_period == -1) {
        # Last day of month: get first day of next month, then subtract one day
        my $next_month = add_months($base_date_obj, 1);
        my $first_of_next = Time::Piece->strptime(sprintf('%04d-%02d-01', $next_month->year, $next_month->mon), '%Y-%m-%d');
        $end_date_obj = $first_of_next - ONE_DAY;
    } else {
        # Specific day of month, clamped to be valid for that month
        my $year  = $base_date_obj->year;
        my $month = $base_date_obj->mon;
        my $day   = $day_of_period;

        my $days_in_target_month = days_in_month($year, $month);
        if ($day > $days_in_target_month) {
            # Clamp to the last day of the month if config is invalid (e.g., day 31 in Feb)
            $day = $days_in_target_month;
        }
        $end_date_obj = Time::Piece->strptime(sprintf('%04d-%02d-%02d', $year, $month, $day), '%Y-%m-%d');
    }

    # This calculation remains the same
    my $start_date_obj = $end_date_obj - (ONE_DAY * ($duration_days - 1));
    return ($start_date_obj, $end_date_obj);
}

# ==============================================================================
# Subroutine to log the rationale for a multiplicative seasonal forecast.
# It provides a clear, aligned, and explanatory summary of how the forecast
# was derived for each profile, matching the format of the standard log.
# ==============================================================================
# ==============================================================================
# SUBROUTINE: _log_seasonal_config_context
# PURPOSE:    Logs the configuration context for a seasonal event.
# ==============================================================================
sub _log_seasonal_config_context {
    my ($fh, $event_config, $context_href) = @_;
    return unless $fh && $event_config;
    my $label_width = 35;
    my $event_name = $event_config->{_eventName} // "Unknown Event";

    print {$fh} "######################################################################\n";
    print {$fh} "# Seasonal Event Configuration: $event_name\n";
    print {$fh} "######################################################################\n";

    # Event Description
    my $desc = $event_config->{description} // "N/A";
    $desc =~ s/^"|"$//g;
    printf {$fh} "  %-${label_width}s : %s\n", "Event Description", $desc;

    # Event Definition
    my $def = $event_config->{dates} // ($event_config->{period} ? $event_config->{period} . " (Day " . ($event_config->{day_of_period} // "?") . ")" : "N/A");
    printf {$fh} "  %-${label_width}s : %s\n", "Event Definition", $def;

    # --- NEW FIELDS ---
    if ($context_href) {
        # Analysis Window
        if ($context_href->{start} && $context_href->{end}) {
            my $days = int(($context_href->{end}->epoch - $context_href->{start}->epoch) / 86400) + 1;
            printf {$fh} "  %-${label_width}s : %s to %s (%d days)\n",
                "Analysis Window", $context_href->{start}->ymd, $context_href->{end}->ymd, $days;
        }

        # Sampling Interval
        if ($context_href->{interval}) {
            printf {$fh} "  %-${label_width}s : %s seconds\n", "Sampling Interval", $context_href->{interval};
        }

        # Projection Horizon (Context specific)
        my $horizon = "Next Occurrence"; # Default for multiplicative/predictive
        if (($event_config->{model}//'') eq 'recency_decay') {
            $horizon = "Standard Growth Projection";
        }
        printf {$fh} "  %-${label_width}s : %s\n", "Projection Horizon", $horizon;
    }
    # ------------------

    # Baseline Lookback
    my $lookback = $event_config->{baseline_period_days} // "N/A";
    printf {$fh} "  %-${label_width}s : %s days\n", "Baseline Lookback", $lookback;

    # Concurrent Event Interaction
    my $policy = lc($event_config->{interaction_policy} // 'exclusive');
    unless ($policy =~ /^(exclusive|combined)$/) {
        $policy = 'exclusive';
    }

    my $interaction_str = "policy=$policy";
    if ($policy eq 'combined') {
        my $dampening = $event_config->{interaction_dampening_factor};
        if (defined $dampening && looks_like_number($dampening)) {
            $interaction_str .= " (dampening: $dampening)";
        }
    }
    printf {$fh} "  %-${label_width}s : %s\n", "Concurrent Event Interaction", $interaction_str;

    # Rule Version
    my $version = $event_config->{last_modified} // "N/A";
    printf {$fh} "  %-${label_width}s : %s\n", "Rule Version", $version;

    # Data Exclusions
    my $exclusions = $event_config->{exclude_dates} // "None";
    printf {$fh} "  %-${label_width}s : %s\n", "Data Exclusions", $exclusions;

    print {$fh} "======================================================================\n";
}

# Logs Rationale for Seasonal Forecasting models
sub log_multiplicative_seasonal_rationale
{
    my ($fh, $event_config, $context) = @_;

    # Ensure the script does not die if the log file handle is not valid.
    return unless $fh;

    # Log Configuration Context
    if (defined $event_config) {
        _log_seasonal_config_context($fh, $event_config, $context);
    }

    # Iterate through each VM that has results, maintaining a consistent order.
    foreach my $vm_name (sort @vm_order)
    {
        # Check if there is seasonal debug information available for this VM.
        next unless exists $seasonal_debug_info{$vm_name};

        # Print a clear, top-level header for each VM in the log.
        print {$fh} "\n######################################################################\n";
        print {$fh} "# Rationale for VM: $vm_name\n";
        print {$fh} "# CPU Forecasting Model: Multiplicative Seasonal Forecast (Event: $apply_seasonality_event)\n";
        print {$fh} "######################################################################\n\n";

        # Iterate through each profile to log its specific forecast calculation.
        foreach my $profile (@profiles)
        {
            my $p_name = $profile->{name};

            # --- Special Handling for P-99W1 ---
            if (defined $MANDATORY_PEAK_PROFILE_FOR_HINT && $p_name eq $MANDATORY_PEAK_PROFILE_FOR_HINT) {
                my $label_width = 38;
                print {$fh} "======================================================================\n";
                printf {$fh} "%-${label_width}s : %s\n", "Profile Processed", parse_profile_name_for_log($p_name);
                print {$fh} "----------------------------------------------------------------------\n";
                print {$fh} "  CPU Forecasting Model: Multiplicative Seasonal Forecast ($apply_seasonality_event)\n\n";
                print {$fh} "  - The $MANDATORY_PEAK_PROFILE_FOR_HINT profile is a measurement-only statistical ceiling.\n";
                print {$fh} "  - It represents the smoothed (1-min SMA) 99.75th percentile of the\n";
                print {$fh} "    unfiltered historical data.\n";
                print {$fh} "  - NO growth, RunQ, or forecasting modifiers are applied to this value.\n\n";

                my $p99_val = $seasonal_debug_info{$vm_name}{$p_name}{forecast} // 0;
                printf {$fh} "  Final Unmodified Value              : %.4f cores\n", ($p99_val // 'N/A');
                print {$fh} "======================================================================\n\n";
                next;
            }

            next unless exists $seasonal_debug_info{$vm_name}{$p_name};

            my $s_data = $seasonal_debug_info{$vm_name}{$p_name};
            my $profile_desc = parse_profile_name_for_log($p_name);

            # Use a fixed width for labels to ensure consistent alignment of colons.
            my $label_width = 36;

            print {$fh} "======================================================================\n";
            printf {$fh} "%-${label_width}s   : %s\n", "Profile Processed", $profile_desc;
            print {$fh} "----------------------------------------------------------------------\n";
            print {$fh} "  CPU Forecasting Model: Multiplicative Seasonal Forecast ($apply_seasonality_event)\n\n";
            printf {$fh} "  %-${label_width}s : %.4f cores\n", "Current Baseline Value", $s_data->{baseline};
            printf {$fh} "  %-${label_width}s : %.4f\n", "Historical Multiplier", $s_data->{multiplier};

            # Add detailed breakdown of how the multiplier was derived
            if (exists $s_data->{historical_multipliers} && ref($s_data->{historical_multipliers}) eq 'ARRAY' && @{$s_data->{historical_multipliers}}) {
                printf {$fh} "  %-${label_width}s : %s\n", "  Multiplier Methodology", "Recency-weighted average of past events:";
                foreach my $hist_entry (@{$s_data->{historical_multipliers}}) {
                    my $hist_date = ref($hist_entry->{date}) ? $hist_entry->{date}->date : $hist_entry->{date};
                    printf {$fh} "  %-${label_width}s   %s: %.4f\n", "", $hist_date, $hist_entry->{value};
                }
            }

            printf {$fh} "  %-${label_width}s : %.4f\n", "Volatility Buffer", $s_data->{volatility};
            printf {$fh} "  %-${label_width}s : %.4f cores\n", "Forecasted Peak Residual", $s_data->{forecasted_residual};
            printf {$fh} "  %-${label_width}s : %.2f\n", "Peak Amplification Factor", $s_data->{amplification_factor};
            printf {$fh} "  %-${label_width}s : %.4f cores\n", "Final Forecasted Value", $s_data->{forecast};
            print {$fh} "    - Calculation : ((Baseline * Multiplier * Buffer) + Residual) * Amplification\n";
            # --- Print outlier warning if it exists ---
            if (defined $s_data->{OutlierWarning} && $s_data->{OutlierWarning} ne '') {
                print {$fh} "\n  --- Workload Volatility Alert ---\n";
                print {$fh} "  " . $s_data->{OutlierWarning} . "\n";
            }
            print {$fh} "======================================================================\n\n";
        }
    }
}

# ==============================================================================
# Subroutine to determine the correct seasonal analysis path.
# It checks if a multiplicative model has enough historical data to run. If not,
# it switches to the defined fallback event. This makes the tool resilient.
# It returns the name of the event that should ultimately be executed.
# Returns undef on cold-start to signal graceful skip.
# ==============================================================================
sub determine_seasonal_analysis_path {
    my ($event_config, $system_cache_dir, $event_name) = @_;

    # This function is only relevant for models that have historical prerequisites.
    my $model_to_run = $event_config->{model} // '';
    return $event_name unless ($model_to_run eq 'multiplicative_seasonal' || $model_to_run eq 'predictive_peak');

    # Use // to allow explicit 0 for cold-start bootstrap runs
    my $min_history_required = $event_config->{min_historical_years} // 1;

    # Read the unified history to count available snapshots for this event.
    my $unified_history = read_unified_history($system_cache_dir);
    my $total_history_months = scalar(keys %$unified_history);

    my $history_count = 0;
    foreach my $month_data (values %$unified_history) {
        if (exists $month_data->{SeasonalEventSnapshots}{$event_name}) {
            $history_count++;
        }
    }

    if ($history_count >= $min_history_required) {
        print STDERR "  [INFO] Historical snapshots for event '$event_name': $history_count (>= min. $min_history_required)\n";
        return $event_name;
    } else {
        my $fallback_event_name = $event_config->{fallback_event} // '';

        print STDERR "  [WARN] Insufficient history for event '$event_name' (found $history_count, min. $min_history_required)\n";

        if ($fallback_event_name ne '' && exists $seasonality_config->{$fallback_event_name}) {
            print STDERR "  Φ Seasonal Analysis [Event selection]: fallback event '$fallback_event_name' selected\n";
            return $fallback_event_name;
        } else {
            # Automatic bootstrap: history-dependent models proceed with baseline-only forecast
            # This is not an error condition - it's the expected first-run behaviour for new events
            print STDERR "  [INFO] Bootstrap run: proceeding with baseline-only forecast (multiplier = 1.0)\n";
            print STDERR "         Subsequent runs will use accumulated historical multipliers.\n";

            # Signal bootstrap state via internal flag (checked by calculate_multiplicative_forecast)
            $event_config->{_bootstrap_active} = 1;

            return $event_name;
        }
    }
}

# ==============================================================================
# SUBROUTINE: _build_nfit_baseline_command
# PURPOSE:    Constructs the specific nfit command for calculating a baseline
#             for a seasonal model. It ensures a consistent command is built
#             and that any conflicting decay or growth flags from the profile
#             are removed. A baseline must be a pure measurement.
# ARGS:
#   1. $profile_flags_in (string): The raw flags from the profile config.
#   2. $baseline_start_str (string): The start date for the analysis.
#   3. $baseline_end_str (string): The end date for the analysis.
#   4. $system_cache_dir (string): Path to the target cache directory.
#   5. $enable_clipping_detection (boolean, optional): If true, adds the
#      --enable-clipping-detection flag to the command.
#   6. $is_generic_baseline    : if true, all decay and time filters are stripped.
#   7. $allow_growth_prediction: if true, growth predictions will be enabled.
#   8. $profile_name_for_label : profile name to record in the results cache (if specified)
#   9. $exclusions_href (hash ref, optional): Hash of VM exclusions.
# RETURNS:
#   - The fully constructed nfit command string.
# ==============================================================================
sub _build_nfit_baseline_command {
    my ($profile_flags_in, $baseline_start_str, $baseline_end_str, $system_cache_dir, $enable_clipping_detection, $is_generic_baseline, $allow_growth_prediction, $profile_name_for_label, $exclusions_href) = @_;

    my $profile_flags = $profile_flags_in; # Work on a copy.

    # --- NEW: Sanitise incoming flags to remove extraneous quotes from config files.
    # This is the primary fix for the "Unknown option" error.
    $profile_flags =~ s/^\s*"?|"?\s*$//g;

    # A baseline is a historical measurement, not a forecast. Growth prediction
    # should almost always be stripped, EXCEPT for the multiplicative model's
    # "CurrentBaseline", which needs to reflect the true current trend.
    unless ($allow_growth_prediction) {
        $profile_flags =~ s/--enable-growth-prediction\s*//g;
        $profile_flags =~ s/--growth-projection-days\s+\d+\s*//g;
        $profile_flags =~ s/--max-growth-inflation-percent\s+\d+\s*//g;
    }

    # The --enable-windowed-decay Model is a trending analysis:
    # Applying a trending model on top of a period that is supposed to be a static baseline measurement would be logically incorrect.
    $profile_flags =~ s/--enable-windowed-decay\s*//g;

    if ($is_generic_baseline) {
        $profile_flags =~ s/--(?:online|batch|no-weekends)\b\s*//g;
        $profile_flags =~ s/--decay\s+[\w-]+\s*//g;
        $profile_flags =~ s/--runq-decay\s+[\w-]+\s*//g;
        $profile_flags =~ s/--avg-method\s+\w+\s*//g;
    }
    $profile_flags =~ s/--decay-over-states\s*//g;

    # --- REFACTOR: Use Manifest for Exclusion Support ---
    # To support complex exclusions (dates/VMs), we must use a manifest.
    # We construct a temporary profile and manifest, inject exclusions, and return the command.

    my $temp_profile = {
        name  => $profile_name_for_label // 'BaselineProfile',
        flags => $profile_flags
    };

    my $runq_behavior = $runq_perc_behavior_mode // 'none';
    my $manifest = build_transform_manifest([$temp_profile], $nfit_runq_avg_method_str, $runq_behavior);

    # Sanitise for history (baselines are historical)
    my $sanitised_manifest = _sanitise_manifest_for_history($manifest);

    # Inject exclusions
    if (defined $exclusions_href) {
        _inject_exclusions_into_manifest($sanitised_manifest, $exclusions_href);
    }

    # Write to temp file
    # We return the File::Temp object to the caller so it persists until they are done.
    my $fh_manifest = File::Temp->new(
        TEMPLATE => 'nfit_baseline_manifest_XXXXXX',
        SUFFIX   => '.json',
        UNLINK   => 1,
        TMPDIR   => 1
    );
    print $fh_manifest JSON->new->pretty->canonical->encode($sanitised_manifest);
    close $fh_manifest; # Flush but keep object alive

    my $manifest_filename = $fh_manifest->filename;

    # Construct the command using the manifest
    my $base_flags = "-q -k --nmondir \"$system_cache_dir\" $rounding_flags_for_nfit";
    $base_flags .= " --startdate $baseline_start_str" if defined $baseline_start_str;
    $base_flags .= " --enddate $baseline_end_str" if defined $baseline_end_str;

    # Note: vm_filter_arg is NOT needed if we rely on the manifest/exclusions,
    # but nfit still respects -vm as a global filter.
    my $vm_filter_arg = defined($target_vm_name) ? " -vm \"$target_vm_name\"" : "";
    my $smt_flag = "--smt $default_smt_arg";

    my $command = "$nfit_script_path --manifest \"$manifest_filename\" $base_flags $vm_filter_arg $smt_flag";

   # Add clipping detection if requested
    if ($enable_clipping_detection) {
        $command .= " --enable-clipping-detection";
    }

    # Append the profile label if provided. This is for metadata and does not affect the L2 cache key.
    if (defined $profile_name_for_label && $profile_name_for_label ne '') {
        $command .= " --profile-label '$profile_name_for_label'";
    }

    return ($command, $fh_manifest);
}

# ==============================================================================
# Main orchestrator for the 'predictive_peak' model.
# This version is enhanced to use the new two-part residual forecasting method.
# ==============================================================================
sub calculate_predictive_peak_forecast {
    my ($system_cache_dir, $system_identifier, $event_name, $event_config, $full_seasonality_config, $adaptive_runq_saturation_thresh, $exclusions_href, $asof_start_obj, $asof_end_obj, $precomputed_baselines_href) = @_;

    print STDERR "\n  ⧉ Executing Predictive Peak Forecast for event $event_name on system $system_identifier\n";

    # --- Step 1: Get Historical Peak and Residual Data ---
    # Calls the enhanced helper to get a hash containing both data series.
    my $historical_data_href = _get_historical_peak_data($system_cache_dir, $event_name, $event_config);

    # --- Step 2: Calculate the Predicted Peak and Residual for each profile ---
    # Calls the enhanced prediction engine to get forecasts for both series.
    print STDERR "  ⧉ Predictive Peak: projecting next peak intensity and residuals using Theil–Sen estimator (robust regression)\n";
    my ($predicted_components_href, $debug_info_for_vms) =
        _calculate_peak_prediction($historical_data_href, $event_config);

    # --- Step 3: Calculate the Non-Peak Baseline (excluding all historical peaks) ---
    # Phase 6: Use pre-computed history-based baseline if provided by orchestrator.
    my $true_baseline_results_href;
    if ($precomputed_baselines_href && ref($precomputed_baselines_href) eq 'HASH' && %$precomputed_baselines_href) {
        $true_baseline_results_href = $precomputed_baselines_href;
        print STDERR "  Φ Predictive Peak [Baseline]: history-based (pre-computed)\n";
    } else {
        # Legacy L1-based path (--use-l1-baseline or pre-Phase 6 compatibility)
        $true_baseline_results_href = _get_true_baseline_results($system_cache_dir, $event_name, $event_config, $full_seasonality_config, $exclusions_href, $asof_start_obj, $asof_end_obj);
    }

    # --- Generate Active Events String ---
    # Even though predictive usually targets one event, we check for concurrent overlap for consistency.
    my ($next_start, $next_end) = determine_event_period($event_config);
    my $forecast_date_obj = $next_start // gmtime();
    my @active_events = detect_active_events($forecast_date_obj, $full_seasonality_config);
    my $active_events_str = join(", ", map { $_->{_eventName} } @active_events);
    # Fallback if detection fails (e.g. fixed dates in past)
    $active_events_str ||= $event_name;

    # --- Step 4: Synthesise Final Results ---
    print STDERR "  ⧉ Completing final forecast synthesis (peak, residuals, baseline)\n";

    my %final_results;
    foreach my $vm_name (keys %{$true_baseline_results_href}) {
        foreach my $profile (@profiles) {
            my $p_name = $profile->{name};
            my $baseline_val = $true_baseline_results_href->{$vm_name}{$p_name};

            # Get the two predicted components from the prediction engine.
            my $predicted_peak_val = $predicted_components_href->{$vm_name}{$p_name}{peak};
            my $predicted_residual_val = $predicted_components_href->{$vm_name}{$p_name}{residual};

            # Combine the signal and volatility forecasts. Undefined values are treated as zero.
            my $combined_prediction;
            if (defined $predicted_peak_val) {
                $combined_prediction = ($predicted_peak_val // 0) + ($predicted_residual_val // 0);
            }

            # The final recommendation is the higher of the baseline or the combined prediction.
            my ($final_value, $source) = (0, 'N/A');
            if (defined $baseline_val && defined $combined_prediction) {
                if ($baseline_val > $combined_prediction) {
                    $final_value = $baseline_val;
                    $source = 'TrueBaseline';
                } else {
                    $final_value = $combined_prediction;
                    $source = 'PeakPrediction';
                }
            } elsif (defined $combined_prediction) {
                $final_value = $combined_prediction;
                $source = 'PeakPrediction';
            } elsif (defined $baseline_val) {
                $final_value = $baseline_val;
                # This case indicates that prediction failed (e.g., insufficient history).
                $source = 'BaselineOnly (NoPrediction)';
            }

            # Apply the optional amplification factor
            my $amplification = $event_config->{peak_amplification_factor} // 1.0;
            my $final_recommendation = $final_value * $amplification;

            $final_results{$vm_name}{$p_name} = $final_recommendation;

            # Store all components in the debug hash for comprehensive logging.
            $seasonal_debug_info{$vm_name}{$p_name} = {
                TrueBaseline       => $baseline_val,
                PredictedPeak      => $predicted_peak_val,
                PredictedResidual  => $predicted_residual_val,
                CombinedPrediction => $combined_prediction,
                FinalSource        => $source,
                AmplificationFactor  => $amplification,
                FinalForecast        => $final_recommendation,
                PredictionDebug    => $debug_info_for_vms->{$vm_name}{$p_name},
                OutlierWarning     => _get_warning_for_vm_forecast($vm_name, \%outlier_warnings),
                active_events      => $active_events_str,
                baseline_source    => "Generic Non-Peak (Filtered)", # Predictive always uses filtered generic
                forecast_model     => "Predictive Linear Regression"
            };
        }
    }
    return \%final_results;
}

# ==============================================================================
# Helper to run nfit and get a "True/Non-Peak Baseline" by excluding all historical peak periods.
#
# ==============================================================================
sub _get_true_baseline_results {
    my ($system_cache_dir, $event_name, $event_config, $full_seasonality_config, $exclusions_href, $asof_start_obj, $asof_end_obj) = @_;

    my $data_cache_file = File::Spec->catfile($system_cache_dir, '.nfit.cache.data');
    return {} unless -f $data_cache_file;

    # --- Stage 1: Build a hash of dates to exclude ---
    my @all_peak_periods = find_all_historical_periods($full_seasonality_config, $asof_start_obj, $asof_end_obj);

    my %days_to_exclude_hash;
    my $total_days_to_process = 0;

    # Pre-calculate total days for an accurate progress bar and validation
    foreach my $period (@all_peak_periods) {
        next unless (defined $period && ref($period) eq 'ARRAY' && @$period == 2);
        my ($start, $end) = @{$period};
        next if ($start > $end);
        $total_days_to_process += (int(($end->epoch - $start->epoch) / 86400) + 1);
    }

    if ($total_days_to_process > 0) {
        print STDERR "  [+] Processing $total_days_to_process days for historical peak exclusion\n";

        # Scope all counters locally to prevent bugs across function calls
        my $days_processed = 0;
        my $last_reported_perc = -1;
        my $period_num = 0;

        foreach my $period (@all_peak_periods) {
            $period_num++;
            next unless (defined $period && ref($period) eq 'ARRAY' && @$period == 2);
            my ($start, $end) = @{$period};
            next if ($start > $end);

            my $current = Time::Piece->new($start->epoch);
            while ($current <= $end) {

                $days_to_exclude_hash{$current->strftime('%Y-%m-%d')} = 1;
                $current += ONE_DAY;

                $days_processed++;

                my $current_perc = int(($days_processed / $total_days_to_process) * 100);
                if ($current_perc > $last_reported_perc) {
                    printf STDERR "\r      Expanding peak-day window: %d%% (%d/%d)",
                        $current_perc, $days_processed, $total_days_to_process;
                    $last_reported_perc = $current_perc;
                }
            }
        }
        print STDERR "\n";
    }

    # --- Stage 2: Create the filtered data using the efficient hash lookup ---
    my ($filtered_fh, $filtered_filename) = tempfile(UNLINK => 1);

    my $unique_days_to_exclude = scalar(keys %days_to_exclude_hash);
    if ($unique_days_to_exclude > 0) {
        print STDERR "  [-] Filtering $unique_days_to_exclude day(s) from baseline data\n";

        open(my $cache_fh, '<', $data_cache_file) or die "Cannot open $data_cache_file: $!";
        my $header = <$cache_fh>;
        print $filtered_fh $header;

        my $line_count = 0;
        while (my $line = <$cache_fh>) {
            $line_count++;
            print STDERR "." if $line_count % 500000 == 0;

            my $line_date = substr($line, 0, 10);
            next if exists $days_to_exclude_hash{$line_date};

            print $filtered_fh $line;
        }
        close $cache_fh;
    } else {
        print STDERR "  [INFO] No peak periods to filter; using full analysis window for baseline\n";
        system("cp '$data_cache_file' '$filtered_filename'");
    }

    close $filtered_fh;

    # --- Stage 3: Execute Single-Pass Analysis on the Filtered Data ---
    print STDERR "\n  ⧉ Synthesising non-peak baseline from filtered data\n";

    # 1. Build and Sanitise Manifest
    #    We use the global @profiles array and RunQ settings
    my $runq_behavior = $runq_perc_behavior_mode // 'none';
    my $tactical_manifest = build_transform_manifest(\@profiles, $nfit_runq_avg_method_str, $runq_behavior);
    my $historical_manifest = _sanitise_manifest_for_history($tactical_manifest);

    # --- INJECT EXCLUSIONS IF PRESENT ---
    if (defined $exclusions_href) {
        _inject_exclusions_into_manifest($historical_manifest, $exclusions_href);
    }

    # 2. Write Manifest to Temp File
    my ($fh_manifest, $manifest_filename) = tempfile(
        'nfit_baseline_manifest_XXXXXX',
        SUFFIX => '.json',
        UNLINK => 1,
        TMPDIR => 1
    );
    print $fh_manifest JSON->new->pretty->canonical->encode($historical_manifest);
    close $fh_manifest;

    # 3. Construct nfit Command
    #    CRITICAL: We point --physc-data and --runq-data to the FILTERED file we just created.
    #    The cache file format (Timestamp, VM, PhysC, RunQ) is compatible with these flags.
    my $nfit_cmd = "$nfit_script_path --manifest $manifest_filename "
                 . " --nmondir \"$system_cache_dir\" "
                 . "--smt $default_smt_arg "
                 . "--runq-avg-method $nfit_runq_avg_method_str "
                 . "--show-progress";

    # 4. Execute
    my $nfit_output = '';
    my $stderr_arg = ">&=" . fileno(STDERR);
    my $pid_nfit = open3(undef, my $stdout_nfit, $stderr_arg, $nfit_cmd);

    while(my $line = <$stdout_nfit>) {
        $nfit_output .= $line;
    }
    waitpid($pid_nfit, 0);

    # 5. Parse and Assimilate Results
    my %baseline_results;
    if ($? == 0) {
        my $parsed = parse_nfit_json_output($nfit_output);

        foreach my $vm (keys %$parsed) {
            # In historical mode (no decay), nfit might return multiple states.
            # We want the aggregated average across the filtered period.
            my @states_for_vm = @{$parsed->{$vm}};

            foreach my $profile (@profiles) {
                my $p_name = $profile->{name};
                # Extract the correct P-metric key (e.g., P95)
                my ($p_val_num) = $profile->{flags} =~ /(?:-p|--percentile)\s+([0-9.]+)/;
                my $p_key = "P" . clean_perc_label($p_val_num // $DEFAULT_PERCENTILE);

                my @valid_vals;
                foreach my $state (@states_for_vm) {
                    my $val = _safe_dig($state, 'metrics', 'physc', $p_name, $p_key);
                    push @valid_vals, $val if (defined $val && looks_like_number($val));
                }

                if (@valid_vals) {
                    # Simple average of the states in the filtered file
                    $baseline_results{$vm}{$p_name} = sum0(@valid_vals) / scalar(@valid_vals);
                }
            }
        }
    } else {
        warn "WARNING: nfit failed during baseline calculation. Exit code: " . ($? >> 8) . "\n";
    }

    # Cleanup temp file if we created one
    unlink $filtered_filename if -f $filtered_filename;

    return \%baseline_results;

}

# ==============================================================================
# SUBROUTINE: _get_historical_peak_data
# PURPOSE:    Reads the snapshot cache for a given event and returns time series
#             data for historical peaks and residuals. This version is enhanced
#             to use the 'unclippedPeakEstimate' from the ClippingInfo block
#             if it exists, ensuring the returned data is corrected for
#             historical saturation.
# ARGS:
#   1. $system_cache_dir (string): Path to the target cache directory.
#   2. $event_name (string): The name of the event to retrieve data for.
#   3. $event_config (hash ref): The configuration for the event.
# RETURNS:
#   - A hash reference containing two keys, 'peaks' and 'residuals', each
#     pointing to a hash of historical data series, structured by profile and VM.
# ==============================================================================
sub _get_historical_peak_data {
    my ($system_cache_dir, $event_name, $event_config) = @_;

    print STDERR "  ⧉ Predictive Peak: assembling saturation-aware historical peak and residual data for event $event_name\n";

    my $unified_history = read_unified_history($system_cache_dir);
    my @event_history;
    # Extract all historical snapshots for the specified event.
    foreach my $month_key (sort keys %$unified_history) {
        my $month_data = $unified_history->{$month_key};
        if (exists $month_data->{SeasonalEventSnapshots}{$event_name}) {
            my $snapshot = $month_data->{SeasonalEventSnapshots}{$event_name};
            $snapshot->{_month_key} = $month_key; # Add date context
            $snapshot->{eventName} //= $event_name;  # Ensure eventName is set for history lookup
            push @event_history, $snapshot;
        }
    }

    my $max_peaks = $event_config->{max_historical_peaks} // 12;
    if (@event_history > $max_peaks) {
        @event_history = @event_history[-$max_peaks..-1]; # Keep only the most recent N peaks.
    }

    my %peaks_by_profile;
    my %residuals_by_profile; # New hash for residual data.

    foreach my $event (@event_history) {
        my $date = $event->{_month_key} . "-01"; # Use the first of the month as the date for the time series.
        my $hist_results = $event->{results};
        my $peak_results = $event->{results}{'PeakValue'} || {};
        my $residual_results = $event->{results}{'PeakResidual'} || {};

        # Process PeakValue data.
        foreach my $vm (keys %$peak_results) {
            foreach my $profile (keys %{$peak_results->{$vm}}) {
                my $value = $peak_results->{$vm}{$profile};
                if (exists $hist_results->{ClippingInfo}{$vm}{$profile}{unclippedPeakEstimate}) {
                    $value = $hist_results->{ClippingInfo}{$vm}{$profile}{unclippedPeakEstimate};
                }
                if (defined $value && looks_like_number($value)) {
                    push @{$peaks_by_profile{$profile}{$vm}}, { value => $value, date => $date };
                }
            }
        }

        # Process PeakResidual data.
        foreach my $vm (keys %$residual_results) {
            foreach my $profile (keys %{$residual_results->{$vm}}) {
                my $value = $residual_results->{$vm}{$profile};
                if (defined $value && looks_like_number($value)) {
                    push @{$residuals_by_profile{$profile}{$vm}}, { value => $value, date => $date };
                }
            }
        }
    }

    # Return a hash containing both data series.
    return {
        peaks => \%peaks_by_profile,
        residuals => \%residuals_by_profile
    };
}

# ==============================================================================
# Core statistical engine for the predictive_peak model.
# This function orchestrates a dual forecast for both the peak signal and the residual.
# This function uses Theil-Sen Estimator (Robust Regression) instead of OLS.
# ==============================================================================
sub _calculate_peak_prediction {
    my ($historical_data_href, $event_config) = @_;

    my $peak_series_per_profile = $historical_data_href->{peaks} || {};
    my $residual_series_per_profile = $historical_data_href->{residuals} || {};

    my %predictions;
    my %debug_info;

    my $min_peaks = $event_config->{min_historical_peaks} // 3;

    # --- Robust Prediction Logic (Theil-Sen) ---
    my $_predict_next_value = sub {
        my ($series_aref) = @_;

        return (undef, "Insufficient history")
            unless (defined $series_aref && ref($series_aref) eq 'ARRAY' && scalar(@$series_aref) >= 2);

        # Extract values. Note: We do NOT strictly need to trim outliers here
        # because Theil-Sen is naturally robust to them (breakdown point ~29%).
        my @values = map { $_->{value} } @$series_aref;

        # 1. Build points [x, y]
        my @points;
        for my $i (0..$#values) {
            push @points, [$i, $values[$i]];
        }

        # 2. Calculate Theil-Sen Slope
        my $sen_result = calculate_sens_slope(\@points);
        unless ($sen_result) {
            return (undef, "Theil-Sen calculation failed (insufficient distinct points)");
        }
        my $slope = $sen_result->{slope};

        # 3. Calculate Robust Intercept
        # Formula: Median(y_i - slope * x_i)
        my @intercepts;
        for my $i (0..$#values) {
            push @intercepts, $values[$i] - ($slope * $i);
        }
        my $intercept = _calculate_median(\@intercepts);

        # 4. Project one step into the future
        my $future_x = scalar(@values); # Next index
        my $projected_value = ($slope * $future_x) + $intercept;

        my $debug_str = sprintf("Theil-Sen (Slope: %.4f, Intercept: %.4f, N: %d)", $slope, $intercept, scalar(@values));

        return ($projected_value, $debug_str);
    };

    foreach my $profile_name (keys %{$peak_series_per_profile}) {
        foreach my $vm_name (keys %{$peak_series_per_profile->{$profile_name}}) {

            my $peak_series_aref = $peak_series_per_profile->{$profile_name}{$vm_name} || [];

            if (scalar(@$peak_series_aref) < $min_peaks) {
                $debug_info{$vm_name}{$profile_name} = {
                    peak_status => "Skipped: Insufficient history (" . scalar(@$peak_series_aref) . "/$min_peaks)",
                    residual_status => "Skipped: Dependent on peak forecast"
                };
                next;
            }

            # Predict Peak
            my ($predicted_peak, $peak_debug) = $_predict_next_value->($peak_series_aref);

            # Predict Residual
            my $residual_series_aref = $residual_series_per_profile->{$profile_name}{$vm_name} || [];
            my ($predicted_residual, $residual_debug) = $_predict_next_value->($residual_series_aref);

            $predictions{$vm_name}{$profile_name} = {
                peak     => $predicted_peak,
                residual => $predicted_residual,
            };

            $debug_info{$vm_name}{$profile_name} = {
                peak_status     => $peak_debug,
                residual_status => $residual_debug,
            };
        }
    }

    return (\%predictions, \%debug_info);
}

# ==============================================================================
# Helper to calculate standard deviation.
# ==============================================================================
sub _calculate_std_dev {
    my ($data_aref) = @_;
    my $n = scalar(@$data_aref);
    return 0 if $n < 2;
    my $mean = sum0(@$data_aref) / $n;
    my $sum_sq_diff = sum0(map { ($_ - $mean)**2 } @$data_aref);
    return sqrt($sum_sq_diff / ($n - 1));
}

# ==============================================================================
# Finds all historical peak periods defined in the seasonality config that
# fall within the date range of the available cache data.
# This version is optimised for performance and robustness by pre-compiling
# regexes, reducing object creation, and preventing runaway loops.
# ==============================================================================
sub find_all_historical_periods {
    my ($full_seasonality_config, $asof_start, $asof_end) = @_;

    # Determine the actual scoped time span of the data in the cache.
    return [] unless $asof_start && $asof_end;

    my @periods;
    my $now = gmtime(); # Cache current time to avoid repeated calls.

    # Pre-compile the regex once, outside the loops.
    my $date_range_regex = qr/(\d{4}-\d{2}-\d{2}):(\d{4}-\d{2}-\d{2})/;

    foreach my $e_name (keys %$full_seasonality_config) {
        my $e_config = $full_seasonality_config->{$e_name};

        # --- Path for events defined with fixed 'dates' ---
        if (defined $e_config->{dates}) {
            my @date_ranges = split /\s*,\s*/, $e_config->{dates};

            foreach my $range (@date_ranges) {
                next unless $range =~ $date_range_regex;

                # Only create Time::Piece objects after a successful regex match.
                my $start = Time::Piece->strptime($1, '%Y-%m-%d');
                my $end = Time::Piece->strptime($2, '%Y-%m-%d');
                push @periods, [$start, $end];
            }

        # --- Path for events defined with a recurring 'period' ---
        } elsif (defined $e_config->{period} && $e_config->{period} eq 'monthly') {
            # Pre-extract config values to avoid repeated hash lookups inside the loop.
            my $day_of_period = $e_config->{day_of_period} // -1;
            my $duration_days = $e_config->{duration_days} // 7;

            my $current_iterator = Time::Piece->new($asof_start->epoch)->truncate(to => 'month');

            # Add runaway protection to prevent infinite loops on very large date ranges.
            my $max_iterations = 1000; # Sensible limit for ~83 years of monthly data.
            my $iteration_count = 0;

            while ($current_iterator <= $asof_end && $iteration_count < $max_iterations) {
                my ($start, $end) = _get_recurring_monthly_period($current_iterator, $day_of_period, $duration_days);

                # Add the period if it's valid, historical, and within the scoped cache's time span.
                if ($start && $end && $end < $now && $end <= $asof_end) {
                    push @periods, [$start, $end];
                }

                $current_iterator = $current_iterator->add_months(1);
                $iteration_count++;
            }

            # Warn the user if the protection limit was reached.
            warn "Monthly iteration limit reached for event '$e_name'" if $iteration_count >= $max_iterations;
        }
    }

    return @periods;
}

# ==============================================================================
# Subroutine to log the rationale for a predictive_peak seasonal forecast.
# ==============================================================================
sub log_predictive_peak_rationale
{
    my ($fh, $event_config, $context) = @_;

    return unless $fh;

    # Log Configuration Context
    if (defined $event_config) {
        _log_seasonal_config_context($fh, $event_config, $context);
    }

    foreach my $vm_name (sort @vm_order) {
        next unless exists $seasonal_debug_info{$vm_name};

        print {$fh} "\n######################################################################\n";
        print {$fh} "# Rationale for VM: $vm_name\n";
        print {$fh} "# CPU Forecasting Model: Predictive Peak Forecast (Event: $apply_seasonality_event)\n";
        print {$fh} "######################################################################\n\n";

        foreach my $profile (@profiles) {
            my $p_name = $profile->{name};

            # --- Special Handling for P-99W1 ---
            if (defined $MANDATORY_PEAK_PROFILE_FOR_HINT && $p_name eq $MANDATORY_PEAK_PROFILE_FOR_HINT) {
                my $label_width = 43;
                print {$fh} "======================================================================\n";
                printf {$fh} "%-${label_width}s : %s\n", "Profile Processed", parse_profile_name_for_log($p_name);
                print {$fh} "----------------------------------------------------------------------\n";
                print {$fh} "  CPU Forecasting Model: Predictive Peak Forecast ($apply_seasonality_event)\n\n";
                print {$fh} "  - The $MANDATORY_PEAK_PROFILE_FOR_HINT profile is a measurement-only statistical ceiling.\n";
                print {$fh} "  - It represents the smoothed (1-min SMA) 99.75th percentile of the\n";
                print {$fh} "    unfiltered historical data.\n";
                print {$fh} "  - NO growth, RunQ, or forecasting modifiers are applied to this value.\n\n";

                # Note: Key is 'FinalForecast' for predictive model
                my $p99_val = $seasonal_debug_info{$vm_name}{$p_name}{FinalForecast} // 0;
                printf {$fh} "  Final Unmodified Value              : %.4f cores\n", ($p99_val // 'N/A');
                print {$fh} "======================================================================\n\n";
                next;
            }

            next unless exists $seasonal_debug_info{$vm_name}{$p_name};

            my $s_data = $seasonal_debug_info{$vm_name}{$p_name};
            my $profile_desc = parse_profile_name_for_log($p_name);
            my $label_width = 38;

            print {$fh} "======================================================================\n";
            printf {$fh} "%-${label_width}s      : %s\n", "Profile Processed", $profile_desc;
            print {$fh} "----------------------------------------------------------------------\n";

            my $predicted_peak_val = $s_data->{PredictedPeak};

            if (!defined $predicted_peak_val) {
                # This block handles cases where prediction was skipped (e.g., insufficient history).
                my $debug_info = $s_data->{PredictionDebug}{peak_status} // "Insufficient history";
                printf {$fh} "  %-${label_width}s : SKIPPED (%s)\n", "Peak Trend Prediction", $debug_info;
                printf {$fh} "  %-${label_width}s : %.4f cores\n", "1. Non-Peak Baseline", ($s_data->{TrueBaseline} // 0);
                print {$fh} "  --------------------------------------------------------------------\n";
                printf {$fh} "  %-${label_width}s : %.4f cores (Source: Baseline Only)\n", "Final Recommendation", ($s_data->{FinalForecast} // 0);

            } else {
                # This block logs a successful, multi-step forecast.
                # Extract trend details from the debug string if available
                my $trend_details = $s_data->{PredictionDebug}{peak_status} // "Details unavailable";

                printf {$fh} "  1. %-${label_width}s : %.4f cores\n", "Non-Peak Baseline (Floor)", ($s_data->{TrueBaseline} // 0);
                printf {$fh} "  2. %-${label_width}s : %.4f cores\n", "Predicted Peak", $predicted_peak_val;
                printf {$fh} "       - Trend Details                      : %s\n", $trend_details;
                printf {$fh} "  3. %-${label_width}s : %.4f cores\n", "Predicted Peak Residual", ($s_data->{PredictedResidual} // 0);
                printf {$fh} "  4. %-${label_width}s : %.2f\n", "Peak Amplification Factor", ($s_data->{AmplificationFactor} // 1.0);
                print {$fh} "  --------------------------------------------------------------------\n";
                printf {$fh} "  Calculation : MAX( Baseline, (Predicted Peak + Predicted Residual) * Amplification )\n";
                print {$fh} "  --------------------------------------------------------------------\n";
                printf {$fh} "  %-${label_width}s : %.4f cores (Source: %s)\n", "Final Forecasted Value",
                    ($s_data->{FinalForecast} // 0),
                    ($s_data->{FinalSource} // 'N/A');
            }

            # --- Print outlier warning if it exists ---
            if (defined $s_data->{OutlierWarning} && $s_data->{OutlierWarning} ne '') {
                print {$fh} "\n  --- Workload Volatility Alert ---\n";
                print {$fh} "  " . $s_data->{OutlierWarning} . "\n";
            }

            print {$fh} "======================================================================\n\n";
        }
    }
}

# ==============================================================================
# Calculate linear regression (slope, intercept, R-squared) manually
# ==============================================================================
sub calculate_manual_linear_regression
{
    my ($points_aref) = @_;
    my $n = scalar @{$points_aref};

    return undef if $n < 2;

    my ($sum_x, $sum_y, $sum_xy, $sum_x_squared, $sum_y_squared) = (0, 0, 0, 0, 0);

    foreach my $point (@{$points_aref})
    {
        my ($x_val, $y_val) = @{$point};
        $sum_x += $x_val;
        $sum_y += $y_val;
        $sum_xy += $x_val * $y_val;
        $sum_x_squared += $x_val**2;
        $sum_y_squared += $y_val**2;
    }

    my $denominator_slope = ($n * $sum_x_squared) - ($sum_x**2);

    if (abs($denominator_slope) > $FLOAT_EPSILON)
    {
        my $slope_calc = (($n * $sum_xy) - ($sum_x * $sum_y)) / $denominator_slope;
        my $intercept_calc = ($sum_y - ($slope_calc * $sum_x)) / $n;
        return {
            slope     => $slope_calc,
            intercept => $intercept_calc,
            n_points  => $n,
        };
    }

    # Cannot reliably determine a linear trend.
    return undef;
}

# ==============================================================================
# SUBROUTINE: read_unified_history
## PURPOSE:   Reads history from either legacy file or partitioned directory.
#             Stitches partitioned files into a single "Virtual Monolith" hash
#             to ensure zero regression for downstream logic.
# ARGUMENTS:
#   1. $system_cache_dir (string): The path to a specific system's cache directory.
# RETURNS:
#   - A hash reference of the decoded JSON data.
#   - Returns an empty hash reference if the file does not exist, is empty,
#     or is corrupt, ensuring a safe default.
sub read_unified_history {
    my ($system_cache_dir) = @_;

    my $legacy_file   = File::Spec->catfile($system_cache_dir, $UNIFIED_HISTORY_FILE);
    my $partition_dir = File::Spec->catfile($system_cache_dir, '.nfit.history');

    my $history_data = {};

    # PATH A: Partitioned History (Preferred)
    if (-d $partition_dir) {
        opendir(my $dh, $partition_dir) or die "FATAL: Cannot open history dir: $!";
        my @files = grep { /^nfit\.hist\.\d{4}-\d{2}\.json$/ } readdir($dh);
        closedir($dh);

        my $json_decoder = JSON->new->allow_nonref;

        foreach my $file (@files) {
            my $path = File::Spec->catfile($partition_dir, $file);
            my $json_text = do {
                open my $fh, '<:encoding(utf8)', $path or next; # Skip unreadable
                local $/; <$fh>;
            };
            next unless $json_text;

            my $chunk = eval { $json_decoder->decode($json_text) };
            if ($@) {
                warn " [WARN] Corrupt history partition $file: $@";
                next;
            }

            # Merge chunk into main hash (Structure: { "YYYY-MM": { ... } })
            my ($key) = keys %$chunk;
            if ($key) {
                $history_data->{$key} = $chunk->{$key};
            }
        }
        return $history_data;
    }

    # PATH B: Legacy Monolith (Fallback)
    if (-f $legacy_file && -s $legacy_file) {
        eval {
            my $json = JSON->new->allow_nonref;
            local $/;
            open my $fh, '<:encoding(utf8)', $legacy_file or die "[ERROR] $!";
            my $json_text = <$fh>;
            close $fh;
            $history_data = $json->decode($json_text);
        };
        if ($@) {
            warn " [WARN] Could not decode legacy history cache. Treating as empty. Error: $@";
            return {};
        }
        return $history_data;
    }

    # PATH C: New System (Empty)
    return {};
}

# ==============================================================================
# SUBROUTINE: write_unified_history
# PURPOSE:    Writes a new monthly entry to the unified history cache file.
#             This function handles file locking to prevent data corruption from
#             concurrent processes. It performs a safe read-modify-write of
#             the entire data structure.
# ARGUMENTS:
#   1. $system_cache_dir (string): The path to a system's cache directory.
#   2. $month_key (string): The key for the new entry (e.g., "2025-07").
#   3. $new_data_for_month_href (hash ref): The data for the new monthly entry.
# RETURNS:
#   - 1 on success, 0 on failure.
# ==============================================================================
sub write_unified_history {
    my ($system_cache_dir, $month_key, $new_data_for_month_href) = @_;

    my $history_file = File::Spec->catfile($system_cache_dir, $UNIFIED_HISTORY_FILE);

    # Acquire global lock
    my ($lock_fh, $lock_path);
    eval {
        ($lock_fh, $lock_path) = _acquire_history_lock($system_cache_dir);
    };
    if ($@) {
        warn " [WARN] Skipping history write due to lock failure: $@";
        return 0;
    }
    # ------------------------------------------------

    my $success = 0;
    eval {
        # Read the existing history file first.
        my $history_data = read_unified_history($system_cache_dir);

        # Add or overwrite the data for the specified month.
        $history_data->{$month_key} = $new_data_for_month_href;

        # Write the entire updated data structure back to the file.
        my $json = JSON->new->pretty->canonical;
        my $json_text = $json->encode($history_data);

        open my $fh, '>:encoding(utf8)', $history_file or die "Could not open '$history_file' for writing: $!";
        print $fh $json_text;
        close $fh;
        $success = 1;
    };
    if ($@) {
        warn " [WARN] An error occurred while writing to the unified history cache '$history_file': $@";
        $success = 0;
    }

    # Release the lock.
    close $lock_fh;

    # Note: We do NOT unlink the global lock file in the new architecture
    # to prevent race conditions on file creation. It remains as a sentinel.

    return $success;
}

# ==============================================================================
# SUBROUTINE: _build_residual_manifest
# PURPOSE:    Creates a manifest for the residual peak profile. The residual
#             profile is a special, ultra-sensitive profile (typically P99.9
#             with W1 window) used to detect volatile workload spikes.
#
# ARGUMENTS:
#   1. $event_config (hash ref): The event configuration containing the
#      residual_peak_profile flags string.
#   2. $exclusions_href (hash ref, optional): Hash of VM exclusions.
#
# RETURNS:
#   - Hash reference to the manifest, or undef if no residual profile defined.
# ==============================================================================
sub _build_residual_manifest {
    my ($event_config, $exclusions_href) = @_;

    # Check if a residual profile is configured
    my $residual_flags_str = $event_config->{residual_peak_profile};
    return undef unless (defined $residual_flags_str && $residual_flags_str ne '');

    # Create a temporary profile object from the flags string
    # This follows the same structure as profiles loaded from config
    my $temp_profile = {
        name  => 'ResidualPeakProfile',  # Fixed name for identification
        flags => $residual_flags_str
    };

    # 2. Build the manifest
    # Run Queue Residual is irrelevant for the standard residual headroom calculation
    my $manifest = build_transform_manifest([$temp_profile], $nfit_runq_avg_method_str, 'none');

    # --- INJECT EXCLUSIONS IF PRESENT ---
    if (defined $exclusions_href) {
        _inject_exclusions_into_manifest($manifest, $exclusions_href);
    }

    # 3. Create a temporary manifest file;
    return $manifest;
}

# ==============================================================================
# SUBROUTINE: _execute_history_pass
# PURPOSE:    Executes a single nfit pass for history priming using a sanitised
#             manifest. This wrapper reduces code duplication and ensures
#             consistent command construction across all history passes.
#
# ARGUMENTS:
#   1. $system_cache_dir (string): Path to the system cache directory
#   2. $manifest_href (hash ref): The sanitised historical manifest
#   3. $start_date (Time::Piece): Start date for analysis
#   4. $end_date (Time::Piece): End date for analysis
#   5. $enable_clipping (boolean): Whether to enable clipping detection
#   6. $pass_name (string): Descriptive name for logging (e.g., "Baseline")
#
# RETURNS:
#   - Hash reference to parsed results (VM -> [states]), or empty hash on failure
# ==============================================================================
sub _execute_history_pass {
    my ($system_cache_dir, $manifest_href, $start_date, $end_date, $enable_clipping, $pass_name) = @_;

    # Validate inputs
    unless (ref($manifest_href) eq 'HASH') {
        warn "WARNING: _execute_history_pass received invalid manifest for $pass_name pass\n";
        return {};
    }

    # Write manifest to temporary file
    use File::Temp qw(tempfile);
    my ($fh_manifest, $manifest_filename) = tempfile(
        "nfit_seasonal_${pass_name}_XXXXXX",
        SUFFIX => '.json',
        UNLINK => 1,
        TMPDIR => 1
    );

    print $fh_manifest JSON->new->pretty->canonical->encode($manifest_href);
    close $fh_manifest;

    # Build the nfit command
    my $nfit_cmd = "$nfit_script_path --manifest $manifest_filename "
                 . "--nmondir \"$system_cache_dir\" "
                 . "--startdate " . $start_date->ymd . " "
                 . "--enddate " . $end_date->ymd . " "
                 . "--smt $default_smt_arg "
                 . "--runq-avg-method $nfit_runq_avg_method_str";

    # Add clipping detection if requested (typically only for peak analysis)
    $nfit_cmd .= " --enable-clipping-detection" if $enable_clipping;

    # Execute the command
    print STDERR "  • Executing $pass_name analysis: " . $start_date->ymd . " to " . $end_date->ymd . "\n";

    my $nfit_output = '';
    my $stderr_arg = ">&=" . fileno(STDERR);
    my $pid = open3(undef, my $stdout_fh, $stderr_arg, $nfit_cmd);

    while (my $line = <$stdout_fh>) {
        $nfit_output .= $line;
    }

    waitpid($pid, 0);
    my $exit_code = $? >> 8;

    # Check for failure
    if ($exit_code != 0) {
        warn "WARNING: nfit failed during $pass_name pass (exit code: $exit_code)\n";
        warn "Command: $nfit_cmd\n";
        return {};
    }

    # Parse and return results
    my $parsed_results = parse_nfit_json_output($nfit_output);

    return $parsed_results;
}

# ==============================================================================
# SUBROUTINE: _assemble_seasonal_snapshot
# PURPOSE:    Assembles the final SeasonalEventSnapshot structure from the
#             results of three nfit passes (baseline, peak, residual). This
#             function contains the battle-tested logic for extracting values,
#             calculating residuals, and building ClippingInfo blocks.
#
# ARGUMENTS:
#   1. $peak_results_href (hash ref): Parsed peak period results
#   2. $baseline_results_href (hash ref): Parsed baseline period results
#   3. $residual_results_href (hash ref): Parsed residual peak results (may be empty)
#   4. $event_config (hash ref): Event configuration
#   5. $event_name (string): Event name for metadata
#   6. $peak_end (Time::Piece): Peak end date for metadata
#   7. $profiles_aref (array ref): Reference to the profiles array
#   8. $baseline_start (Time::Piece): Baseline analysis period start
#   9. $baseline_end (Time::Piece): Baseline analysis period end
#
# RETURNS:
#   - Hash reference to the complete snapshot structure, or undef on failure
# ==============================================================================
sub _assemble_seasonal_snapshot {
    my ($peak_results_href, $baseline_results_href, $residual_results_href,
        $event_config, $event_name, $peak_end, $profiles_aref, $baseline_start, $baseline_end) = @_;

    # Validate that profiles array was passed correctly
    unless (ref($profiles_aref) eq 'ARRAY' && @$profiles_aref) {
        die "FATAL: _assemble_seasonal_snapshot requires a valid profiles array reference\n";
    }

    # Determine the correct key names based on model type
    my $model_type = $event_config->{model} // '';
    my ($peak_key, $baseline_key) = ('HistoricPeak', 'HistoricBaseline');
    if ($model_type eq 'predictive_peak' or $model_type eq 'adaptive_peak') {
        ($peak_key, $baseline_key) = ('PeakValue', 'BaselineValue');
    }

    # Initialise result structures
    my %peak_period_results;
    my %baseline_period_results;
    my %clipping_info_results;
    my %residual_results_final;

    # ======================================================================
    # STEP 1: Extract baseline values for all VMs and profiles
    # ======================================================================
    # CRITICAL FIX: When nfit runs without decay (historical mode), it may
    # return multiple states per VM. Profiles with time filters (e.g. -online)
    # may appear in earlier states, whilst general profiles appear in later
    # states. We must search through ALL states to find each profile's value.
    # ======================================================================
    foreach my $vm_name (keys %$baseline_results_href) {
        my $states_aref = $baseline_results_href->{$vm_name};
        next unless ref($states_aref) eq 'ARRAY' && @$states_aref;

        # Extract values for each profile
        foreach my $profile (@$profiles_aref) {
            my $profile_name = $profile->{name};
            my $p_key = "P" . clean_perc_label(($profile->{flags} =~ /-p\s+([0-9.]+)/) ? $1 : $DEFAULT_PERCENTILE);

            # Search through all states (newest to oldest) to find this profile's value
            # We search backwards because the most recent state is usually most complete
            my $metric_val = undef;
            for (my $i = $#{$states_aref}; $i >= 0; $i--) {
                my $state = $states_aref->[$i];
                $metric_val = _safe_dig($state, 'metrics', 'physc', $profile_name, $p_key);
                last if defined $metric_val;  # Found it, stop searching
            }

            if (defined $metric_val) {
                $baseline_period_results{$vm_name}{$profile_name} = $metric_val;
            }
        }
    }

    # ======================================================================
    # STEP 2: Extract sanitised peak values for residual calculation
    # ======================================================================
    my %sanitised_peak_values;  # Used for residual calculation

    if (defined $event_config->{residual_peak_profile} &&
        $event_config->{residual_peak_profile} ne '' &&
        ref($residual_results_href) eq 'HASH' &&
        scalar keys %$residual_results_href) {

        foreach my $vm_name (keys %$residual_results_href) {
            my $states_aref = $residual_results_href->{$vm_name};
            next unless ref($states_aref) eq 'ARRAY' && @$states_aref;

            # Extract the residual profile's percentile
            my $residual_flags = $event_config->{residual_peak_profile};
            my $p_key = "P" . clean_perc_label(($residual_flags =~ /-p\s+([0-9.]+)/) ? $1 : $DEFAULT_PERCENTILE);

            # Search through all states (newest to oldest) to find the residual value
            my $metric_val = undef;
            for (my $i = $#{$states_aref}; $i >= 0; $i--) {
                my $state = $states_aref->[$i];
                $metric_val = _safe_dig($state, 'metrics', 'physc', 'ResidualPeakProfile', $p_key);
                last if defined $metric_val;  # Found it, stop searching
            }

            if (defined $metric_val) {
                $sanitised_peak_values{$vm_name} = $metric_val;
            }
        }
    }

    # ======================================================================
    # STEP 3: Extract peak values and clipping info for all profiles
    # ======================================================================
    # Same multi-state search logic as baseline extraction above.
    # ======================================================================
    foreach my $vm_name (keys %$peak_results_href) {
        my $states_aref = $peak_results_href->{$vm_name};
        next unless ref($states_aref) eq 'ARRAY' && @$states_aref;

        # Extract values for each profile
        foreach my $profile (@$profiles_aref) {
            my $profile_name = $profile->{name};
            my $p_key = "P" . clean_perc_label(($profile->{flags} =~ /-p\s+([0-9.]+)/) ? $1 : $DEFAULT_PERCENTILE);

            # Search through all states (newest to oldest) to find this profile's value
            my $standard_peak_value = undef;
            my $clipping_metrics = undef;
            my $source_state = undef;

            for (my $i = $#{$states_aref}; $i >= 0; $i--) {
                my $state = $states_aref->[$i];
                my $val = _safe_dig($state, 'metrics', 'physc', $profile_name, $p_key);
                if (defined $val) {
                    $standard_peak_value = $val;
                    $source_state = $state;
                    last;  # Found it, stop searching
                }
            }

            if (defined $standard_peak_value) {
                $peak_period_results{$vm_name}{$profile_name} = $standard_peak_value;
            }

            # Extract clipping information from the same state where we found the value
            my $max_capacity = 0;
            if (defined $source_state) {
                $clipping_metrics = _safe_dig($source_state, 'metrics', 'physc', $profile_name, 'ClippingInfo');
                # Note: The --config CSV file that is supported by this program captures max_capacity as 'maxcpu'.
                #       In .nfit.cache.states, the value (of the maximum usable CPU) is stored as 'max_capacity'.
                $max_capacity = _safe_dig($source_state, 'metadata', 'max_capacity') //  _safe_dig($source_state, 'metadata', 'max_cpu') // 0;
            }

            if ($clipping_metrics && ref($clipping_metrics) eq 'HASH' && exists $clipping_metrics->{isClipped}) {
                my $unmet_demand_est = $clipping_metrics->{unmetDemandEstimate} // 0;

                # Assemble ClippingInfo with the exact structure expected by consumers
                $clipping_info_results{$vm_name}{$profile_name} = {
                    isClipped               => $clipping_metrics->{isClipped},
                    clippingConfidence      => $clipping_metrics->{clippingConfidence} // 'unknown',
                    capacityLimit           => $max_capacity,
                    unmetDemandEstimate     => $unmet_demand_est,
                    unclippedPeakEstimate   => ($standard_peak_value // 0) + $unmet_demand_est,
                    platformSpecificMarkers => {
                        aix_runq_saturation => _safe_dig($clipping_metrics, 'platformMarkers', 'aix_runq_saturation') // 0
                    }
                };
            }

            # Calculate peak residual if we have sanitised peak data
            if (defined $standard_peak_value && exists $sanitised_peak_values{$vm_name}) {
                my $residual = $sanitised_peak_values{$vm_name} - $standard_peak_value;
                $residual_results_final{$vm_name}{$profile_name} = ($residual > 0) ? $residual : 0;
            }
        }
    }

    # ======================================================================
    # STEP 3b: Extract Metadata Peak (Absolute Maximum) for each VM
    # ======================================================================
    # The "Peak" column is a metadata field (-k), not a profile P-metric.
    # It represents the absolute raw maximum PhysC observed during the peak period.
    # We must explicitly harvest it since it's not captured by profile iteration.
    # ======================================================================
    my %metadata_peak_results;
    foreach my $vm_name (keys %$peak_results_href) {
        my $states_aref = $peak_results_href->{$vm_name};
        next unless ref($states_aref) eq 'ARRAY' && @$states_aref;

        my @peak_values;
        foreach my $state (@$states_aref) {
            # Extract the raw Peak metric from the P-99W1 profile (used for hinting)
            my $peak_val = _safe_dig($state, 'metrics', 'physc', $MANDATORY_PEAK_PROFILE_FOR_HINT, 'Peak');
            push @peak_values, $peak_val if (defined $peak_val && looks_like_number($peak_val));
        }

        if (@peak_values) {
            $metadata_peak_results{$vm_name} = max(@peak_values);
        }
    }

    # ======================================================================
    # STEP 4: Validate and return
    # ======================================================================
    unless (scalar keys %peak_period_results && scalar keys %baseline_period_results) {
        warn "      - WARNING: Could not generate valid peak/baseline data for event '$event_name'. Skipping snapshot.\n";
        return undef;
    }

    # Assemble the final snapshot structure
    return {
        eventName     => $event_name,
        periodEndDate => $peak_end->ymd,
        baselinePeriod => {
            startDate => $baseline_start ? $baseline_start->ymd : undef,
            endDate   => $baseline_end ? $baseline_end->ymd : undef,
        },
        periodEndDate => $peak_end->ymd,
        generatedOn   => localtime()->datetime(),
        results       => {
            $peak_key        => \%peak_period_results,
            $baseline_key    => \%baseline_period_results,
            'ClippingInfo'   => \%clipping_info_results,
            'PeakResidual'   => \%residual_results_final,
            'MetadataPeak'   => \%metadata_peak_results,  # Absolute raw peak per VM
        }
    };

}

# ==============================================================================
# SUBROUTINE: _store_model_forecast_to_history
# PURPOSE:    Stores the forecast results from a seasonal model run into the
#             unified history under ModelForecasts for the appropriate event.
#             This enables nfit-forecast to treat these as candidate forecasts.
#
# ARGUMENTS:
#   1. $system_cache_dir      - Path to the system's cache directory
#   2. $event_name            - Name of the seasonal event (e.g., 'month-end')
#   3. $model_type            - Model identifier (e.g., 'predictive_peak')
#   4. $forecast_results_href - Hash ref of {vm => {profile => value}}
#   5. $event_config          - Event configuration hash (optional)
#
# SCHEMA:
#   SeasonalEventSnapshots.{event}.ModelForecasts.{model} = {
#     _meta: { model_version, generated_on, analysis_window, fingerprints },
#     VM01: { forecast: {P-99W1: val}, baseline: {P-99W1: val} },
#     VM02: { ... }
#   }
# ==============================================================================
sub _store_model_forecast_to_history {
    my ($system_cache_dir, $event_name, $model_type, $forecast_results_href, $event_config, $horizon_meta, $analysis_context, $explicit_anchor_bucket, $fingerprints_href, $baseline_meta_href) = @_;

    return unless ($system_cache_dir && $event_name && $model_type && $forecast_results_href);
    return unless (scalar keys %$forecast_results_href > 0);

    print STDERR "  • Writing $model_type forecast to history (event '$event_name')\n";

    # Resolve effective analysis window (from analysis_context; fallback to cache span)
    my $data_cache_file = File::Spec->catfile($system_cache_dir, '.nfit.cache.data');
    my ($cache_start, $cache_end) = _get_cache_date_range($data_cache_file);

    my $analysis_start_obj = (defined $analysis_context->{start} && ref($analysis_context->{start}) eq 'Time::Piece')
        ? $analysis_context->{start}->truncate(to => 'day')
        : $cache_start->truncate(to => 'day');

    my $analysis_end_obj = (defined $analysis_context->{end} && ref($analysis_context->{end}) eq 'Time::Piece')
        ? $analysis_context->{end}->truncate(to => 'day')
        : $cache_end->truncate(to => 'day');

    unless ($analysis_start_obj && $analysis_end_obj) {
        print STDERR "  [WARN] Could not determine analysis date range. Forecasting cannot proceed\n";
        return;
    }

    # --- PHASE 1: DETERMINISTIC ANCHOR BUCKET RESOLUTION ---
    # If an explicit anchor bucket is provided by the orchestrator, use it.
    # This ensures deterministic history storage based on the resolved anchor,
    # not on which partitions happen to exist.

    my $target_month_key;

    if (defined $explicit_anchor_bucket && $explicit_anchor_bucket =~ /^\d{4}-\d{2}$/) {
        # Use the orchestrator-provided anchor bucket directly
        $target_month_key = $explicit_anchor_bucket;
        print STDERR "    → Target month (anchor bucket): $target_month_key\n";
    }
    elsif (defined $analysis_context && defined $analysis_context->{anchor_bucket}
           && $analysis_context->{anchor_bucket} =~ /^\d{4}-\d{2}$/) {
        # Fallback to anchor_bucket from analysis_context if available
        $target_month_key = $analysis_context->{anchor_bucket};
        print STDERR "    → Target month (from context): $target_month_key\n";
    }
    else {
        # Legacy fallback: determine from the analysis end date
        $target_month_key = $analysis_end_obj->strftime('%Y-%m');
        print STDERR "    → Target month (inferred): $target_month_key\n";
    }

    my $analysis_days = int(($analysis_end_obj->epoch - $analysis_start_obj->epoch) / ONE_DAY) + 1;
    my $analysis_start_date = $analysis_start_obj->strftime('%Y-%m-%d');
    my $analysis_end_date = $analysis_end_obj->strftime('%Y-%m-%d');

    # Read current history
    print STDERR "  • Loading unified partitioned history\n";
    my $history_data = read_unified_history($system_cache_dir);

    # --- PHASE 1: Relaxed partition existence check ---
    # We no longer require MonthlyWorkloadAnalysis to exist for the target month.
    # The seasonal snapshot can be stored even if the monthly analysis hasn't run yet.

    unless (exists $history_data->{$target_month_key}) {
        print STDERR "    [INFO] Creating new history partition for $target_month_key\n";
        $history_data->{$target_month_key} = {};
    }

    # Optional: inform if MonthlyWorkloadAnalysis is missing (for information only)
    unless (exists $history_data->{$target_month_key}{MonthlyWorkloadAnalysis}) {
        print STDERR "    [INFO] MonthlyWorkloadAnalysis not present for $target_month_key (OK for seasonal-only runs)\n";
    }

    if ($analysis_start_obj && $analysis_end_obj) {
        $analysis_days = int(($analysis_end_obj->epoch - $analysis_start_obj->epoch) / ONE_DAY) + 1;
    }

    # Ensure the structural hierarchy exists
    $history_data->{$target_month_key} //= {};
    $history_data->{$target_month_key}{SeasonalEventSnapshots} //= {};
    $history_data->{$target_month_key}{SeasonalEventSnapshots}{$event_name} //= {
        eventName => $event_name,
    };

    # Ensure periodEndDate is populated if missing (crucial for time-series plotting)
    $history_data->{$target_month_key}{SeasonalEventSnapshots}{$event_name}{periodEndDate} //= $analysis_end_date;

    # Initialise ModelForecasts container if absent
    my $event_snapshot = $history_data->{$target_month_key}{SeasonalEventSnapshots}{$event_name};
    $event_snapshot->{ModelForecasts} //= {};

    my %model_data;

    # Metadata block (underscore prefix distinguishes from VM keys)
    my $meta_block = {
        model_version   => 1,
        generated_on    => localtime()->datetime(),
        analysis_window => {
            days    => $analysis_days,
            start   => $analysis_start_date,
            end     => $analysis_end_date,
        },
    };

    # Inject the detailed AnalysisContext if provided
    if ($analysis_context) {

        # Recalculate window days from the context objects for precision
        my $window_days_calc = 0;
        if ($analysis_context->{start} && $analysis_context->{end}) {
            $window_days_calc = int(($analysis_context->{end}->epoch - $analysis_context->{start}->epoch) / 86400) + 1;
        }

        $meta_block->{AnalysisContext} = {
            # Data Boundaries (The "Ground Truth" for Backfill)
            cache_start            => $cache_start->datetime,
            cache_end              => $cache_end->datetime,

            # Effective analysis window (what this run used)
            analysis_start => $analysis_start_obj->datetime,
            analysis_end => $analysis_end_obj->datetime,

            # Statistical Context
            sampling_interval_seconds => ($analysis_context->{interval} // 0) + 0, # Ensure numeric
            analysis_window_days      => $window_days_calc,
            adaptive_horizon_days     => ($horizon_meta->{days} // 0) + 0,

            # Execution Context (as requested)
            analysis_start_epoch      => $analysis_context->{analysis_start},

            # Phase 3.1: bucket provenance (audit only)
            anchor_bucket             => $analysis_context->{anchor_bucket},
            anchor_source             => $analysis_context->{anchor_source},
            bucket_occurrence_count   => $analysis_context->{bucket_occurrence_count},
            bucket_occurrence_end_dates => $analysis_context->{bucket_occurrence_end_dates},
        };
    }

    # Inject Horizon Metadata if provided
    if ($horizon_meta) {
        $meta_block->{forecast_target_date}  = $horizon_meta->{target_date};
        $meta_block->{forecast_horizon_days} = $horizon_meta->{days};
    }

    # Phase 4: Inject fingerprints for idempotency (if provided)
    if ($fingerprints_href && ref($fingerprints_href) eq 'HASH') {
        $meta_block->{fingerprints} = {
            combined       => $fingerprints_href->{combined},
            event_def      => $fingerprints_href->{event_def},
            exec_ctx       => $fingerprints_href->{exec_ctx},
            chain          => $fingerprints_href->{chain},
            engine_version => $fingerprints_href->{engine_version},
        };
    }

    # Phase 5: Inject baseline metadata for staleness detection (if provided)
    # These fields sit at the _meta level alongside 'fingerprints', enabling
    # _should_reprocess_bucket to detect baseline changes independently of
    # the bucket fingerprint chain.
    if ($baseline_meta_href && ref($baseline_meta_href) eq 'HASH') {
        $meta_block->{baseline_fingerprint}  = $baseline_meta_href->{fingerprint};
        $meta_block->{baseline_source}       = $baseline_meta_href->{baseline_source} // 'history';
        $meta_block->{baseline_span}         = ($baseline_meta_href->{span_start} && $baseline_meta_href->{span_end})
                                             ? "$baseline_meta_href->{span_start}:$baseline_meta_href->{span_end}"
                                             : undef;
        $meta_block->{baseline_months_read}  = $baseline_meta_href->{months_read};
        $meta_block->{baseline_coverage_pct} = _calculate_aggregate_coverage(
                                                   $baseline_meta_href->{coverage_by_vm});
    }

    $model_data{'_meta'} = $meta_block;

    # Build per-VM forecast and baseline data
    my $vm_count = 0;
    foreach my $vm_name (sort keys %$forecast_results_href) {
        my $vm_results = $forecast_results_href->{$vm_name};
        next unless (ref($vm_results) eq 'HASH' && scalar keys %$vm_results > 0);

        my %forecast_by_profile;
        my %baseline_by_profile;
        my %components_by_profile;

        foreach my $profile_key (keys %$vm_results) {
            my $value = $vm_results->{$profile_key};

            # Store forecast value
            if (defined $value && looks_like_number($value)) {
                $forecast_by_profile{$profile_key} = sprintf("%.4f", $value) + 0;
            }

            # Extract baseline and components from seasonal_debug_info if available
            if (exists $seasonal_debug_info{$vm_name}{$profile_key}) {
                my $debug = $seasonal_debug_info{$vm_name}{$profile_key};

                # Model-specific baseline and component extraction
                my $baseline_val;
                if ($model_type eq 'predictive_peak') {
                    $baseline_val = $debug->{TrueBaseline};
                    # Capture prediction components for audit
                    if (defined $debug->{PredictedPeak} || defined $debug->{PredictedResidual}) {
                        $components_by_profile{$profile_key} = {
                            predicted_peak      => $debug->{PredictedPeak},
                            predicted_residual  => $debug->{PredictedResidual},
                            combined_prediction => $debug->{CombinedPrediction},
                            final_source        => $debug->{FinalSource},
                        };
                    }
                }
                elsif ($model_type eq 'multiplicative_seasonal') {
                    $baseline_val = $debug->{baseline};
                    # Capture the seasonal multiplier for audit
                    if (defined $debug->{multiplier}) {
                        $components_by_profile{$profile_key} = {
                            seasonal_multiplier  => $debug->{multiplier},
                            trend_factor         => $debug->{trend_factor},
                            volatility_buffer    => $debug->{volatility},
                            forecasted_residual  => $debug->{forecasted_residual},
                            amplification_factor => $debug->{amplification_factor},
                        };
                    }
                    # Phase 3.2: synthesis distribution metadata (if aggregated)
                    if (exists $debug->{synthesis} && ref($debug->{synthesis}) eq 'HASH') {
                        $components_by_profile{$profile_key} //= {};
                        $components_by_profile{$profile_key}{synthesis} = $debug->{synthesis};
                    }
                }

                if (defined $baseline_val && looks_like_number($baseline_val)) {
                    $baseline_by_profile{$profile_key} = sprintf("%.4f", $baseline_val) + 0;
                }
            }
        }

        # Only store VMs that have actual forecast data
        if (scalar keys %forecast_by_profile > 0) {
            $model_data{$vm_name} = {
                forecast => \%forecast_by_profile,
            };
            $model_data{$vm_name}{baseline} = \%baseline_by_profile if %baseline_by_profile;
            $model_data{$vm_name}{components} = \%components_by_profile if %components_by_profile;
            $vm_count++;
        }
    }

    # Only write if we have actual data
    if ($vm_count > 0) {
        $event_snapshot->{ModelForecasts}{$model_type} = \%model_data;

        # Phase 3.2: Also populate results block (Zone 1) for models that generate observable facts
        # This enables multiplier calculation for subsequent occurrences within the same run
        if ($model_type eq 'multiplicative_seasonal') {
            $event_snapshot->{results} //= {};
            $event_snapshot->{results}{HistoricPeak} //= {};
            $event_snapshot->{results}{HistoricBaseline} //= {};

            foreach my $vm_name (keys %model_data) {
                foreach my $profile_key (keys %{ $model_data{$vm_name}{forecast} // {} }) {
                    # HistoricPeak: use observed_peak from debug_info, fallback to forecast
                    my $peak_val = undef;
                    if (exists $seasonal_debug_info{$vm_name}{$profile_key}{observed_peak}) {
                        $peak_val = $seasonal_debug_info{$vm_name}{$profile_key}{observed_peak};
                    }
                    $peak_val //= $model_data{$vm_name}{forecast}{$profile_key};

                    if (defined $peak_val && looks_like_number($peak_val)) {
                        $event_snapshot->{results}{HistoricPeak}{$vm_name}{$profile_key} = $peak_val + 0;
                    }

                    # HistoricBaseline: from model_data baseline
                    my $base_val = $model_data{$vm_name}{baseline}{$profile_key};
                    if (defined $base_val && looks_like_number($base_val)) {
                        $event_snapshot->{results}{HistoricBaseline}{$vm_name}{$profile_key} = $base_val + 0;
                    }
                }
            }
        }


        # Phase 3.3: Capture Zone 1 results for recency_decay
        # Although recency_decay doesn't consume historical snapshots, recording what it
        # observed enables historical audit, accuracy analysis, and cross-month comparison.
        # For recency_decay, the "forecast" IS the observed peak (measurement of last peak period).
        if ($model_type eq 'recency_decay') {
            $event_snapshot->{results} //= {};
            $event_snapshot->{results}{PeakValue} //= {};

            foreach my $vm_name (keys %model_data) {
                foreach my $profile_key (keys %{ $model_data{$vm_name}{forecast} // {} }) {
                    # PeakValue: For recency_decay, the forecast IS the observed peak
                    my $peak_val = $model_data{$vm_name}{forecast}{$profile_key};
                    if (defined $peak_val && looks_like_number($peak_val)) {
                        $event_snapshot->{results}{PeakValue}{$vm_name}{$profile_key} = $peak_val + 0;
                    }
                }
            }

            # Capture ClippingInfo from %seasonal_debug_info if available
            if (%seasonal_debug_info) {
                $event_snapshot->{results}{ClippingInfo} //= {};
                foreach my $vm_name (keys %seasonal_debug_info) {
                    foreach my $profile_key (keys %{$seasonal_debug_info{$vm_name}}) {
                        my $clip_info = $seasonal_debug_info{$vm_name}{$profile_key}{ClippingInfo};
                        if (ref($clip_info) eq 'HASH' && scalar keys %$clip_info > 0) {
                            $event_snapshot->{results}{ClippingInfo}{$vm_name}{$profile_key} = $clip_info;
                        }
                    }
                }
            }
        }

        # Update metadata timestamp
        $history_data->{$target_month_key}{Metadata} //= {};
        $history_data->{$target_month_key}{Metadata}{LastUpdated} = localtime()->datetime();

        # Write back to history
        _write_full_history($system_cache_dir, $history_data);

        print STDERR "  ✓ Stored $model_type forecasts for $vm_count VM(s) in $target_month_key history\n";
    }
    else {
        print STDERR "  [WARN] No valid forecast data to store for $model_type in history\n";
    }
}

# ==============================================================================
# SUBROUTINE: _mark_scores_invalid_for_snapshot
# PURPOSE:    Phase 7 — Cross-zone score invalidation.
#             When a snapshot is reprocessed due to baseline changes, Zone 3
#             (nfit-forecast.py) tournament scores become stale because they
#             were calculated against the previous forecast values.
#
#             This function sets a staleness marker at the event snapshot level
#             that Zone 3 detects via its S6 scoring gate. Zone 2 NEVER deletes
#             or modifies Zone 3 artefacts (scores, candidates); it only sets
#             the cross-zone signal. Zone 3 is responsible for rescoring.
#
# ARGUMENTS:
#   $system_cache_dir  - Path to the system's cache directory
#   $event_name        - Name of the seasonal event
#   $month_key         - Target month key (YYYY-MM)
#   $reason            - Human-readable reason string for audit trail
#
# ZONE BOUNDARY:  Zone 2 writes → Zone 3 reads (one-way signal)
# ==============================================================================
sub _mark_scores_invalid_for_snapshot {
    my ($system_cache_dir, $event_name, $month_key, $reason) = @_;

    return unless ($system_cache_dir && $event_name && $month_key);

    my $partition_dir = File::Spec->catfile($system_cache_dir, '.nfit.history');
    my $filepath = File::Spec->catfile($partition_dir, "nfit.hist.${month_key}.json");

    return unless (-f $filepath && -s $filepath);

    # Read the single partition
    my $partition_data;
    eval {
        open my $fh, '<:encoding(utf8)', $filepath
            or die "Cannot open $filepath: $!";
        local $/;
        $partition_data = JSON->new->allow_nonref->decode(<$fh>);
        close $fh;
    };
    if ($@ || !$partition_data || ref($partition_data) ne 'HASH') {
        warn "  [WARN] Phase 7: Could not read partition $month_key for score invalidation: $@\n";
        return;
    }

    # Navigate to the event snapshot
    my $month_data = $partition_data->{$month_key};
    return unless (ref($month_data) eq 'HASH');

    my $event_snapshot = $month_data->{SeasonalEventSnapshots}{$event_name} // undef;
    return unless (ref($event_snapshot) eq 'HASH');

    # Guard: only set marker if Zone 3 has actually scored this snapshot.
    # If AdaptiveForecast.scoring does not exist, there are no scores to invalidate.
    my $adaptive = $event_snapshot->{AdaptiveForecast} // {};
    unless (ref($adaptive) eq 'HASH' && exists $adaptive->{scoring}) {
        if (($verbose // 0) >= 2) {
            print STDERR "      ↳  Phase 7: No Zone 3 scoring block for $event_name / $month_key — no scores to invalidate\n";
        }
        return;
    }

    # Set the cross-zone staleness marker at event snapshot level.
    # This is deliberately placed OUTSIDE AdaptiveForecast to maintain
    # zone boundary clarity — Zone 2 owns the event snapshot root,
    # Zone 3 owns AdaptiveForecast internals.
    $event_snapshot->{scores_valid}        = 0;
    $event_snapshot->{baseline_changed_at} = localtime()->datetime();
    $event_snapshot->{score_invalidation_reason} = $reason // 'baseline inputs changed';

    # Atomic write-back (single partition)
    my $json_encoder = JSON->new->pretty->canonical;
    my $tmp_path = "${filepath}.tmp.$$";

    eval {
        my $json_text = $json_encoder->encode($partition_data);
        open my $fh, '>:encoding(utf8)', $tmp_path
            or die "Open temp failed: $!";
        print {$fh} $json_text or die "Write failed: $!";
        close $fh or die "Close failed: $!";
        rename $tmp_path, $filepath
            or die "Rename failed ($tmp_path → $filepath): $!";
    };
    if ($@) {
        warn "  [WARN] Phase 7: Score invalidation write failed for $month_key: $@\n";
        unlink $tmp_path if (-f $tmp_path);
        return;
    }

    print STDERR "    [!] Scores marked invalid for $event_name / $month_key ($reason)\n";
}

# ==============================================================================
# SUBROUTINE: _generate_seasonal_snapshot_for_period
# PURPOSE:    Performs single-pass analysis for a seasonal event's peak and
#             baseline periods using the manifest-driven Single Pass Engine
#             architecture. This function executes nfit three times (baseline,
#             peak, residual) instead of 3 * N times (where N = number of profiles).
#
#             This refactored version achieves the same I/O reduction as the
#             monthly analysis (typically 16x for a 16-profile configuration).
# ==============================================================================
sub _generate_seasonal_snapshot_for_period {
    my ($system_cache_dir, $event_config, $event_name, $peak_start, $peak_end, $baseline_start, $baseline_end, $exclusions_href) = @_;

    print STDERR "  ⧉ Executing Single Pass Engine for seasonal event $event_name (baseline, peak, residual)\n";

    # ======================================================================
    # STEP 1: Build and sanitise the main manifest (all profiles)
    # ======================================================================
    my $runq_behavior = $runq_perc_behavior_mode // 'none';
    my $tactical_manifest = build_transform_manifest(\@profiles, $nfit_runq_avg_method_str, $runq_behavior);
    my $main_historical_manifest = _sanitise_manifest_for_history($tactical_manifest);

    # --- INJECT EXCLUSIONS IF PRESENT ---
    if (defined $exclusions_href) {
        _inject_exclusions_into_manifest($main_historical_manifest, $exclusions_href);
    }

    # ======================================================================
    # STEP 2: Build and sanitise the residual manifest (if configured)
    # ======================================================================
    my $residual_historical_manifest = undef;

    if (defined $event_config->{residual_peak_profile} && $event_config->{residual_peak_profile} ne '') {
        my $residual_tactical_manifest = _build_residual_manifest($event_config);
        if (defined $residual_tactical_manifest) {
            $residual_historical_manifest = _sanitise_manifest_for_history($residual_tactical_manifest);

            # Inject exclusions into residual manifest too
            if (defined $exclusions_href) {
                _inject_exclusions_into_manifest($residual_historical_manifest, $exclusions_href);
            }
        }
    }

    # ======================================================================
    # STEP 3: Execute three nfit passes
    # ======================================================================

    # Pass 1: Baseline period (no clipping detection needed)
    my $baseline_results = _execute_history_pass(
        $system_cache_dir,
        $main_historical_manifest,
        $baseline_start,
        $baseline_end,
        0,  # enable_clipping = false
        "Baseline"
    );

    # Pass 2: Standard peak period (with clipping detection)
    my $peak_results = _execute_history_pass(
        $system_cache_dir,
        $main_historical_manifest,
        $peak_start,
        $peak_end,
        1,  # enable_clipping = true
        "Peak"
    );

    # Pass 3: Residual peak period (only if configured)
    my $residual_results = {};
    if (defined $residual_historical_manifest) {
        $residual_results = _execute_history_pass(
            $system_cache_dir,
            $residual_historical_manifest,
            $peak_start,
            $peak_end,
            0,  # enable_clipping = false
            "Residual"
        );
    }

    # ======================================================================
    # STEP 4: Assemble the final snapshot structure
    # ======================================================================
    my $snapshot_result = _assemble_seasonal_snapshot(
        $peak_results,
        $baseline_results,
        $residual_results,
        $event_config,
        $event_name,
        $peak_end,
        \@profiles,  # Pass profiles array explicitly to ensure correct scoping
        $baseline_start,
        $baseline_end,
    );

    return $snapshot_result;
}

# ==============================================================================
# SUBROUTINE: update_monthly_history (PRODUCTION FIX - Unified In-Memory)
# PURPOSE:    Orchestrates a robust, multi-phase strategy to create and update
#             the unified monthly history cache using a single in-memory hash
#             to prevent data corruption from conflicting file writes.
# ==============================================================================
sub update_monthly_history {
    my ($system_cache_dir, $system_identifier, $seasonality_config, $min_days_for_history, $adaptive_runq_saturation_thresh, $force_flag) = @_;

    print STDERR "▶ Updating Unified History for system: $system_identifier\n";

    # In-Flight auto-migration to per-month history
    # We acquire the lock here to perform the check and potential migration atomically.
    {
        my ($lock_fh, $lock_path);
        eval { ($lock_fh, $lock_path) = _acquire_history_lock($system_cache_dir); };
        if (!$@) {
            if (-f File::Spec->catfile($system_cache_dir, '.nfit.history.json') &&
                !-d File::Spec->catfile($system_cache_dir, '.nfit.history')) {

                _migrate_history_to_partitioned($system_cache_dir);
            }
            close $lock_fh;
        }
    }
    # --------------------------------------------

    my $data_cache_file = File::Spec->catfile($system_cache_dir, '.nfit.cache.data');
    my ($cache_start_obj, $cache_end_obj) = _get_cache_date_range($data_cache_file);

    unless ($cache_start_obj && $cache_end_obj) {
        warn "  [WARN] Unable to determine date range of data cache (.nfit.cache.data). Cannot update history.\n";
        return;
    }
    print STDERR "  ⧗ Data cache time range: " . $cache_start_obj->date . " - " . $cache_end_obj->date . "\n";

    # --- Step 1: Load existing history ONCE ---
    my $history_data = read_unified_history($system_cache_dir);
    my @months_to_process;

    # --- Step 2: Planning Stage - Discover unprocessed months ---
    print STDERR "  ↳  Scanning data cache for unprocessed or incomplete months\n";
    my $iterator_month = Time::Piece->new($cache_start_obj->epoch)->truncate(to => 'month');
    while ($iterator_month <= $cache_end_obj) {
        my $month_key = $iterator_month->strftime('%Y-%m');
        my $month_start_obj = Time::Piece->new($iterator_month->epoch);
        my $month_end_obj   = Time::Piece->new($month_start_obj->epoch)->add_months(1) - ONE_DAY;
        my $effective_start_obj = ($cache_start_obj > $month_start_obj) ? $cache_start_obj : $month_start_obj;
        my $effective_end_obj   = ($cache_end_obj   < $month_end_obj)   ? $cache_end_obj   : $month_end_obj;
        my $days_in_period = ($effective_start_obj <= $effective_end_obj) ? (int(($effective_end_obj->epoch - $effective_start_obj->epoch) / 86400) + 1) : 0;

        my $should_process = 0;

        # Logic: Process if new, if data is more complete, OR if forced.
        if (exists $history_data->{$month_key}) {
            my $stored_days = $history_data->{$month_key}{Metadata}{ProcessedDays} // 0;

            # --- FIX: Check force_flag here ---
            if ($force_flag) {
                print STDERR "  ↳  Forcing re-processing of $month_key (user request)\n";
                $should_process = 1;
            }
            elsif ($days_in_period > $stored_days) {
                 print STDERR "  ↳  Discovered additional data for $month_key ($days_in_period > $stored_days days)\n";
                 $should_process = 1;
            }
        } elsif ($days_in_period >= $min_days_for_history) {
            # New month detected
            $should_process = 1;
        }

        if ($should_process) {
            push @months_to_process, { key => $month_key, start_obj => $effective_start_obj, end_obj => $effective_end_obj, days => $days_in_period };
        }
        $iterator_month = $iterator_month->add_months(1);
    }
    print STDERR "  ✓ Data Cache examination complete\n";

    # Pre-execution validation for the first month to be processed
    if (@months_to_process) {
        my $first_month_key = $months_to_process[0]{key};
        _validate_pre_update_conditions($system_cache_dir, $first_month_key, $force_flag);

        # Create checkpoint backup before any modifications
        print STDERR "  • Creating pre-tactical checkpoint backup\n";
        _create_checkpoint_backup($system_cache_dir, 'pre-tactical', 1);
    }

    # --- Step 3: Execution Stage ---
    if (@months_to_process) {
        print STDERR "  ⧗ Identified " . scalar(@months_to_process) . " month(s) to process: " . join(", ", map { $_->{key} } @months_to_process) . "\n";

        my @all_historical_peaks = find_all_historical_periods($seasonality_config, $cache_start_obj, $cache_end_obj);

        # This single function call modifies the in-memory $history_data hash.
        _generate_monthly_and_seasonal_history(
            $system_cache_dir, \@months_to_process, $seasonality_config,
            \@all_historical_peaks, $history_data, $adaptive_runq_saturation_thresh
        );
    } else {
        print STDERR "  ✓ No new months to process. History is up to date\n";
    }


    # Track if this was a first run (no pre-existing history file)
    my $history_file = File::Spec->catfile($system_cache_dir, '.nfit.history.json');
    my $was_first_run = !(-f $history_file);

    # --- Phase 3: Enrich with Growth Rationale from L2 Cache ---
    my $was_enriched = _enrich_history_with_growth_rationale($system_cache_dir, $history_data);

    # --- Step 4: Final Write ---
    # The history file is written only ONCE at the end if any changes were made.
    if (@months_to_process || $was_enriched) {
        print "  ↳  Finalising updates to history cache.\n";

        _write_full_history($system_cache_dir, $history_data);
        print "  ✓ History cache successfully updated.\n";

        # If this was the first run, create a post-write backup to protect the newly created file
        if ($was_first_run && -f $history_file) {
            print "\n--- Creating Post-Initial-Write Backup ---\n";
            _create_checkpoint_backup($system_cache_dir, 'post-initial-write', 1);
        }
    } else {
        print "  ✓ History cache is already up to date.\n";
    }
}

# ==============================================================================
# SUBROUTINE: _generate_monthly_and_seasonal_history (PRODUCTION FIX - COMBINED)
# PURPOSE:    Processes months to generate both generic analysis and snapshots,
#             modifying a single in-memory hash to prevent data corruption.
# ==============================================================================
sub _generate_monthly_and_seasonal_history {
    my ($system_cache_dir, $months_to_process_aref, $seasonality_config, $all_historical_peaks_aref, $history_data_href, $adaptive_runq_saturation_thresh) = @_;

    foreach my $month_job (@$months_to_process_aref) {
        my $month_key = $month_job->{key};
        my $start_str = $month_job->{start_obj}->strftime('%Y-%m-%d');
        my $end_str   = $month_job->{end_obj}->strftime('%Y-%m-%d');

        # --- PHASE 1: Generate MonthlyWorkloadAnalysis ---
        print "\n--- Processing Month: $month_key (Period: $start_str to $end_str) ---\n";
        my $workload_analysis = _generate_monthly_workload_analysis($system_cache_dir, $start_str, $end_str, $adaptive_runq_saturation_thresh);

        # --- Populate the in-memory hash ---
        $history_data_href->{$month_key} = {
            MonthlyWorkloadAnalysis => $workload_analysis,
            Metadata => {
                ProcessedStartDate => $start_str,
                ProcessedEndDate   => $end_str,
                ProcessedDays      => $month_job->{days},
                LastUpdated        => localtime()->datetime(),
            }
        };

        # --- PHASE 2: Generate SeasonalEventSnapshots for this month ---
        print "  - Discovering seasonal events for month: $month_key...\n";
        my %seasonal_snapshots_for_month;
        my %calculation_cache;

        foreach my $event_name (sort keys %$seasonality_config) {
            next if $event_name eq 'Global';
            my $event_config = $seasonality_config->{$event_name};
            next if (($event_config->{model} // '') eq 'recency_decay');

            my ($peak_start, $peak_end) = determine_event_period($event_config, $month_job->{start_obj});

            # Parse Exclusions
            my $exclusions = undef;
            my $exclusion_fingerprint = 'NONE';
            if (defined $event_config->{exclude_dates}) {
                # We need known_vms for wildcard expansion.
                # Optimisation: You might want to fetch this ONCE outside the loop if performance matters,
                # but fetching it here is safe.
                my $known_vms = _get_known_vms_from_cache($system_cache_dir);
                $exclusions = _parse_exclusion_dates($event_config->{exclude_dates}, $known_vms);
                $exclusion_fingerprint = _compute_exclusion_fingerprint($exclusions);
            }

            if ($peak_start && $peak_end && $peak_start <= $month_job->{end_obj} && $peak_end >= $month_job->{start_obj}) {
                print "    - Found active event: '$event_name'\n";
                my $baseline_days = $event_config->{baseline_period_days} // 16;
                my $potential_baseline_start = $peak_start - ($baseline_days * ONE_DAY);
                my $latest_preceding_peak_end;
                foreach my $p (@{$all_historical_peaks_aref}) {
                    if ($p->[1] < $peak_start && (!defined $latest_preceding_peak_end || $p->[1] > $latest_preceding_peak_end)) {
                        $latest_preceding_peak_end = $p->[1];
                    }
                }
                my $baseline_start_obj = $potential_baseline_start;
                if (defined $latest_preceding_peak_end && $potential_baseline_start <= $latest_preceding_peak_end) {
                    $baseline_start_obj = $latest_preceding_peak_end + ONE_DAY;
                    my $available_days = ($peak_start->epoch - $baseline_start_obj->epoch) / ONE_DAY;
                    print "      - WARNING: Baseline for this event truncated to " . int($available_days) . " day(s) to avoid overlap.\n";
                }
                my $baseline_end_obj = $peak_start - ONE_SECOND;
                my $model_type_for_key = $event_config->{model} // 'unknown';

                # Update Cache Key with Fingerprint
                my $cache_key = $peak_start->date . ":" . $peak_end->date . ":" . $baseline_start_obj->date .  ":" . $model_type_for_key . ":" . $exclusion_fingerprint;

                my $snapshot_results;
                if (exists $calculation_cache{$cache_key}) {
                    print "      - INFO: Reusing cached calculations for identical time period and model type.\n";
                    $snapshot_results = $calculation_cache{$cache_key};
                } else {
                    $snapshot_results = _generate_seasonal_snapshot_for_period(
                        $system_cache_dir,
                        $event_config,
                        $event_name,
                        $peak_start, $peak_end,
                        $baseline_start_obj, $baseline_end_obj,
                        $exclusions
                    );
                    $calculation_cache{$cache_key} = $snapshot_results;
                }
                if ($snapshot_results) {
                    $snapshot_results->{eventName} = $event_name;
                    $seasonal_snapshots_for_month{$event_name} = $snapshot_results;
                }
            }
        }

        if (scalar keys %seasonal_snapshots_for_month > 0) {
            $history_data_href->{$month_key}{SeasonalEventSnapshots} = \%seasonal_snapshots_for_month;
            $history_data_href->{$month_key}{Metadata}{LastUpdated} = localtime()->datetime();
        }
    }
}

# ==============================================================================
# SUBROUTINE: _validate_history_structure
# PURPOSE:    Performs structural validation on the history data structure before
#             writing to disk. This safety check prevents corruption by detecting
#             missing or empty blocks that would break downstream consumers.
#
# ARGUMENTS:
#   1. $history_href (hash ref): The complete history data structure
#
# RETURNS:
#   - 1 on success (structure is valid)
#   - Dies with detailed error message on validation failure
#
# VALIDATION CHECKS:
#   1. Required blocks exist (MonthlyWorkloadAnalysis, Metadata)
#   2. Blocks are not empty
#   3. Required fields are present in nested structures
#   4. Basic data type validation (hashes where expected, etc.)
# ==============================================================================
sub _validate_history_structure {
    my ($history_href) = @_;

    # Validate input type
    unless (ref($history_href) eq 'HASH') {
        die "FATAL: History structure validation failed - not a hash reference\n";
    }

    # Check that we have at least one month of data
    my @months = keys %$history_href;
    unless (@months) {
        die "FATAL: History structure validation failed - no monthly data present\n";
    }

    # Validate each month's structure
    foreach my $month_key (sort @months) {
        my $month_data = $history_href->{$month_key};

        # Check that month data is a hash
        unless (ref($month_data) eq 'HASH') {
            die "FATAL: History structure validation failed for $month_key - month data is not a hash\n";
        }

        # Check for required top-level blocks
        unless (exists $month_data->{MonthlyWorkloadAnalysis}) {
            die "FATAL: History structure validation failed for $month_key - missing MonthlyWorkloadAnalysis block\n";
        }

        unless (exists $month_data->{Metadata}) {
            die "FATAL: History structure validation failed for $month_key - missing Metadata block\n";
        }

        # Validate MonthlyWorkloadAnalysis is not empty
        my $workload_analysis = $month_data->{MonthlyWorkloadAnalysis};
        unless (ref($workload_analysis) eq 'HASH') {
            die "FATAL: History structure validation failed for $month_key - MonthlyWorkloadAnalysis is not a hash\n";
        }

        unless (scalar keys %$workload_analysis) {
            die "FATAL: History structure validation failed for $month_key - MonthlyWorkloadAnalysis is empty\n";
        }

        # Validate that each VM in MonthlyWorkloadAnalysis has required fields
        foreach my $vm_name (keys %$workload_analysis) {
            my $vm_data = $workload_analysis->{$vm_name};

            unless (ref($vm_data) eq 'HASH') {
                die "FATAL: History structure validation failed for $month_key VM $vm_name - VM data is not a hash\n";
            }

            # Check for required fields
            my @required_fields = qw(ProfileValues Hint Pattern Pressure);
            foreach my $field (@required_fields) {
                unless (exists $vm_data->{$field}) {
                    die "FATAL: History structure validation failed for $month_key VM $vm_name - missing required field: $field\n";
                }
            }

            # Validate ProfileValues is a non-empty hash
            unless (ref($vm_data->{ProfileValues}) eq 'HASH' && scalar keys %{$vm_data->{ProfileValues}}) {
                die "FATAL: History structure validation failed for $month_key VM $vm_name - ProfileValues is empty or invalid\n";
            }
        }

        # Validate Metadata structure
        my $metadata = $month_data->{Metadata};
        unless (ref($metadata) eq 'HASH') {
            die "FATAL: History structure validation failed for $month_key - Metadata is not a hash\n";
        }

        # Check for essential metadata fields
        unless (exists $metadata->{ProcessedDays}) {
            die "FATAL: History structure validation failed for $month_key - Metadata missing ProcessedDays\n";
        }

        # Optional: Validate SeasonalEventSnapshots structure if present
        if (exists $month_data->{SeasonalEventSnapshots}) {
            my $seasonal = $month_data->{SeasonalEventSnapshots};
            unless (ref($seasonal) eq 'HASH') {
                die "FATAL: History structure validation failed for $month_key - SeasonalEventSnapshots is not a hash\n";
            }

            # Validate each event's structure
            foreach my $event_name (keys %$seasonal) {
                my $event_data = $seasonal->{$event_name};

                # Allow entry if it has 'results' (Snapshot) OR 'ModelForecasts'/'AdaptiveForecast' (Pure Forecast)
                # Required since recency_decay models (and nfit-forecast backfills) create entries that contain only ModelForecasts (predictions), not results (ingredients).
                my $has_results = (exists $event_data->{results} && ref($event_data->{results}) eq 'HASH');
                my $has_forecast = (exists $event_data->{ModelForecasts} || exists $event_data->{AdaptiveForecast});

                unless (ref($event_data) eq 'HASH' && ($has_results || $has_forecast)) {
                    die "FATAL: History structure validation failed for $month_key event $event_name - missing both 'results' (Snapshot) and 'ModelForecasts' (Forecast) blocks\n";
                }
            }

        }
    }

    # All validation checks passed
    return 1;
}

# ==============================================================================
# SUBROUTINE: _write_full_history (Partition Aware)
# PURPOSE:    Writes the in-memory history hash to the partitioned directory structure.
#             Creates the directory if missing. Enforces Global Lock.
#             Includes structural validation before writing to prevent corruption.
#             Refactored to use the standardised _acquire_history_lock for safety.
# ARGUMENTS:
#   1. $system_cache_dir (string): The path to a system's cache directory.
#   2. $history_data_href (hash ref): The complete history data structure.
# RETURNS:
#   - None
# ==============================================================================
sub _write_full_history {
    my ($system_cache_dir, $history_data_href) = @_;

    # 1. Validation
    # ======================================================================
    # CRITICAL SAFETY CHECK: Validate structure before writing
    # ======================================================================
    # This validation prevents corrupted or incomplete data from being
    # persisted to disk. It's better to fail fast with a clear error
    # than to write broken data that silently breaks downstream consumers.
    # ======================================================================
    eval { _validate_history_structure($history_data_href); };
    if ($@) {
        die "FATAL: Aborting history write due to validation failure:\n$@";
    }

    _add_lightweight_metadata($history_data_href, 'nfit-profile');

    # 2. Global Lock Acquisition
    my ($lock_fh, $lock_path);
    eval { ($lock_fh, $lock_path) = _acquire_history_lock($system_cache_dir); };
    if ($@) { die "FATAL: Could not acquire global history lock: $@"; }

    # 3. Directory Setup
    my $partition_dir = File::Spec->catfile($system_cache_dir, '.nfit.history');
    unless (-d $partition_dir) {
        make_path($partition_dir) or die "FATAL: Cannot create history directory: $!";
    }

    # 4. Write Partitions
    my $json_encoder = JSON->new->pretty->canonical;
    my $write_errors = 0;

    # Sort to give deterministic write order (helps logs/debugging)
    foreach my $month_key (sort keys %$history_data_href) {

        # Skip metadata/non-month keys if any creep in
        next unless $month_key =~ /^\d{4}-\d{2}$/;

        my $filename = "nfit.hist.${month_key}.json";
        my $filepath = File::Spec->catfile($partition_dir, $filename);

        # Wrap in month key for consistency
        my $payload = { $month_key => $history_data_href->{$month_key} };

        my $json_text;
        my $tmp_path = "${filepath}.tmp.$$";
        my $bak_path = "${filepath}.bak";

        eval {
            # Encode FIRST (prevents truncation if encode dies)
            $json_text = $json_encoder->encode($payload);

            # Write to temp file in same directory
            open my $fh, '>:encoding(utf8)', $tmp_path
                or die "Open temp failed ($tmp_path): $!";

            print {$fh} $json_text
                or die "Write temp failed ($tmp_path): $!";

            close $fh
                or die "Close temp failed ($tmp_path): $!";

            # Optional: keep last-good backup (best-effort)
            if (-f $filepath) {
                # If a prior .bak exists, rotate it away to avoid rename failure
                unlink $bak_path if -f $bak_path;
                rename($filepath, $bak_path)
                    or die "Backup rename failed ($filepath -> $bak_path): $!";
            }

            # Atomic replace
            rename($tmp_path, $filepath)
                or die "Atomic rename failed ($tmp_path -> $filepath): $!";

            1;
        } or do {
            my $err = $@ || 'unknown error';

            # Cleanup temp file if it exists
            unlink $tmp_path if -f $tmp_path;

            # If we created/rotated a backup but failed before final rename,
            # try to restore the backup best-effort (avoid leaving file missing)
            if (!-f $filepath && -f $bak_path) {
                rename($bak_path, $filepath);  # best-effort; do not die here
            }

            warn "Error writing partition $filename: $err";
            $write_errors++;
        };
    }

    # 5. Legacy Cleanup (only if ALL partitions succeeded)
    # If we successfully wrote partitions, we ensure no stale monolith exists
    # to confuse readers, though _migrate should have handled this.
    my $legacy_file = File::Spec->catfile($system_cache_dir, $UNIFIED_HISTORY_FILE);
    if (-f $legacy_file && !$write_errors) {
        rename($legacy_file, $legacy_file . ".migrated");
    }

    close $lock_fh;

    if ($write_errors) {
        die "FATAL: Errors occurred writing history partitions. Existing history was preserved. Check logs.";
    }

    return 1;
}

# ==============================================================================
# HISTORY BASED BASELINING — PHASE 1: Date-Set Merge Primitives
# ==============================================================================
# These foundational functions implement correct, date-set-based update
# detection and first-write-wins merge semantics for DailyProfileSeries
# data stored in partitioned history months.
#
# Design reference: nFit - History Based Baselining Implementation Plan v1.3
# Constraint: No existing code is modified — these are pure additive helpers.
# ==============================================================================

# ==============================================================================
# SUBROUTINE: _extract_date_set_from_series
# PURPOSE:    Extracts the set of all unique dates present across every VM and
#             profile in a DailyProfileSeries structure.  The returned hashref
#             is keyed on 'YYYY-MM-DD' strings for O(1) membership testing.
#
# ARGUMENTS:
#   1. $daily_series_href (hashref): DailyProfileSeries structure:
#      { VM => { Profile => { 'YYYY-MM-DD' => value, ... }, ... }, ... }
#
# RETURNS:
#   - Hashref: { 'YYYY-MM-DD' => 1, ... }
#   - Empty hashref if input is undef, empty, or malformed.
# ==============================================================================
sub _extract_date_set_from_series {
    my ($daily_series_href) = @_;

    my %date_set;
    return \%date_set unless ($daily_series_href && ref($daily_series_href) eq 'HASH');

    foreach my $vm (keys %$daily_series_href) {
        unless (ref($daily_series_href->{$vm}) eq 'HASH') {
            warn "  [WARN] _extract_date_set_from_series: VM '$vm' value is not a hashref, skipping\n";
            next;
        }
        foreach my $profile (keys %{$daily_series_href->{$vm}}) {
            unless (ref($daily_series_href->{$vm}{$profile}) eq 'HASH') {
                warn "  [WARN] _extract_date_set_from_series: $vm/$profile value is not a hashref, skipping\n";
                next;
            }
            foreach my $date (keys %{$daily_series_href->{$vm}{$profile}}) {
                # Lightweight format guard — accept only YYYY-MM-DD
                unless ($date =~ /^\d{4}-\d{2}-\d{2}$/) {
                    warn "  [WARN] _extract_date_set_from_series: malformed date '$date' in $vm/$profile, skipping\n";
                    next;
                }
                $date_set{$date} = 1;
            }
        }
    }

    return \%date_set;
}

# ==============================================================================
# SUBROUTINE: _needs_history_update
# PURPOSE:    Determines whether a month's DailyProfileSeries in history needs
#             updating based on the date set present in the current L1 cache.
#
#             This replaces the legacy count-based comparison
#             ($days_in_period > $stored_days) which fails on partial extensions.
#             Date-set comparison correctly detects:
#               - Forward extension  (history has Jan 01-25, L1 adds 26-31)
#               - Backward extension (history has Jan 10-31, L1 adds 01-09)
#               - Internal gap fill  (history missing Jan 12-14, L1 fills them)
#
# ARGUMENTS:
#   1. $existing_date_set_href (hashref | undef): Dates currently in history
#      for this month.  Keys are 'YYYY-MM-DD'.  undef or {} = new month.
#   2. $l1_date_set_href (hashref | undef): Dates present in L1 for this month.
#
# RETURNS:
#   Hashref:
#   {
#       needs_update => 0 | 1,
#       reason       => 'new_month' | 'has_new_dates' | 'no_change' | 'empty_l1',
#       new_dates    => [ ... ],   # Dates in L1 absent from history (sorted)
#   }
# ==============================================================================
sub _needs_history_update {
    my ($existing_date_set_href, $l1_date_set_href) = @_;

    # Guard: empty L1 means nothing to contribute
    unless ($l1_date_set_href && ref($l1_date_set_href) eq 'HASH' && %$l1_date_set_href) {
        return { needs_update => 0, reason => 'empty_l1', new_dates => [] };
    }

    # New month — no existing history for this month
    if (!$existing_date_set_href || ref($existing_date_set_href) ne 'HASH' || !%$existing_date_set_href) {
        return {
            needs_update => 1,
            reason       => 'new_month',
            new_dates    => [ sort keys %$l1_date_set_href ],
        };
    }

    # Core logic: find dates in L1 that are NOT yet in existing history
    my @new_dates;
    foreach my $date (keys %$l1_date_set_href) {
        push @new_dates, $date unless exists $existing_date_set_href->{$date};
    }

    if (@new_dates) {
        return {
            needs_update => 1,
            reason       => 'has_new_dates',
            new_dates    => [ sort @new_dates ],
        };
    }

    return { needs_update => 0, reason => 'no_change', new_dates => [] };
}

# ==============================================================================
# SUBROUTINE: _merge_daily_profile_series
# PURPOSE:    Merges a DailyProfileSeries from L1 into an existing history
#             series using FIRST-WRITE-WINS semantics.  Existing date values
#             are never overwritten — only genuinely new dates are added.
#
#             This provides a predictable, auditable merge contract:
#               - Once a date is committed to history, its value is immutable.
#               - Re-runs of --update-history are idempotent for existing dates.
#
# ARGUMENTS:
#   1. $existing_series_href (hashref): Current DailyProfileSeries from history.
#      May be undef or {} for a brand-new month.
#   2. $l1_series_href (hashref): DailyProfileSeries from the current L1 run.
#
# RETURNS:
#   Hashref:
#   {
#       merged_series => { VM => { Profile => { date => val, ... } } },
#       dates_added   => [ ... ],   # New dates written (sorted)
#       dates_skipped => [ ... ],   # Existing dates preserved (sorted)
#       meta => {
#           first_date    => 'YYYY-MM-DD',
#           last_date     => 'YYYY-MM-DD',
#           ProcessedDays => N,      # Unique date count across all VMs/profiles
#       },
#   }
# ==============================================================================
sub _merge_daily_profile_series {
    my ($existing_series_href, $l1_series_href) = @_;

    # Initialise tracking
    my %dates_added_set;
    my %dates_skipped_set;

    # Build merged structure — copy-on-write per VM/profile for memory efficiency
    my %merged;
    if ($existing_series_href && ref($existing_series_href) eq 'HASH') {
        foreach my $vm (keys %$existing_series_href) {
            next unless ref($existing_series_href->{$vm}) eq 'HASH';
            foreach my $profile (keys %{$existing_series_href->{$vm}}) {
                next unless ref($existing_series_href->{$vm}{$profile}) eq 'HASH';
                # Shallow copy at date-value level (values are scalars)
                $merged{$vm}{$profile} = { %{$existing_series_href->{$vm}{$profile}} };
            }
        }
    }

    # Merge L1 data with first-write-wins
    if ($l1_series_href && ref($l1_series_href) eq 'HASH') {
        foreach my $vm (keys %$l1_series_href) {
            next unless ref($l1_series_href->{$vm}) eq 'HASH';
            foreach my $profile (keys %{$l1_series_href->{$vm}}) {
                next unless ref($l1_series_href->{$vm}{$profile}) eq 'HASH';

                # Ensure target profile exists in merged structure
                $merged{$vm}{$profile} //= {};

                foreach my $date (keys %{$l1_series_href->{$vm}{$profile}}) {
                    # Lightweight format guard
                    next unless $date =~ /^\d{4}-\d{2}-\d{2}$/;

                    if (exists $merged{$vm}{$profile}{$date}) {
                        # First-write-wins: existing value preserved
                        $dates_skipped_set{$date} = 1;
                    } else {
                        # Genuinely new date — add it
                        $merged{$vm}{$profile}{$date} = $l1_series_href->{$vm}{$profile}{$date};
                        $dates_added_set{$date} = 1;
                    }
                }
            }
        }
    }

    # Compute merged metadata from the complete date universe
    my $merged_date_set = _extract_date_set_from_series(\%merged);
    my @all_dates = sort keys %$merged_date_set;

    my $first_date    = $all_dates[0]  // undef;
    my $last_date     = $all_dates[-1] // undef;
    my $processed_days = scalar @all_dates;

    return {
        merged_series => \%merged,
        dates_added   => [ sort keys %dates_added_set  ],
        dates_skipped => [ sort keys %dates_skipped_set ],
        meta => {
            first_date    => $first_date,
            last_date     => $last_date,
            ProcessedDays => $processed_days,
        },
    };
}

# ==============================================================================
# SUBROUTINE: _update_month_metadata
# PURPOSE:    Builds or updates the _meta block for a history month after a
#             merge operation.  The content_version is incremented ONLY when
#             new dates were actually added, preventing spurious fingerprint
#             invalidation in downstream staleness gates.
#
# ARGUMENTS:
#   1. $existing_meta_href (hashref | undef): Current _meta block from history.
#      undef indicates a brand-new month with no prior metadata.
#   2. $merge_result_href (hashref): Output from _merge_daily_profile_series,
#      containing 'dates_added' and 'meta' sub-keys.
#
# RETURNS:
#   Hashref — the new _meta block:
#   {
#       ProcessedDays   => N,
#       first_date      => 'YYYY-MM-DD',
#       last_date       => 'YYYY-MM-DD',
#       content_version => M,
#       last_modified   => 'YYYY-MM-DDTHH:MM:SSZ',
#   }
# ==============================================================================
sub _update_month_metadata {
    my ($existing_meta_href, $merge_result_href) = @_;

    # Determine content_version
    my $content_version;
    my $dates_added = $merge_result_href->{dates_added} // [];

    if (!$existing_meta_href || ref($existing_meta_href) ne 'HASH') {
        # Brand-new month — initial version
        $content_version = 1;
    } elsif (scalar @$dates_added > 0) {
        # Existing month with new content — increment
        $content_version = ($existing_meta_href->{content_version} // 0) + 1;
    } else {
        # No change — preserve existing version
        $content_version = $existing_meta_href->{content_version} // 1;
    }

    # Extract merged metadata
    my $merge_meta = $merge_result_href->{meta} // {};

    return {
        ProcessedDays   => $merge_meta->{ProcessedDays} // 0,
        first_date      => $merge_meta->{first_date},
        last_date       => $merge_meta->{last_date},
        content_version => $content_version,
        last_modified   => gmtime()->strftime('%Y-%m-%dT%H:%M:%SZ'),
    };
}

# ==============================================================================
# HISTORY BASED BASELINING — PHASE 2: Bounded Month Loader Primitives
# ==============================================================================
# These functions implement safe, bounded I/O for loading only the history
# months that intersect the canonical baseline window.  They replace the
# unbounded read_unified_history() call path for baseline computation.
#
# Design reference: nFit - History Based Baselining Implementation Plan v1.3
# ==============================================================================

# ==============================================================================
# SUBROUTINE: _get_month_boundaries
# PURPOSE:    Returns correct first and last calendar day of a month using
#             proper calendar arithmetic.  This replaces the fragile string
#             approximation pattern ("${month_key}-31") which silently produces
#             invalid dates for months with fewer than 31 days.
#
# ARGUMENTS:
#   1. $month_key (string): 'YYYY-MM' format month identifier.
#
# RETURNS:
#   List of two Time::Piece objects: ($month_start, $month_end)
#   - $month_start: first day of the month (e.g., 2026-02-01)
#   - $month_end:   last day of the month  (e.g., 2026-02-28 or 2026-02-29)
#
#   Returns empty list if $month_key is malformed.
# ==============================================================================
sub _get_month_boundaries {
    my ($month_key) = @_;

    unless ($month_key && $month_key =~ /^\d{4}-\d{2}$/) {
        warn "  [WARN] _get_month_boundaries: malformed month_key '$month_key'\n";
        return ();
    }

    my $month_start = eval { Time::Piece->strptime("${month_key}-01", '%Y-%m-%d') };
    if ($@ || !$month_start) {
        warn "  [WARN] _get_month_boundaries: failed to parse '${month_key}-01': $@\n";
        return ();
    }

    # Last day = first day of next month minus one day
    my $month_end = $month_start->add_months(1) - ONE_DAY;

    return ($month_start, $month_end);
}

# ==============================================================================
# SUBROUTINE: _get_history_month_index
# PURPOSE:    Lists available history month files without loading their content.
#             Performs a lightweight directory scan of the partitioned history
#             directory and returns month keys for O(1) lookup.
#
# ARGUMENTS:
#   1. $system_cache_dir (string): Path to the system's staging directory.
#
# RETURNS:
#   Hashref: { 'YYYY-MM' => 1, ... }
#   Empty hashref if the history directory doesn't exist or is empty.
# ==============================================================================
sub _get_history_month_index {
    my ($system_cache_dir) = @_;

    my %index;
    my $partition_dir = File::Spec->catfile($system_cache_dir, '.nfit.history');

    return \%index unless (-d $partition_dir);

    opendir(my $dh, $partition_dir) or do {
        warn "  [WARN] _get_history_month_index: cannot open '$partition_dir': $!\n";
        return \%index;
    };

    while (my $entry = readdir($dh)) {
        if ($entry =~ /^nfit\.hist\.(\d{4}-\d{2})\.json$/) {
            $index{$1} = 1;
        }
    }
    closedir($dh);

    return \%index;
}

# ==============================================================================
# SUBROUTINE: _determine_required_months
# PURPOSE:    Given a canonical window (end_date, lookback_days) and an index
#             of available history months, determines which months intersect
#             the window and therefore need to be loaded.
#
#             Uses correct calendar-based overlap testing:
#               month overlaps window ⟺ month_end ≥ window_start
#                                    AND month_start ≤ window_end
#
# ARGUMENTS:
#   1. $end_date_obj (Time::Piece): Anchor date — right edge of window.
#   2. $lookback_days (integer): Canonical window size in days (e.g. 90).
#   3. $history_index_href (hashref): Month keys available in history
#      (output of _get_history_month_index).
#
# RETURNS:
#   Arrayref of month keys in chronological order, e.g.:
#   ['2025-12', '2026-01', '2026-02', '2026-03']
#
#   Empty arrayref if no months overlap the window.
# ==============================================================================
sub _determine_required_months {
    my ($end_date_obj, $lookback_days, $history_index_href) = @_;

    return [] unless ($end_date_obj && $lookback_days && $lookback_days > 0
                      && $history_index_href && ref($history_index_href) eq 'HASH'
                      && %$history_index_href);

    # Calculate window boundaries (inclusive)
    # N days ending on end_date → start = end - (N - 1) days
    my $window_start = $end_date_obj - (($lookback_days - 1) * ONE_DAY);
    my $window_end   = $end_date_obj;

    my @required;
    foreach my $month_key (keys %$history_index_href) {
        my ($m_start, $m_end) = _get_month_boundaries($month_key);
        next unless ($m_start && $m_end);

        # Standard interval overlap test
        next if $m_end   < $window_start;
        next if $m_start > $window_end;

        push @required, $month_key;
    }

    return [ sort @required ];
}

# ==============================================================================
# SUBROUTINE: _read_bounded_history_months
# PURPOSE:    Reads only the specified month partition files from history,
#             avoiding the unbounded read_unified_history() call path.
#
#             This is the I/O primitive for baseline computation.  It reads
#             exactly the months requested (as determined by
#             _determine_required_months), never the full history directory.
#
#             The JSON envelope convention is identical to read_unified_history:
#             each partition file contains { "YYYY-MM": { ... } } and the
#             inner payload is extracted and returned.
#
# ARGUMENTS:
#   1. $system_cache_dir (string): Path to the system's staging directory.
#   2. $month_keys_aref (arrayref): Month keys to load, e.g. ['2025-04','2025-05'].
#
# RETURNS:
#   Hashref with four fields:
#   {
#       months  => { 'YYYY-MM' => { ... }, ... },   # Successfully decoded payloads
#       loaded  => ['YYYY-MM', ...],                 # Month keys loaded (sorted)
#       missing => ['YYYY-MM', ...],                 # Requested but file absent (sorted)
#       errors  => { 'YYYY-MM' => 'reason', ... },   # File found but decode failed
#   }
# ==============================================================================
sub _read_bounded_history_months {
    my ($system_cache_dir, $month_keys_aref) = @_;

    my %result = (
        months  => {},
        loaded  => [],
        missing => [],
        errors  => {},
    );

    return \%result unless ($system_cache_dir && $month_keys_aref
                            && ref($month_keys_aref) eq 'ARRAY'
                            && @$month_keys_aref);

    my $partition_dir = File::Spec->catfile($system_cache_dir, '.nfit.history');

    # If the partition directory doesn't exist at all, every month is missing
    unless (-d $partition_dir) {
        $result{missing} = [ sort @$month_keys_aref ];
        return \%result;
    }

    # Use the same decoder settings as read_unified_history for consistency
    my $json_decoder = JSON->new->allow_nonref;

    foreach my $month_key (sort @$month_keys_aref) {
        # Guard: validate month_key format before constructing file path
        unless ($month_key =~ /^\d{4}-\d{2}$/) {
            $result{errors}{$month_key} = "Invalid month key format: '$month_key'";
            next;
        }

        my $file = "nfit.hist.${month_key}.json";
        my $path = File::Spec->catfile($partition_dir, $file);

        # Check existence
        unless (-f $path) {
            push @{$result{missing}}, $month_key;
            next;
        }

        # Check non-empty (same guard as read_unified_history's 'next unless $json_text')
        unless (-s $path) {
            $result{errors}{$month_key} = "File exists but is empty: $file";
            next;
        }

        # Read file — same encoding as read_unified_history
        my $json_text = do {
            open my $fh, '<:encoding(utf8)', $path or do {
                $result{errors}{$month_key} = "Cannot open $file: $!";
                next;
            };
            local $/;
            <$fh>;
        };

        unless ($json_text && $json_text =~ /\S/) {
            $result{errors}{$month_key} = "File is blank or whitespace-only: $file";
            next;
        }

        # Decode JSON — envelope: { "YYYY-MM": { ... } }
        my $chunk = eval { $json_decoder->decode($json_text) };
        if ($@ || !$chunk || ref($chunk) ne 'HASH') {
            my $err = $@ || 'Decoded value is not a hashref';
            $result{errors}{$month_key} = "JSON decode failed for $file: $err";
            next;
        }

        # Unwrap envelope — extract the month payload
        # Defensive: use the envelope key that matches the requested month_key,
        # falling back to the first key if different (with a warning).
        my $payload;
        if (exists $chunk->{$month_key}) {
            $payload = $chunk->{$month_key};
        }
        else {
            my ($found_key) = keys %$chunk;
            if ($found_key) {
                warn "  [WARN] _read_bounded_history_months: envelope key '$found_key' "
                   . "doesn't match requested '$month_key' in $file — using found key\n";
                $payload = $chunk->{$found_key};
            }
            else {
                $result{errors}{$month_key} = "Envelope is empty hash in $file";
                next;
            }
        }

        $result{months}{$month_key} = $payload;
        push @{$result{loaded}}, $month_key;
    }

    return \%result;
}

# ==============================================================================
# HISTORY BASED BASELINING — PHASE 3: Daily Series Stitcher and Exclusions
# ==============================================================================
# These functions stitch DailyProfileSeries from multiple history months into
# a unified view for baseline computation, and apply exclusion filters at read
# time to preserve history purity.
#
# Design reference: nFit - History Based Baselining Implementation Plan v1.3
# ==============================================================================

# ==============================================================================
# SUBROUTINE: _expand_peak_periods_to_dates
# PURPOSE:    Expands peak period time ranges into a flat hash of date strings
#             for O(1) exclusion lookup.  Uses the existing _expand_date_range
#             helper for correctness and consistency (including the 366-day
#             per-range safety limit).
#
# ARGUMENTS:
#   1. $peak_periods_aref (arrayref): Output from find_all_historical_periods.
#      Each element is [Time::Piece start, Time::Piece end].
#
# RETURNS:
#   Hashref: { 'YYYY-MM-DD' => 1, ... }
#   Empty hashref if input is undef/empty or all ranges invalid.
# ==============================================================================
sub _expand_peak_periods_to_dates {
    my ($peak_periods_aref) = @_;

    my %date_set;
    return \%date_set unless ($peak_periods_aref && ref($peak_periods_aref) eq 'ARRAY'
                              && @$peak_periods_aref);

    foreach my $pair (@$peak_periods_aref) {
        next unless (ref($pair) eq 'ARRAY' && scalar(@$pair) >= 2);

        my ($start_obj, $end_obj) = @$pair;

        # Accept both Time::Piece objects and plain 'YYYY-MM-DD' strings
        my $start_str = ref($start_obj) ? $start_obj->strftime('%Y-%m-%d') : $start_obj;
        my $end_str   = ref($end_obj)   ? $end_obj->strftime('%Y-%m-%d')   : $end_obj;

        # Delegate to existing helper — it handles swap, safety limit, and error logging
        my @expanded = _expand_date_range($start_str, $end_str);
        $date_set{$_} = 1 for @expanded;
    }

    return \%date_set;
}

# ==============================================================================
# SUBROUTINE: _stitch_daily_series_bounded
# PURPOSE:    Stitches DailyProfileSeries from multiple partitioned history
#             months within a bounded lookback window.  This is the read-side
#             primitive for history-based baselining.
#
#             Orchestrates Phase 2 functions:
#               _get_history_month_index  → discover available months
#               _determine_required_months → select months in window
#               _read_bounded_history_months → load only those months
#
#             Then extracts, filters, and merges DailyProfileSeries from each
#             loaded month into a single unified structure.
#
# ARGUMENTS:
#   1. $args_href (hashref):
#      {
#          system_cache_dir => '/path/to/staging',       # Required
#          end_date_obj     => Time::Piece object,       # Required: right edge of window
#          lookback_days    => 90,                        # Default 90, hard cap 120
#          profiles_needed  => ['G3-95W15', ...],         # Optional: skip other profiles
#          vms_needed       => ['VM01', ...],             # Optional: skip other VMs
#      }
#
# RETURNS:
#   Hashref:
#   {
#       series           => { $vm => { $profile => { 'YYYY-MM-DD' => $value } } },
#       months_read      => ['YYYY-MM', ...],
#       months_missing   => ['YYYY-MM', ...],
#       span_start       => 'YYYY-MM-DD',       # Earliest actual date in series
#       span_end         => 'YYYY-MM-DD',        # Latest actual date in series
#       content_versions => { 'YYYY-MM' => N },  # For fingerprinting
#   }
# ==============================================================================
sub _stitch_daily_series_bounded {
    my ($args_href) = @_;

    my %result = (
        series           => {},
        months_read      => [],
        months_missing   => [],
        span_start       => undef,
        span_end         => undef,
        content_versions => {},
    );

    return \%result unless ($args_href && ref($args_href) eq 'HASH');

    my $system_cache_dir = $args_href->{system_cache_dir};
    my $end_date_obj     = $args_href->{end_date_obj};

    return \%result unless ($system_cache_dir && $end_date_obj);

    # --- Hard cap enforcement ---
    my $lookback_days = $args_href->{lookback_days} // 90;
    my $HARD_CAP = 120;
    if ($lookback_days > $HARD_CAP) {
        print STDERR "  [WARN] _stitch_daily_series_bounded: lookback_days $lookback_days "
                   . "exceeds hard cap $HARD_CAP — clamping\n";
        $lookback_days = $HARD_CAP;
    }
    $lookback_days = 1 if $lookback_days < 1;

    # --- Build optional filter lookup hashes ---
    my %vm_filter;
    if ($args_href->{vms_needed} && ref($args_href->{vms_needed}) eq 'ARRAY'
        && @{$args_href->{vms_needed}}) {
        %vm_filter = map { $_ => 1 } @{$args_href->{vms_needed}};
    }

    my %profile_filter;
    if ($args_href->{profiles_needed} && ref($args_href->{profiles_needed}) eq 'ARRAY'
        && @{$args_href->{profiles_needed}}) {
        %profile_filter = map { $_ => 1 } @{$args_href->{profiles_needed}};
    }

    # --- Phase 2 orchestration: discover, select, load ---
    my $index = _get_history_month_index($system_cache_dir);
    my $required = _determine_required_months($end_date_obj, $lookback_days, $index);

    if (!@$required) {
        return \%result;
    }

    my $load_result = _read_bounded_history_months($system_cache_dir, $required);

    $result{months_read}    = $load_result->{loaded};
    $result{months_missing} = $load_result->{missing};

    # --- Calculate window boundaries for date clipping ---
    # Window is inclusive: N days ending on end_date
    my $window_start_str = ($end_date_obj - (($lookback_days - 1) * ONE_DAY))->strftime('%Y-%m-%d');
    my $window_end_str   = $end_date_obj->strftime('%Y-%m-%d');

    # --- Stitch loaded months ---
    my $global_min_date;
    my $global_max_date;

    foreach my $month_key (@{$load_result->{loaded}}) {
        my $month_data = $load_result->{months}{$month_key};
        next unless (ref($month_data) eq 'HASH');

        # Record content_version (from Phase 1 _meta if available, else 0 for legacy)
        $result{content_versions}{$month_key} =
            (exists $month_data->{_meta} && ref($month_data->{_meta}) eq 'HASH')
                ? ($month_data->{_meta}{content_version} // 0)
                : 0;

        # Navigate to MonthlyWorkloadAnalysis
        my $mwa = $month_data->{MonthlyWorkloadAnalysis};
        next unless (ref($mwa) eq 'HASH');

        foreach my $vm (keys %$mwa) {
            # VM filter: skip if filter is active and VM not in list
            next if (%vm_filter && !$vm_filter{$vm});

            my $dps = $mwa->{$vm}{DailyProfileSeries};
            next unless (ref($dps) eq 'HASH');

            foreach my $profile (keys %$dps) {
                # Profile filter: skip if filter is active and profile not in list
                next if (%profile_filter && !$profile_filter{$profile});

                my $date_values = $dps->{$profile};
                next unless (ref($date_values) eq 'HASH');

                foreach my $date (keys %$date_values) {
                    # Date format guard
                    next unless ($date =~ /^\d{4}-\d{2}-\d{2}$/);

                    # Window clipping (string comparison is correct for YYYY-MM-DD)
                    next if ($date lt $window_start_str);
                    next if ($date gt $window_end_str);

                    # Add to stitched series
                    $result{series}{$vm}{$profile}{$date} = $date_values->{$date};

                    # Track actual data span
                    if (!defined $global_min_date || $date lt $global_min_date) {
                        $global_min_date = $date;
                    }
                    if (!defined $global_max_date || $date gt $global_max_date) {
                        $global_max_date = $date;
                    }
                }
            }
        }
    }

    $result{span_start} = $global_min_date;
    $result{span_end}   = $global_max_date;

    return \%result;
}

# ==============================================================================
# SUBROUTINE: _apply_exclusions_to_stitched_series
# PURPOSE:    Applies all exclusion filters to a stitched DailyProfileSeries
#             at READ time, preserving history purity.  The processing order
#             is strictly defined for deterministic, auditable behaviour.
#
#             Processing order (mandatory):
#               1. VM inclusions  — keep only listed VMs
#               2. VM exclusions  — remove listed VMs
#               3. Global date exclusions — remove dates from all VMs/profiles
#               4. VM-specific date exclusions — remove dates per-VM
#               5. Peak period exclusions — remove dates within peak ranges
#
#             Each date removal is attributed to exactly one category (the
#             first matching rule wins) for unambiguous audit stats.
#
# ARGUMENTS:
#   1. $args_href (hashref):
#      {
#          stitched_series => { VM => { Profile => { date => val } } },
#          vm_scope_filter => {                    # From _parse_vm_scope_filter or undef
#              include => { VM => 1, ... },        # or undef
#              exclude => { VM => 1, ... },        # or undef
#          },
#          date_exclusions => {                    # From _parse_exclusion_dates or undef
#              global      => ['YYYY-MM-DD', ...],
#              vm_specific => { VM => ['YYYY-MM-DD', ...] },
#          },
#          peak_periods    => [                    # From find_all_historical_periods or undef
#              [Time::Piece start, Time::Piece end],
#              ...
#          ],
#      }
#
# RETURNS:
#   Hashref:
#   {
#       filtered_series => { ... },   # Same ref as input (mutated in place)
#       exclusion_stats => {
#           vms_included      => N,   # VMs remaining after include filter (0 = no filter)
#           vms_excluded      => M,   # VMs removed by include + exclude steps
#           dates_excluded    => {
#               global        => K,   # Date-point removals attributed to global rule
#               vm_specific   => L,   # Date-point removals attributed to VM-specific rule
#               peak_periods  => P,   # Date-point removals attributed to peak rule
#           },
#       },
#   }
#
# NOTE: This function MUTATES the input stitched_series for memory efficiency
#       (consistent with existing _apply_vm_scope_filter pattern).  The caller
#       should not reference the input series after this call.
# ==============================================================================
sub _apply_exclusions_to_stitched_series {
    my ($args_href) = @_;

    my %stats = (
        vms_included   => 0,
        vms_excluded   => 0,
        dates_excluded => {
            global       => 0,
            vm_specific  => 0,
            peak_periods => 0,
        },
    );

    my $series = ($args_href && ref($args_href) eq 'HASH')
               ? $args_href->{stitched_series}
               : undef;

    # If no series or empty, return immediately
    unless ($series && ref($series) eq 'HASH' && %$series) {
        return { filtered_series => ($series // {}), exclusion_stats => \%stats };
    }

    my $vm_scope    = $args_href->{vm_scope_filter};
    my $date_excl   = $args_href->{date_exclusions};
    my $peak_aref   = $args_href->{peak_periods};

    # ---------------------------------------------------------------
    # STEP 1: VM Inclusions — keep only listed VMs
    # ---------------------------------------------------------------
    my $include_filter = ($vm_scope && ref($vm_scope) eq 'HASH')
                       ? $vm_scope->{include} : undef;

    if ($include_filter && ref($include_filter) eq 'HASH' && %$include_filter) {
        my @to_remove;
        foreach my $vm (keys %$series) {
            unless ($include_filter->{$vm}) {
                push @to_remove, $vm;
            }
        }
        delete $series->{$_} for @to_remove;
        $stats{vms_excluded} += scalar @to_remove;
        $stats{vms_included} = scalar keys %$series;
    }

    # ---------------------------------------------------------------
    # STEP 2: VM Exclusions — remove listed VMs
    # ---------------------------------------------------------------
    my $exclude_filter = ($vm_scope && ref($vm_scope) eq 'HASH')
                       ? $vm_scope->{exclude} : undef;

    if ($exclude_filter && ref($exclude_filter) eq 'HASH' && %$exclude_filter) {
        my $removed = 0;
        foreach my $vm (keys %$series) {
            if ($exclude_filter->{$vm}) {
                delete $series->{$vm};
                $removed++;
            }
        }
        $stats{vms_excluded} += $removed;
    }

    # Early exit if no VMs remain after VM filtering
    unless (%$series) {
        return { filtered_series => $series, exclusion_stats => \%stats };
    }

    # ---------------------------------------------------------------
    # STEP 3-5 preparation: build lookup hashes for date exclusions
    # ---------------------------------------------------------------

    # Global date exclusions → hash for O(1) lookup
    my %global_dates;
    if ($date_excl && ref($date_excl) eq 'HASH'
        && $date_excl->{global} && ref($date_excl->{global}) eq 'ARRAY') {
        %global_dates = map { $_ => 1 } @{$date_excl->{global}};
    }

    # VM-specific date exclusions → hash of hashes
    my %vm_specific_dates;
    if ($date_excl && ref($date_excl) eq 'HASH'
        && $date_excl->{vm_specific} && ref($date_excl->{vm_specific}) eq 'HASH') {
        foreach my $vm (keys %{$date_excl->{vm_specific}}) {
            my $dates_aref = $date_excl->{vm_specific}{$vm};
            if (ref($dates_aref) eq 'ARRAY') {
                $vm_specific_dates{$vm} = { map { $_ => 1 } @$dates_aref };
            }
        }
    }

    # Peak period dates → flat hash (delegates to _expand_peak_periods_to_dates)
    my $peak_dates_href = _expand_peak_periods_to_dates($peak_aref);

    # Short-circuit: if no date exclusions of any kind, return early
    unless (%global_dates || %vm_specific_dates || %$peak_dates_href) {
        return { filtered_series => $series, exclusion_stats => \%stats };
    }

    # ---------------------------------------------------------------
    # STEPS 3-5: Apply date exclusions with elsif priority
    #   global > vm_specific > peak_periods
    # ---------------------------------------------------------------
    foreach my $vm (keys %$series) {
        my $vm_profiles = $series->{$vm};
        next unless (ref($vm_profiles) eq 'HASH');

        # Pre-fetch VM-specific lookup for this VM (may be undef)
        my $this_vm_excl = $vm_specific_dates{$vm};

        foreach my $profile (keys %$vm_profiles) {
            my $date_values = $vm_profiles->{$profile};
            next unless (ref($date_values) eq 'HASH');

            my @dates_to_delete;

            foreach my $date (keys %$date_values) {
                # STEP 3: Global exclusion (highest priority)
                if ($global_dates{$date}) {
                    push @dates_to_delete, $date;
                    $stats{dates_excluded}{global}++;
                }
                # STEP 4: VM-specific exclusion
                elsif ($this_vm_excl && $this_vm_excl->{$date}) {
                    push @dates_to_delete, $date;
                    $stats{dates_excluded}{vm_specific}++;
                }
                # STEP 5: Peak period exclusion
                elsif ($peak_dates_href->{$date}) {
                    push @dates_to_delete, $date;
                    $stats{dates_excluded}{peak_periods}++;
                }
            }

            delete $date_values->{$_} for @dates_to_delete;
        }
    }

    return { filtered_series => $series, exclusion_stats => \%stats };
}

# ==============================================================================
# HISTORY BASED BASELINING — PHASE 4: Baseline Computation and Fingerprinting
# ==============================================================================
# These functions compute P95 baselines from stitched/filtered series, track
# per-VM/profile coverage, and produce deterministic fingerprints for staleness
# detection.
#
# Design reference: nFit - History Based Baselining Implementation Plan v1.3
# ==============================================================================

# ==============================================================================
# SUBROUTINE: _compute_profile_set_hash
# PURPOSE:    Deterministic hash of requested profile names for fingerprinting.
# ARGUMENTS:  $profiles_aref (arrayref of profile name strings)
# RETURNS:    MD5 hex digest string
# ==============================================================================
sub _compute_profile_set_hash {
    my ($profiles_aref) = @_;
    return '' unless ($profiles_aref && ref($profiles_aref) eq 'ARRAY' && @$profiles_aref);
    require Digest::MD5;
    return Digest::MD5::md5_hex(join('|', sort @$profiles_aref));
}

# ==============================================================================
# SUBROUTINE: _compute_vm_set_hash
# PURPOSE:    Deterministic hash of requested VM names for fingerprinting.
#             Returns literal 'ALL' when no VM filter is active.
# ARGUMENTS:  $vms_aref (arrayref of VM name strings, or undef for all)
# RETURNS:    MD5 hex digest string, or 'ALL'
# ==============================================================================
sub _compute_vm_set_hash {
    my ($vms_aref) = @_;
    return 'ALL' unless ($vms_aref && ref($vms_aref) eq 'ARRAY' && @$vms_aref);
    require Digest::MD5;
    return Digest::MD5::md5_hex(join('|', sort @$vms_aref));
}

# ==============================================================================
# SUBROUTINE: _compute_months_content_hash
# PURPOSE:    Deterministic hash of month keys and their content versions.
#             Changes when history is reprocessed (content_version increments).
# ARGUMENTS:  $content_versions_href ({ 'YYYY-MM' => version_int, ... })
# RETURNS:    MD5 hex digest string, or 'NONE' if empty
# ==============================================================================
sub _compute_months_content_hash {
    my ($content_versions_href) = @_;
    return 'NONE' unless ($content_versions_href && ref($content_versions_href) eq 'HASH'
                          && %$content_versions_href);
    my @parts;
    foreach my $month (sort keys %$content_versions_href) {
        push @parts, "$month:$content_versions_href->{$month}";
    }
    require Digest::MD5;
    return Digest::MD5::md5_hex(join('|', @parts));
}

# ==============================================================================
# SUBROUTINE: _compute_vm_scope_hash
# PURPOSE:    Deterministic hash of VM scope filter (include/exclude lists).
# ARGUMENTS:  $vm_scope_filter_href ({ include => { VM => 1 }, exclude => { VM => 1 } })
# RETURNS:    MD5 hex digest string, or 'NONE' if no filter active
# ==============================================================================
sub _compute_vm_scope_hash {
    my ($vm_scope_filter_href) = @_;
    return 'NONE' unless ($vm_scope_filter_href && ref($vm_scope_filter_href) eq 'HASH');

    my @parts;
    my $include = $vm_scope_filter_href->{include};
    if ($include && ref($include) eq 'HASH' && %$include) {
        push @parts, 'I:' . join(',', sort keys %$include);
    }
    my $exclude = $vm_scope_filter_href->{exclude};
    if ($exclude && ref($exclude) eq 'HASH' && %$exclude) {
        push @parts, 'E:' . join(',', sort keys %$exclude);
    }

    return 'NONE' unless @parts;
    require Digest::MD5;
    return Digest::MD5::md5_hex(join('|', @parts));
}

# ==============================================================================
# SUBROUTINE: _calculate_percentile_from_sorted
# PURPOSE:    Computes a percentile value from a pre-sorted array using linear
#             interpolation.  Identical algorithm to calculate_percentile in
#             nfit.pl — reproduced here for nfit-profile.pl self-containment.
#
# ARGUMENTS:
#   1. $sorted_aref (arrayref): Numerically sorted values (ascending)
#   2. $p (number): Percentile to compute (0-100)
#
# RETURNS:    Numeric value, or undef if input is empty
# ==============================================================================
sub _calculate_percentile_from_sorted {
    my ($sorted_aref, $p) = @_;

    return undef unless ($sorted_aref && ref($sorted_aref) eq 'ARRAY');
    my $n = scalar @$sorted_aref;
    return undef if $n == 0;
    return $sorted_aref->[0] if $n == 1;

    return $sorted_aref->[0]      if $p == 0;
    return $sorted_aref->[$n - 1] if $p == 100;

    my $rank = ($p / 100) * ($n - 1);
    my $k    = int($rank);
    my $d    = $rank - $k;

    if ($k >= $n - 1) {
        return $sorted_aref->[$n - 1];
    }

    return $sorted_aref->[$k] + $d * ($sorted_aref->[$k + 1] - $sorted_aref->[$k]);
}

# ==============================================================================
# SUBROUTINE: _calculate_baseline_from_stitched
# PURPOSE:    Calculates P95 baselines from a filtered stitched series with
#             per-VM/profile coverage assessment.  Coverage is computed against
#             canonical_days (the requested window), not actual available days.
#
# ARGUMENTS:
#   1. $args_href (hashref):
#      {
#          filtered_series    => { VM => { Profile => { date => val } } },
#          canonical_days     => 90,
#          coverage_threshold => 0.7,
#      }
#
# RETURNS:
#   Hashref:
#   {
#       baselines => {
#           $vm => { $profile => P95_value | undef },
#       },
#       coverage => {
#           $vm => {
#               $profile => {
#                   days_available  => N,
#                   pct_coverage    => 0.XX,
#                   status          => 'sufficient' | 'degraded' | 'no_data',
#               },
#           },
#       },
#   }
# ==============================================================================
sub _calculate_baseline_from_stitched {
    my ($args_href) = @_;

    my %baselines;
    my %coverage;

    return { baselines => \%baselines, coverage => \%coverage }
        unless ($args_href && ref($args_href) eq 'HASH');

    my $series             = $args_href->{filtered_series} // {};
    my $canonical_days     = $args_href->{canonical_days}     // 90;
    my $coverage_threshold = $args_href->{coverage_threshold} // 0.7;

    # Guard: canonical_days must be positive
    $canonical_days = 1 if $canonical_days < 1;

    foreach my $vm (keys %$series) {
        my $vm_profiles = $series->{$vm};
        next unless (ref($vm_profiles) eq 'HASH');

        foreach my $profile (keys %$vm_profiles) {
            my $date_values = $vm_profiles->{$profile};

            # Collect numeric values — skip undef/non-numeric
            my @values;
            if (ref($date_values) eq 'HASH') {
                foreach my $val (values %$date_values) {
                    push @values, $val if (defined $val && looks_like_number($val));
                }
            }

            my $days_available = scalar @values;
            my $pct_coverage   = $days_available / $canonical_days;

            if ($days_available == 0) {
                $baselines{$vm}{$profile} = undef;
                $coverage{$vm}{$profile}  = {
                    days_available => 0,
                    pct_coverage   => 0,
                    status         => 'no_data',
                };
                next;
            }

            # Sort once, compute P95
            my @sorted = sort { $a <=> $b } @values;
            my $p95    = sum0(@values) / scalar(@values);

            my $status = ($pct_coverage >= $coverage_threshold)
                       ? 'sufficient'
                       : 'degraded';

            $baselines{$vm}{$profile} = $p95;
            $coverage{$vm}{$profile}  = {
                days_available => $days_available,
                pct_coverage   => $pct_coverage,
                status         => $status,
            };
        }
    }

    return { baselines => \%baselines, coverage => \%coverage };
}

# ==============================================================================
# SUBROUTINE: _compute_baseline_fingerprint
# PURPOSE:    Computes a deterministic MD5 fingerprint from the meta block of a
#             history-based baseline result.  Any change to inputs that could
#             affect the baseline will produce a different fingerprint.
#
# ARGUMENTS:  $meta_href (hashref): The assembled meta block
# RETURNS:    MD5 hex digest string
#
# FINGERPRINT INPUTS (order-sensitive, pipe-delimited):
#   baseline_source | canonical_days | lookback_cap_days | required_days |
#   coverage_threshold | span_start | span_end | excluded_dates_hash |
#   vm_scope_hash | months_content_hash | profile_set_hash | vm_set_hash
#
# NOTE: actual_data_span and coverage_by_vm are NOT in the fingerprint —
#       they are audit/reporting fields that vary with data availability.
# ==============================================================================
sub _compute_baseline_fingerprint {
    my ($meta_href) = @_;
    return 'NONE' unless ($meta_href && ref($meta_href) eq 'HASH');

    require Digest::MD5;

    my @components = (
        $meta_href->{baseline_source},
        $meta_href->{canonical_days},
        $meta_href->{lookback_cap_days},
        $meta_href->{required_days},
        $meta_href->{coverage_threshold},
        $meta_href->{span_start},
        $meta_href->{span_end},
        $meta_href->{excluded_dates_hash},
        $meta_href->{vm_scope_hash},
        $meta_href->{months_content_hash},
        $meta_href->{profile_set_hash} // '',
        $meta_href->{vm_set_hash}      // '',
    );

    return Digest::MD5::md5_hex(join('|', map { $_ // '' } @components));
}

# ==============================================================================
# SUBROUTINE: _compute_exclusion_hash
# PURPOSE:    Computes a deterministic MD5 hash of the exclusion configuration
#             for use in baseline fingerprinting.  Unlike _compute_exclusion_
#             fingerprint (which returns 'NONE' for empty input), this always
#             returns a 32-char MD5 hex digest for consistent fingerprint
#             composition in _compute_baseline_fingerprint.
# ARGUMENTS:  $date_exclusions - hashref { global => [...], vm_specific => { VM => [...] } }
# RETURNS:    String - 32-character lowercase MD5 hex digest.
# PHASE:      History Based Baselining - Phase 4
# ==============================================================================
sub _compute_exclusion_hash {
    my ($date_exclusions) = @_;

    # Empty/undef exclusions still produce a deterministic hash
    return Digest::MD5::md5_hex('') unless ($date_exclusions
                                            && ref($date_exclusions) eq 'HASH');

    # Canonical representation: sorted for determinism
    my @parts;
    push @parts, "G:" . join(",", sort @{$date_exclusions->{global} // []});

    foreach my $vm (sort keys %{$date_exclusions->{vm_specific} // {}}) {
        push @parts, "$vm:" . join(",", sort @{$date_exclusions->{vm_specific}{$vm}});
    }

    return Digest::MD5::md5_hex(join("|", @parts));
}

# ==============================================================================
# SUBROUTINE: _get_baseline_from_history
# PURPOSE:    Primary entry point for history-based baseline calculation.
#             Orchestrates the full pipeline: stitch → exclusions → baseline
#             computation → canonical span → fingerprint assembly.
# ARGUMENTS:  $args_href - hashref:
#               system_cache_dir   => '/path/to/staging'
#               end_date_obj       => Time::Piece object
#               canonical_days     => 90          (default)
#               coverage_threshold => 0.7         (default)
#               vm_scope_filter    => { ... } | undef
#               date_exclusions    => { ... } | undef
#               peak_periods       => [ ... ] | undef
#               profiles           => ['G3-95W15', ...]
#               vms                => ['VM01', ...] | undef  (undef = all VMs)
# RETURNS:    Hashref { baselines => { ... }, meta => { ... } }
# THROWS:     die "No history exists..." if no history files found
# PHASE:      History Based Baselining - Phase 4
# ==============================================================================
sub _get_baseline_from_history {
    my ($args_href) = @_;

    my $system_cache_dir   = $args_href->{system_cache_dir};
    my $end_date_obj       = $args_href->{end_date_obj};
    my $canonical_days     = $args_href->{canonical_days}     // 90;
    my $coverage_threshold = $args_href->{coverage_threshold} // 0.7;
    my $required_days      = int($canonical_days * $coverage_threshold);

    # ------------------------------------------------------------------
    # Step 1: Stitch bounded history
    # ------------------------------------------------------------------
    my $stitch_result = _stitch_daily_series_bounded({
        system_cache_dir => $system_cache_dir,
        end_date_obj     => $end_date_obj,
        lookback_days    => $canonical_days,
        profiles_needed  => $args_href->{profiles},
        vms_needed       => $args_href->{vms},
    });

    # ------------------------------------------------------------------
    # Step 2: No history check
    # ------------------------------------------------------------------
    if (!$stitch_result->{months_read} || !@{$stitch_result->{months_read}}) {
        die "No history exists. Run `nfit-profile --update-history` first.\n";
    }

    # ------------------------------------------------------------------
    # Step 3: Apply exclusions
    # ------------------------------------------------------------------
    my $excl_result = _apply_exclusions_to_stitched_series({
        stitched_series => $stitch_result->{series},
        vm_scope_filter => $args_href->{vm_scope_filter},
        date_exclusions => $args_href->{date_exclusions},
        peak_periods    => $args_href->{peak_periods},
    });

    # ------------------------------------------------------------------
    # Step 4: Compute baselines with coverage assessment
    # ------------------------------------------------------------------
    my $baseline_result = _calculate_baseline_from_stitched({
        filtered_series    => $excl_result->{filtered_series},
        canonical_days     => $canonical_days,
        coverage_threshold => $coverage_threshold,
    });

    # ------------------------------------------------------------------
    # Step 5: Canonical span (deterministic from inputs, NOT actual data)
    #
    # INCLUSIVE window calculation (matches existing L1 convention):
    #   span_start = end_date - (N - 1) days
    #   span_end   = end_date
    # Example: 90 days ending Jan 31 → span_start = Nov 03
    #
    # Rationale: The fingerprint must be deterministic from inputs alone.
    # Using actual data range would cause divergence between
    # _calculate_expected_baseline_fingerprint (Phase 5, lightweight path)
    # and the real baseline path.  The actual data range is captured in
    # actual_data_span and coverage_by_vm for reporting/audit purposes.
    # ------------------------------------------------------------------
    my $canonical_span_start = ($end_date_obj - (($canonical_days - 1) * ONE_DAY))->strftime('%Y-%m-%d');
    my $canonical_span_end   = $end_date_obj->strftime('%Y-%m-%d');

    # ------------------------------------------------------------------
    # Step 6: Assemble meta with fingerprint components
    # ------------------------------------------------------------------
    my $meta = {
        baseline_source     => 'history',
        canonical_days      => $canonical_days,
        lookback_cap_days   => 120,
        required_days       => $required_days,
        coverage_threshold  => $coverage_threshold,
        span_start          => $canonical_span_start,
        span_end            => $canonical_span_end,
        actual_data_span    => {
            first_date => $stitch_result->{span_start},
            last_date  => $stitch_result->{span_end},
        },
        excluded_dates_hash  => _compute_exclusion_hash($args_href->{date_exclusions}),
        vm_scope_hash        => _compute_vm_scope_hash($args_href->{vm_scope_filter}),
        months_read          => $stitch_result->{months_read},
        months_content_hash  => _compute_months_content_hash($stitch_result->{content_versions}),
        profile_set_hash     => _compute_profile_set_hash($args_href->{profiles} // []),
        vm_set_hash          => _compute_vm_set_hash($args_href->{vms}),
        coverage_by_vm       => $baseline_result->{coverage},
        exclusion_stats      => $excl_result->{exclusion_stats},
    };

    # ------------------------------------------------------------------
    # Step 7: Compute combined fingerprint
    # ------------------------------------------------------------------
    $meta->{fingerprint} = _compute_baseline_fingerprint($meta);

    return {
        baselines => $baseline_result->{baselines},
        meta      => $meta,
    };
}

# ==============================================================================
# SUBROUTINE: _read_month_metadata_only
# PURPOSE:    Lightweight reader that extracts ONLY the _meta block from a
#             month partition JSON file, without loading the full series data.
#             Used by _calculate_expected_baseline_fingerprint for efficient
#             staleness detection.
# ARGUMENTS:
#   $system_cache_dir - Path to system cache directory
#   $month_key        - Month key (YYYY-MM)
# RETURNS:    Hashref of _meta block, or undef if not found/corrupt.
# PHASE:      History Based Baselining - Phase 5
# ==============================================================================
sub _read_month_metadata_only {
    my ($system_cache_dir, $month_key) = @_;
    return undef unless ($system_cache_dir && $month_key && $month_key =~ /^\d{4}-\d{2}$/);

    my $partition_dir = File::Spec->catfile($system_cache_dir, '.nfit.history');
    my $path = File::Spec->catfile($partition_dir, "nfit.hist.${month_key}.json");
    return undef unless (-f $path && -s $path);

    my $meta;
    eval {
        open my $fh, '<:encoding(utf8)', $path or die "Cannot open: $!";
        local $/;
        my $json_text = <$fh>;
        close $fh;

        my $data = JSON->new->allow_nonref->decode($json_text);
        return unless (ref($data) eq 'HASH');

        # Navigate to the month payload (keyed by month_key or first key)
        my $payload = exists $data->{$month_key}
                    ? $data->{$month_key}
                    : $data->{(keys %$data)[0]};

        if (ref($payload) eq 'HASH' && exists $payload->{_meta}
            && ref($payload->{_meta}) eq 'HASH') {
            $meta = $payload->{_meta};
        }
    };
    if ($@) {
        warn "  [WARN] _read_month_metadata_only: failed for $month_key: $@\n";
    }
    return $meta;
}

# ==============================================================================
# SUBROUTINE: _calculate_expected_baseline_fingerprint
# PURPOSE:    Computes what the baseline fingerprint WOULD be for given inputs,
#             without actually computing the full baseline.  Uses the same
#             shared hash helpers and fingerprint builder as the real baseline
#             path (_get_baseline_from_history) to guarantee non-divergence.
#
#             This enables efficient staleness detection: if the expected
#             fingerprint differs from the stored one, the baseline must be
#             recomputed.
# ARGUMENTS:  $args_href - hashref:
#               system_cache_dir   => '/path/to/staging'
#               end_date_obj       => Time::Piece object
#               canonical_days     => 90          (default)
#               coverage_threshold => 0.7         (default)
#               vm_scope_filter    => { ... } | undef
#               date_exclusions    => { ... } | undef
#               profiles           => ['G3-95W15', ...]
#               vms                => ['VM01', ...] | undef
# RETURNS:    String - MD5 hex fingerprint (or undef if no history exists)
# PHASE:      History Based Baselining - Phase 5
# ==============================================================================
sub _calculate_expected_baseline_fingerprint {
    my ($args_href) = @_;

    my $canonical_days     = $args_href->{canonical_days}     // 90;
    my $coverage_threshold = $args_href->{coverage_threshold} // 0.7;
    my $required_days      = int($canonical_days * $coverage_threshold);

    # 1. Determine required months (same logic as stitcher)
    my $history_index = _get_history_month_index($args_href->{system_cache_dir});
    my $required_months = _determine_required_months(
        $args_href->{end_date_obj},
        $canonical_days,
        $history_index
    );

    return undef unless ($required_months && @$required_months);

    # 2. Load ONLY metadata from those months (not full series data)
    my %content_versions;
    foreach my $month_key (@$required_months) {
        my $meta = _read_month_metadata_only($args_href->{system_cache_dir}, $month_key);
        $content_versions{$month_key} = ($meta ? ($meta->{content_version} // 0) : 0);
    }

    # 3. Calculate span (canonical window bounds — deterministic, matches Phase 4)
    #    INCLUSIVE: N days ending on end_date → start = end - (N - 1)
    my $span_start = ($args_href->{end_date_obj} - (($canonical_days - 1) * ONE_DAY))->strftime('%Y-%m-%d');
    my $span_end   = $args_href->{end_date_obj}->strftime('%Y-%m-%d');

    # 4. Assemble meta block using SHARED helpers (non-divergence guarantee)
    my $meta = {
        baseline_source      => 'history',
        canonical_days       => $canonical_days,
        lookback_cap_days    => 120,
        required_days        => $required_days,
        coverage_threshold   => $coverage_threshold,
        span_start           => $span_start,
        span_end             => $span_end,
        excluded_dates_hash  => _compute_exclusion_hash($args_href->{date_exclusions}),
        vm_scope_hash        => _compute_vm_scope_hash($args_href->{vm_scope_filter}),
        months_content_hash  => _compute_months_content_hash(\%content_versions),
        profile_set_hash     => _compute_profile_set_hash($args_href->{profiles} // []),
        vm_set_hash          => _compute_vm_set_hash($args_href->{vms}),
    };

    # 5. Use SHARED fingerprint builder
    return _compute_baseline_fingerprint($meta);
}

# ==============================================================================
# SUBROUTINE: _calculate_aggregate_coverage
# PURPOSE:    Computes a single aggregate coverage percentage from per-VM
#             coverage data returned by _calculate_baseline_from_stitched.
#             Uses the minimum coverage across all VM/profile pairs to
#             represent the worst-case data quality.
# ARGUMENTS:  $coverage_by_vm - hashref { VM => { profile => { pct_coverage => N } } }
# RETURNS:    Number (0.0–1.0), or 0 if no coverage data.
# PHASE:      History Based Baselining - Phase 5
# ==============================================================================
sub _calculate_aggregate_coverage {
    my ($coverage_by_vm) = @_;
    return 0 unless ($coverage_by_vm && ref($coverage_by_vm) eq 'HASH' && %$coverage_by_vm);

    my $min_coverage = 1.0;
    my $has_data = 0;

    foreach my $vm (keys %$coverage_by_vm) {
        my $profiles = $coverage_by_vm->{$vm};
        next unless (ref($profiles) eq 'HASH');
        foreach my $profile (keys %$profiles) {
            my $info = $profiles->{$profile};
            next unless (ref($info) eq 'HASH' && exists $info->{pct_coverage});
            $has_data = 1;
            $min_coverage = $info->{pct_coverage}
                if ($info->{pct_coverage} < $min_coverage);
        }
    }

    return $has_data ? $min_coverage : 0;
}

# ==============================================================================
# SUBROUTINE: _create_checkpoint_backup (Directory Aware & Compressed)
# PURPOSE:    Creates an atomic, compressed snapshot of the history.
#             Handles both Legacy (File) and Modern (Directory) structures.
#             Uses Archive::Tar for cross-platform (AIX/Linux) compatibility.
# ARGUMENTS:
#   1. $system_cache_dir: Path to the staging cache
#   2. $checkpoint_type:  Label for the backup (e.g., 'pre-tactical')
#   3. $include_l2_cache: Boolean, whether to include L2 results
# RETURNS:
#   Path to the created backup file, or undef on failure.
# ==============================================================================
sub _create_checkpoint_backup {
    my ($system_cache_dir, $checkpoint_type, $include_l2_cache) = @_;

    # Derive serial number and paths
    my $serial = basename($system_cache_dir);
    my $root_dir = dirname(dirname($system_cache_dir));
    my $backup_root = File::Spec->catfile($root_dir, 'backups', $serial);

    make_path($backup_root) unless -d $backup_root;

    my $timestamp = localtime->strftime('%Y%m%d_%H%M%S');

    # Identify the source (Directory takes precedence over File)
    my $history_dir_source  = File::Spec->catfile($system_cache_dir, '.nfit.history');
    my $history_file_source = File::Spec->catfile($system_cache_dir, '.nfit.history.json');

    my $source_path;
    my $is_directory = 0;

    if (-d $history_dir_source) {
        $source_path = $history_dir_source;
        $is_directory = 1;
    } elsif (-f $history_file_source) {
        $source_path = $history_file_source;
        $is_directory = 0;
    } else {
        print "  - No existing history found to backup (First Run)\n";
        return undef;
    }

    # Generate unique backup filename (.tar.gz for dirs, .json.gz for files)
    my $extension = $is_directory ? "tar.gz" : "json.gz";
    my $backup_target = _get_next_available_filename(
        $backup_root,
        ".nfit.history.${timestamp}",
        $extension
    );

    # Perform the Backup
    print "  • Creating compressed checkpoint (${checkpoint_type})\n";

    eval {
        if ($is_directory) {
            # Create a compressed tarball of the directory contents
            my $tar = Archive::Tar->new;

            # We cd into the directory to keep paths relative inside the tar
            my $cwd = Cwd::getcwd();
            chdir($system_cache_dir);

            # Add the folder (e.g., .nfit.history)
            # This recursively adds contents
            $tar->add_files(basename($source_path));

            # Write compressed file
            $tar->write($backup_target, COMPRESS_GZIP);

            chdir($cwd); # Restore PWD
        } else {
            # Gzip the single legacy file
            # We use IO::Zlib or just simple system gzip if we want to be lazy,
            # but let's stick to Perl for safety.
            # Actually, for a single file, copying then gzipping is safest
            # to avoid race conditions reading the source.

            require File::Copy;
            # Copy to .tmp first
            my $tmp_target = "$backup_target.tmp";
            File::Copy::copy($source_path, $tmp_target) or die "Copy failed: $!";

            # Compress in place
            system("gzip -f \"$tmp_target\""); # Standard on AIX/Linux
            rename("$tmp_target.gz", $backup_target);
        }
        print "  ✓ Created: " . basename($backup_target) . "\n";
    };

    if ($@) {
        warn "  [WARN] Could not create backup at '$backup_target': $@";
        return undef;
    }

    # Backup L2 Cache (Standard Copy - these are transient/rebuildable)
    if ($include_l2_cache) {
        foreach my $cache_file ('.nfit.cache.results', '.nfit.cache.manifest') {
            my $source = File::Spec->catfile($system_cache_dir, $cache_file);
            next unless -f $source;

            # We don't compress L2 cache backups to keep this step fast,
            # as these files are smaller than history.
            my $ext = ($cache_file =~ /\.(\w+)$/) ? $1 : '';
            my $cache_backup = _get_next_available_filename($backup_root, "${cache_file}.${timestamp}", $ext);

            require File::Copy;
            File::Copy::copy($source, $cache_backup) or warn "Could not backup $cache_file: $!";
        }
    }

    _prune_old_backups($backup_root, 24);
    return $backup_target;
}

# ==============================================================================
# SUBROUTINE: _get_next_available_filename
# PURPOSE:    Generates a unique filename using rolling numbering (001, 002, etc.)
#             to prevent collisions, similar to CSV output collision prevention.
# ARGUMENTS:
#   1. $dir (string): Directory where file will be created
#   2. $base_name (string): Base filename without extension
#   3. $extension (string): File extension (without leading dot)
# RETURNS:
#   - Full path to available filename
# ==============================================================================
sub _get_next_available_filename {
    my ($dir, $base_name, $extension) = @_;

    my $counter = 1;
    my $candidate;

    do {
        my $suffix = sprintf(".%03d", $counter);
        if ($extension) {
            $candidate = File::Spec->catfile($dir, "${base_name}${suffix}.${extension}");
        } else {
            $candidate = File::Spec->catfile($dir, "${base_name}${suffix}");
        }
        $counter++;
    } while (-e $candidate && $counter < 1000); # Safety limit

    die "Could not generate unique filename after 999 attempts" if $counter >= 1000;

    return $candidate;
}

# ==============================================================================
# SUBROUTINE: _prune_old_backups
# PURPOSE:    Removes old backup files to prevent unbounded disk usage.
#             Keeps the N most recent backup files based on modification time.
# ARGUMENTS:
#   1. $backup_dir (string): Directory containing backups
#   2. $keep_count (integer): Number of recent backups to retain
# RETURNS:
#   - None
# ==============================================================================
sub _prune_old_backups {
    my ($backup_dir, $keep_count) = @_;

    opendir(my $dh, $backup_dir) or return;
    my @backups = grep { /^\.nfit\.history\.\d{8}_\d{6}\.\d{3}\.json$/ }
                  readdir($dh);
    closedir($dh);

    return if @backups <= $keep_count;

    # Sort by modification time (oldest first)
    my @sorted = map { $_->[0] }
                 sort { $a->[1] <=> $b->[1] }
                 map {
                     my $path = File::Spec->catfile($backup_dir, $_);
                     [$_, (stat($path))[9]]
                 } @backups;

    # Remove oldest files beyond keep_count
    my $remove_count = @sorted - $keep_count;
    for (my $i = 0; $i < $remove_count; $i++) {
        my $old_file = File::Spec->catfile($backup_dir, $sorted[$i]);
        unlink $old_file or warn "Could not remove old backup $old_file: $!";
        print "  - Pruned old backup: " . basename($old_file) . "\n";
    }
}

# ==============================================================================
# SUBROUTINE: _validate_pre_update_conditions
# PURPOSE:    Enforces strict pre-conditions before allowing --update-history to
#             proceed. Ensures both decay models have been executed for complete
#             results, and prevents accidental corruption of enriched months.
# ARGUMENTS:
#   1. $system_cache_dir (string): Path to the system's staging cache
#   2. $month_key (string): The month key being updated (e.g., "2025-09")
#   3. $force_flag (boolean): Whether --force flag was specified
# RETURNS:
#   - Dies with error message if validation fails (unless --force specified)
# ==============================================================================
sub _validate_pre_update_conditions {
    my ($system_cache_dir, $month_key, $force_flag) = @_;

    print "\n--- Pre-Update Validation ---\n";

    # Check 1: Verify both decay models have been executed
    my $l2_cache_file = File::Spec->catfile($system_cache_dir, '.nfit.cache.results');

    if (-f $l2_cache_file) {
        my $json = JSON->new->relaxed;
        my $l2_data;

        eval {
            open my $fh, '<:encoding(utf8)', $l2_cache_file
                or die "Could not open L2 cache: $!";
            my $json_text = do { local $/; <$fh> };
            close $fh;
            $l2_data = $json->decode($json_text);
        };

        if ($@) {
            warn "  [WARN] Unable to read results cache (.nfit.cache.results) for validation: $@\n";
        } else {
            my $has_decay_over_states = 0;
            my $has_windowed_decay = 0;

            # Scan L2 cache for analysisType values
            foreach my $l2_key (keys %$l2_data) {
                my $l2_results_aref = $l2_data->{$l2_key};
                next unless (ref($l2_results_aref) eq 'ARRAY' && @$l2_results_aref);

                my $first_result = $l2_results_aref->[0];
                my $analysis_type = $first_result->{analysisType} // '';

                $has_decay_over_states = 1 if $analysis_type eq 'hybrid_decay_aggregated';
                $has_windowed_decay = 1 if $analysis_type eq 'windowed_decay_aggregated';

                # Early exit if both found
                last if ($has_decay_over_states && $has_windowed_decay);
            }

            unless ($has_decay_over_states && $has_windowed_decay) {
                if ($force_flag) {
                    warn "⚠  Results for one or both decay models are absent (maximum enrichment potential will be lost). Proceeding due to --force flag.\n";
                    if (!$has_decay_over_states) {
                        warn "  ↳  Results for the Hybrid State-Time Decay model (--decay-over-states) are absent.\n";
                    }
                    if (!$has_windowed_decay) {
                        warn "  ↳  Results for the Time-Based Windowed Decay model (--enable-windowed-decay) are absent.\n";
                    }
                } else {
                    die "[ERROR] Incomplete tactical analysis detected.\n" .
                        "Both --decay-over-states AND --enable-windowed-decay must be " .
                        "executed before --update-history.\n" .
                        "Current state: decay-over-states=$has_decay_over_states, " .
                        "windowed-decay=$has_windowed_decay\n" .
                        "Use --force to override this check.\n";
                }
            } else {
                print "  ✓ Results for all decay models are present\n";
            }
        }
    } else {
        if ($force_flag) {
            warn "⚠  Results cache not found. Proceeding due to --force flag.\n";
        } else {
            die "[ERROR] Results cache could not found at $l2_cache_file\n" .
                "Tactical analysis must be completed before attempting to execute `--update-history`.\n" .
                "Use `--force` to override this check.\n";
        }
    }

    # Check 2: Prevent overwriting already-enriched months
    my $history_data = read_unified_history($system_cache_dir);

    if (exists $history_data->{$month_key}) {
        if (exists $history_data->{$month_key}{SeasonalEventSnapshots}) {
            # Check if any snapshot has forecast enrichment
            my $snapshots = $history_data->{$month_key}{SeasonalEventSnapshots};
            foreach my $event_name (keys %$snapshots) {
                if (exists $snapshots->{$event_name}{forecast}) {
                    if ($force_flag) {
                        warn "⚠  Month $month_key already enriched with forecasts. " .
                             "This will be overwritten. Proceeding due to --force flag.\n";
                    } else {
                        die "FATAL: Month $month_key already contains forecast enrichment.\n" .
                            "Re-running tactical analysis would corrupt forecast consistency.\n" .
                            "If intentional, use --force to override.\n" .
                            "Consider restoring from ROOT/backups/<serial>/ if this is an error.\n";
                    }
                    last;
                }
            }
        }
    }

    print "  ✓ Pre-update validation passed\n";
}

# ==============================================================================
# SUBROUTINE: _add_lightweight_metadata
# PURPOSE:    Adds lightweight modification tracking metadata to each month entry
#             in the history cache, providing audit trails without the complexity
#             of cryptographic checksums.
# ARGUMENTS:
#   1. $history_data_href (hash ref): The complete history data structure
#   2. $updated_by (string): Tool name that performed the update
# RETURNS:
#   - None (modifies hash reference in place)
# ==============================================================================
sub _add_lightweight_metadata {
    my ($history_data_href, $updated_by) = @_;

    foreach my $month_key (keys %$history_data_href) {
        $history_data_href->{$month_key}{_metadata} = {
            last_updated => scalar(localtime->strftime('%Y-%m-%dT%H:%M:%S%z')),
            updated_by => $updated_by,
            toolkit_version => $VERSION,
            backup_recommendation => "Restore from ROOT/backups/<serial>/ if corruption suspected"
        };
    }
}

# ==============================================================================
# SUBROUTINE: _enrich_history_with_growth_rationale
# PURPOSE:    Performs a final enrichment pass on the unified history data (which is passed as an argument).
#             It robustly scans the nfit L2 Results Cache for growth-enabled
#             model runs, reads the embedded 'profileLabel' from the data, and
#             injects the 'growthRationale' block into the correct location,
#             keyed by the profile name.
# ==============================================================================
sub _enrich_history_with_growth_rationale {
    my ($system_cache_dir, $history_data) = @_;

    print "\n--- Phase 3: Enriching History with Growth Rationale from L2 Cache ---\n";

    my $l2_cache_file = File::Spec->catfile($system_cache_dir, '.nfit.cache.results');

    unless (-f $l2_cache_file) {
        print "  - INFO: L2 results cache not found. Skipping enrichment.\n";
        return 0; # Return false (was not modified)
    }

    # --- Step 1: Load both the history and the L2 cache ---
    my $l2_cache_data = eval {
        local $/;
        open my $fh, '<:encoding(utf8)', $l2_cache_file or die "Could not open L2 cache '$l2_cache_file': $!";
        my $json_text = <$fh>;
        close $fh;
        JSON->new->allow_nonref->decode($json_text);
    };
    if ($@ || !ref($l2_cache_data) eq 'HASH') {
        warn "  [WARN] Unable to decode results cache ($l2_cache_file). Enrichment cannot proceed.\n";
        return 0; # Return false (was not modified)
    }

    # --- Step 2: Scan L2 cache and build a structured hash of all available growth data ---
    # This hash is the single source of truth for all growth rationale.
    # Structure: {vm_name}{profile_name}{model_type} = rationale_hash
    my %growth_data_found;
    foreach my $l2_key (keys %$l2_cache_data) {
        my $l2_results_aref = $l2_cache_data->{$l2_key};
        next unless (ref($l2_results_aref) eq 'ARRAY' && @$l2_results_aref);

        my $first_result = $l2_results_aref->[0];
        my $analysis_type = $first_result->{analysisType} // '';
        my $model_type;

        if ($analysis_type eq 'hybrid_decay_aggregated') {
            $model_type = 'decay_over_states';
        } elsif ($analysis_type eq 'windowed_decay_aggregated') {
            $model_type = 'windowed_decay';
        } else {
            next; # Not a growth-enabled model result, skip this L2 cache entry.
        }

        foreach my $result (@$l2_results_aref) {
            my $vm = $result->{vmName};
            next unless $vm;

            # Instead of a top-level profileLabel, iterate the metrics.physc
            # block to find all profiles and their nested rationales.
            my $physc_metrics = _safe_dig($result, 'metrics', 'physc');
            next unless (ref($physc_metrics) eq 'HASH');

            foreach my $profile_name (keys %$physc_metrics) {
                my $profile_data = $physc_metrics->{$profile_name};

                # Check for the rationale (the trigger)
                if (ref($profile_data) eq 'HASH' && $profile_data->{growthRationale}) {
                    # --- Store the rationale AND its associated tactical values ---
                    $growth_data_found{$vm}{$profile_name}{$model_type} = {
                        rationale  => $profile_data->{growthRationale},
                        BaseValue  => $profile_data->{BaseValue},
                        FinalValue => $profile_data->{FinalValue}
                    };
                }
           }
        }
    }

    # --- Step 3: Iterate through history and inject the found growth data ---
    my $was_modified = 0;
    foreach my $month_key (keys %$history_data) {
        next unless (ref($history_data->{$month_key}) eq 'HASH' && $history_data->{$month_key}{MonthlyWorkloadAnalysis});
        my $workload_analysis = $history_data->{$month_key}{MonthlyWorkloadAnalysis};

        foreach my $vm_name (keys %$workload_analysis) {
            # Skip VMs for which no growth data was ever found.
            next unless exists $growth_data_found{$vm_name};

            my $vm_analysis = $workload_analysis->{$vm_name};

            # Inject all found growth data for this VM.
            foreach my $profile_name (keys %{$growth_data_found{$vm_name}}) {
                foreach my $model_type (keys %{$growth_data_found{$vm_name}{$profile_name}}) {

                    # --- Unpack the new data wrapper ---
                    my $new_data_wrapper = $growth_data_found{$vm_name}{$profile_name}{$model_type}; # This is the hash {rationale, BaseValue, FinalValue}
                    my $new_rationale    = $new_data_wrapper->{rationale};

                    # Ensure the nested structure exists for Rationale.
                    $vm_analysis->{growthRationale} //= {};
                    $vm_analysis->{growthRationale}{$profile_name} //= {};

                    # --- Create new block for Tactical Values ---
                    # This is the new block to store tactical results
                    $vm_analysis->{TacticalValues} //= {};
                    $vm_analysis->{TacticalValues}{$profile_name} //= {};

                    # --- Check if the Rationale data is new or different ---
                    # We use JSON generation to compare structures. This handles:
                    # 1. Deep nesting (recursive)
                    # 2. Key sorting (canonical)
                    # 3. Numeric/String normalization (mostly)

                    my $json_cmp = JSON->new->canonical->allow_nonref;

                    if (!exists $vm_analysis->{growthRationale}{$profile_name}{$model_type}) {
                        $was_modified = 1;
                    } else {
                        # Compare the serialized canonical strings
                        my $existing_json = $json_cmp->encode($vm_analysis->{growthRationale}{$profile_name}{$model_type});
                        my $new_json      = $json_cmp->encode($new_rationale);

                        if ($existing_json ne $new_json) {
                            $was_modified = 1;
                        }
                    }

                    if ($was_modified) {
                         $vm_analysis->{growthRationale}{$profile_name}{$model_type} = $new_rationale;
                    }

                    # --- Harvest BaseValue and FinalValue ---
                    # This archives the tactical model's input (BaseValue)
                    # and output (FinalValue) for long-term auditing.
                    my $base_val = $new_data_wrapper->{BaseValue};
                    my $final_val = $new_data_wrapper->{FinalValue};

                    if (!exists $vm_analysis->{TacticalValues}{$profile_name}{$model_type} ||
                        $vm_analysis->{TacticalValues}{$profile_name}{$model_type}{BaseValue} ne $base_val ||
                        $vm_analysis->{TacticalValues}{$profile_name}{$model_type}{FinalValue} ne $final_val)
                    {
                        # Add the tactical results for this profile AND model
                        $vm_analysis->{TacticalValues}{$profile_name}{$model_type} = {
                            BaseValue  => $base_val,
                            FinalValue => $final_val
                        };
                        $was_modified = 1;
                    }
                }
            }
        }
    }

    # --- Step 4: Write back to the history file ONLY if it was modified ---
    if ($was_modified) {
        print "  - INFO: Found new/updated growth rationale data in L2 cache. Updating history file...\n";
        _write_full_history($system_cache_dir, $history_data);
    } else {
        print "  - INFO: No new growth rationale data found in L2 cache. History is up to date.\n";
    }

    # Return the modification status to the caller
    return $was_modified;
}

# ==============================================================================
# HELPER SUBROUTINE for the enrichment process.
# PURPOSE:    A robust replication of nfit.pl's generate_canonical_key function.
#             It builds the precise key used to store results in the L2 cache for
#             a specific growth analysis run.
# ==============================================================================
sub _generate_l2_cache_key_for_nfit {
    my ($profile_obj, $model_type, $system_cache_dir) = @_;

    my @key_parts;
    my $flags = $profile_obj->{flags};

    # This simulates the arguments nfit.pl would receive from a typical
    # nfit-profile run that enables growth prediction.

    # --- Flags passed from nfit-profile to nfit ---
    push @key_parts, "--nmondir $system_cache_dir";
    push @key_parts, "--smt $default_smt_arg";
    push @key_parts, "--runq-avg-method $nfit_runq_avg_method_str";
    push @key_parts, "--peak"; # nfit-profile always adds this

    # --- Flags from the profile definition ---
    # Parse them from the string to handle them individually
    if ($flags =~ /--percentile\s+([0-9.]+)/ || $flags =~ /-p\s+([0-9.]+)/) { push @key_parts, "--percentile $1"; }
    if ($flags =~ /--window\s+(\d+)/ || $flags =~ /-w\s+(\d+)/) { push @key_parts, "--window $1"; }
    if ($flags =~ /--avg-method\s+(\w+)/) { push @key_parts, "--avg-method $1"; }
    if ($flags =~ /--decay\s+([\w-]+)/) { push @key_parts, "--decay $1"; }
    if ($flags =~ /--runq-decay\s+([\w-]+)/) { push @key_parts, "--runq-decay $1"; }
    if ($flags =~ /--filter-above-perc\s+([0-9.]+)/) { push @key_parts, "--filter-above-perc $1"; }
    if ($flags =~ /--online/) { push @key_parts, "--online"; }
    if ($flags =~ /--batch/) { push @key_parts, "--batch"; }
    if ($flags =~ /--no-weekends/) { push @key_parts, "--no-weekends"; }

    # --- Flags specific to the growth model ---
    if ($model_type eq 'decay_over_states') {
        push @key_parts, "--decay-over-states";
    } elsif ($model_type eq 'windowed_decay') {
        push @key_parts, "--enable-windowed-decay";
        push @key_parts, "--process-window-unit $nfit_window_unit_str";
        push @key_parts, "--process-window-size $nfit_window_size_val";
    }

    # Growth flags are present in the profile, so they are included automatically
    if ($flags =~ /--enable-growth-prediction/) {
        push @key_parts, "--enable-growth-prediction";
        if ($flags =~ /--growth-projection-days\s+(\d+)/) { push @key_parts, "--growth-projection-days $1"; }
        if ($flags =~ /--max-growth-inflation-percent\s+(\d+)/) { push @key_parts, "--max-growth-inflation-percent $1"; }
    }

    # Note: This replication assumes a standard set of flags passed from nfit-profile.
    # It prioritizes the flags that define the analysis type and profile uniqueness.
    # This is far more robust than the previous simple string concatenation.

    return join(" ", sort @key_parts);
}

# ==============================================================================
# SUBROUTINE: _assemble_monthly_analysis
# PURPOSE:    Assembles the MonthlyWorkloadAnalysis data structure from parsed
#             nfit JSON output. This function contains the battle-tested logic
#             for extracting profile values, calculating sizing hints, building
#             RunQ metrics, and assembling ClippingInfo blocks.
#
# ARGUMENTS:
#   1. $parsed_results_href (hash ref): Parsed JSON from nfit (VM -> [states])
#   2. $adaptive_runq_saturation_thresh (float): Adaptive saturation threshold
#   3. Start time of analysis time-range
#   4. End time of analysis time-range
#
# RETURNS:
#   - Hash reference containing the complete MonthlyWorkloadAnalysis structure
#     keyed by VM name.
#
# EXTRACTED FROM:
#   This function was extracted from _generate_monthly_workload_analysis to
#   enable code reuse and preserve all recent bug fixes to ClippingInfo
#   assembly and RunQ metrics extraction.
# ==============================================================================
sub _assemble_monthly_analysis {
    my ($parsed_results_href, $adaptive_runq_saturation_thresh, $states_db, $month_start_str, $month_end_str) = @_;

    # Validate inputs
    unless (ref($parsed_results_href) eq 'HASH') {
        die "FATAL: _assemble_monthly_analysis requires parsed results hash reference";
    }

    # Parse boundaries for monthly configuration state overlap check
    my $month_start_obj = Time::Piece->strptime($month_start_str, "%Y-%m-%d");
    my $month_end_obj   = Time::Piece->strptime($month_end_str, "%Y-%m-%d") + ONE_DAY - 1;

    my %analysis_results;
    my %collated_results_table;  # Temporary table for hint generation

    # ======================================================================
    # STEP 1: Build collated results table for ALL profiles
    # ======================================================================
    # This step harvests all P-metrics for the MonthlyWorkloadAnalysis.ProfileValues
    # block, fixing the historical data regression.
    # It ALSO surgically extracts the max P-99W1 'Peak' metric, which is
    # a critical, separate input for the hint-generation logic in STEP 2.
    # ======================================================================
    foreach my $vm_name (keys %$parsed_results_href) {
        my $states_aref = $parsed_results_href->{$vm_name};
        next unless ref($states_aref) eq 'ARRAY' && @$states_aref;

        my @p99w1_peak_values_for_hinting; # Array to store P-99W1 'Peak' metrics

        # Iterate all configured profiles to get their specific P-metric
        foreach my $profile (@profiles) {
            my $profile_name = $profile->{name};

            # Find the P-metric for this profile (e.g., "P95")
            my ($p_val_num) = $profile->{flags} =~ /(?:-p|--percentile)\s+([0-9.]+)/;
            my $p_metric_key = "P" . clean_perc_label($p_val_num // $DEFAULT_PERCENTILE);

            my $metric_val = undef;

            # Iterate ALL states (newest to oldest)
            for (my $i = $#{$states_aref}; $i >= 0; $i--) {
                my $state = $states_aref->[$i];

                # A. Find the P-metric value (if we haven't already)
                $metric_val //= _safe_dig($state, 'metrics', 'physc', $profile_name, $p_metric_key);

                # B. If this is the P-99W1 profile, find its 'Peak' metric for hinting
                if ($profile_name eq $MANDATORY_PEAK_PROFILE_FOR_HINT) {
                    my $profile_peak_val = _safe_dig($state, 'metrics', 'physc', $profile_name, 'Peak');
                    push @p99w1_peak_values_for_hinting, $profile_peak_val if (defined $profile_peak_val && looks_like_number($profile_peak_val));
                }
            } # end state loop

            # Store the final P-metric value for the ProfileValues block
            if (defined $metric_val && looks_like_number($metric_val)) {
                $collated_results_table{$vm_name}{$profile_name} = $metric_val + 0;
            }
        } # end profile loop

        # Store the max P-99W1 Profile Peak for hint logic, using the original key
        if (@p99w1_peak_values_for_hinting) {
            $collated_results_table{$vm_name}{$PEAK_PROFILE_NAME} = max(@p99w1_peak_values_for_hinting);
        }
    }

    # ======================================================================
    # STEP 2: Generate hints and extract metrics for each VM
    # ======================================================================
    foreach my $vm_name (keys %collated_results_table) {
        my $vm_config_ref = $vm_config_data{$vm_name};
        my $states_aref = $parsed_results_href->{$vm_name};
        my $last_state_data = $states_aref->[-1] // {};

        # ------------------------------------------------------------------
        # Extract configuration metadata
        # ------------------------------------------------------------------
        my @config_timeline;
        my $last_state_generic = {};
        my $last_state_platform = {};

        # Retrieve authoritative states from cache
        my $auth_states = $states_db->{$vm_name};

        if (defined $auth_states && ref($auth_states) eq 'ARRAY') {
            foreach my $state (@$auth_states) {
                # 1. Check Date Overlap
                my $s_start = $state->{start_epoch};
                my $s_end   = $state->{end_epoch};

                # Logic: State overlaps Month if (StateEnd >= MonthStart) AND (StateStart <= MonthEnd)
                next unless ($s_end >= $month_start_obj->epoch && $s_start <= $month_end_obj->epoch);

                my $md = $state->{metadata} || {};

                # 2. Extract Raw Attributes (Snake_Case from .states file)
                my $ent       = $md->{entitlement} // 0;
                my $vcpu      = $md->{virtual_cpus} // 0;
                my $pool_cpu  = $md->{pool_cpu} // 0;
                my $is_capped = $md->{capped} // 0;
                my $smt       = $md->{smt} // $default_smt_arg;
                my $proc_type = $md->{proc_type} // "Unknown";

                # 3. Calculate MaxUsableCPU (Replicating nfit logic)
                # If capped: Entitlement
                # If uncapped: min(vCPU, Pool)
                my $calc_max;
                if ($is_capped) {
                    $calc_max = $ent;
                } else {
                    # Uncapped logic
                    if ($pool_cpu > 0 && $vcpu > 0) {
                        $calc_max = ($vcpu < $pool_cpu) ? $vcpu : $pool_cpu;
                    } else {
                        $calc_max = ($vcpu > 0) ? $vcpu : $ent; # Fallback to Ent if vCPU missing
                    }
                }
                # Safety clamps
                $calc_max = 0 if $calc_max < 0;

                # 4. Build Generic Shape
                my $generic = {
                    Architecture => 'ppc64',
                    OS_Type      => $md->{os_type} // 'AIX', # Default = AIX
                    MaxUsableCPU => 0 + sprintf("%.2f", $calc_max),
                    RAM_GB       => undef
                };

                # 5. Build Platform Shape
                my $platform = {
                    AIX => {
                        Entitlement   => 0 + $ent,
                        VirtualCPUs   => 0 + $vcpu,
                        Capped        => $is_capped ? JSON::true : JSON::false,
                        Pool_CPU      => $md->{pool_cpu},
                        Pool_ID       => $md->{pool_id},
                        Shared_Weight => $md->{shared_weight} // 128,
                        Proc_Type     => $proc_type,
                        SMT_Level     => 0 + $smt,
                    }
                };

                $last_state_generic = $generic;
                $last_state_platform = $platform;

                push @config_timeline, {
                    Start        => gmtime($s_start)->datetime . "Z",
                    End          => gmtime($s_end)->datetime . "Z",
                    DurationDays => sprintf("%.2f", ($s_end - $s_start) / 86400) + 0,
                    Generic      => $generic,
                    Platform     => $platform
                };
            }
        }

        # Assemble the Configuration Block
        my $configuration_block = {
            Generic => $last_state_generic,
            Configuration_States => \@config_timeline
        };

        # --- Legacy Support for Hint Generation ---
        # We still use the MaxUsableCPU we just calculated for the hint logic
        my $max_cpu_from_state = $last_state_generic->{MaxUsableCPU} // 0;
        my $smt_from_state     = $last_state_platform->{SMT_Level} // $default_smt_arg;

        # ------------------------------------------------------------------
        # Build the VM map structure for sizing hint generation
        # ------------------------------------------------------------------
        my %vm_map = (
            Configuration => {
                vm_name     => $vm_name,
                max_cpu     => $max_cpu_from_state,
                smt         => $smt_from_state,
                entitlement => $vm_config_ref->{entitlement} // 0,
                %{$vm_config_ref || {}}
            },
            CoreResults => {
                ProfileValues => $collated_results_table{$vm_name}
            },
            RunQMetrics => _safe_dig($last_state_data, 'metrics', 'runq') || {}
        );

        # ------------------------------------------------------------------
        # Generate sizing hint using existing, proven logic
        # ------------------------------------------------------------------
        my ($hint, $pattern, $pressure, $pressure_detail, $rationale_text, $has_abs_pressure, $has_norm_pressure) =
            generate_sizing_hint(\%vm_map, undef, $adaptive_runq_saturation_thresh);

        # ------------------------------------------------------------------
        # Extract RunQ metrics from the last state
        # ------------------------------------------------------------------
        my $runq_metrics_block = $last_state_data->{metrics}{runq} || {};

        # ------------------------------------------------------------------
        # Build ClippingInfo for all profiles
        # ------------------------------------------------------------------
        # This logic iterates through all profiles and extracts clipping
        # detection results. The structure here has been carefully debugged
        # and must be preserved exactly.
        # ------------------------------------------------------------------
        my $clipping_info_for_vm = {};

        foreach my $profile_name (keys %{$last_state_data->{metrics}{physc} || {}}) {
            # Skip if this isn't a real profile (e.g., metadata keys)
            next unless ref($last_state_data->{metrics}{physc}{$profile_name}) eq 'HASH';

            # Extract clipping data from this profile's output
            my $clipping_metrics = _safe_dig($last_state_data, 'metrics', 'physc', $profile_name, 'ClippingInfo');

            # Try alternate path if first doesn't work (backwards compatibility)
            $clipping_metrics //= _safe_dig($last_state_data, 'metrics', 'physc', $profile_name, 'Clipping');

            # Only process if we found valid clipping data
            if ($clipping_metrics && ref($clipping_metrics) eq 'HASH' && exists $clipping_metrics->{isClipped}) {
                my $unmet_demand_est = $clipping_metrics->{unmetDemandEstimate} // 0;

                # Get the base value for this profile to calculate unclipped peak estimate
                my $profile_obj = (grep { $_->{name} eq $profile_name } @profiles)[0];
                next unless $profile_obj;  # Safety check

                my $p_key = "P" . clean_perc_label(($profile_obj->{flags} =~ /-p\s+([0-9.]+)/) ? $1 : $DEFAULT_PERCENTILE);
                my $base_peak_for_unclip = _safe_dig($last_state_data, 'metrics', 'physc', $profile_name, $p_key);

                # Get MaxCPU from VM config
                # IMPORTANT: ClippingInfo uses 'max_capacity' (lowercase), not 'max_cpu'
                my $max_capacity = $max_cpu_from_state // $vm_config_ref->{max_capacity} // 0;

                # Assemble the ClippingInfo structure with exact key names
                # that downstream consumers (nfit-forecast, GUI) expect
                $clipping_info_for_vm->{$profile_name} = {
                    isClipped               => $clipping_metrics->{isClipped},
                    clippingConfidence      => $clipping_metrics->{clippingConfidence} // 'unknown',
                    capacityLimit           => $max_capacity,
                    unmetDemandEstimate     => $unmet_demand_est,
                    unclippedPeakEstimate   => ((defined $base_peak_for_unclip && looks_like_number($base_peak_for_unclip)) ? $base_peak_for_unclip : 0) + $unmet_demand_est,
                    platformSpecificMarkers => {
                        aix_runq_saturation => _safe_dig($clipping_metrics, 'platformMarkers', 'aix_runq_saturation') // 0
                    }
                };
            }

            # --- Harvest DailyProfileSeries Data ---
            # Extract the daily time-series map
            my $daily_series = _safe_dig($last_state_data, 'metrics', 'physc', $profile_name, 'DailySeries');
            if (ref($daily_series) eq 'HASH' && scalar(keys %$daily_series) > 0) {
                # Temporarily store in collated table for final assembly
                $collated_results_table{$vm_name}{_DailySeries}{$profile_name} = $daily_series;
            }
        }

        # ------------------------------------------------------------------
        # Assemble the final data structure for this VM
        # ------------------------------------------------------------------
        # CRITICAL: This structure must NOT include growthRationale or any
        # other predictive metadata. MonthlyWorkloadAnalysis is pure historical.
        # The growthRationale is harvested separately by the enrichment phase.
        # ------------------------------------------------------------------
        $analysis_results{$vm_name} = {
            Hint           => $hint,
            Pattern        => $pattern,
            Pressure       => $pressure,
            PressureDetail => $pressure_detail,
            Configuration  => $configuration_block,
            RunQMetrics    => {
                # Extract the specific RunQ metrics needed by downstream logic
                AbsRunQ_P90   => _safe_dig($runq_metrics_block, 'absolute', $MANDATORY_PEAK_PROFILE_FOR_HINT, 'P90'),
                AbsRunQ_P99   => _safe_dig($runq_metrics_block, 'absolute', $MANDATORY_PEAK_PROFILE_FOR_HINT, 'P99'),
                AbsRunQ_P99W1 => _safe_dig($runq_metrics_block, 'absolute', $MANDATORY_PEAK_PROFILE_FOR_HINT, 'P99W1'),
                NormRunQ_P50  => _safe_dig($runq_metrics_block, 'normalized', $MANDATORY_PEAK_PROFILE_FOR_HINT, 'P50'),
                NormRunQ_P90  => _safe_dig($runq_metrics_block, 'normalized', $MANDATORY_PEAK_PROFILE_FOR_HINT, 'P90')
            },
            ProfileValues  => $collated_results_table{$vm_name},
            DailyProfileSeries => $collated_results_table{$vm_name}{_DailySeries} || {},
            ClippingInfo   => $clipping_info_for_vm
        };

        # Cleanup: Remove the temporary _DailySeries key from ProfileValues to keep the schema clean
        delete $analysis_results{$vm_name}{ProfileValues}{_DailySeries};
    }

    return \%analysis_results;
}

# ==============================================================================
# SUBROUTINE: _generate_monthly_workload_analysis
# PURPOSE:    Performs a full single-pass analysis for a given time period
#             (typically one month) using the manifest-driven Single Pass Engine
#             architecture, then assembles the comprehensive MonthlyWorkloadAnalysis
#             data structure.
#
#             This function has been refactored to use the SPE to achieve dramatic
#             I/O reduction (16x for a typical 16-profile configuration).
# ==============================================================================
sub _generate_monthly_workload_analysis {
    my ($system_cache_dir, $start_date_str, $end_date_str, $adaptive_runq_saturation_thresh) = @_;

    print STDERR "\n  [+] Single Pass Engine (monthly analysis)\n";

    # --- Load Configuration States Cache (for this analysis cycle) ---
    my $states_cache_file = File::Spec->catfile($system_cache_dir, '.nfit.cache.states');
    my $states_db = {};
    if (-f $states_cache_file) {
        my $json_text = do {
            open my $fh, '<:encoding(utf8)', $states_cache_file or die "Cannot open states cache: $!";
            local $/; <$fh>;
        };
        $states_db = decode_json($json_text);
    }

    # ======================================================================
    # STEP 1: Build the tactical manifest
    # ======================================================================
    # Use the existing, battle-tested build_transform_manifest function.
    # This function handles all the complexity of profile parameter extraction,
    # transform key generation, and percentile de-duplication.
    # ======================================================================
    my $runq_behavior = $runq_perc_behavior_mode // 'none';
    my $tactical_manifest = build_transform_manifest(\@profiles, $nfit_runq_avg_method_str, $runq_behavior);

    # ======================================================================
    # STEP 2: Create the PURE HISTORICAL manifest
    # ======================================================================
    # The sanitisation function is the primary data purity guardrail.
    # It strips all predictive elements (growth, decay) whilst preserving
    # the profile's fundamental measurement definition.
    # ======================================================================
    my $historical_manifest = _sanitise_manifest_for_history($tactical_manifest);

    # ======================================================================
    # STEP 3: Write manifest to temporary file
    # ======================================================================
    use File::Temp qw(tempfile);
    my ($fh_manifest, $manifest_filename) = tempfile(
        'nfit_history_manifest_XXXXXX',
        SUFFIX => '.json',
        UNLINK => 1,
        TMPDIR => 1
    );

    # Write the manifest using canonical JSON encoding for consistency
    print $fh_manifest JSON->new->pretty->canonical->encode($historical_manifest);
    close $fh_manifest;

    # ======================================================================
    # STEP 4: Build and execute the single nfit command
    # ======================================================================
    my $nfit_cmd = "$nfit_script_path --manifest $manifest_filename "
                 . "--nmondir \"$system_cache_dir\" "
                 . "--startdate $start_date_str "
                 . "--enddate $end_date_str "
                 . "--smt $default_smt_arg "
                 . "--runq-avg-method $nfit_runq_avg_method_str "
                 . "--enable-clipping-detection "  # Historical capacity measurement
                 . "--show-progress";

    # Execute nfit once for all profiles
    print STDERR "  • Invoking nFit engine for date range: $start_date_str to $end_date_str\n";

    my $nfit_output = '';
    my $nfit_error  = '';

    # Capture both STDOUT (JSON) and STDERR (progress/errors)
    my $stderr_arg = ">&=" . fileno(STDERR);
    my $pid = open3(undef, my $stdout_fh, $stderr_arg, $nfit_cmd);

    while (my $line = <$stdout_fh>) {
        $nfit_output .= $line;
    }

    waitpid($pid, 0);
    my $exit_code = $? >> 8;

    # Check for execution failure
    if ($exit_code != 0) {
        die "FATAL: nfit engine failed during monthly analysis.\n"
            . "Exit code: $exit_code\n"
            . "Command: $nfit_cmd\n"
            . "Check STDERR output above for details.\n";
    }

    # ======================================================================
    # STEP 5: Parse the JSON output
    # ======================================================================
    # Use the existing, proven parse_nfit_json_output function.
    # It correctly handles multi-line JSON output where each line is a
    # separate VM result object.
    # ======================================================================
    my $parsed_results = parse_nfit_json_output($nfit_output);

    # Sanity check: ensure we got results
    unless (ref($parsed_results) eq 'HASH' && scalar keys %$parsed_results) {
        warn "WARNING: nfit returned no results for monthly analysis period $start_date_str to $end_date_str\n";
        return {};
    }

    print STDERR "  ✓ Successfully processed " . scalar(keys %$parsed_results) . " VM(s)\n";

    # ======================================================================
    # STEP 6: Assemble the final MonthlyWorkloadAnalysis structure
    # ======================================================================
    # Call the extracted assembly logic which contains all the recent
    # bug fixes for ClippingInfo, RunQ metrics, and profile value extraction.
    # ======================================================================
    my $analysis_results = _assemble_monthly_analysis(
        $parsed_results,
        $adaptive_runq_saturation_thresh,
        $states_db,       # Configuration states
        $start_date_str,  # Boundary for filtering
        $end_date_str     # Boundary for filtering
    );

    return $analysis_results;
}

# ==============================================================================
# SUBROUTINE: detect_sampling_interval
# Robustly detects the NMON sampling interval from the .nfit.cache.data file.
# It reads the start of the cache, isolates timestamps for the first VM found,
# calculates the time difference between consecutive samples, and finds the mode
# of these deltas to determine the most likely interval.
#
# Note: This version introduces jitter bucketing: Before counting the frequency,
#  it checks if a delta is close to a standard interval (60, 300, 600, 900 seconds)
#  within a tolerance ($EPS = 3). If it is, it snaps the value to that standard
#  interval before incrementing the count.
# Returns:
#   - The detected interval in seconds (e.g., 60, 300), or undef on failure.
sub detect_sampling_interval {
    my ($data_cache_file) = @_;

    my $SAMPLES_TO_GATHER       = 50;
    my $MAX_LINES_TO_SCAN       = 5000;
    my $MIN_SAMPLES_FOR_DETECTION = 5;   # required count in a single interval bucket
    my @STD = (60, 300, 600, 900);
    my $EPS = 3;                          # jitter tolerance (seconds)

    return undef unless (-f $data_cache_file && -s $data_cache_file);

    open my $fh, '<:encoding(utf8)', $data_cache_file or do {
        warn " [WARN] Could not open data cache '$data_cache_file' for interval detection: $!";
        return undef;
    };

    <$fh>; # Skip header

    my $target_vm_for_detection;
    my @timestamps;
    my $lines_scanned = 0;

    while (my $line = <$fh>) {
        $lines_scanned++;
        last if $lines_scanned > $MAX_LINES_TO_SCAN;

        my ($ts_str, $vm_name) = ($line =~ /^(\d{4}-\d{2}-\d{2}\s\d{2}:\d{2}:\d{2}),([^,]+)/);
        next unless ($ts_str && $vm_name);

        $target_vm_for_detection //= $vm_name;
        next unless $vm_name eq $target_vm_for_detection;

        eval {
            push @timestamps, Time::Piece->strptime($ts_str, "%Y-%m-%d %H:%M:%S")->epoch;
        };
        last if @timestamps >= $SAMPLES_TO_GATHER;
    }
    close $fh;

    # Need enough timestamps to produce at least MIN_SAMPLES_FOR_DETECTION deltas
    return undef if @timestamps < ($MIN_SAMPLES_FOR_DETECTION + 1);

    @timestamps = sort { $a <=> $b } @timestamps;

    my %delta_counts;
    for (my $i = 1; $i < @timestamps; $i++) {
        my $delta = $timestamps[$i] - $timestamps[$i-1];
        next unless ($delta > 10 && $delta < 1810);

        # Jitter bucketing: snap near-standards to the standard value
        my $bucket = $delta;
        my $best_d = $EPS + 1;
        for my $s (@STD) {
            my $d = abs($delta - $s);
            if ($d < $best_d) { $best_d = $d; $bucket = ($d <= $EPS) ? $s : $bucket; }
        }
        $delta_counts{$bucket}++;
    }

    return undef unless %delta_counts;

    # Find max frequency
    my $max_count = 0;
    foreach my $delta (keys %delta_counts) {
        $max_count = $delta_counts{$delta} if $delta_counts{$delta} > $max_count;
    }

    # Require minimum evidence in the winning bucket
    return undef if $max_count < $MIN_SAMPLES_FOR_DETECTION;

    # Collect all tied winners
    my @tied_deltas = grep { $delta_counts{$_} == $max_count } keys %delta_counts;

    # Deterministic tie-break: closest to a standard, then smallest delta
    if (@tied_deltas > 1) {
        my %distances;
        for my $delta (@tied_deltas) {
            my $min_dist = 1e9;
            foreach my $std (@STD) {
                my $dist = abs($delta - $std);
                $min_dist = $dist if $dist < $min_dist;
            }
            $distances{$delta} = $min_dist;
        }
        @tied_deltas = sort { $distances{$a} <=> $distances{$b} || $a <=> $b } @tied_deltas;
    }

    return $tied_deltas[0];
}

# ==============================================================================
# --- _build_adaptive_thresholds_hash ---
# Takes a detected sampling interval, snaps it to a standard value (e.g., 1, 5, 10 min),
# calculates the adaptive thresholds for saturation and efficiency, and logs the rationale.
# This function does NOT modify global state; it returns the calculated values.
#
# Returns:
#   - A list of three values:
#     1. The new saturation threshold (float).
#     2. The new target for efficiency calculation (float).
#     3. The new max downsizing percentage (as a factor, e.g., 0.05 for 5%).
sub _build_adaptive_thresholds_hash {
    my ($raw_interval_seconds, $log_fh) = @_;

    # --- Baseline (1-min) thresholds ---
    my $sat_thresh = $RUNQ_PRESSURE_P90_SATURATION_THRESHOLD;
    my $target_norm_runq = $DEFAULT_TARGET_NORM_RUNQ_FOR_EFFICIENCY_CALC;
    my $max_downsize_perc_factor = $MAX_EFFICIENCY_REDUCTION_PERCENTAGE;

    my $snapped_minutes = 1.0;
    my $detection_method_log = "cache-based detection using mode of deltas with epsilon bucketing";
    my $log_message;

    if (!defined $raw_interval_seconds) {
        $log_message = "No reliable interval found; defaulting to 1.0 min; thresholds at 1-min baseline.";
    } else {
        # Snap the detected raw seconds to the nearest standard interval with a tolerance
        my $epsilon = 5; # Allow +/- 5 seconds
        if    (abs($raw_interval_seconds - 60) <= $epsilon)  { $snapped_minutes = 1; }
        elsif (abs($raw_interval_seconds - 300) <= $epsilon) { $snapped_minutes = 5; }
        elsif (abs($raw_interval_seconds - 600) <= $epsilon) { $snapped_minutes = 10; }
        elsif (abs($raw_interval_seconds - 900) <= $epsilon) { $snapped_minutes = 15; }
        else {
             # If it doesn't snap cleanly, default to 1 min but log the anomaly.
            $log_message = sprintf("Unusual interval %ds detected; did not snap to a standard value. Defaulting to 1.0 min.", $raw_interval_seconds);
            $snapped_minutes = 1.0;
        }
    }

    if ($snapped_minutes > 1) {
        # It's a non-default interval, so calculate the adaptive thresholds.
        my $k    = $snapped_minutes;
        my $base = $RUNQ_PRESSURE_P90_SATURATION_THRESHOLD; # Follow the global 1-min baseline
        my $beta = 0.4; # Decay exponent, could be made configurable in the future
        # General formula: 1 + (base-1) * k^(-beta)
        # This removes the hard-coded '0.8' and derives it from the baseline (1.8 - 1.0)
        $sat_thresh = 1 + (($base - 1.0) * ($k**-$beta));
        # Downsizing thresholds: Use the lookup table for conservatism
        my %downsize_targets = ( 5 => 0.75, 10 => 0.72, 15 => 0.70 );
        my %downsize_caps    = ( 5 => 5,    10 => 5,    15 => 5 ); # Cap is 5% for all coarser intervals

        $target_norm_runq = $downsize_targets{$k} // $target_norm_runq;
        # NOTE: The cap is stored as a percentage and converted to a factor here.
        my $cap_perc = $downsize_caps{$k};
        if (defined $cap_perc) {
            $max_downsize_perc_factor = $cap_perc / 100.0;
        }

        $log_message = sprintf("Sampling interval detected %.1fs -> snapped to %.1f min.", $raw_interval_seconds, $snapped_minutes) if defined $raw_interval_seconds;
    } elsif (!defined $log_message) {
         $log_message = sprintf("Sampling interval detected %.1fs -> snapped to 1.0 min. Using baseline thresholds.", $raw_interval_seconds) if defined $raw_interval_seconds;
    }


    # --- Rationale Logging ---
    if ($log_fh) {
        print {$log_fh} "----------------------------------------------------------------------\n";
        print {$log_fh} "Adaptive Threshold Calculation\n";
        print {$log_fh} "  - Method                   : $detection_method_log\n";
        print {$log_fh} "  - Status                   : $log_message\n";
        print {$log_fh} "  - Saturation Threshold     : " . sprintf("%.2f", $sat_thresh) . " x LCPU\n";
        print {$log_fh} "  - Downsizing Target        : " . sprintf("%.2f", $target_norm_runq) . " / LCPU\n";
        print {$log_fh} "  - Downsizing Cap           : " . sprintf("%.1f%%", $max_downsize_perc_factor * 100) . "\n";
        print {$log_fh} "----------------------------------------------------------------------\n";
    }

    # --- Additional derived policy knobs (plumbing only) ---
    # Baseline divisor (60s) remains the global constant; coarser intervals may map to a more conservative divisor.
    my $runq_width_divisor_base = $RUNQ_TARGET_HEADROOM_FACTOR;
    $runq_width_divisor_base = 0.80 if (!defined $runq_width_divisor_base || $runq_width_divisor_base <= 0);

    # NOTE: These values are not yet consumed in calculations (Patch 1 plumbing only).
    my %width_divisor_lookup = (
        5  => 0.70,
        10 => 0.65,
        15 => 0.60,
    );

    my $runq_width_divisor = $runq_width_divisor_base;
    if ($snapped_minutes > 1) {
        $runq_width_divisor = $width_divisor_lookup{$snapped_minutes} // $runq_width_divisor_base;
    }
    $runq_width_divisor = $runq_width_divisor_base if (!defined $runq_width_divisor || $runq_width_divisor <= 0);

    my $snapped_interval_secs = int(($snapped_minutes * 60) + 0.5);

    return {
        # Interval context
        interval_secs_raw     => $raw_interval_seconds,
        interval_secs_snapped => $snapped_interval_secs,
        snapped_minutes       => $snapped_minutes + 0,

        # Adaptive thresholds currently used by RunQ logic
        saturation    => $sat_thresh + 0,
        target        => $target_norm_runq + 0,
        max_reduction => $max_downsize_perc_factor + 0,

        # Future (plumbing only for now)
        runq_width_divisor_base => $runq_width_divisor_base + 0,
        runq_width_divisor      => $runq_width_divisor + 0,

        # Diagnostics (not consumed by calculations)
        detection_method => $detection_method_log,
        status           => $log_message,
    };
}

# --- set_adaptive_thresholds ---
# Backwards-compatible wrapper: returns the three threshold scalars used by existing logic.
# Prefer calling _build_adaptive_thresholds_hash() for full context (interval + policy knobs).
sub set_adaptive_thresholds {
    my ($raw_interval_seconds, $log_fh) = @_;
    my $href = _build_adaptive_thresholds_hash($raw_interval_seconds, $log_fh);
    return ($href->{saturation}, $href->{target}, $href->{max_reduction});
}

# --- parse_vm_tier_overrides ---
# Parses a simple INI-style file to allow users to override the auto-detected
# tier for specific VMs, giving them fine-grained control.
sub parse_vm_tier_overrides {
    my ($filepath) = @_;
    my %overrides;
    return \%overrides unless (-f $filepath);

    print STDERR "  ↳  VM-specific tier override configuration file: $filepath\n";
    open my $fh, '<:encoding(utf8)', $filepath or do {
        warn " [WARN] Could not open VM tier override file '$filepath': $!. Skipping overrides.";
        return \%overrides;
    };

    my $current_vm = '';
    while (my $line = <$fh>) {
        chomp $line;
        $line =~ s/\s*[#;].*//;
        $line =~ s/^\s+|\s+$//g;
        next if $line eq '';

        if ($line =~ /^\s*\[\s*([^\]]+?)\s*\]\s*$/) {
            $current_vm = $1;
        } elsif ($current_vm ne '' && $line =~ /^\s*tier\s*=\s*(.+)$/i) {
            my $tier = uc($1);
            $tier =~ s/\s+//g;
            $overrides{$current_vm} = $tier;
            $current_vm = ''; # Reset after reading the tier
        }
    }
    close $fh;
    print STDERR "    ✓ Loaded " . scalar(keys %overrides) . " VM-specific tier overrides\n";
    return \%overrides;
}

# ==============================================================================
# SUBROUTINE: build_transform_manifest
# PURPOSE:    Parses all loaded profiles and aggregates their computational
#             requirements into a single "transform manifest". This manifest
#             identifies each unique data transformation needed for PhysC,
#             NormRunQ, and AbsRunQ, ensuring calculation parameters are
#             correctly isolated.
# ==============================================================================

sub build_transform_manifest {
    my ($profiles_ref, $global_runq_avg_method, $runq_behavior) = @_;
    my %manifest;
    $runq_behavior //= 'none'; # Default safety

    # Helper to create a consistent transform key.
    my $_get_transform_key = sub {
        my ($metric, $method, $window, $decay, $filter, $time, $weekends) = @_;
        return join(":", $metric, $method, $window, $decay // '', $filter, $time, $weekends);
    };

    foreach my $profile (@$profiles_ref) {
        my $flags = $profile->{flags};
        my $p_name = $profile->{name};

        # --- Extract all relevant parameters from the profile's flags ---
        # Note: We re-parse these to ensure we respect the exact definition of each profile.
        my ($p_perc)      = $flags =~ /-p\s+([0-9.]+)/;
        my ($w_min)       = $flags =~ /-w\s+(\d+)/;
        my ($avg_method)  = $flags =~ /--avg-method\s+(\w+)/;
        my ($decay)       = $flags =~ /--decay\s+([\w-]+)/;
        my ($runq_decay)  = $flags =~ /--runq-decay\s+([\w-]+)/;
        my ($filter_perc) = $flags =~ /--filter-above-perc\s+([0-9.]+)/;
        my ($rq_norm_str) = $flags =~ /--runq-norm-perc\s+"?([0-9.,\s]+)"?/;
        my ($rq_abs_str)  = $flags =~ /--runq-abs-perc\s+"?([0-9.,\s]+)"?/;
        my $time_filter = 'none';
        if ($flags =~ / -online\b/)     { $time_filter = 'online'; }
        elsif ($flags =~ / -batch\b/)   { $time_filter = 'batch'; }
        my $no_weekends = ($flags =~ / -no-weekends\b/) ? 1 : 0;

        # --- 1. PhysC Transform ---
        my $physc_method = $avg_method // $DEFAULT_AVG_METHOD;
        my $physc_decay = $decay // $DEFAULT_DECAY_LEVEL;
        my $physc_window = $w_min // 15;
        my $physc_filter = $filter_perc // '0';

        my $physc_key = $_get_transform_key->('PhysC', $physc_method, $physc_window, $physc_decay, $physc_filter, $time_filter, $no_weekends);

        # Find or create the transform. CRITICAL: Initialize 'profiles' with a NEW hash ref '{}'.
        $manifest{$physc_key} //= {
            metric => 'PhysC', method => $physc_method, window => $physc_window, decay => $physc_decay,
            filter_perc => $physc_filter, time_filter => $time_filter, no_weekends => $no_weekends,
            profiles => {}
        };

        # Add this profile's directives under the correct transform.
        if (defined $p_perc) {
            my $p_directives = $manifest{$physc_key}{profiles}{$p_name} //= {};
            push @{ $p_directives->{percentiles} }, $p_perc;

            $p_directives->{enable_growth} = 1 if ($flags =~ /--enable-growth-prediction\b/);
            $p_directives->{enable_clipping} = 1 if ($flags =~ /--enable-clipping-detection\b/);
            $p_directives->{calculate_peak} = 1 if ($p_name eq $MANDATORY_PEAK_PROFILE_FOR_HINT || $flags =~ /-k\b|--peak\b/);
        }

        # Auto-Match RunQ Logic (--runq-perc-behavior match)
        # If behaviour is 'match', create a NormRunQ entry mirroring the PhysC profile
        if ($runq_behavior eq 'match' && defined $p_perc) {
            # NormRunQ key: Metric=NormRunQ, Method=Global, Filter=0, Time=Same as PhysC
            # Note: We use '0' for filter_perc as RunQ is rarely filtered by value like PhysC
            my $norm_runq_key = $_get_transform_key->(
                'NormRunQ', $global_runq_avg_method, $w_min, $decay, '0', $time_filter, $no_weekends
            );

            $manifest{$norm_runq_key} //= {
                metric => 'NormRunQ', method => $global_runq_avg_method, window => $w_min, decay => $decay,
                filter_perc => '0', time_filter => $time_filter, no_weekends => $no_weekends,
                profiles => {}
            };

            # Add the same percentile
            my $nr_directives = $manifest{$norm_runq_key}{profiles}{$p_name} //= {};
            push @{ $nr_directives->{percentiles} }, $p_perc;
        }

        # --- 2. RunQ Transforms ---
        my $runq_eff_decay = $runq_decay // $decay // 'medium';
        my $runq_method = ($global_runq_avg_method eq 'none') ? 'none' : ($global_runq_avg_method // $DEFAULT_NFIT_RUNQ_AVG_METHOD);
        my $runq_window = $w_min // 15;

        if (defined $rq_norm_str) {
            my @norm_percs = grep { looks_like_number($_) } split /[,\s]+/, $rq_norm_str;
            push @norm_percs, (25, 50, 75, 90);
            my %seen; @norm_percs = grep { !$seen{$_}++ } @norm_percs;

            if (@norm_percs) {
                my $norm_key = $_get_transform_key->('NormRunQ', $runq_method, $runq_window, $runq_eff_decay, '0', $time_filter, $no_weekends);
                $manifest{$norm_key} //= {
                    metric => 'NormRunQ', method => $runq_method, window => $runq_window, decay => $runq_eff_decay,
                    filter_perc => '0', time_filter => $time_filter, no_weekends => $no_weekends,
                    profiles => {}
                };
                if (@norm_percs) {
                    my $n_directives = $manifest{$norm_key}{profiles}{$p_name} //= {};
                    push @{ $n_directives->{percentiles} }, @norm_percs;
                }
            }
        }

        if (defined $rq_abs_str) {
            my @abs_percs = grep { looks_like_number($_) } split /[,\s]+/, $rq_abs_str;
            push @abs_percs, $p_perc if ($runq_perc_behavior_mode eq 'match' && defined $p_perc);
            my %seen; @abs_percs = grep { !$seen{$_}++ } @abs_percs;

            if (@abs_percs) {
                my $abs_key = $_get_transform_key->('AbsRunQ', $runq_method, $runq_window, $runq_eff_decay, '0', $time_filter, $no_weekends);
                $manifest{$abs_key} //= {
                    metric => 'AbsRunQ', method => $runq_method, window => $runq_window, decay => $runq_eff_decay,
                    filter_perc => '0', time_filter => $time_filter, no_weekends => $no_weekends,
                    profiles => {}
                };
                if (@abs_percs) {
                    my $a_directives = $manifest{$abs_key}{profiles}{$p_name} //= {};
                    push @{ $a_directives->{percentiles} }, @abs_percs;
                }
            }
        }
    }

    # Final pass to de-duplicate percentiles
    foreach my $key (keys %manifest) {
        foreach my $p_name (keys %{ $manifest{$key}{profiles} }) {
            if (exists $manifest{$key}{profiles}{$p_name}{percentiles}) {
                my $percs_ref = \@{ $manifest{$key}{profiles}{$p_name}{percentiles} };
                my %seen; @$percs_ref = grep { !$seen{$_}++ } @$percs_ref;
            }
        }
    }

    # remove profiles that never got valid percentiles:
    foreach my $key (keys %manifest) {
        foreach my $pname (keys %{ $manifest{$key}{profiles} }) {
            my $pd = $manifest{$key}{profiles}{$pname};
            delete $manifest{$key}{profiles}{$pname}
                unless (ref($pd) eq 'HASH' && exists $pd->{percentiles} && @{ $pd->{percentiles} });
        }
    }

    return \%manifest;
}

# ==============================================================================
# SUBROUTINE: _sanitise_manifest_for_history
# PURPOSE:    Transforms a "tactical" manifest (from build_transform_manifest)
#             into a "historical" manifest by surgically removing all predictive
#             and time-weighting directives whilst preserving the profile's
#             fundamental historical definition.
#
# ARGUMENTS:
#   1. $manifest_href (hash ref): The tactical manifest to sanitise.
#
# RETURNS:
#   - A hash reference to the sanitised manifest.
#
# CRITICAL DESIGN PRINCIPLES:
#   This function is the primary data purity guardrail for the history priming
#   process. It implements the rules defined in:
#   "nFit - Maintaining Cache Structure Purity.md"
#
#   What is REMOVED (predictive/time-weighting):
#     - decay keys (inter-period time-weighting)
#     - runq_decay keys (RunQ time-weighting)
#     - enable_growth directives (forward-looking predictions)
#
#   What is PRESERVED (profile definition/historical measurement):
#     - method (sma/ema/wma) - defines how noise is smoothed
#     - window - defines the smoothing window size
#     - time_filter (online/batch) - defines WHAT data to measure
#     - no_weekends - defines operational time boundaries
#     - filter_perc - defines data quality thresholds
#     - calculate_peak - historical maximum measurement
#     - enable_clipping - historical capacity limit detection
#
# RATIONALE:
#   A workload profile like "G3-95W15" is not just "P95". It is explicitly
#   defined as "the P95 of a 15-minute simple moving average, excluding
#   weekends, during online hours". This complete definition must be preserved
#   in the historical record, as it defines the measurement methodology.
#
#   Time-context filters (-online, -no-weekends) are NOT predictive. They
#   define WHAT is being measured (operational hours vs. all hours). Removing
#   them would corrupt the historical record by blending non-operational data
#   into profiles explicitly designed to exclude it.
# ==============================================================================
sub _sanitise_manifest_for_history {
    my ($manifest_href) = @_;

    # Validate input to catch programming errors early
    unless (ref($manifest_href) eq 'HASH') {
        die "FATAL: _sanitise_manifest_for_history requires a hash reference. Received: "
            . (defined $manifest_href ? ref($manifest_href) || 'scalar' : 'undef');
    }

    # Perform a true deep copy to avoid any possibility of modifying the
    # original tactical manifest. This is critical for maintainability.
    use Storable qw(dclone);
    my $sanitised_href = dclone($manifest_href);

    # Iterate through each transform in the manifest
    foreach my $transform_key (keys %$sanitised_href) {
        my $transform = $sanitised_href->{$transform_key};

        # Sanity check: ensure we're working with a proper transform structure
        next unless ref($transform) eq 'HASH';

        # ======================================================================
        # STEP 1: Remove predictive time-weighting (decay)
        # ======================================================================
        # Decay applies recency weighting across time periods, making older
        # data contribute less to the final metric. This is predictive logic
        # (assumes recent behaviour is more relevant for forecasting).
        #
        # For historical records, we want pure statistical aggregation where
        # all time periods in the analysis window are weighted equally.
        #
        # We DELETE the keys entirely rather than setting them to 'none' or 'low'
        # because:
        #   a) Absence of the key causes nfit to perform pure aggregation
        #   b) Even 'low' decay applies some time-weighting
        #   c) Explicit deletion makes the intent unambiguous
        # ======================================================================
        delete $transform->{decay};
        delete $transform->{runq_decay} if exists $transform->{runq_decay};

        # ======================================================================
        # STEP 2: PRESERVE smoothing method and window
        # ======================================================================
        # The method (sma/ema) and window define HOW the profile smooths noise
        # within each time period. This is NOT predictiveÃ¢â‚¬â€it's the measurement
        # methodology.
        #
        # Example: "G3-95W15" explicitly means "P95 of 15-minute SMA"
        #          Changing this would create a different metric entirely.
        #
        # NO CHANGES to: $transform->{method}, $transform->{window}
        # ======================================================================

        # ======================================================================
        # STEP 3: PRESERVE time-context filters
        # ======================================================================
        # Time-context filters define WHAT operational state is being measured:
        #   - time_filter (online/batch): Operational hours vs. batch windows
        #   - no_weekends: Weekday operations vs. full week
        #   - filter_perc: Data quality threshold (exclude idle periods)
        #
        # These are fundamental to the profile's definition and MUST be preserved.
        #
        # A profile defined with "-online -no-weekends" is explicitly measuring
        # "workload during operational business hours". Stripping these filters
        # would contaminate the measurement by including non-operational data.
        #
        # NO CHANGES to: $transform->{time_filter}, $transform->{no_weekends},
        #                $transform->{filter_perc}
        # ======================================================================

        # ======================================================================
        # STEP 4: Sanitise profile-level directives
        # ======================================================================
        next unless exists $transform->{profiles} && ref($transform->{profiles}) eq 'HASH';

        foreach my $profile_name (keys %{ $transform->{profiles} }) {
            my $directives = $transform->{profiles}{$profile_name};
            next unless ref($directives) eq 'HASH';

            # ------------------------------------------------------------------
            # Remove growth prediction (CRITICAL for data purity)
            # ------------------------------------------------------------------
            # Growth prediction is forward-looking and must never appear in
            # historical records. The history cache must contain only "what
            # actually happened", not "what we predict will happen".
            #
            # We use DELETE rather than setting to 0 to make the absence
            # explicit and unambiguous.
            # ------------------------------------------------------------------
            delete $directives->{enable_growth};

            # ------------------------------------------------------------------
            # PRESERVE historical measurement flags
            # ------------------------------------------------------------------
            # These directives capture factual historical measurements:
            #   - calculate_peak: The actual maximum value observed
            #   - enable_clipping: Detection of capacity saturation
            #
            # Both are measurements of "what happened" and belong in the
            # historical record.
            #
            # NO CHANGES to: $directives->{calculate_peak},
            #                $directives->{enable_clipping}
            # ------------------------------------------------------------------
        }
    }

    return $sanitised_href;
}

# ==============================================================================
# SUBROUTINE: run_single_pass_analysis
# PURPOSE:    Orchestrates the single-pass analysis for a given system cache.
#             It builds the manifest, assembles and executes the nfit command
#             with all necessary global flags, and returns the parsed results.
# ==============================================================================
sub run_single_pass_analysis {
    my ($system_cache_dir, $profiles_ref, $args_ref) = @_;

    print STDERR "    [+] Single Pass Engine Analysis\n";

    # 1. Build the manifest using the function already added.
    print STDERR "      • Building transform manifest\n";

    # [FIX] Extract runq_perc_behavior from args (default to 'none' if missing)
    my $runq_behavior = $args_ref->{runq_perc_behavior} // 'none';

    # [FIX] Pass it to the builder function
    my $transform_manifest = build_transform_manifest($profiles_ref, $args_ref->{runq_avg_method}, $runq_behavior);

    # --- INJECT EXCLUSIONS IF PRESENT ---
    if (defined $args_ref->{exclusions}) {
        _inject_exclusions_into_manifest($transform_manifest, $args_ref->{exclusions});
    }

    # *** ADD THIS COMPLETE DIAGNOSTIC BLOCK TO PROVE THE CORRECTNESS OF THE MANIFEST***
#    {
#        warn "\n" . "=" x 70 . "\n";
#        warn "STEP 1: MANIFEST DEBUG IN nfit-profile.pl (After build_transform_manifest)\n";
#        warn "=" x 70 . "\n\n";
#
#        # Test profile to trace
#        my $test_profile = 'B3-95W15';
#
#        # Count PhysC transforms
#        my @physc_transforms = grep {
#            $transform_manifest->{$_}{metric} eq 'PhysC'
#        } keys %$transform_manifest;
#        warn "Total PhysC transforms created: " . scalar(@physc_transforms) . "\n\n";
#
#        # Check which transforms contain our test profile
#        my @transforms_with_test = grep {
#            $transform_manifest->{$_}{metric} eq 'PhysC' &&
#            exists $transform_manifest->{$_}{profiles}{$test_profile}
#        } keys %$transform_manifest;
#
#        warn "Transforms containing '$test_profile': " . scalar(@transforms_with_test) . "\n";
#
#        if (@transforms_with_test) {
#            foreach my $key (sort @transforms_with_test) {
#                my $prof_data = $transform_manifest->{$key}{profiles}{$test_profile};
#                my $has_pct = defined $prof_data->{percentiles};
#                my $pct_val = $has_pct ? "[" . join(",", @{$prof_data->{percentiles}}) . "]" : "UNDEFINED";
#                warn "  - $key\n";
#                warn "    Percentiles: $pct_val\n";
#            }
#        } else {
#            warn "  ERROR: '$test_profile' not found in ANY PhysC transform!\n";
#        }
#
#        warn "\n";
#
#        # Show summary of ALL PhysC transforms and their profiles
#        warn "Complete PhysC Transform Summary:\n";
#        warn "-" x 70 . "\n";
#        foreach my $key (sort @physc_transforms) {
#            my @profiles_in_transform = sort keys %{$transform_manifest->{$key}{profiles}};
#            warn "Transform: $key\n";
#            warn "  Profiles (" . scalar(@profiles_in_transform) . "): " . join(", ", @profiles_in_transform) . "\n";
#
#            # Check if percentiles are defined for each
#            my @with_pct = grep {
#                defined $transform_manifest->{$key}{profiles}{$_}{percentiles}
#            } @profiles_in_transform;
#            my @without_pct = grep {
#                !defined $transform_manifest->{$key}{profiles}{$_}{percentiles}
#            } @profiles_in_transform;
#
#            if (@without_pct) {
#                warn "  WARNING: Profiles without percentiles: " . join(", ", @without_pct) . "\n";
#            }
#        }
#
#        warn "\n" . "=" x 70 . "\n";
#        warn "END STEP 1 MANIFEST DEBUG\n";
#        warn "=" x 70 . "\n\n";
#    }
#    # *** END DIAGNOSTIC BLOCK ***

    # Check if any profile in the manifest requires growth prediction.
    my $any_profile_has_growth = 0;
    foreach my $transform (values %$transform_manifest) {
        foreach my $profile_directives (values %{ $transform->{profiles} }) {
            if ($profile_directives->{enable_growth}) {
                $any_profile_has_growth = 1;
                last;
            }
        }
        last if $any_profile_has_growth;
    }

    # 2. Create a temporary file for the manifest.
    my ($fh_manifest, $manifest_filename) = tempfile(UNLINK => 1);
    print $fh_manifest JSON->new->pretty->encode($transform_manifest);
    close $fh_manifest;

    # --- BEGIN STEP 1 DEBUGGING ---
#    print STDERR "--- DEBUG: TRANSFORM MANIFEST ---\n";
#    my $manifest_json_for_debug = JSON->new->pretty->encode($transform_manifest);
#    print STDERR $manifest_json_for_debug . "\n";
#    print STDERR "Start Date:            " . ($args_ref->{start_date} // 'undef') . "\n";
#    print STDERR "End Date:              " . ($args_ref->{end_date} // 'undef') . "\n";
#    print STDERR "Analysis Reference:    " . ($args_ref->{analysis_reference_date} // 'undef') . "\n";
#    print STDERR "Enable Windowed Decay: " . ($args_ref->{enable_windowed_decay} // '0') . "\n";
#    print STDERR "--- MANIFEST DEBUG END. SCRIPT WILL NOW EXIT. ---\n";
#    exit 0; # Exit early for debugging
    # --- END STEP 1 DEBUGGING ---

    # 3. Assemble the complete nfit command with all preserved global flags.
    my $command = $args_ref->{nfit_path} . " --manifest $manifest_filename";

    # Add required I/O and pass-through global flags.
    $command .= " --nmondir \"$system_cache_dir\"";
    $command .= " " . $args_ref->{rounding_flags} if $args_ref->{rounding_flags};
    $command .= " --smt $args_ref->{default_smt}" if defined $args_ref->{default_smt};
    $command .= " --runq-avg-method $args_ref->{runq_avg_method}" if defined $args_ref->{runq_avg_method};

    # Add optional filtering flags.
    $command .= " --startdate $args_ref->{start_date}" if defined $args_ref->{start_date};
    $command .= " --enddate $args_ref->{end_date}" if defined $args_ref->{end_date};
    $command .= " --vm \"$args_ref->{vm_name}\"" if defined $args_ref->{vm_name};

    # Add flags for specific analysis models, auto-enabling decay for growth
    my $standard_model = 0;
    my $decay_mode_set = 0;
    if ($args_ref->{enable_windowed_decay}) {
        $command .= " --enable-windowed-decay";
        $decay_mode_set = 1;
        $standard_model = 1;
    }
    if ($args_ref->{decay_over_states}) {
        $command .= " --decay-over-states";
        $decay_mode_set = 1;
        $standard_model = 1;
    }

    # If any profile needs growth, a decay model is required. Auto-enable one if not set.
    if ($any_profile_has_growth && !$decay_mode_set) {
        $command .= " --enable-windowed-decay";
        # Optionally print a notice to stderr that a default mode was activated.
        print STDERR "  Φ Auto-enabling windowed-decay for profile-level growth prediction\n";
    }

    # Pass the global growth flag if needed, and other flags like clipping detection
    $command .= " --enable-growth-prediction" if $any_profile_has_growth;
    $command .= " --analysis-reference-date $args_ref->{analysis_reference_date}" if defined $args_ref->{analysis_reference_date};
    $command .= " --enable-clipping-detection" if $args_ref->{enable_clipping_detection}; # Pass this through
    $command .= " -q" unless ($standard_model);  # execute nfit engine silently

    # 4. Execute the nfit engine and capture its output.
    print STDERR "      • Executing nFit Engine";
    printf STDERR " (%s - %s)", $args_ref->{start_date} // '-', $args_ref->{end_date} // '-' if (defined $args_ref->{start_date} or defined $args_ref->{end_date});
    print STDERR "\n";
    my $raw_nfit_output = `$command`;
    my $exit_status = $? >> 8;

    if ($exit_status != 0) {
        die "      [ERROR] nFit Single Pass Engine execution failed for system '$system_cache_dir' with exit code $exit_status.\nCommand: $command\nOutput: $raw_nfit_output";
    }

    # --- BEGIN STEP 2 DEBUGGING ---
    #print STDERR "--- DEBUG: RAW JSON OUTPUT FROM nfit.pl ---\n";
    #print STDERR "nfit.pl Exit Status: " . $exit_status . "\n";
    #print STDERR "Command Executed:\n$command\n\n";
    #print STDERR "Output Received:\n";
    #print STDERR $raw_nfit_output . "\n";
    #print STDERR "--- JSON DEBUG END. SCRIPT WILL NOW EXIT. ---\n";
    #exit 0; # Exit early for debugging
    # --- END STEP 2 DEBUGGING ---

    # 5. Parse and return the final results.
    return parse_nfit_json_output($raw_nfit_output);
}

# Helper for hash references (with safe fallback)
sub _safe_get {
    my ($href, $key, $fallback) = @_;
    return (ref($href) eq 'HASH' && exists $href->{$key}) ? $href->{$key} : $fallback;
}

# ==============================================================================
# SUBROUTINE: build_assimilation_map
# PURPOSE:    Acts as an anti-corruption layer by consuming the raw, nested JSON
#             output from the nfit.pl engine and transforming it into a stable,
#             predictable, and mostly flat Perl hash structure. This map becomes
#             the single source of truth for all subsequent consumer logic within
#             nfit-profile.pl.
# ARGUMENTS:
#   1. $parsed_nfit_results_href (hash ref): The raw, decoded Perl hash from nfit.pl.
#   2. $profiles_aref (array ref): A reference to the global @profiles array.
#   3. $adaptive_runq_saturation_thresh (float): The dynamically calculated threshold for detecting Absolute RunQ saturation.
#        This value is passed directly to 'generate_sizing_hint' to determine the VM's pressure status.
#
# RETURNS:
#   - A hash reference to the fully populated assimilation map.
# ==============================================================================
sub build_assimilation_map {
    my ($parsed_nfit_results_href, $profiles_aref, $adaptive_runq_saturation_thresh) = @_;

    my %assimilation_map;

    # Iterate through each VM returned by the nfit engine.
    foreach my $vm_name (sort keys %{$parsed_nfit_results_href}) {
        my @states_for_vm = @{ $parsed_nfit_results_href->{$vm_name} };
        next unless @states_for_vm;

        # The representative state is used to source common metadata and RunQ metrics.
        my $representative_state = $states_for_vm[-1];
        my $is_aggregated = (ref($representative_state) eq 'HASH' && ($representative_state->{analysisType} // '') =~ /aggregated/i);

        # Initialise the map entry for this VM with a predictable structure.
        my $vm_map = $assimilation_map{$vm_name} = {
            Configuration    => {},
            CoreResults      => { ProfileValues => {}, PeakValue => undef },
            Hinting          => {},
            RunQMetrics      => {},
            Growth           => { min_adj => 0.0, max_adj => 0.0, adjustment => 0.0, rationale => {} },
            RawNfitStates    => \@states_for_vm,
            SeasonalForecast => {},
            CSVModifiers     => {},
            # GrowthRationaleByProfile will store ALL rationales for logging
            GrowthRationaleByProfile => {},
        };

        # --- Block 1: Populate Configuration ---
        my $metadata_block = _safe_dig($representative_state, 'metadata') || {};
        $vm_map->{Configuration} = {
            smt           => _safe_dig($representative_state, 'metadata', 'smt'),
            entitlement   => _safe_dig($representative_state, 'metadata', 'entitlement'),
            max_cpu       => _safe_dig($representative_state, 'metadata', 'max_cpu'),
            virtual_cpus  => _safe_dig($representative_state, 'metadata', 'virtual_cpus'),
            is_capped     => _safe_dig($representative_state, 'metadata', 'capped'),
            pool_id       => _safe_dig($representative_state, 'metadata', 'pool_id'),
            pool_cpu      => _safe_dig($representative_state, 'metadata', 'pool_cpu'),
            serial_number => _safe_dig($representative_state, 'metadata', 'serial_number'),
            proc_type     => _safe_dig($representative_state, 'metadata', 'proc_type'),
            proc_version  => _safe_dig($representative_state, 'metadata', 'proc_version'),
            proc_clock    => _safe_dig($representative_state, 'metadata', 'proc_clock'),
        };

        # --- Block 2: Populate CoreResults (ProfileValues and PeakValue) ---
        if ($is_aggregated) {
            # For aggregated runs, data is already per-profile.
            # Growth is now calculated per profile inside nfit.pl.
            my $physc_metrics = _safe_dig($representative_state, 'metrics', 'physc') || {};
            my @growth_values_for_vm;

            foreach my $profile_name (keys %$physc_metrics) {
                my $profile_data = $physc_metrics->{$profile_name};

                # CRITICAL: Extract the growth-inclusive baseline for RunQ modifier processing.
                # BaseValue from nfit is ALWAYS pre-growth. FinalValue is BaseValue + GrowthAdj.
                my $base_val   = _safe_dig($profile_data, 'BaseValue');
                my $growth_adj = _safe_dig($profile_data, 'GrowthAdj');
                my $final_val  = _safe_dig($profile_data, 'FinalValue'); # Growth-inclusive
                my $rationale  = _safe_dig($profile_data, 'growthRationale');

                # Store the growth-inclusive FinalValue as the starting point for nfit-profile modifiers.
                # This ensures that profile metrics in the CSV include the GrowthAdj.
                # Fallback: If FinalValue is not available, compute it as BaseValue + GrowthAdj.
                my $growth_inclusive_value = $final_val;
                if (!defined $growth_inclusive_value || !looks_like_number($growth_inclusive_value)) {
                    my $base_numeric = (defined $base_val && looks_like_number($base_val)) ? $base_val : 0;
                    my $growth_numeric = (defined $growth_adj && looks_like_number($growth_adj)) ? $growth_adj : 0;
                    $growth_inclusive_value = $base_numeric + $growth_numeric;
                }
                $vm_map->{CoreResults}{ProfileValues}{$profile_name} = $growth_inclusive_value;

                # Store the pre-growth BaseValue separately for audit trail purposes
                $vm_map->{Growth}{base_values}{$profile_name} = $base_val // 0;

                # Store per-profile growth adjustment for later use
                my $growth_adj_for_profile = (defined $growth_adj && looks_like_number($growth_adj)) ? $growth_adj : 0;
                push @growth_values_for_vm, $growth_adj_for_profile;
                $vm_map->{Growth}{adjustments}{$profile_name} = $growth_adj_for_profile;

                # Harvest ALL rationales for audit logging
                if (ref($rationale) eq 'HASH' && scalar keys %$rationale) {
                    $vm_map->{GrowthRationaleByProfile}{$profile_name} = $rationale;
                }

                # ------------------------------------------------------------------
                # Diagnostics (AIX-agnostic): PhysC daily-series diagnostics per profile
                # ------------------------------------------------------------------
                # NOTE: These diagnostics are computed from DailySeries (daily aggregated values),
                #       not raw 1-minute samples. They are intended as *diagnostic signals*.
                my $daily_series = _safe_dig($profile_data, 'DailySeries');
                if (ref($daily_series) eq 'HASH' && scalar(keys %$daily_series) > 0) {
                    my @physc_daily_vals = grep { defined($_) && looks_like_number($_) } values %$daily_series;
                    my $n_daily = scalar @physc_daily_vals;

                    if ($n_daily > 0) {
                        @physc_daily_vals = sort { $a <=> $b } @physc_daily_vals;

                        my $p25 = _calculate_percentile_from_sorted(\@physc_daily_vals, 25);
                        my $p50 = _calculate_percentile_from_sorted(\@physc_daily_vals, 50);
                        my $p75 = _calculate_percentile_from_sorted(\@physc_daily_vals, 75);

                        my $physc_iqrc;
                        if (defined $p50 && $p50 > $FLOAT_EPSILON && defined $p75 && defined $p25) {
                            $physc_iqrc = ($p75 - $p25) / $p50;
                        }

                        # PAE (Peak Above Entitlement / PhysC Above Entitlement) and AAE (Area Above Entitlement),
                        # derived from daily PhysC points (diagnostic approximation).
                        my $ent_diag = (defined $vm_map->{Configuration}{entitlement} && looks_like_number($vm_map->{Configuration}{entitlement}))
                                     ? ($vm_map->{Configuration}{entitlement} + 0)
                                     : 0.0;

                        my $pae_cnt = 0;
                        my $sum_physc = 0.0;
                        my $sum_excess = 0.0;
                        foreach my $v (@physc_daily_vals) {
                            $sum_physc += $v;
                            if ($v > $ent_diag) {
                                $pae_cnt++;
                                $sum_excess += ($v - $ent_diag);
                            }
                        }

                        my $pae = ($n_daily > 0) ? ($pae_cnt / $n_daily) : undef;
                        my $aae = ($sum_physc > $FLOAT_EPSILON) ? ($sum_excess / $sum_physc) : undef;

                        $vm_map->{Diagnostics}{PhysC}{$profile_name} = {
                            DailyCount => $n_daily,
                            P25        => (defined $p25 ? ($p25 + 0) : undef),
                            P50        => (defined $p50 ? ($p50 + 0) : undef),
                            P75        => (defined $p75 ? ($p75 + 0) : undef),
                            IQRC       => (defined $physc_iqrc ? ($physc_iqrc + 0) : undef),
                            PAE        => (defined $pae ? ($pae + 0) : undef),
                            AAE        => (defined $aae ? ($aae + 0) : undef),
                        };
                    }
                }
            }

            # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
            # Calculate min/max from non-zero growth values only
            # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
            # RATIONALE: Zero growth means "no trend detected", not "minimum is zero".
            # Capacity planners need the actual range of predicted growth values.
            # - If all profiles have 0 growth → min=0, max=0 (correct: no growth)
            # - If some profiles have growth → show min/max of non-zero values
            # - Negative growth (declining trends) is preserved in the range
            # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

            # Filter to non-zero growth values (preserving negatives)
            my @non_zero_growth = grep { abs($_) > 1e-9 } @growth_values_for_vm;

            if (@non_zero_growth) {
                # Calculate range from actual growth predictions
                $vm_map->{Growth}{max_adj} = max(@non_zero_growth);
                $vm_map->{Growth}{min_adj} = min(@non_zero_growth);
            } else {
                # No growth detected on any profile
                $vm_map->{Growth}{max_adj} = 0.0;
                $vm_map->{Growth}{min_adj} = 0.0;
            }

            # The PeakValue comes from the raw peak tracker, not a specific profile's aggregated value.
            $vm_map->{CoreResults}{PeakValue} = _safe_dig($representative_state, 'metadata', 'peakPhyscFromLatestState');

       } else {
            # Logic for per-state runs remains correct.
            my %profile_sums;
            my %profile_counts;
            foreach my $profile (@$profiles_aref) {
                my ($p_val_num) = $profile->{flags} =~ /(?:-p|--percentile)\s+([0-9.]+)/;
                my $p_metric_key = "P" . clean_perc_label($p_val_num // $DEFAULT_PERCENTILE);
                foreach my $state (@states_for_vm) {
                    my $metric_val = _safe_dig($state, 'metrics', 'physc', $profile->{name}, $p_metric_key);
                    if (defined $metric_val && looks_like_number($metric_val)) {
                        $profile_sums{$profile->{name}} += $metric_val;
                        $profile_counts{$profile->{name}}++;
                    }
                }
            }
            foreach my $profile_name (keys %profile_sums) {
                if ($profile_counts{$profile_name} > 0) {
                    $vm_map->{CoreResults}{ProfileValues}{$profile_name} = $profile_sums{$profile_name} / $profile_counts{$profile_name};
                }
            }
            my @peak_values;
            foreach my $state (@states_for_vm) {
                my $peak_val = _safe_dig($state, 'metrics', 'physc', $MANDATORY_PEAK_PROFILE_FOR_HINT, 'Peak');
                push @peak_values, $peak_val if (defined $peak_val && looks_like_number($peak_val));
            }
            $vm_map->{CoreResults}{PeakValue} = max(@peak_values) if @peak_values;
        }

        # --- Block 3: Populate RunQMetrics (Comprehensive) ---
        # This loop iterates through ALL profiles to find and collate every RunQ
        # metric calculated by the nfit engine into a single, unified block.
        $vm_map->{RunQMetrics}{SourceProfile} = $MANDATORY_PEAK_PROFILE_FOR_HINT; # P-99W1 remains the default source
        foreach my $profile (@$profiles_aref) {
            my $p_name = $profile->{name};
            my $runq_norm_metrics = _safe_dig($representative_state, 'metrics', 'runq', 'normalized', $p_name) || {};
            my $runq_abs_metrics  = _safe_dig($representative_state, 'metrics', 'runq', 'absolute', $p_name) || {};
            foreach my $key (keys %$runq_norm_metrics) {
                $vm_map->{RunQMetrics}{"NormRunQ_$key"} //= $runq_norm_metrics->{$key};
            }
            foreach my $key (keys %$runq_abs_metrics) {
                $vm_map->{RunQMetrics}{"AbsRunQ_$key"} //= $runq_abs_metrics->{$key};
            }
        }

        # Calculate IQRC.
        my ($p25, $p50, $p75) = ($vm_map->{RunQMetrics}{'NormRunQ_P25'}, $vm_map->{RunQMetrics}{'NormRunQ_P50'}, $vm_map->{RunQMetrics}{'NormRunQ_P75'});
        if (defined $p50 && $p50 > $FLOAT_EPSILON && defined $p75 && defined $p25) {
            $vm_map->{RunQMetrics}{IQRC} = ($p75 - $p25) / $p50;
        }

        # --- Block 4: Populate Growth ---
        if ($is_aggregated) {
            # In this new logic, we intentionally leave Growth.adjustment and
            # Growth.rationale EMPTY.
            # They will be populated dynamically in the consumer functions
            # (log_profile_rationale and _write_standard_csv_report)
            # based on the correct profile context.
            $vm_map->{Growth}{adjustment} = 0.0; # Default to 0
            $vm_map->{Growth}{rationale}  = {};  # Default to empty
        }
    }
    return \%assimilation_map;
}

# ==============================================================================
# SUBROUTINE: _acquire_history_lock
# PURPOSE:    Acquires an EXCLUSIVE lock on the history infrastructure.
#             This enables safe read-modify-write operations across the
#             transition from Monolith -> Partitioned architecture.
# RETURNS:    ($fh, $lock_path) - Filehandle and path. Keep $fh open to hold lock.
# ==============================================================================
sub _acquire_history_lock {
    my ($system_cache_dir) = @_;

    my $lock_file = File::Spec->catfile($system_cache_dir, $HISTORY_LOCK_FILENAME);

    open my $lock_fh, '>', $lock_file
        or die "FATAL: Could not create lock file '$lock_file': $!";

    # Blocking lock - wait until acquired
    flock($lock_fh, LOCK_EX)
        or die "FATAL: Could not acquire exclusive lock on '$lock_file': $!";

    # Auto-flush to ensure lock intent is registered if we write pid later
    my $old_fh = select($lock_fh); $| = 1; select($old_fh);

    return ($lock_fh, $lock_file);
}

# ==============================================================================
# SUBROUTINE: _migrate_history_to_partitioned
# PURPOSE:    One-time migration of monolithic history file to partitioned layout.
#             Preserves data integrity by creating the directory, populating it,
#             and renaming the legacy file only upon success.
# ==============================================================================
sub _migrate_history_to_partitioned {
    my ($system_cache_dir) = @_;

    my $legacy_file = File::Spec->catfile($system_cache_dir, '.nfit.history.json');
    my $partition_dir = File::Spec->catfile($system_cache_dir, '.nfit.history');

    return unless -f $legacy_file; # Nothing to migrate

    # This function is called INSIDE a lock, so we don't re-acquire it here.

    print "  - Migrating legacy history to partitioned format.\n";

    # 1. Load Legacy Data
    my $json_text = do {
        open my $fh, '<:encoding(utf8)', $legacy_file or die "FATAL: Cannot read legacy history: $!";
        local $/; <$fh>;
    };
    my $data = JSON->new->decode($json_text);

    # 2. Create Directory
    unless (-d $partition_dir) {
        make_path($partition_dir) or die "FATAL: Cannot create history partition directory: $!";
    }

    # 3. Write Partition Files
    my $json_encoder = JSON->new->pretty->canonical;

    foreach my $month_key (keys %$data) {
        # We wrap the data in the month key to maintain the exact structure
        # { "YYYY-MM": { ... } } inside the file. This simplifies stitching.
        my $month_payload = { $month_key => $data->{$month_key} };

        # Naming Convention: nfit.hist.YYYY-MM.json
        my $filename = "nfit.hist.${month_key}.json";
        my $filepath = File::Spec->catfile($partition_dir, $filename);

        open my $fh, '>:encoding(utf8)', $filepath
            or die "FATAL: Cannot write partition file $filename: $!";
        print $fh $json_encoder->encode($month_payload);
        close $fh;
    }

    # 4. Rename Legacy File (The "Commit" operation)
    my $migrated_name = $legacy_file . ".migrated";
    rename($legacy_file, $migrated_name)
        or die "FATAL: Failed to rename legacy file after migration: $!";

    print "  - Migration complete. Legacy file renamed to .migrated.\n";
}

# ==============================================================================
# SUBROUTINE: calculate_sens_slope
# PURPOSE:    Calculate Theil-Sen slope estimator (median of pairwise slopes)
# ARGUMENTS:
#   1. $series_aref - Array ref of [x, y] pairs (time series points)
# RETURNS:
#   Hash ref: { slope => $value, n_slopes => $count, method => 'exact_pairs' }
#   Returns undef if insufficient data (n < 2)
# ==============================================================================
sub calculate_sens_slope {
    my ($series_aref) = @_;
    my $n = scalar @{$series_aref};

    # Need at least 2 points to calculate a slope
    return undef if $n < 2;

    my @slopes;
    my $FLOAT_EPSILON = 1e-9;

    # Calculate all pairwise slopes: slope_ij = (y_j - y_i) / (x_j - x_i)
    for my $i (0 .. $n-2) {
        for my $j ($i+1 .. $n-1) {
            my ($x1, $y1) = @{$series_aref->[$i]};
            my ($x2, $y2) = @{$series_aref->[$j]};

            my $delta_x = $x2 - $x1;

            # Only calculate slope if x values are distinct
            if (abs($delta_x) > $FLOAT_EPSILON) {
                push @slopes, ($y2 - $y1) / $delta_x;
            }
        }
    }

    # If no valid slopes could be calculated, return undef
    return undef if @slopes == 0;

    # Sen's slope is the median of all pairwise slopes
    my $median_slope = _calculate_median(\@slopes);

    return {
        slope     => $median_slope,
        n_slopes  => scalar(@slopes),
        method    => 'exact_pairs',
    };
}

# ==============================================================================
# SUBROUTINE: _calculate_median
# PURPOSE:    Calculate median of an array of numbers
# ARGUMENTS:
#   1. $values_aref - Array ref of numeric values
# RETURNS:
#   Median value (scalar)
# ==============================================================================
sub _calculate_median {
    my ($values_aref) = @_;

    # Sort values in ascending order
    my @sorted = sort { $a <=> $b } @{$values_aref};
    my $n = scalar @sorted;

    # For odd n, return middle value
    if ($n % 2 == 1) {
        return $sorted[int($n/2)];
    }
    # For even n, return average of two middle values
    else {
        return ($sorted[$n/2 - 1] + $sorted[$n/2]) / 2;
    }
}

# --- usage_wrapper ---
# Generates and returns the usage/help message for the script.
sub usage_wrapper
{
    my $script_name = $0;
    $script_name =~ s{.*/}{}; # Get only script name, remove path
    return <<END_USAGE;
Usage: $script_name (--mgsys <serial> | --nmondir <directory>) <pc_file> [options]

nfit-profile is a workload profiling and forecasting orchestrator for capacity planning.

It executes the nfit profiling engine across configured workload profiles and models to
characterise baseline behaviour, operational demand, and future peak risk.
The resulting outputs are designed for forensic analysis, operational decision-making,
and forward-looking capacity forecasts.

Supported capabilities include:
  - Workload profiling (state-based, forensic)
  - Operational forecasting (hybrid state–time decay, run-queue aware)
  - Seasonal and business-cycle forecasting (recency-anchored decay, multiplicative seasonal, predictive peak)

CSV reports and model-specific rationale logs are written to $LOG_FILE_DIR.

INPUT SELECTION:
  --mgsys <serial>          : Managed system serial to analyse.
                              If --nmondir is not provided, the staged data cache
                               is derived from the default base path ('./stage/').
                              May be combined with --nmondir to select a system
                               from a non-default staged cache hierarchy.

  --nmondir <directory>     : Path to a staged data cache directory.
                                The directory must be one of the following:
                                 - a base directory containing multiple systems (multi-cache mode), or
                                 - a single system cache directory (single-cache mode).

  --default-smt, --smt <N>  : Optional default SMT level for RunQ calculations.
                               (Default: $DEFAULT_SMT_VALUE_PROFILE)

OVERVIEW OF WORKLOAD PROFILING, MODELLING, AND FORECASTING MODES:
  (default)
        Workload Profiling
        State-based, forensic workload characterisation without time decay.
        Suitable for historical analysis and configuration investigations.

  --enable-windowed-decay
        Operational Forecasting (Time-Based Windowed Decay)
        Applies recency weighting over a moving time window to produce
        short-term, operational capacity forecasts.

  --decay-over-states
        Operational Forecasting (Hybrid State–Time Decay)
        Synthesises state-based results first, then applies recency decay.
        Designed for stable environments with infrequent configuration changes.

        Both operational forecasting modes may incorporate:
          - Run-queue pressure signals
          - Growth adjustment using robust trend estimation (Theil–Sen)

  --apply-seasonality <event>
        Seasonal Forecasting (Event-Specific)
        Executes the forecasting model configured for a single specified event.
        Models are configured in `etc/nfit.seasonality.cfg` and may include:
          - Recency-Anchored Decay (recency_decay)
          - Multiplicative Seasonal (multiplicative_seasonal)
          - Predictive Peak (robust regression: Theil–Sen) (predictive_peak)

  --seasonal
        Seasonal Forecasting (Automatic, comprehensive)
        Executes all applicable seasonal forecasting models mentioned above,
        as defined in etc/nfit.seasonality.cfg.

RUN-QUEUE METRIC CONFIGURATION:

  --runq-norm-percentiles <list>            : Percentiles for normalised run-queue analysis
                                              (Default: "$DEFAULT_RUNQ_NORM_PERCS").
                                              The specified list is combined with profile-specific settings and ensures P50,P90.

  --runq-abs-percentiles <list>             : Percentiles for absolute run-queue analysis
                                              (Default: "$DEFAULT_RUNQ_ABS_PERCS").
                                              The specified list is combined with profile-specific settings and ensures P90.

  --runq-perc-behavior <mode>               : Manages how run-queue pressure is translated into additive CPU recommendations.
                                              fixed (default) : Uses the 90th percentile of run-queue pressure for additive CPU calculations.
                                              match           : Matches the run-queue percentile to the workload profile’s PhysC percentile.

SEASONALITY AND BUSINESS-CYCLE FORECASTING:

Seasonal analysis is driven by the configuration in etc/nfit.seasonality.cfg.

Model behaviour depends on the event configuration:
  - Multiplicative seasonal models
      Produce multiple CSV outputs (final forecast, current baseline, and historical snapshot)
  - Recency-decay models (e.g. month-end)
      Anchor the forecast to the last completed peak period to avoid start-of-period dilution
  - Predictive peak models
      Estimate future peak demand using robust statistical projection

  --apply-seasonality <event>               : Execute the configured forecasting model for a specific business event.

  --seasonal                                : Execute all configured seasonal models applicable to the analysis period.

  --analysis-reference-date <YYYY-MM-DD>    : Seasonal Analysis Reference Date (As-of Anchor)
                                              Sets the "as-of" date used to evaluate whether seasonal events are
                                              complete/active and therefore eligible for forecasting.

                                              Default: effective analysis end date (-e), otherwise the last
                                              recorded date in the staged data cache.
                                              Must precede --enddate if both are set.

  --update-history                          : Populate or extend the historical system snapshot cache.
                                              Discovers completed months and seasonal events from the staged data cache
                                              and records workload and forecasting snapshots for advanced analysis and adaptive scoring.

  --min-history-days <N>                    : When updating history, allow partial months with at least N days of data
                                              (Default: 28 days).

  --force                                   : Override validation checks. Use with caution.
  --reset-seasonal-cache                    : Remove cached seasonal snapshots (this feature is not yet implemented).

RECENCY AND DECAY CONTROLS:

  --enable-windowed-decay                   : Enable time-based windowed processing.
  --decay-over-states                       : Enable hybrid state–time decay.
                                              (Mutually exclusive with windowed decay)

  --process-window-unit <days|weeks>        : Unit for the processing window size.
                                              (Default: $DEFAULT_PROCESS_WINDOW_UNIT_FOR_NFIT)
  --process-window-size <N>                 : Size of the processing window in the selected unit.
                                              (Default: $DEFAULT_PROCESS_WINDOW_SIZE_FOR_NFIT)

  --decay-half-life-days <N>                : Half-life for recency weighting.
                                             (Default: $DEFAULT_DECAY_HALF_LIFE_DAYS_FOR_NFIT)

  --runq-avg-method <none|sma|ema>          : Averaging method applied to RunQ data before percentile calculation.
                                              (Default: $DEFAULT_NFIT_RUNQ_AVG_METHOD)

CONFIGURATION FILES:
  -config <vm_cfg_csv>                      : Optional. VM configuration CSV file.
  --profiles-config <path>                  : Optional. Profiles definition file (INI format).
                                              Can contain 'runq_modifier_behavior = additive_only' per profile,
                                              and profile-specific 'nfit_flags' including --runq-norm-perc/--runq-abs-perc.

ANALYSIS FILTERING OPTIONS:
  -s, --startdate <YYYY-MM-DD>              : Global start date for analysis. Optional.
                                              Limits the data considered for analysis and forecasting.
                                              Data earlier than this date will be ignored, even if present
                                              in the current staged data cache.
  -e, --enddate <YYYY-MM-DD>                : Global end date for analysis. Optional.
                                              Limits the data considered for analysis and forecasting.
                                              Data later than this date will be ignored, even if present
                                              in the current staged data cache.

  -vm, --lpar <name>                        : Analyse only the specified VM/LPAR name (passed to nfit). Optional.

ROUNDING OPTIONS:
  -r[=increment]                            : Optional. nfit rounds results to NEAREST increment.
  -u[=increment]                            : Optional. nfit rounds results UP to nearest increment.
                                              (Default increment: $DEFAULT_ROUND_INCREMENT)
OTHER:
  --nfit-path <path>                        : Optional. Path to the 'nfit' script.
  -h, --help                                : Display this help message.
  -v, --version                             : Display script version and nfit version used.

CSV OUTPUT OPTIONS:
  --excel-formulas[=<true|false>]           : Optional. Add Excel-specific formula columns and summary footer.
                                              (Default: false).

OUTPUT CSV COLUMNS (MODEL-DEPENDENT):

All CSV outputs share a common base structure:
  VM, TIER, Hint, Pattern, Pressure, PressureDetail, SMT, Serial, SystemType, Pool Name, Pool ID, Peak, [Profile Columns], Current - ENT
  (Note: Workload profile values are potentially growth-adjusted & RunQ-modified)

Additional columns are model-dependent:
  Operational Forecasting (Windowed / Hybrid Decay):
    RunQ_Tactical, RunQ_Strategic, RunQ_Potential, RunQ_Source
    GrowthAdj, GrowthAdj_Min, GrowthAdj_Max, GrowthAdj_Source, ProjectionDays, GrowthMethod, GrowthConfidence, GrowthTrend, GrowthSignificant

  Seasonal Forecasting Models:
    Recency-Anchored Decay:
      GrowthAdj, GrowthAdj_Min, GrowthAdj_Max, GrowthAdj_Source, ProjectionDays, GrowthMethod, GrowthConfidence, GrowthTrend, GrowthSignificance
    Multiplicative Seasonal:
      ActiveEvents, Baseline_P99, BaselineSource, ForecastModel
    Predictive Peak:
      ActiveEvents, P99_PredictedDelta, Baseline_P99, BaselineSource, ForecastModel

END_USAGE
}

# ==============================================================================
# SUBROUTINE: _parse_exclusion_dates
# PURPOSE:    Parses the exclude_dates configuration into a structured format,
#             expanding wildcards against the known VM list.
# ARGUMENTS:
#   1. $raw_value    - Raw string value from config (may be multiline/accumulated)
#   2. $known_vms_aref - Reference to array of known VM names (for wildcard expansion)
# RETURNS:
#   Hash ref with 'global' and 'vm_specific' arrays
# ==============================================================================
sub _parse_exclusion_dates {
    my ($raw_value, $known_vms_aref) = @_;

    return undef unless defined $raw_value && $raw_value ne '';

    my %exclusions = (
        global      => [],
        vm_specific => {},
    );

    # Build VM lookup for wildcard matching
    my @known_vms = @{$known_vms_aref // []};

    # Split on commas and newlines, trim whitespace
    my @entries = grep { $_ ne '' }
                  map { s/^\s+|\s+$//gr }
                  split /[,\n]+/, $raw_value;

    foreach my $entry (@entries) {
        # Expected format: VM_PATTERN:START_DATE:END_DATE
        # Robust parsing with whitespace tolerance
        my @parts = split /:/, $entry;
        next unless scalar @parts >= 2;

        my $vm_pattern = $parts[0] // '';
        my $start_date = $parts[1] // '';
        my $end_date   = $parts[2] // $start_date;  # Single date if no end

        # Trim whitespace from each part
        s/^\s+|\s+$//g for ($vm_pattern, $start_date, $end_date);

        # Validate date format (YYYY-MM-DD)
        unless ($start_date =~ /^\d{4}-\d{2}-\d{2}$/ && $end_date =~ /^\d{4}-\d{2}-\d{2}$/) {
            warn "WARNING: Invalid date format in exclude_dates: '$entry'. Skipping.\n";
            next;
        }

        # Expand date range into individual dates
        my @date_range = _expand_date_range($start_date, $end_date);

        # Handle VM pattern
        if ($vm_pattern eq '*' || uc($vm_pattern) eq 'ALL') {
            # Global exclusion - applies to all VMs
            push @{$exclusions{global}}, @date_range;
        }
        elsif ($vm_pattern =~ /\*/) {
            # Wildcard pattern (e.g., "DB-*") - expand against known VMs
            my $regex = $vm_pattern;
            $regex =~ s/\*/\.\*/g;  # Convert glob to regex
            $regex = qr/^$regex$/i;

            my @matching_vms = grep { $_ =~ $regex } @known_vms;

            if (scalar @matching_vms == 0) {
                warn "WARNING: Wildcard pattern '$vm_pattern' matched no known VMs. Skipping.\n";
                next;
            }

            foreach my $matched_vm (@matching_vms) {
                push @{$exclusions{vm_specific}{$matched_vm}}, @date_range;
            }
        }
        else {
            # Explicit VM name
            push @{$exclusions{vm_specific}{$vm_pattern}}, @date_range;
        }
    }

    # De-duplicate all arrays
    my %seen;
    @{$exclusions{global}} = grep { !$seen{$_}++ } @{$exclusions{global}};

    foreach my $vm (keys %{$exclusions{vm_specific}}) {
        my %vm_seen;
        @{$exclusions{vm_specific}{$vm}} = grep { !$vm_seen{$_}++ }
                                            @{$exclusions{vm_specific}{$vm}};
    }

    return \%exclusions;
}

# ==============================================================================
# SUBROUTINE: _expand_date_range
# PURPOSE:    Expands a start/end date pair into a list of individual dates.
# ==============================================================================
sub _expand_date_range {
    my ($start_str, $end_str) = @_;

    my @dates;

    eval {
        my $current = Time::Piece->strptime($start_str, '%Y-%m-%d');
        my $end     = Time::Piece->strptime($end_str, '%Y-%m-%d');

        # Sanity check: end must be >= start
        if ($end < $current) {
            warn "WARNING: End date '$end_str' is before start date '$start_str'. Swapping.\n";
            ($current, $end) = ($end, $current);
        }

        # Safety limit: prevent runaway expansion
        my $max_days = 366;  # One year maximum
        my $day_count = 0;

        while ($current <= $end && $day_count < $max_days) {
            push @dates, $current->strftime('%Y-%m-%d');
            $current += ONE_DAY;
            $day_count++;
        }

        if ($day_count >= $max_days) {
            warn "WARNING: Date range '$start_str:$end_str' exceeds $max_days days. Truncated.\n";
        }
    };

    if ($@) {
        warn "WARNING: Failed to parse date range '$start_str:$end_str': $@\n";
        return ();
    }

    return @dates;
}

# ==============================================================================
# SUBROUTINE: _get_known_vms_from_cache
# PURPOSE:    Retrieves the list of known VM names from the L1 cache or states.
# ARGUMENTS:
#   1. $system_cache_dir - Path to the system's cache directory
# RETURNS:
#   Array reference of VM names
# ==============================================================================
sub _get_known_vms_from_cache {
    my ($system_cache_dir) = @_;

    my @known_vms;

    # Try states cache first (faster, structured)
    my $states_file = File::Spec->catfile($system_cache_dir, '.nfit.cache.states');
    if (-f $states_file) {
        eval {
            open my $fh, '<:encoding(utf8)', $states_file or die $!;
            local $/;
            my $states = decode_json(<$fh>);
            close $fh;
            @known_vms = keys %$states;
        };
        return \@known_vms if @known_vms;
    }

    # Fallback: scan data cache header or first N lines
    my $data_file = File::Spec->catfile($system_cache_dir, '.nfit.cache.data');
    if (-f $data_file) {
        eval {
            open my $fh, '<', $data_file or die $!;
            <$fh>;  # Skip header
            my %seen_vms;
            my $line_count = 0;
            while (my $line = <$fh>) {
                last if $line_count++ > 10000;  # Sample first 10K lines
                my (undef, $vm) = split ',', $line, 3;
                $seen_vms{$vm} = 1 if defined $vm;
            }
            close $fh;
            @known_vms = keys %seen_vms;
        };
    }

    return \@known_vms;
}

# ==============================================================================
# SUBROUTINE: _compute_exclusion_fingerprint
# PURPOSE:    Computes a deterministic fingerprint for the exclusion set.
# ==============================================================================
sub _compute_exclusion_fingerprint {
    my ($exclusions_href) = @_;
    return 'NONE' unless (defined $exclusions_href &&
                          (scalar @{$exclusions_href->{global} // []} > 0 ||
                           scalar keys %{$exclusions_href->{vm_specific} // {}} > 0));

    # Canonical representation for deterministic hashing
    my @parts;
    push @parts, "G:" . join(",", sort @{$exclusions_href->{global} // []});

    foreach my $vm (sort keys %{$exclusions_href->{vm_specific} // {}}) {
        push @parts, "$vm:" . join(",", sort @{$exclusions_href->{vm_specific}{$vm}});
    }

    require Digest::MD5;
    return Digest::MD5::md5_hex(join("|", @parts));
}

# ==============================================================================
# SUBROUTINE: _compute_event_definition_fingerprint (Phase 4: Idempotency)
# PURPOSE:    Computes a deterministic fingerprint for a normalised event
#             definition. Changes to any configuration element that affects
#             forecast computation will produce a different fingerprint.
# ARGUMENTS:
#   $event_cfg (hashref) - The resolved event configuration
# RETURNS:
#   String - MD5 hex digest of the canonical event representation
# ==============================================================================
sub _compute_event_definition_fingerprint {
    my ($event_cfg) = @_;
    return 'NONE' unless (defined $event_cfg && ref($event_cfg) eq 'HASH');

    require Digest::MD5;

    # Canonical keys that affect computation (alphabetically sorted for stability)
    # Includes user-modifiable fields that should trigger reprocessing:
    #   - dates, exclude_dates, vms, exclude_vms, last_modified
    my @fingerprint_keys = qw(
        aggregation_method
        baseline_period_days
        confidence_level
        dates
        day_of_period
        description
        duration_days
        exclude_dates
        exclude_vms
        fallback_event
        interaction_dampening_factor
        interaction_policy
        last_modified
        min_history_days
        min_historical_peaks
        min_historical_years
        model
        outlier_detection
        peak_amplification_factor
        period
        priority
        regime_detection
        residual_peak_profile
        seasonal_confidence_level
        seasonal_lookback_days
        source_event
        vms
        volatility_adjustment
    );

    my @parts;
    foreach my $key (sort @fingerprint_keys) {
        my $val = $event_cfg->{$key};
        if (defined $val) {
            # Normalise whitespace and case for string values
            if (!ref($val)) {
                $val =~ s/^\s+|\s+$//g;  # Trim
                $val =~ s/\s+/ /g;       # Collapse internal whitespace
            }
            push @parts, "$key=$val";
        }
    }

    # Include VM scope filter fingerprint if present
    if (exists $event_cfg->{_vm_scope_filter} && ref($event_cfg->{_vm_scope_filter}) eq 'HASH') {
        my $scope = $event_cfg->{_vm_scope_filter};
        if ($scope->{include} && ref($scope->{include}) eq 'HASH') {
            push @parts, "vms_include=" . join(",", sort keys %{$scope->{include}});
        }
        if ($scope->{exclude} && ref($scope->{exclude}) eq 'HASH') {
            push @parts, "vms_exclude=" . join(",", sort keys %{$scope->{exclude}});
        }
    }

    my $canonical = join("|", @parts);
    return Digest::MD5::md5_hex($canonical);
}

# ==============================================================================
# SUBROUTINE: _compute_execution_context_fingerprint (Phase 4: Idempotency)
# PURPOSE:    Computes a deterministic fingerprint for the execution context
#             of a bucket run. This captures runtime parameters that affect
#             the result.
# ARGUMENTS:
#   $exec_ctx (hashref) - The bucket execution context
#   $model_type (string) - Model identifier
# RETURNS:
#   String - MD5 hex digest of the canonical execution context
# ==============================================================================
sub _compute_execution_context_fingerprint {
    my ($exec_ctx, $model_type) = @_;
    return 'NONE' unless (defined $exec_ctx && ref($exec_ctx) eq 'HASH');

    require Digest::MD5;

    my @parts;

    # Core temporal boundaries
    if ($exec_ctx->{effective_start_date} && ref($exec_ctx->{effective_start_date}) eq 'Time::Piece') {
        push @parts, "start=" . $exec_ctx->{effective_start_date}->ymd;
    }
    if ($exec_ctx->{effective_end_date} && ref($exec_ctx->{effective_end_date}) eq 'Time::Piece') {
        push @parts, "end=" . $exec_ctx->{effective_end_date}->ymd;
    }

    # Anchor information
    push @parts, "anchor_bucket=" . ($exec_ctx->{anchor_bucket} // 'unknown');
    push @parts, "model=" . ($model_type // 'unknown');

    # Cache bounds (ground truth)
    if ($exec_ctx->{cache_start_date} && ref($exec_ctx->{cache_start_date}) eq 'Time::Piece') {
        push @parts, "cache_start=" . $exec_ctx->{cache_start_date}->ymd;
    }
    if ($exec_ctx->{cache_end_date} && ref($exec_ctx->{cache_end_date}) eq 'Time::Piece') {
        push @parts, "cache_end=" . $exec_ctx->{cache_end_date}->ymd;
    }

    # Engine version (ensures stale results are invalidated on logic changes)
    push @parts, "engine_version=$SEASONAL_ENGINE_VERSION";

    my $canonical = join("|", sort @parts);
    return Digest::MD5::md5_hex($canonical);
}

# ==============================================================================
# SUBROUTINE: _compute_bucket_fingerprint (Phase 4: Idempotency)
# PURPOSE:    Computes the combined fingerprint for a bucket. This is the
#             primary fingerprint used for skip/redo decisions.
# ARGUMENTS:
#   $event_cfg (hashref)  - The resolved event configuration
#   $exec_ctx (hashref)   - The bucket execution context
#   $model_type (string)  - Model identifier
#   $prev_bucket_fp (string|undef) - Fingerprint of the prior bucket (chain-of-custody)
# RETURNS:
#   Hashref containing:
#     combined     - The combined bucket fingerprint
#     event_def    - Event definition fingerprint
#     exec_ctx     - Execution context fingerprint
#     chain        - Chain-of-custody fingerprint (may be 'NONE' for first bucket)
#     engine_version - Current engine version
# ==============================================================================
sub _compute_bucket_fingerprint {
    my ($event_cfg, $exec_ctx, $model_type, $prev_bucket_fp, $dependency_fp, $baseline_fp) = @_;

    require Digest::MD5;

    my $event_def_fp = _compute_event_definition_fingerprint($event_cfg);
    my $exec_ctx_fp  = _compute_execution_context_fingerprint($exec_ctx, $model_type);

    # Chain-of-custody: hash of previous bucket's combined fingerprint
    my $chain_fp = 'NONE';
    if (defined $prev_bucket_fp && $prev_bucket_fp ne '' && $prev_bucket_fp ne 'NONE') {
        $chain_fp = Digest::MD5::md5_hex("chain:$prev_bucket_fp");
    }

    # Upstream Dependencies (Phase 4 addition)
    # Defaults to 'NONE' to ensure the hash is stable even if dependencies are empty
    my $dep_fp_str = (defined $dependency_fp && $dependency_fp ne '') ? $dependency_fp : 'NONE';

    # Phase 5: Baseline fingerprint (History Based Baselining)
    # When provided, a change in baseline data triggers staleness detection.
    # Defaults to 'NONE' for backward compatibility with pre-baselining snapshots.
    my $baseline_fp_str = (defined $baseline_fp && $baseline_fp ne '') ? $baseline_fp : 'NONE';

    # Canonical Assembly
    # We use a hash structure and JSON encoding to ensure deterministic key sorting
    # and clean extensibility without manually managing pipe delimiters.
    my $fingerprint_components = {
        baseline       => $baseline_fp_str,
        event_def      => $event_def_fp,
        exec_ctx       => $exec_ctx_fp,
        chain          => $chain_fp,
        dependencies   => $dep_fp_str,
        # Ensure engine version is captured if available in scope, else default
        engine_version => $SEASONAL_ENGINE_VERSION // '1.0',
    };

    # Encode with options to ensure stability:
    # canonical (sort keys), utf8, allow_nonref
    my $json_struct = JSON->new->utf8->canonical->allow_nonref->encode($fingerprint_components);

    # Combined fingerprint
    my $combined = Digest::MD5::md5_hex($json_struct);

    return {
        combined       => $combined,
        baseline       => $baseline_fp_str,
        event_def      => $event_def_fp,
        exec_ctx       => $exec_ctx_fp,
        chain          => $chain_fp,
        dependencies   => $dep_fp_str,
        engine_version => $fingerprint_components->{engine_version},
    };
}

# ==============================================================================
# SUBROUTINE: _load_stored_bucket_fingerprint (Phase 4: Idempotency)
# PURPOSE:    Loads the stored fingerprint metadata for a bucket from history.
#             Uses memoisation to avoid repeated disk reads within the same
#             nfit-profile invocation.
# ARGUMENTS:
#   $system_cache_dir (string) - Path to system cache directory
#   $event_name (string)       - Event name
#   $model_type (string)       - Model identifier
#   $bucket_key (string)       - Bucket key (YYYY-MM)
# RETURNS:
#   Hashref with fingerprint metadata, or undef if not found
# ==============================================================================
{
    # Memoisation cache - scoped to avoid polluting global namespace
    my %_fingerprint_history_cache;

    sub _load_stored_bucket_fingerprint {
        my ($system_cache_dir, $event_name, $model_type, $bucket_key) = @_;

        # ------------------------------------------------------------------
        # Phase 4 DEBUG: lookup probe (only when -v / -vv)
        # Shows exactly what month key we search, and what we find (or don't).
        # ------------------------------------------------------------------
        if ($debug) {
            my $bk_ref = ref($bucket_key) || '';
            my $bucket_preview = $bk_ref ? "$bucket_key" : (defined $bucket_key ? $bucket_key : '(undef)');
            print STDERR "      ↳ [DEBUG] Phase 4 load-stored: event=$event_name model=$model_type bucket_key=$bucket_preview"
                . " bucket_ref=" . ($bk_ref || '(scalar)')
                . "\n";
        }

        # Memoise: load history once per system_cache_dir per invocation
        my $cache_key = $system_cache_dir;
        unless (exists $_fingerprint_history_cache{$cache_key}) {
            $_fingerprint_history_cache{$cache_key} = read_unified_history($system_cache_dir);
        }
        if ($debug) {
            my @months = sort keys %{$_fingerprint_history_cache{$cache_key} // {}};
            print STDERR "      ↳ [DEBUG] Phase 4 load-stored: unified history months=" . scalar(@months)
                . " first=" . ($months[0] // '(none)') . " last=" . ($months[-1] // '(none)') . "\n";
        }

        my $history_data = $_fingerprint_history_cache{$cache_key};
        return undef unless (defined $history_data && ref($history_data) eq 'HASH');

        # Navigate to the stored _meta block
        my $meta = $history_data->{$bucket_key}
                              ->{SeasonalEventSnapshots}
                              ->{$event_name}
                              ->{ModelForecasts}
                              ->{$model_type}
                              ->{'_meta'};

        return undef unless (defined $meta && ref($meta) eq 'HASH');

        # Return fingerprint block if present, enriched with baseline metadata
        # (Phase 5: History Based Baselining staleness detection)
        if (exists $meta->{fingerprints}) {
            # Shallow copy to avoid mutating the memoised cache
            my %result = %{$meta->{fingerprints}};

            # Carry baseline fields from _meta into the returned structure
            # so _should_reprocess_bucket can perform baseline-specific checks
            $result{baseline_fingerprint} = $meta->{baseline_fingerprint}
                if exists $meta->{baseline_fingerprint};
            $result{baseline_source} = $meta->{baseline_source}
                if exists $meta->{baseline_source};

            return \%result;
        }

        return undef;
    }

    # Helper to clear memoisation cache (useful if history is modified mid-run)
    sub _clear_fingerprint_history_cache {
        %_fingerprint_history_cache = ();
    }
}

# ==============================================================================
# SUBROUTINE: _should_reprocess_bucket (Phase 4: Idempotency)
# PURPOSE:    Determines whether a bucket should be reprocessed or skipped.
#             Implements the deterministic skip/redo gate.
# ARGUMENTS:
#   $system_cache_dir (string)  - Path to system cache directory
#   $event_name (string)        - Event name
#   $event_cfg (hashref)        - Event configuration
#   $model_type (string)        - Model identifier
#   $bucket_key (string)        - Bucket key (YYYY-MM)
#   $exec_ctx (hashref)         - Bucket execution context
#   $prev_bucket_fp (string|undef) - Previous bucket's combined fingerprint
#   $force_flag (boolean)       - Whether --force was specified
# RETURNS:
#   Hashref with:
#     reprocess        - Boolean: true if bucket should be reprocessed
#     reason           - String: explanation of decision
#     reason_category  - String: 'skip'|'config_changed'|'rules_changed'|
#                                'chain_changed'|'missing_prior'|'force'
#     current_fp       - Current computed fingerprint hashref
#     stored_fp        - Stored fingerprint hashref (or undef)
# ==============================================================================
sub _should_reprocess_bucket {
    my ($system_cache_dir, $event_name, $event_cfg, $model_type, $bucket_key, $exec_ctx, $prev_bucket_fp, $dependency_fp, $force_flag, $baseline_fp_href) = @_;

    # ------------------------------------------------------------------
    # Idempotency & Dependency Awareness: DEBUG: argument sanity probe (only when --debug)
    # This is designed to catch reference issues such as:
    # bucket_key accidentally arriving as a HASH ref -> "HASH(0x...)"
    # ------------------------------------------------------------------
    if ($debug) {
        my $bk_ref  = ref($bucket_key) || '';
        my $ec_ref  = ref($exec_ctx)   || '';
        my $ev_ref  = ref($event_cfg)  || '';
        my $sc_ref  = ref($seasonality_config) || '';

        my $bucket_preview = $bk_ref ? "$bucket_key" : (defined $bucket_key ? $bucket_key : '(undef)');
        print STDERR "      ↳ [DEBUG] _should_reprocess_bucket(): event=$event_name model=$model_type bucket_key=$bucket_preview"
            . " bucket_ref=" . ($bk_ref || '(scalar)')
            . " exec_ctx_ref=" . ($ec_ref || '(undef)')
            . " event_cfg_ref=" . ($ev_ref || '(undef)')
            . " seasonality_cfg_ref=" . ($sc_ref || '(undef)')
            . "\n";

        if ($bk_ref) {
            print STDERR "      ↳ [DEBUG] _should_reprocess_bucket(): WARN: bucket_key is a reference; prior fingerprint lookup will never match months like '2025-04'\n";
            require Data::Dumper;
            local $Data::Dumper::Terse  = 1;
            local $Data::Dumper::Indent = 0;
            print STDERR "      ↳ [DEBUG] _should_reprocess_bucket(): bucket_key dump: " . Data::Dumper::Dumper($bucket_key) . "\n";
        }
    }

    if ($verbose) {
        print STDERR "      ↳  Dependency: " . substr(($dependency_fp // ''), 0, 12) . "\n";
    }

    # Phase 5: Extract baseline fingerprint for bucket fingerprint composition
    my $baseline_fp_str = ($baseline_fp_href && ref($baseline_fp_href) eq 'HASH')
                        ? $baseline_fp_href->{fingerprint} : undef;

    # Compute current fingerprint (always, to ensure provenance is persisted even under --force)
    my $current_fp = _compute_bucket_fingerprint($event_cfg, $exec_ctx, $model_type, $prev_bucket_fp, $dependency_fp, $baseline_fp_str);

    # Always reprocess if forced (but still return fingerprint for storage)
    if ($force_flag) {
        return {
            reprocess        => 1,
            reason           => 'User requested --force',
            reason_category  => 'force',
            current_fp       => $current_fp,
            stored_fp        => undef,
        };
    }

    # Load stored fingerprint
    my $stored_fp = _load_stored_bucket_fingerprint($system_cache_dir, $event_name, $model_type, $bucket_key);

    # No prior fingerprint - must process (bootstrap)
    unless (defined $stored_fp) {
        return {
            reprocess        => 1,
            reason           => "No prior fingerprint (bootstrap or first run for bucket $bucket_key)",
            reason_category  => 'missing_prior',
            current_fp       => $current_fp,
            stored_fp        => undef,
        };
    }

    # Compare fingerprints
    my @diff_reasons;

    # Check engine version first (trumps other checks)
    if (($stored_fp->{engine_version} // '') ne ($current_fp->{engine_version} // '')) {
        push @diff_reasons, sprintf("engine version changed (%s -> %s)",
            $stored_fp->{engine_version} // 'unknown',
            $current_fp->{engine_version} // 'unknown');
    }

    # Check Upstream Dependencies
    if (($stored_fp->{dependencies} // 'NONE') ne ($current_fp->{dependencies} // 'NONE')) {
        push @diff_reasons, "upstream history inputs changed (backfill or regeneration)";
    }

    # Phase 5: Check baseline source mode change (L1 <-> history)
    if ($baseline_fp_href && ref($baseline_fp_href) eq 'HASH') {
        my $stored_bl_source  = $stored_fp->{baseline_source}      // 'unknown';
        my $current_bl_source = $baseline_fp_href->{source}        // 'history';
        if ($stored_bl_source ne 'unknown' && $stored_bl_source ne $current_bl_source) {
            push @diff_reasons, sprintf("baseline mode changed (%s -> %s)",
                $stored_bl_source, $current_bl_source);
        }

        # Check baseline fingerprint (within same mode)
        my $stored_bl_fp  = $stored_fp->{baseline_fingerprint} // 'none';
        my $current_bl_fp = $baseline_fp_href->{fingerprint}   // 'none';
        if ($stored_bl_fp ne 'none' && $stored_bl_fp ne $current_bl_fp) {
            push @diff_reasons, "baseline inputs changed (history content or exclusion configuration)";
        }
    }

    # Check event definition fingerprint (config changes)
    if (($stored_fp->{event_def} // '') ne ($current_fp->{event_def} // '')) {
        push @diff_reasons, "event configuration changed (dates, vms, exclude_dates, etc.)";
    }

    # Check execution context fingerprint (window/anchor changes)
    if (($stored_fp->{exec_ctx} // '') ne ($current_fp->{exec_ctx} // '')) {
        push @diff_reasons, "execution context changed (analysis window or cache bounds)";
    }

    # Check chain-of-custody fingerprint (upstream dependency changed)
    if (($stored_fp->{chain} // '') ne ($current_fp->{chain} // '')) {
        push @diff_reasons, "upstream dependency changed (prior bucket was reprocessed)";
    }

    # If any differences found, reprocess
    if (@diff_reasons) {
        my $reason_category = 'config_changed';
        if (grep { /engine version/ } @diff_reasons) {
            $reason_category = 'rules_changed';
        } elsif (grep { /baseline/ } @diff_reasons) {
            $reason_category = 'baseline_changed';
        } elsif (grep { /upstream dependency/ } @diff_reasons) {
            $reason_category = 'chain_changed';
        }

        return {
            reprocess        => 1,
            reason           => join("; ", @diff_reasons),
            reason_category  => $reason_category,
            current_fp       => $current_fp,
            stored_fp        => $stored_fp,
        };
    }

    # All fingerprints match - skip
    return {
        reprocess        => 0,
        reason           => "Fingerprints match (combined: " . substr($current_fp->{combined}, 0, 12) . "...)",
        reason_category  => 'skip',
        current_fp       => $current_fp,
        stored_fp        => $stored_fp,
    };
}

# ==============================================================================
# SUBROUTINE: _resolve_upstream_dependency_fingerprint
# PURPOSE:    Resolves the fingerprints of the historical inputs required for this anchor date.
# Performs lightweight JSON reads of the history files to avoid loading full datasets.
sub _resolve_upstream_dependency_fingerprint {
    my ($cache_path, $event_cfg, $anchor_date_obj) = @_;

    return 'NONE' unless ($cache_path && -d $cache_path);
    require Digest::MD5;

    my @dependency_parts;
    my $target_month_idx = $anchor_date_obj->mon;
    my $model_type = $event_cfg->{model} // 'multiplicative_seasonal';
    my $lookback_days = $event_cfg->{seasonal_lookback_days} // 730;

    # 1. Identify which past months are dependencies
    #    For multiplicative models, we only care about the same calendar month in prior years.
    my $is_month_matched = ($model_type eq 'multiplicative_seasonal');

    # Scan back 5 years (or lookback limit) to find relevant history files
    my $start_year = $anchor_date_obj->year - 5;
    my $end_year   = $anchor_date_obj->year;

    for my $y ($start_year .. $end_year) {
        for my $m (1..12) {
            # Skip future/current
            my $candidate_str = sprintf("%04d-%02d", $y, $m);
            my $candidate_date = Time::Piece->strptime($candidate_str, "%Y-%m");
            next if $candidate_date >= $anchor_date_obj->truncate(to => 'month');

            # Apply Model Logic (Same Month)
            if ($is_month_matched) {
                next unless $m == $target_month_idx;
            }

            # 2. Check if file exists and read its meta fingerprint
            #    Assumes standard file naming: "$cache_path/$candidate_str.json"
            #    (Adjust filename pattern if your system uses a prefix)
            my $file = File::Spec->catfile($cache_path, "$candidate_str.json");

            if (-f $file) {
                my $fp = 'missing_meta';
                # Lightweight Read: We only need the top-level structure or specific event key
                eval {
                    local $/;
                    open(my $fh, '<', $file) or die $!;
                    my $json_text = <$fh>;
                    close $fh;
                    my $data = decode_json($json_text);
                    my $evt = $event_cfg->{_eventName};

                    # Grab the stored fingerprint if it exists
                    # Navigate to: SeasonalEventSnapshots -> {event} -> ModelForecasts -> {model} -> _meta -> fingerprints
                    my $model_meta = $data->{SeasonalEventSnapshots}{$evt}{ModelForecasts}{$model_type}{'_meta'};
                    if ($model_meta && ref($model_meta) eq 'HASH' && $model_meta->{fingerprints}) {
                        # Use the combined fingerprint as the dependency marker
                        $fp = $model_meta->{fingerprints}{combined} // 'missing_combined';
                    }
                };
                push @dependency_parts, "$candidate_str=$fp";
            }
        }
    }

    return 'NONE' unless @dependency_parts;
    return Digest::MD5::md5_hex(join("|", @dependency_parts));
}

# ==============================================================================
# SUBROUTINE: _inject_exclusions_into_manifest
# PURPOSE:    Adds the _exclusions and _exclusion_fingerprint blocks to a
#             manifest for transmission to the nfit engine.
# ==============================================================================
sub _inject_exclusions_into_manifest {
    my ($manifest_href, $exclusions_href) = @_;

    return $manifest_href unless defined $exclusions_href;

    # Validate: no wildcards should be present at this stage
    foreach my $vm (keys %{$exclusions_href->{vm_specific} // {}}) {
        if ($vm =~ /\*/) {
            die "FATAL: Unexpanded wildcard '$vm' in exclusions. " .
                "Wildcards must be expanded before manifest injection.\n";
        }
    }

    # Only inject if there are actual exclusions
    my $has_exclusions = (scalar @{$exclusions_href->{global} // []} > 0) ||
                         (scalar keys %{$exclusions_href->{vm_specific} // {}} > 0);

    if ($has_exclusions) {
        $manifest_href->{_exclusions} = $exclusions_href;
        $manifest_href->{_exclusion_fingerprint} = _compute_exclusion_fingerprint($exclusions_href);
    }

    return $manifest_href;
}

# ==============================================================================
# SUBROUTINE: _group_events_by_fingerprint
# PURPOSE:    Groups events that share identical computational requirements
#             (boundaries + exclusions) to enable nfit pass consolidation.
# ==============================================================================
sub _group_events_by_fingerprint {
    my ($events_aref, $seasonality_config) = @_;

    my %groups;

    foreach my $event_cfg (@$events_aref) {
        my $event_name = $event_cfg->{_eventName};

        # Determine boundaries
        my ($peak_start, $peak_end) = determine_event_period($event_cfg);
        my $baseline_days = $event_cfg->{baseline_period_days} // 16;
        my $baseline_start = $peak_start ? ($peak_start - ($baseline_days * ONE_DAY)) : undef;

        # Parse exclusions (Note: we need known_vms here for full expansion, but for grouping
        # we might just use the raw config if we assume consistent expansion later.
        # However, to be safe, we should probably expand.
        # For now, let's use a simplified fingerprint based on the raw config string if available,
        # or just skip optimisation for now to keep it simple and safe as per instructions.)

        # Actually, let's skip the complex grouping optimisation for this first iteration
        # to ensure safety and correctness, as the user emphasized "production ready" and "safe".
        # We can implement a simple 1-to-1 mapping for now.

        my $fingerprint = $event_name; # Simple unique fingerprint
        push @{$groups{$fingerprint}}, $event_cfg;
    }

    return %groups;
}

# ==============================================================================
# SUBROUTINE: _translate_skip_reason
# PURPOSE:    Converts internal reason categories to user-friendly messages
# ==============================================================================
sub _translate_skip_reason {
    my ($category) = @_;
    return 'unchanged since last analysis'      if $category eq 'skip';
    return 'no changes detected'                if !$category;
    return $category;  # fallback
}

# ==============================================================================
# SUBROUTINE: _translate_change_reason
# PURPOSE:    Converts internal change reasons to user-friendly messages
# ==============================================================================
sub _translate_change_reason {
    my ($category, $detail) = @_;

    return 'configuration changed'              if $category eq 'config_changed' && $detail =~ /event configuration/;
    return 'analysis window changed'            if $category eq 'config_changed' && $detail =~ /execution context/;
    return 'upstream history updated'           if $category eq 'config_changed' && $detail =~ /upstream history/;
    return 'engine version updated'             if $category eq 'rules_changed';
    return 'baseline data changed'              if $category eq 'baseline_changed';
    return 'prior bucket was reprocessed'       if $category eq 'chain_changed';
    return 'changes detected';  # fallback
}

# ==============================================================================
# SUBROUTINE: _execute_automatic_seasonal_discovery
#
# PURPOSE:
#   Orchestrates the execution of seasonal forecasting events as-of a specific
#   analysis reference date.
#
#   This subroutine is the single authoritative entry point for all seasonal
#   model execution in nfit-profile, and is used by both:
#     - `--seasonal`                : (execute all active seasonal events)
#     - `--apply-seasonality <E>`   : (execute a single explicit event, if active)
#
#   PHASE 1 ENHANCEMENT:
#   This function now performs CENTRALISED ANCHOR RESOLUTION before dispatching
#   to model executors. This ensures:
#     - Deterministic anchor assignment across all model types
#     - Correct history partition (anchor bucket) for storage
#     - Consistent time-blinded analysis windows
#
# ARGUMENTS:
#   $seasonality_config   (hashref)
#   $current_cache_path   (string)
#   $profiles_aref        (arrayref)
#   $args_href            (hashref) - includes cache bounds, effective dates, etc.
#   $analysis_anchor_date (Time::Piece | undef)
#   $event_filter         (string | undef)
#
# RETURNS:
#   None.
#
# ==============================================================================
sub _execute_automatic_seasonal_discovery {
    my ($seasonality_config, $current_cache_path, $profiles_aref, $args_href, $analysis_anchor_date, $event_filter) = @_;

    _phase('Seasonal Discovery');

    # 1. Resolve the canonical reference date ONCE
    # This date is used for BOTH eligibility checking AND anchor resolution,
    # ensuring they cannot diverge.
    #
    # Priority:
    #   1. CLI override (--analysis-reference-date / -e)
    #   2. Passed-in anchor date (from caller context)
    #   3. Effective end date from cache/CLI
    #   4. System time (last resort)

    # 1. Determine if CLI override is explicitly present
    my $cli_override_present = (defined $nfit_analysis_reference_date_str
                                && length $nfit_analysis_reference_date_str);
    my $cli_override_date = undef;

    if ($cli_override_present) {
        $cli_override_date = _parse_anchor_date_string($nfit_analysis_reference_date_str);
        unless ($cli_override_date) {
            warn "  [WARN] Could not parse --analysis-reference-date '$nfit_analysis_reference_date_str'\n";
            $cli_override_present = 0;
        }
    }

    # 2. Resolve canonical reference date for ELIGIBILITY checking
    # Priority: CLI override > passed-in anchor > effective end > cache end > now
    my $canonical_reference_date = $cli_override_date
                                // $analysis_anchor_date
                                // $args_href->{effective_end_date}
                                // $args_href->{cache_end_date}
                                // gmtime();

    # 3. Detect active events using the canonical date
    my @active_events = detect_active_events($canonical_reference_date, $seasonality_config);

    # Filter (in case --apply-seasonality was specified)
    if (defined $event_filter && length $event_filter) {
        @active_events = grep { ($_->{_eventName} // '') eq $event_filter } @active_events;

        if (scalar @active_events == 0) {
            print STDERR "  [INFO] Seasonal event $event_filter is not active for reference date (" . $canonical_reference_date->ymd . ")\n";
            return;
        }
    }

    if (scalar @active_events == 0) {
        print STDERR "  [INFO] No active seasonal events found for reference date (" . $canonical_reference_date->ymd . ")\n";
        return;
    }

    print STDERR "  [INFO] Detected " . scalar(@active_events) . " active seasonal event(s): " . join(", ", map { $_->{_eventName} } @active_events) . "\n";


    # ----------------------------------------------------------------------
    # Phase 4 Idempotency Gate: DEBUG PROBE (no behavioural change)
    # Purpose: quickly confirm whether Phase 4 subs exist/are compiled and
    # whether the orchestrator is invoking them.
    # ----------------------------------------------------------------------
    if ($debug) {
        my @need = qw(
            _should_reprocess_bucket
            _compute_bucket_fingerprint
            _load_stored_bucket_fingerprint
        );
        my @present = grep { defined &{$_} } @need;
        my @missing = grep { !defined &{$_} } @need;

        print STDERR "  [DEBUG] Phase 4 probe: idempotency subs present: "
            . (@present ? join(', ', @present) : '(none)')
            . "\n";
        if (@missing) {
            print STDERR "  [DEBUG] Phase 4 probe: idempotency subs missing: "
                . join(', ', @missing)
                . "\n";
            print STDERR "  [DEBUG] Phase 4 probe: gate cannot execute; run will always behave as pre-Phase-4\n";
        }
    }

    # 2. Iterate and execute with CENTRALISED ANCHOR RESOLUTION
    foreach my $event_cfg (@active_events) {
        my $event_name = $event_cfg->{_eventName};
        print STDERR "\n  ⧉ Executing seasonal forecast for event $event_name\n";

        # Phase 3: Anchor buckets
        #
        # If the user explicitly provided --analysis-reference-date, we treat it
        # as an intentional anchor override and execute a single run (no bucket loop).
        #
        # Otherwise, if the event has enumerable occurrences, we bucketise completed
        # occurrences by anchor month (YYYY-MM) and execute either:
        #   - latest bucket only (default)
        #   - all buckets (chronological) when --seasonal-scope all

        my @bucket_keys_to_run;
        my $buckets_href = {};
        my $occurrences_aref = _enumerate_event_occurrences(
            $event_cfg,
            $args_href->{effective_start_date},
            $canonical_reference_date
        );

        if (!$cli_override_present && $occurrences_aref && @$occurrences_aref) {
            $buckets_href = _bucketise_completed_occurrences_by_month($occurrences_aref, $canonical_reference_date);
            my @keys = sort keys %$buckets_href;

            if (!@keys) {
                print STDERR "    [-] Skipping event $event_name (no completed occurrences as-of reference date " . $canonical_reference_date->ymd . ")\n";
                next;
            }

            if ($seasonal_scope eq 'all') {
                @bucket_keys_to_run = @keys; # chronological
            } else {
                @bucket_keys_to_run = ($keys[-1]); # latest only (default)
            }
        } else {
            # Fallback to Phase 2 single-anchor resolution (also covers free-running events)
            @bucket_keys_to_run = ('__single__');
        }

        # Phase 4: Chain-of-custody tracking for idempotency
        # Track the previous bucket's combined fingerprint to establish dependency chain
        my $prev_bucket_combined_fp = undef;

        foreach my $bucket_key (@bucket_keys_to_run) {
            # Reset global state to ensure clean execution per bucket
            _reset_seasonal_state();

            my $anchor_info;

            if ($bucket_key eq '__single__') {
                $anchor_info = _resolve_seasonal_anchor_date(
                    $event_cfg,
                    $args_href->{effective_start_date},
                    $canonical_reference_date,
                    $cli_override_present ? $nfit_analysis_reference_date_str : undef
                );
            } else {
                my $bucket_occ_aref = $buckets_href->{$bucket_key} // [];
                my $bucket_anchor_end = _max_occurrence_end($bucket_occ_aref);

                unless ($bucket_anchor_end) {
                    print STDERR "    [WARN] Could not determine bucket anchor for $event_name ($bucket_key)\n";
                    next;
                }

                # For single-occurrence buckets, calculate the actual event PEAK start
                # (not the calendar month start) for proper window bounding.
                #
                # For month-end events (day_of_period=-1), the occurrence 'start' from
                # _enumerate_monthly_occurrences is the first of the month, but the
                # actual event peak starts (duration_days - 1) days before month end.
                my $actual_peak_start = undef;
                if ($bucket_occ_aref && @$bucket_occ_aref) {
                    my $last_occ = $bucket_occ_aref->[-1];  # Use last occurrence in bucket
                    if ($last_occ && $last_occ->{end}) {
                        my $occ_end = $last_occ->{end}->truncate(to => 'day');
                        my $duration = $event_cfg->{duration_days} // 4;
                        # Peak starts (duration - 1) days before the end
                        # e.g., duration=4: May 31 - 3 = May 28
                        $actual_peak_start = $occ_end - (($duration - 1) * ONE_DAY);
                    }
                }

                # Important: force a scalar context on the truncate() to ensure object context (or else it will return an empty list)
                $anchor_info = {
                    anchor_date    => scalar($bucket_anchor_end->truncate(to => 'day')),
                    anchor_bucket  => $bucket_key,
                    anchor_source  => 'bucket_latest_end',
                    occurrence_start => $actual_peak_start,
                    occurrence_end => scalar($bucket_anchor_end->truncate(to => 'day')),
                    is_complete    => 1,
                    # Phase 3.1: attach the evidence used to form this bucket
                    bucket_occurrences => $bucket_occ_aref,
                };
            }

            # Log anchor resolution
            if ($anchor_info->{anchor_date} && ref($anchor_info->{anchor_date}) eq 'Time::Piece' && $anchor_info->{anchor_date}->epoch > 0) {

                my $anchor_ymd   = $anchor_info->{anchor_date}->ymd;
                my $bucket_month = $anchor_info->{anchor_bucket} // 'unknown';
                my $event_label  = $event_name // 'event';

                # Default: always show the anchoring milestone
                print STDERR "    Φ History month $bucket_month: anchored to reference date $anchor_ymd\n";

                # -v: user-friendly context about what data is being used
                if (($verbose // 0) >= 1) {
                    my $occ_n = 0;
                    if ($anchor_info->{bucket_occurrences} && ref($anchor_info->{bucket_occurrences}) eq 'ARRAY') {
                        $occ_n = scalar @{$anchor_info->{bucket_occurrences}};
                    }

                    # Translate anchor_source to plain English
                    my $src = $anchor_info->{anchor_source} // 'unknown';
                    my $src_desc = $src eq 'bucket_latest_end'   ? 'latest completed event occurrence'
                                 : $src eq 'bucket_first_end'    ? 'earliest completed event occurrence'
                                 : $src eq 'bucket_aggregated'   ? 'aggregated event occurrences'
                                 : $src;

                    my $occ_label = $occ_n == 1 ? '1 occurrence' : "$occ_n occurrences";
                    print STDERR "      ↳  Analysis basis: $src_desc ($occ_label)\n";

                    # Show occurrence end dates if multiple
                    if ($occ_n > 1 && $anchor_info->{bucket_occurrences}) {
                        my @end_dates = map { $_->{end} ? $_->{end}->ymd : () } @{$anchor_info->{bucket_occurrences}};
                        print STDERR "      ↳  Occurrence end dates: " . join(', ', @end_dates) . "\n";
                    }
                }

            } else {
                print STDERR "    [WARN] Could not resolve anchor date for event $event_name\n";
                next;
            }

            # Skip incomplete occurrences - do not produce partial/incorrect results
            unless ($anchor_info->{is_complete}) {
                print STDERR "    [-] Skipping event $event_name (occurrence is not complete as-of reference date " . $canonical_reference_date->ymd . ")\n";
                next;
            }

            if (($verbose // 0) >= 1 && $anchor_info->{occurrence_start} && $anchor_info->{occurrence_end}) {
                print STDERR "      ↳  Occurrence Window: " . $anchor_info->{occurrence_start}->ymd . " ↔ " . $anchor_info->{occurrence_end}->ymd . "\n";
            }

            # Build the execution context with anchor information
            my $exec_ctx = _build_anchor_execution_context($anchor_info, $args_href, $event_cfg);
            $exec_ctx->{debug}   = $debug ? 1 : 0;
            $exec_ctx->{verbose} = $verbose // 0;   # if not already present everywhere

            # --- PHASE 4: IDEMPOTENCY GATE ---

            # Resolve Upstream Dependencies
            # We explicitly calculate the state of the history required by this bucket.
            # If a history month was backfilled/regenerated, its fingerprint will have changed.
            my $dependency_fp = _resolve_upstream_dependency_fingerprint(
                $current_cache_path,
                $event_cfg,
                $anchor_info->{anchor_date}
            );

            # --- PHASE 6: HISTORY-BASED BASELINE FINGERPRINT FOR STALENESS ---
            # Compute the expected baseline fingerprint BEFORE the idempotency gate.
            # This is lightweight (metadata-only) and allows staleness detection
            # when baseline inputs change (content_version, exclusions, mode switch).
            my $baseline_fp_href;

            if (!$use_l1_baseline) {
                # History-based path (default)
                my $bl_exclusions;
                if (defined $event_cfg->{exclude_dates}) {
                    my $known_vms = _get_known_vms_from_cache($current_cache_path);
                    $bl_exclusions = _parse_exclusion_dates($event_cfg->{exclude_dates}, $known_vms);
                }
                my $bl_vm_scope = _parse_vm_scope_filter($event_cfg);
                my $bl_canonical_days = $event_cfg->{baseline_canonical_days} // 90;

                my $expected_bl_fp = _calculate_expected_baseline_fingerprint({
                    system_cache_dir   => $current_cache_path,
                    end_date_obj       => $exec_ctx->{effective_end_date},
                    canonical_days     => $bl_canonical_days,
                    coverage_threshold => $event_cfg->{baseline_coverage_threshold} // 0.7,
                    date_exclusions    => $bl_exclusions,
                    vm_scope_filter    => $bl_vm_scope,
                    profiles           => [map { $_->{name} } @$profiles_aref],
                });

                if (defined $expected_bl_fp) {
                    $baseline_fp_href = {
                        fingerprint => $expected_bl_fp,
                        source      => 'history',
                    };
                }
            } else {
                # L1 escape hatch: deterministic fingerprint based on canonical span
                require Digest::MD5;
                my $bl_canonical_days = $event_cfg->{baseline_canonical_days} // 90;
                my $span_start = ($exec_ctx->{effective_end_date} - (($bl_canonical_days - 1) * ONE_DAY))->strftime('%Y-%m-%d');
                my $span_end   = $exec_ctx->{effective_end_date}->strftime('%Y-%m-%d');
                $baseline_fp_href = {
                    fingerprint => Digest::MD5::md5_hex("l1_cache:$bl_canonical_days:$span_start:$span_end"),
                    source      => 'l1_cache',
                };
            }

            # Override: predictive_peak always uses L1 baseline, so its staleness
            # fingerprint must reflect L1 provenance rather than history content.
            # Without this, history content changes (e.g. new month committed)
            # would trigger unnecessary reprocessing for predictive_peak buckets
            # whose baseline is unaffected by history.
            my $model_type = $event_cfg->{model} // '';
            if ($model_type eq 'predictive_peak' && !$use_l1_baseline
                && $baseline_fp_href && $baseline_fp_href->{source} eq 'history') {
                require Digest::MD5;
                my $bl_canonical_days = $event_cfg->{baseline_canonical_days} // 90;
                my $span_start = ($exec_ctx->{effective_end_date} - (($bl_canonical_days - 1) * ONE_DAY))->strftime('%Y-%m-%d');
                my $span_end   = $exec_ctx->{effective_end_date}->strftime('%Y-%m-%d');
                $baseline_fp_href = {
                    fingerprint => Digest::MD5::md5_hex("l1_cache:$bl_canonical_days:$span_start:$span_end"),
                    source      => 'l1_cache',
                };
            }

            # Check if this bucket needs reprocessing or can be skipped
            my $effective_bucket_key = ($bucket_key eq '__single__')
                ? ($anchor_info->{anchor_bucket} // 'unknown')
                : $bucket_key;

            my $reprocess_decision = _should_reprocess_bucket(
                $current_cache_path,
                $event_name,
                $event_cfg,
                $model_type,
                $effective_bucket_key,
                $exec_ctx,
                $prev_bucket_combined_fp,
                $dependency_fp,
                $force_update,       # Use the global --force flag
                $baseline_fp_href    # Phase 6: baseline staleness input
            );

            if (!$reprocess_decision->{reprocess}) {
                # Default: show skip with user-friendly reason
                # Skip this bucket - fingerprints match
                my $skip_reason = _translate_skip_reason($reprocess_decision->{reason_category});
                print STDERR "      ↳  $effective_bucket_key: $skip_reason\n";

                # -vv only: show fingerprint details
                if (($verbose // 0) >= 2) {
                    my $fp_short = substr($reprocess_decision->{stored_fp}{combined} // '', 0, 12);
                    print STDERR "        ↳  Fingerprint: $fp_short...\n";
                }

                # Update chain-of-custody with stored fingerprint for next bucket
                if ($reprocess_decision->{stored_fp} && $reprocess_decision->{stored_fp}{combined}) {
                    $prev_bucket_combined_fp = $reprocess_decision->{stored_fp}{combined};
                }
                next;
            }

            # Log reprocess reason
            my $reason_cat = $reprocess_decision->{reason_category} // 'unknown';
            if ($reason_cat eq 'missing_prior') {
                print STDERR "    ↳  First analysis for $effective_bucket_key\n";
            } elsif ($reason_cat eq 'force') {
                print STDERR "    ↳  Forced re-processing for $effective_bucket_key\n";
                print STDERR "      ↳  Reason: $reprocess_decision->{reason}\n" if (($verbose // 0) >= 1);
            } else {
                my $change_desc = _translate_change_reason($reason_cat, $reprocess_decision->{reason});
                print STDERR "    ↳  $effective_bucket_key: $change_desc\n";
            }

            # --debug only: developer diagnostics
            if ($debug) {
                print STDERR "      ↳  [DEBUG] Category: $reason_cat\n";
                print STDERR "      ↳  [DEBUG] Detail: $reprocess_decision->{reason}\n";
                if ($reprocess_decision->{current_fp}) {
                    require Data::Dumper;
                    local $Data::Dumper::Terse = 1;
                    local $Data::Dumper::Indent = 0;
                    print STDERR "      ↳  [DEBUG] Current fingerprint: " . Data::Dumper::Dumper($reprocess_decision->{current_fp}) . "\n";
                }
            }

            # Attach fingerprints to execution context for downstream storage
            $exec_ctx->{_fingerprints} = $reprocess_decision->{current_fp};

            # --- PHASE 6: FULL BASELINE COMPUTATION (only on reprocess) ---
            # Compute the complete baseline from history for consumer use.
            # Skipped when --use-l1-baseline is active (consumers fall back to L1 path).
            if (!$use_l1_baseline) {
                my $bl_exclusions;
                if (defined $event_cfg->{exclude_dates}) {
                    my $known_vms = _get_known_vms_from_cache($current_cache_path);
                    $bl_exclusions = _parse_exclusion_dates($event_cfg->{exclude_dates}, $known_vms);
                }
                my $bl_vm_scope = _parse_vm_scope_filter($event_cfg);
                my $bl_canonical_days = $event_cfg->{baseline_canonical_days} // 90;

                my @all_peak_periods = find_all_historical_periods(
                    $seasonality_config,
                    $args_href->{effective_start_date},
                    $exec_ctx->{effective_end_date}
                );

                eval {
                    my $baseline_result = _get_baseline_from_history({
                        system_cache_dir   => $current_cache_path,
                        end_date_obj       => $exec_ctx->{effective_end_date},
                        canonical_days     => $bl_canonical_days,
                        coverage_threshold => $event_cfg->{baseline_coverage_threshold} // 0.7,
                        vm_scope_filter    => $bl_vm_scope,
                        date_exclusions    => $bl_exclusions,
                        peak_periods       => \@all_peak_periods,
                        profiles           => [map { $_->{name} } @$profiles_aref],
                    });

                    $exec_ctx->{_baseline_result} = $baseline_result;
                    $exec_ctx->{_baseline_meta}   = $baseline_result->{meta};

                    if (($verbose // 0) >= 1) {
                        my $bl_status = ($baseline_result->{meta}{coverage_by_vm})
                            ? 'computed' : 'no coverage data';
                        print STDERR "      ↳  Baseline from history: $bl_status\n";
                    }
                };
                if ($@) {
                    # History-based baseline failed — this is non-fatal.
                    # Consumers will detect the absence and use legacy L1 path.
                    my $err_msg = $@;
                    chomp $err_msg;
                    warn "  [WARN] History-based baseline unavailable: $err_msg\n";
                    warn "  [WARN] Consumers will use legacy L1 path for this bucket.\n"
                        if (($verbose // 0) >= 1);
                }
            }

            # --- DISPATCH TO MODEL EXECUTOR ---

            # Gate: suppress history-based baseline for predictive_peak.
            # The predictive_peak model uses baseline in a max() gate where
            # small differences can flip the governing branch. Mean-of-daily-
            # aggregates cannot reproduce the pooled-percentile precision that
            # the legacy L1 single-pass engine provides. Multiplicative models
            # are unaffected because baseline enters as a ratio multiplicand
            # where day-weighting bias cancels between numerator and denominator.
            #
            # Replace history baseline with an L1 metadata stub so that:
            #  - The consumer falls through to _get_true_baseline_results (L1)
            #  - The storage function records baseline_source = 'l1_cache'
            #  - Snapshot auditability is preserved
            if ($model_type eq 'predictive_peak' && !$use_l1_baseline) {
                delete $exec_ctx->{_baseline_result};
                $exec_ctx->{_baseline_meta} = {
                    baseline_source => 'l1_cache',
                    fingerprint     => $baseline_fp_href->{fingerprint},
                    span_start      => undef,
                    span_end        => undef,
                    months_read     => undef,
                    coverage_by_vm  => undef,
                };
                if (($verbose // 0) >= 1) {
                    print STDERR "      ↳  Baseline: using staged data cache as data source (predictive_peak precision requirement)\n";
                }
            }

            if ($model_type eq 'multiplicative_seasonal') {
                # Phase 3.2: if Phase 3.1 bucket provenance exists and we have multiple occurrences,
                # aggregate multiplicative forecasts within the bucket.
                if (exists $anchor_info->{bucket_occurrences}
                    && ref($anchor_info->{bucket_occurrences}) eq 'ARRAY'
                    && scalar(@{ $anchor_info->{bucket_occurrences} }) > 1) {
                    _execute_multiplicative_seasonal_bucket_aggregated(
                        $event_name, $event_cfg, $current_cache_path, $profiles_aref, $exec_ctx, $seasonality_config,
                        $anchor_info->{bucket_occurrences}
                    );
                } else {
                    _execute_multiplicative_seasonal($event_name, $event_cfg, $current_cache_path, $profiles_aref, $exec_ctx, $seasonality_config);
                }
            }
            elsif ($model_type eq 'recency_decay') {
                _execute_recency_decay($event_name, $event_cfg, $current_cache_path, $profiles_aref, $exec_ctx, $seasonality_config);
            }
            elsif ($model_type eq 'predictive_peak') {
                _execute_predictive_peak($event_name, $event_cfg, $current_cache_path, $profiles_aref, $exec_ctx, $seasonality_config);
            }
            else {
                warn "  WARNING: Unknown or unsupported model type '$model_type' for event '$event_name'. Skipping.\n";
            }

            # Phase 7: Cross-zone score invalidation for baseline changes
            # When the reprocess reason includes baseline changes, any existing
            # Zone 3 tournament scores are stale (they referenced old forecasts).
            # Set the staleness marker so Zone 3 detects and rescores.
            if ($reason_cat eq 'baseline_changed') {
                _mark_scores_invalid_for_snapshot(
                    $current_cache_path,
                    $event_name,
                    $effective_bucket_key,
                    $reprocess_decision->{reason}
                );
            }

            # Phase 4: Update chain-of-custody for next bucket
            if ($exec_ctx->{_fingerprints} && $exec_ctx->{_fingerprints}{combined}) {
                $prev_bucket_combined_fp = $exec_ctx->{_fingerprints}{combined};
            }
        }

    }

    print STDERR "\n✔ Completed Automatic Seasonal Discovery\n";
}

# ==============================================================================
# SUBROUTINE: _reset_seasonal_state
# PURPOSE:    Resets global variables to a clean state between event executions.
# ==============================================================================
sub _reset_seasonal_state {
    # Reset globals used by nfit-profile logic
    $nfit_decay_over_states = 0;
    $nfit_enable_windowed_decay = 0;
    $is_multiplicative_forecast_run = 0;
    $is_predictive_peak_model_run = 0;

    # Clear any other globals that might persist (e.g., from previous runs in the same process)
    # Note: @vm_order and %assimilation_map are usually rebuilt per run, so they should be fine.
}

# ==============================================================================
# SUBROUTINE: _resolve_seasonal_anchor_date
#
# PURPOSE:
#   Determines the appropriate analysis reference date (anchor) for a seasonal
#   event. This is the SINGLE authoritative function for anchor resolution,
#   ensuring deterministic and consistent behaviour across all model types.
#
#   The anchor date is the point in time "as-of" which the analysis is performed.
#   It defines:
#     - The temporal boundary for time-blinded analysis
#     - The decay weight reference point (Time Zero)
#     - The history partition where results are stored
#
# RESOLUTION PRIORITY:
#   1. User-specified CLI argument (--analysis-reference-date / -e)
#   2. Smart default for month-end events (last completed month-end)
#   3. Smart default for date-range events (event end date if completed)
#   4. Fallback to effective end date from cache/CLI
#
# ARGUMENTS:
#   $event_config       (hashref)     - Event configuration from seasonality config
#   $effective_end_date (Time::Piece) - Cache end or CLI-specified end date
#   $cli_reference_date (string|undef)- User-specified reference date (YYYY-MM-DD)
#
# RETURNS:
#   hashref with:
#     anchor_date      => Time::Piece object (the resolved anchor)
#     anchor_bucket    => string (YYYY-MM format for history storage)
#     anchor_source    => string (explanation of how anchor was determined)
#     occurrence_end   => Time::Piece object (event occurrence end, if applicable)
#     is_complete      => boolean (whether the event occurrence is complete)
#
# ==============================================================================
# ==============================================================================
# SUBROUTINE: _enumerate_event_occurrences
#
# PURPOSE:
#   Enumerates candidate occurrences for an event definition within the supplied
#   analysis window. This is a pure calendar operation: it does not decide
#   completion, anchoring, or storage.
#
# ARGUMENTS:
#   $event_config       (hashref)     - Event configuration
#   $analysis_start     (Time::Piece) - Start of analysis window
#   $analysis_end       (Time::Piece) - End of analysis window (also serves as default reference date)
#
# RETURNS:
#   Arrayref of occurrence hashrefs:
#     { start => Time::Piece, end => Time::Piece, kind => 'monthly'|'dates' }
#
# ==============================================================================
sub _enumerate_event_occurrences {
    my ($event_config, $analysis_start, $analysis_end) = @_;

    return [] unless ($event_config && $analysis_start && $analysis_end);

    my $period        = $event_config->{period} // '';
    my $day_of_period = $event_config->{day_of_period} // 0;

    # Month-end events: monthly with day_of_period == -1
    if ($period eq 'monthly' && $day_of_period == -1) {
        return _enumerate_monthly_occurrences($analysis_start, $analysis_end);
    }

    # Explicit date events: dates = ...
    if (defined $event_config->{dates} && length($event_config->{dates})) {
        return _enumerate_explicit_date_occurrences($event_config->{dates}, $analysis_start, $analysis_end);
    }

    # No enumerable occurrences for free-running events
    return [];
}

# ==============================================================================
# SUBROUTINE: _enumerate_monthly_occurrences
#
# PURPOSE:
#   Produces month-end occurrences between the supplied window bounds.
#
# RETURNS:
#   Arrayref of { start => month_start, end => month_end, kind => 'monthly' }
#
# ==============================================================================
sub _enumerate_monthly_occurrences {
    my ($analysis_start, $analysis_end) = @_;

    return [] unless ($analysis_start && $analysis_end);

    my @occ;

    # ------------------------------------------------------------
    # Always include the most recent completed month-end <= analysis_end,
    # even if it falls before analysis_start. This prevents narrow -s windows
    # from accidentally producing "no month-end candidates" mid-month.
    # ------------------------------------------------------------
    my $last_month_end = ($analysis_end->truncate(to => 'month') - 1)->truncate(to => 'day');
    if ($last_month_end->epoch <= $analysis_end->truncate(to => 'day')->epoch) {
        my $last_month_start = $last_month_end->truncate(to => 'month')->truncate(to => 'day');
        push @occ, { start => $last_month_start, end => $last_month_end, kind => 'monthly' };
    }

    my $cursor = $analysis_start->truncate(to => 'month');
    my $end_month = $analysis_end->truncate(to => 'month');

    while ($cursor->epoch <= $end_month->epoch) {
        my $month_start = $cursor->truncate(to => 'day');
        my $month_end   = ($cursor->add_months(1) - 1)->truncate(to => 'day');

        # Only include if the occurrence end is within the analysis window bounds
        if ($month_end->epoch >= $analysis_start->truncate(to => 'day')->epoch &&
            $month_end->epoch <= $analysis_end->truncate(to => 'day')->epoch) {
            push @occ, { start => $month_start, end => $month_end, kind => 'monthly' };
        }

        $cursor = $cursor->add_months(1);
    }

    # De-duplicate in case the "last completed month-end" is also within bounds
    my %seen_end;
    @occ = grep { !$seen_end{ $_->{end}->ymd }++ } @occ;

    return \@occ;
}

################################################################################
# Phase 3 helpers: bucketisation for high-frequency / multi-occurrence events
################################################################################

# ==============================================================================
# SUBROUTINE: _bucketise_completed_occurrences_by_month
#
# PURPOSE:
#   Groups occurrences into anchor buckets (YYYY-MM) based on occurrence end date,
#   keeping only those occurrences completed as-of the supplied reference date.
#
# RETURNS:
#   Hashref: { 'YYYY-MM' => [ {start=>..., end=>..., kind=>...}, ... ], ... }
# ==============================================================================
sub _bucketise_completed_occurrences_by_month {
    my ($occurrences_aref, $reference_date) = @_;

    return {} unless ($occurrences_aref && ref($occurrences_aref) eq 'ARRAY' && @$occurrences_aref);
    return {} unless ($reference_date && ref($reference_date) eq 'Time::Piece');

    my %buckets;
    foreach my $occ (@$occurrences_aref) {
        next unless $occ && $occ->{end} && ref($occ->{end}) eq 'Time::Piece';
        next unless _occurrence_is_completed_asof($occ->{end}, $reference_date);

        my $bucket = $occ->{end}->strftime('%Y-%m');
        push @{$buckets{$bucket}}, $occ;
    }

    return \%buckets;
}

# ==============================================================================
# SUBROUTINE: _max_occurrence_end
#
# PURPOSE:
#   Returns the maximum (latest) occurrence end date from a bucket list.
#
# RETURNS:
#   Time::Piece | undef
# ==============================================================================
sub _max_occurrence_end {
    my ($bucket_occ_aref) = @_;
    return undef unless ($bucket_occ_aref && ref($bucket_occ_aref) eq 'ARRAY' && @$bucket_occ_aref);

    my $max_end = undef;
    foreach my $occ (@$bucket_occ_aref) {
        next unless $occ && $occ->{end} && ref($occ->{end}) eq 'Time::Piece';
        $max_end = $occ->{end} unless defined $max_end;
        $max_end = $occ->{end} if ($occ->{end}->epoch > $max_end->epoch);
    }
    return $max_end;
}

# ==============================================================================
# SUBROUTINE: _enumerate_explicit_date_occurrences
#
# PURPOSE:
#   Parses an explicit 'dates' configuration value into candidate occurrences.
#
# SUPPORTED FORMS:
#   - YYYY-MM-DD
#   - YYYY-MM-DD:YYYY-MM-DD
#   - Comma-separated list of the above
#
# RETURNS:
#   Arrayref of { start => Time::Piece, end => Time::Piece, kind => 'dates' }
#
# ==============================================================================
sub _enumerate_explicit_date_occurrences {
    my ($dates_str, $analysis_start, $analysis_end) = @_;

    return [] unless (defined $dates_str && length($dates_str) && $analysis_start && $analysis_end);

    my @occ;
    my @items = split /\s*,\s*/, $dates_str;

    foreach my $item (@items) {
        next unless defined $item && length $item;

        my ($start_s, $end_s);
        if ($item =~ /:/) {
            ($start_s, $end_s) = split /\s*:\s*/, $item, 2;
        } else {
            $start_s = $item;
            $end_s   = $item;
        }

        my $start_obj = _parse_anchor_date_string($start_s);
        my $end_obj   = _parse_anchor_date_string($end_s);

        next unless ($start_obj && $end_obj);

        $start_obj = $start_obj->truncate(to => 'day');
        $end_obj   = $end_obj->truncate(to => 'day');

        # Normalise inverted ranges defensively
        if ($end_obj->epoch < $start_obj->epoch) {
            my $tmp = $start_obj;
            $start_obj = $end_obj;
            $end_obj = $tmp;
        }

        # Include if the occurrence end is within the analysis window bounds
        if ($end_obj->epoch >= $analysis_start->truncate(to => 'day')->epoch &&
            $end_obj->epoch <= $analysis_end->truncate(to => 'day')->epoch) {
            push @occ, { start => $start_obj, end => $end_obj, kind => 'dates' };
        }
    }

    return \@occ;
}

# ==============================================================================
# SUBROUTINE: _select_latest_completed_occurrence
#
# PURPOSE:
#   Selects the most recent completed occurrence (by end date) as-of the supplied
#   reference date.
#
# RETURNS:
#   Occurrence hashref, or undef if none completed.
#
# ==============================================================================
sub _select_latest_completed_occurrence {
    my ($occurrences_aref, $reference_date) = @_;

    return undef unless ($occurrences_aref && ref($occurrences_aref) eq 'ARRAY' && @$occurrences_aref);
    return undef unless ($reference_date && ref($reference_date) eq 'Time::Piece');

    my @completed = grep {
        $_->{end} && _occurrence_is_completed_asof($_->{end}, $reference_date)
    } @$occurrences_aref;

    return undef unless @completed;

    @completed = sort { $a->{end}->epoch <=> $b->{end}->epoch } @completed;
    return $completed[-1];
}

# ==============================================================================
# SUBROUTINE: _resolve_seasonal_anchor_date
#
# PURPOSE:
#   Determines the appropriate analysis reference date (anchor) for a seasonal
#   event. This is the SINGLE authoritative function for anchor resolution.
#
#   Phase 2: Priority 2/3 are implemented via occurrence enumeration and latest
#   completed occurrence selection (month-end + explicit dates), rather than
#   inline heuristics.
# ==============================================================================
sub _resolve_seasonal_anchor_date {
    my ($event_config, $analysis_start_date, $effective_end_date, $cli_reference_date) = @_;

    my $result = {
        anchor_date    => undef,
        anchor_bucket  => undef,
        anchor_source  => 'unknown',
        occurrence_end => undef,
        is_complete    => 0,
    };

    # Validate inputs
    unless ($event_config && $effective_end_date && ref($effective_end_date) eq 'Time::Piece') {
        warn "  [WARN] _resolve_seasonal_anchor_date: Missing required arguments\n";
        return $result;
    }

    # Default analysis start (needed for occurrence enumeration)
    if (!defined $analysis_start_date || ref($analysis_start_date) ne 'Time::Piece') {
        $analysis_start_date = $effective_end_date->truncate(to => 'day');
    }

    # --- Priority 1: User-specified CLI override ---
    # Explicit CLI override is treated as an intentional anchor and is NOT subject
    # to smart snapping. We still compute occurrence_end for metadata.
    if (defined $cli_reference_date && length $cli_reference_date) {
        my $cli_anchor = _parse_anchor_date_string($cli_reference_date);
        if ($cli_anchor) {
            $cli_anchor = $cli_anchor->truncate(to => 'day');

            $result->{anchor_date}   = $cli_anchor;
            $result->{anchor_bucket} = $cli_anchor->strftime('%Y-%m');
            $result->{anchor_source} = 'cli_override';
            $result->{is_complete}   = 1;

            my ($occ_start, $occ_end) = determine_event_period($event_config, $cli_anchor, 'historical');
            $result->{occurrence_end} = $occ_end if $occ_end;

            return $result;
        }
    }

    # --- Priority 2/3: Event-bound anchoring via occurrence enumeration ---
    # For month-end and explicit 'dates' events, enumerate candidate occurrences
    # within the analysis window and select the latest completed occurrence.
    my $occurrences_aref = _enumerate_event_occurrences($event_config, $analysis_start_date, $effective_end_date);
    if ($occurrences_aref && ref($occurrences_aref) eq 'ARRAY' && @$occurrences_aref) {
        my $latest = _select_latest_completed_occurrence($occurrences_aref, $effective_end_date);

        if ($latest && $latest->{end}) {
            my $anchor = $latest->{end}->truncate(to => 'day');

            $result->{anchor_date}    = $anchor;
            $result->{anchor_bucket}  = $anchor->strftime('%Y-%m');
            $result->{occurrence_start} = $latest->{start} ? $latest->{start}->truncate(to => 'day') : undef;
            $result->{anchor_source}  = 'occurrence_end';
            $result->{occurrence_end} = $anchor;
            $result->{is_complete}    = 1;

            return $result;
        }

        # Occurrences exist, but none completed as-of reference date
        my @sorted = sort { $a->{end}->epoch <=> $b->{end}->epoch } @$occurrences_aref;
        my $most_recent = $sorted[-1];
        $result->{occurrence_end} = $most_recent->{end} if ($most_recent && $most_recent->{end});

        $result->{anchor_date}   = $effective_end_date->truncate(to => 'day');
        $result->{anchor_bucket} = $result->{anchor_date}->strftime('%Y-%m');
        $result->{anchor_source} = 'no_completed_occurrence';
        $result->{is_complete}   = 0;

        return $result;
    }

    # --- Priority 4: Fallback to effective end date for free-running events ---
    $result->{anchor_date}   = $effective_end_date->truncate(to => 'day');
    $result->{anchor_bucket} = $result->{anchor_date}->strftime('%Y-%m');
    $result->{anchor_source} = 'fallback_effective_end';
    $result->{is_complete}   = 1;

    # Best-effort occurrence metadata (may be undef for free-running events)
    my ($occ_start, $occ_end) = determine_event_period($event_config, $effective_end_date, 'historical');
    $result->{occurrence_start} = $occ_start if $occ_start;
    $result->{occurrence_end} = $occ_end if $occ_end;

    return $result;
}

# ==============================================================================
# SUBROUTINE: _occurrence_is_completed_asof
#
# PURPOSE:
#   Determines whether an event occurrence is complete as-of a reference date.
#   An occurrence is complete if its end date is on or before the reference date.
#
# ARGUMENTS:
#   $occurrence_end (Time::Piece) - The end date of the event occurrence
#   $reference_date (Time::Piece) - The date against which to evaluate completion
#
# RETURNS:
#   1 if the occurrence is complete, 0 otherwise
#
# NOTES:
#   - Comparison is done at day granularity (time-of-day is ignored)
#   - An occurrence ending on the reference date IS considered complete
#     (the reference date spans the entire day)
#
# ==============================================================================
sub _occurrence_is_completed_asof {
    my ($occurrence_end, $reference_date) = @_;

    return 0 unless ($occurrence_end && $reference_date);

    # Compare at day granularity
    my $occ_day = $occurrence_end->truncate(to => 'day');
    my $ref_day = $reference_date->truncate(to => 'day');

    # Completed if occurrence end is on or before reference date
    return ($occ_day->epoch <= $ref_day->epoch) ? 1 : 0;
}

# ==============================================================================
# SUBROUTINE: _build_anchor_execution_context
#
# PURPOSE:
#   Builds a complete execution context for a seasonal model run, incorporating
#   the resolved anchor information. This context is passed to model executors.
#
# ARGUMENTS:
#   $anchor_info   (hashref) - Output from _resolve_seasonal_anchor_date
#   $base_args     (hashref) - Base args_href from caller
#   $event_config  (hashref) - Event configuration
#
# RETURNS:
#   hashref with execution context including:
#     - All base args
#     - Resolved anchor date and bucket
#     - Effective analysis window (clamped to anchor)
#     - Anchor metadata for logging/storage
#
# ==============================================================================
sub _build_anchor_execution_context {
    my ($anchor_info, $base_args, $event_config) = @_;

    # Start with a copy of base args
    my %ctx = %$base_args;

    # Inject anchor information
    $ctx{anchor_date}        = $anchor_info->{anchor_date};
    $ctx{anchor_bucket}      = $anchor_info->{anchor_bucket};
    $ctx{anchor_source}      = $anchor_info->{anchor_source};
    $ctx{anchor_is_complete} = $anchor_info->{is_complete};

    # Propagate occurrence boundaries for downstream event-period filtering
    $ctx{occurrence_start}   = $anchor_info->{occurrence_start};
    $ctx{occurrence_end}     = $anchor_info->{occurrence_end};

    # Clamp effective_end_date to anchor if anchor is earlier
    # This ensures time-blinded analysis: we cannot use data beyond the anchor
    if ($anchor_info->{anchor_date} && $ctx{effective_end_date}) {
        if ($anchor_info->{anchor_date}->epoch < $ctx{effective_end_date}->epoch) {
            $ctx{effective_end_date} = $anchor_info->{anchor_date};
        }
    }

    # Clamp effective_start_date based on occurrence start and baseline_period_days
    # This prevents prior event occurrences from contaminating the analysis window
    if ($anchor_info->{occurrence_start} && $event_config) {
        my $baseline_days = $event_config->{baseline_period_days} // 16;

        # Analysis should start at: occurrence_start - baseline_period_days
        # This gives enough room for baseline calculation without including prior events
        my $bounded_start = $anchor_info->{occurrence_start} - ($baseline_days * ONE_DAY);

        # Only tighten the window; never expand beyond original bounds
        if ($ctx{effective_start_date} && $bounded_start->epoch > $ctx{effective_start_date}->epoch) {
            $ctx{effective_start_date} = $bounded_start;
        }
    }

    # Compute the effective analysis window string for the nfit engine
    $ctx{analysis_reference_date_str} = $anchor_info->{anchor_date}
        ? $anchor_info->{anchor_date}->ymd
        : $ctx{effective_end_date}->ymd;

    # ----------------------------------------------------------------------
    # Phase 3.1: bucket provenance (auditability only; does not alter maths)
    # ----------------------------------------------------------------------
    if ($anchor_info->{bucket_occurrences} && ref($anchor_info->{bucket_occurrences}) eq 'ARRAY') {
        $ctx{anchor_bucket_key} = $anchor_info->{anchor_bucket};
        $ctx{bucket_occurrences} = $anchor_info->{bucket_occurrences};
        $ctx{bucket_occurrence_count} = scalar @{$anchor_info->{bucket_occurrences}};
        $ctx{bucket_occurrence_end_dates} = [
            map { ($_->{end} && ref($_->{end}) eq 'Time::Piece') ? $_->{end}->ymd : () }
            @{$anchor_info->{bucket_occurrences}}
        ];
    }

    return \%ctx;
}

# ==============================================================================
# SUBROUTINE: _parse_anchor_date_string
#
# PURPOSE:
#   Safely parses a date string (YYYY-MM-DD format) into a Time::Piece object.
#
# ARGUMENTS:
#   $date_str (string) - Date in YYYY-MM-DD format
#
# RETURNS:
#   Time::Piece object on success, undef on failure
#
# ==============================================================================
sub _parse_anchor_date_string {
    my ($date_str) = @_;

    return undef unless (defined $date_str && $date_str =~ /^\d{4}-\d{2}-\d{2}$/);

    my $parsed;
    eval {
        $parsed = Time::Piece->strptime($date_str, '%Y-%m-%d');
    };

    return $@ ? undef : $parsed;
}

# ==============================================================================
# SUBROUTINE: _execute_multiplicative_seasonal
# PURPOSE:    Executes the Multiplicative Seasonal model for a specific event.
# ==============================================================================
sub _execute_multiplicative_seasonal {
    my ($event_name, $event_config, $current_cache_path, $profiles_aref, $exec_ctx, $seasonality_config, $opts_href) = @_;

    # Phase 3.2: Output control flags (suppressed during intra-bucket aggregation)
    my $store_history  = 1;
    my $emit_csv       = 1;
    my $emit_rationale = 1;

    if ($opts_href && ref($opts_href) eq 'HASH') {
        $store_history  = ($opts_href->{store_history}  // 1) ? 1 : 0;
        $emit_csv       = ($opts_href->{emit_csv}       // 1) ? 1 : 0;
        $emit_rationale = ($opts_href->{emit_rationale} // 1) ? 1 : 0;
    }

    # 1. Determine analysis path (history check) & Load Config
    my $effective_event_name = determine_seasonal_analysis_path(
        $event_config, $current_cache_path, $event_name
    );

    # Handle cold-start graceful skip
    unless (defined $effective_event_name) {
        print STDERR "  [-] Skipping forecast for event $event_name (cold start; execution of `--update-history` is required)\n";
        return;
    }

    my $effective_config = $event_config;
    if ($effective_event_name ne $event_name) {
        $effective_config = $seasonality_config->{$effective_event_name} // $event_config;
        print STDERR "  [INFO] Falling back to event '$effective_event_name' (due to history constraints)\n";
    }

    # 2. Setup Flags
    $is_multiplicative_forecast_run = 1;
    $apply_seasonality_event = $event_name; # Set global for logging context

    # 3. Prepare Exclusions
    my $exclusions = undef;
    if (defined $effective_config->{exclude_dates}) {
        my $known_vms = _get_known_vms_from_cache($current_cache_path);
        $exclusions = _parse_exclusion_dates($effective_config->{exclude_dates}, $known_vms);
    }

    # 4. Run nFit Analysis
    my $parsed_results = run_single_pass_analysis(
        $current_cache_path,
        $profiles_aref,
        {
            %$exec_ctx,
            enable_windowed_decay   => 1,  # Required for growth/baseline stats
            decay_over_states       => 0,  # Ensure we decay over TIME, not state changes
            enable_growth_prediction => 1, # Must be True if windowed_decay is True
            vm_name => undef,
            exclusions => $exclusions,
            start_date => $exec_ctx->{effective_start_date}->ymd,
            end_date   => $exec_ctx->{effective_end_date}->ymd,
        }
    );

    # 5. Assimilate Results
    my $assimilation_map = build_assimilation_map($parsed_results, $profiles_aref, $exec_ctx->{adaptive_saturation_thresh});
    @vm_order = sort keys %{$assimilation_map};

    # 5a. Phase 3.3: Apply VM scope filter if configured (supports 'vms' and 'exclude_vms')
    my $vm_scope_filter = _parse_vm_scope_filter($event_config);
    my $vm_scope_meta = _apply_vm_scope_filter(
        $vm_scope_filter,
        \@vm_order,
        $event_name,
        $assimilation_map  # Use assimilation_map keys as cache VM set
    );

    # Early exit if all VMs filtered out
    if ($vm_scope_meta && scalar(@vm_order) == 0) {
        print STDERR "  [-] Skipping forecast storage for event '$event_name' (no matching VMs after scope filter)\n";
        return {};
    }

    # 5b. Correct PeakValue to reflect EVENT PERIOD only, not baseline period
    # The engine window may include baseline days (for efficiency), but PeakValue
    # must be the maximum observed during the actual event occurrence(s).
    my $event_start_epoch = $exec_ctx->{occurrence_start}
        ? $exec_ctx->{occurrence_start}->epoch
        : $exec_ctx->{effective_end_date}->epoch - (($effective_config->{duration_days} // 1) * 86400);
    my $event_end_epoch = $exec_ctx->{effective_end_date}->epoch;

    foreach my $vm_name (@vm_order) {
        my $states_aref = $parsed_results->{$vm_name};
        next unless ref($states_aref) eq 'ARRAY' && @$states_aref;

        my @event_period_peaks;
        foreach my $state (@$states_aref) {
            # Check if this state falls within the event period
            my $state_end_str = _safe_dig($state, 'metadata', 'end_date');
            next unless $state_end_str;

            my $state_end_obj;
            eval { $state_end_obj = Time::Piece->strptime($state_end_str, '%Y-%m-%d'); };
            next if $@ || !$state_end_obj;

            # Only include states that end within or after the event start
            if ($state_end_obj->epoch >= $event_start_epoch) {
                my $peak_val = _safe_dig($state, 'metrics', 'physc', $MANDATORY_PEAK_PROFILE_FOR_HINT, 'Peak');
                push @event_period_peaks, $peak_val if (defined $peak_val && looks_like_number($peak_val));
            }
        }

        # Override PeakValue with max from event period only
        if (@event_period_peaks) {
            $assimilation_map->{$vm_name}{CoreResults}{PeakValue} = max(@event_period_peaks);
        }
    }

    # Phase 6: Extract pre-computed history baseline (undef if --use-l1-baseline)
    my $precomputed_baselines = ($exec_ctx->{_baseline_result})
        ? $exec_ctx->{_baseline_result}{baselines} : undef;

    # 6. Calculate Forecast
    my ($forecast_results, $historic_data) = calculate_multiplicative_forecast(
        $current_cache_path,
        $exec_ctx->{system_identifier},
        $effective_event_name,
        $effective_config,
        $seasonality_config,
        $assimilation_map,
        $exclusions,
        $exec_ctx->{effective_start_date},
        $exec_ctx->{effective_end_date},
        $precomputed_baselines     # Phase 6: history-based baseline (undef → L1 fallback)
    );

    # Merge Forecast Results (preserving event-period measurement values)
    # The assimilation_map was built from the engine run on the event window.
    # PeakValue and P-99W1 in assimilation_map represent the EVENT PERIOD observations.
    # We must preserve these before overwriting ProfileValues with forecast results.

    # Pre-extract P-99W1 metric key for fallback extraction from parsed_results
    my $p99w1_p_metric_key;
    my ($p99w1_profile) = grep { $_->{name} eq $MANDATORY_PEAK_PROFILE_FOR_HINT } @$profiles_aref;
    if ($p99w1_profile) {
        my ($p_val_num) = $p99w1_profile->{flags} =~ /(?:-p|--percentile)\s+([0-9.]+)/;
        $p99w1_p_metric_key = "P" . clean_perc_label($p_val_num // $DEFAULT_PERCENTILE);
    }

    foreach my $vm_name (@vm_order) {
        # Preserve measurement values BEFORE overwriting
        my $preserved_peak_value = $assimilation_map->{$vm_name}{CoreResults}{PeakValue};
        my $preserved_p99w1 = $assimilation_map->{$vm_name}{CoreResults}{ProfileValues}{$MANDATORY_PEAK_PROFILE_FOR_HINT};

        # Fallback: If P-99W1 not in assimilation_map, extract directly from parsed_results
        if (!defined $preserved_p99w1 && $p99w1_p_metric_key) {
            my $states_aref = $parsed_results->{$vm_name};
            if (ref($states_aref) eq 'ARRAY' && @$states_aref) {
                my @p99w1_values;
                foreach my $state (@$states_aref) {
                    # Try aggregated format first (FinalValue)
                    my $val = _safe_dig($state, 'metrics', 'physc', $MANDATORY_PEAK_PROFILE_FOR_HINT, 'FinalValue');
                    # Fallback to P-metric key
                    $val //= _safe_dig($state, 'metrics', 'physc', $MANDATORY_PEAK_PROFILE_FOR_HINT, $p99w1_p_metric_key);
                    push @p99w1_values, $val if (defined $val && looks_like_number($val));
                }
                $preserved_p99w1 = max(@p99w1_values) if @p99w1_values;
            }
        }

        # Merge forecasted profile values (overwrites ProfileValues hash)
        if (exists $forecast_results->{$vm_name}) {
            $assimilation_map->{$vm_name}{CoreResults}{ProfileValues} = $forecast_results->{$vm_name};
        }

        # Restore PeakValue (immutable historical fact - not a forecast)
        if (defined $preserved_peak_value && looks_like_number($preserved_peak_value)) {
            $assimilation_map->{$vm_name}{CoreResults}{PeakValue} = $preserved_peak_value;
        }

        # Restore P-99W1 (measurement profile - exempt from forecasting)
        if (defined $preserved_p99w1 && looks_like_number($preserved_p99w1)) {
            $assimilation_map->{$vm_name}{CoreResults}{ProfileValues}{$MANDATORY_PEAK_PROFILE_FOR_HINT} = $preserved_p99w1;
        }
    }

    # DEBUG: Trace PeakValue source
    if (($verbose // 0) > 1) {
        my $sample_vm = (keys %$assimilation_map)[0];
        if ($sample_vm && ($opts_href->{emit_csv} // 1)) {
            my $peak_val = $assimilation_map->{$sample_vm}{CoreResults}{PeakValue};
            print STDERR "  [DEBUG] CSV-emitting run for bucket: " . ($exec_ctx->{anchor_bucket} // 'N/A') . "\n";
            print STDERR "  [DEBUG] effective_start_date: " . ($exec_ctx->{effective_start_date} ? $exec_ctx->{effective_start_date}->ymd : 'undef') . "\n";
            print STDERR "  [DEBUG] effective_end_date: " . ($exec_ctx->{effective_end_date} ? $exec_ctx->{effective_end_date}->ymd : 'undef') . "\n";
            print STDERR "  [DEBUG] occurrence_start: " . ($exec_ctx->{occurrence_start} ? $exec_ctx->{occurrence_start}->ymd : 'undef') . "\n";
            print STDERR "  [DEBUG] Sample VM '$sample_vm' PeakValue: " . ($peak_val // 'undef') . "\n";
        }
    }

    # --- ENRICHMENT LOOP ---
    foreach my $vm_name (@vm_order) {
        my $vm_map_ref = $assimilation_map->{$vm_name};
        $vm_map_ref->{Configuration}{vm_name} = $vm_name;
        $vm_map_ref->{RunQMetrics} //= $per_profile_runq_metrics{$vm_name} || {};

        # Generate Hints
        my ($hint_type_tier, $hint_pattern_shape, $hint_pressure_bool, $pressure_detail_str, $rationale, $has_abs, $has_norm) =
            generate_sizing_hint(
                $vm_map_ref,
                undef,
                $exec_ctx->{adaptive_saturation_thresh}
            );

        # Store Hints & Globals
        $vm_map_ref->{Hinting}{AutoTier}       = $hint_type_tier;
        $vm_map_ref->{Hinting}{Pattern}        = $hint_pattern_shape;
        $vm_map_ref->{Hinting}{Pressure}       = $hint_pressure_bool;
        $vm_map_ref->{Hinting}{PressureDetail} = $pressure_detail_str // 'N/A';
        $hint_tier_for_csv{$vm_name}           = $hint_type_tier;
        $hint_pattern_for_csv{$vm_name}        = $hint_pattern_shape;

        # Populate FinalTierForVM (Critical for STD logic)
        $vm_map_ref->{Hinting}{FinalTierForVM} = $vm_tier_overrides{$vm_name} // $hint_type_tier;
        $tier_override_for_csv{$vm_name}       = $vm_tier_overrides{$vm_name} // "";

        # Prepare for RunQ Modifiers
        my $user_tier = $vm_tier_overrides{$vm_name} // "";
        my $pattern_source = ($user_tier ne "") ? $user_tier : $hint_type_tier;
        my ($pattern_char) = ($pattern_source =~ /^([A-Z])/);
        $pattern_char //= 'G';
        my %pat_map = ('O'=>'O3-95W15', 'B'=>'B3-95W15', 'G'=>'G3-95W15', 'P'=>'G3-95W15');
        my $runq_source_profile_name = $pat_map{$pattern_char} // 'G3-95W15';

        foreach my $profile (@$profiles_aref) {
            my $p_name = $profile->{name};

            # --- GUARD: Skip RunQ Modifiers for P-99W1 ---
            next if ($p_name eq $MANDATORY_PEAK_PROFILE_FOR_HINT);

            my %pressure_flags = ( abs_pressure => $has_abs, norm_pressure => $has_norm );
            my %adaptive_thresholds = %{ $exec_ctx->{adaptive_thresholds_href} // {} };
            $adaptive_thresholds{saturation}    //= $exec_ctx->{adaptive_saturation_thresh};
            $adaptive_thresholds{target}        //= $exec_ctx->{adaptive_target_norm_runq};
            $adaptive_thresholds{max_reduction} //= $exec_ctx->{adaptive_max_efficiency_reduction};

            my ($adjusted_physc, $debug_info_ref) = calculate_runq_modified_physc(
                $vm_name, $vm_map_ref, $profile, \%pressure_flags, \%adaptive_thresholds
            );

            if (looks_like_number($adjusted_physc)) {
                $vm_map_ref->{CoreResults}{ProfileValues}{$p_name} = sprintf("%.4f", $adjusted_physc);
            }

            # Persist per-profile debug info for downstream consumers (CSV rVCPU scaling, later rationale refinements, etc.)
            if (defined $debug_info_ref && ref($debug_info_ref) eq 'HASH') {
                $vm_map_ref->{CoreResults}{ProfileDebugInfo}{$p_name} = $debug_info_ref;
            }

            # --- rVCPU (per profile): stress-driven, slot-aware, rENT-bounded, peak-aware ---
            # Store for CSV + rationale (do NOT recompute later in CSV scope).
            my $rvcpu_final = undef;
            my $rvcpu_raw = undef;
            my $rvcpu_min = undef;
            my $rvcpu_max = undef;
            my $p99w1_floor_vcpu = undef;
            my $p99w1_factor_eff = undef;
            my $p99w1_mult = undef;
            my $cred_ratio = 1.00;
            my $p99w1_adjusted;
            my $p99w1_cores;

            my $rENT_P = (defined $adjusted_physc && looks_like_number($adjusted_physc)) ? ($adjusted_physc + 0) : undef;
            my $smt_used = (defined $vm_map_ref->{Configuration}{smt} && looks_like_number($vm_map_ref->{Configuration}{smt}))
                         ? ($vm_map_ref->{Configuration}{smt} + 0)
                         : $DEFAULT_SMT;

            # Use the SAME AbsRunQ key/value selected inside calculate_runq_modified_physc()
            # to avoid divergence between rationale, modifier logic, and rVCPU.
            #
            # IMPORTANT: keep the key/value paired. If the value comes from AbsRunQValueUsedForCalc,
            # then the key MUST come from AbsRunQKeyUsed (or a safe fallback), otherwise we risk
            # misleading output such as "RunQ=AbsRunQ_P95=<value from P50>".
            my $runq_key = undef;
            my $runq_hi_threads = undef;

            if (defined $debug_info_ref && ref($debug_info_ref) eq 'HASH') {
                if (defined $debug_info_ref->{AbsRunQKeyUsed} && $debug_info_ref->{AbsRunQKeyUsed} =~ /^AbsRunQ_P\d+$/) {
                    $runq_key = $debug_info_ref->{AbsRunQKeyUsed};
                }

                if (defined $debug_info_ref->{AbsRunQValueUsedForCalc} && looks_like_number($debug_info_ref->{AbsRunQValueUsedForCalc})) {
                    $runq_hi_threads = $debug_info_ref->{AbsRunQValueUsedForCalc} + 0;
                    # If we got a value but the key is missing/invalid, fall back to the headroom key
                    # (which is derived from the same selection chain) or to a conservative default.
                    if (!defined $runq_key) {
                        if (defined $debug_info_ref->{Headroom_AbsRunQKey} && $debug_info_ref->{Headroom_AbsRunQKey} =~ /^AbsRunQ_P\d+$/) {
                            $runq_key = $debug_info_ref->{Headroom_AbsRunQKey};
                        } else {
                            $runq_key = 'AbsRunQ_P90';
                        }
                    }
                }
            }

            # Fallback chain only if the selected value is not available.
            if (!defined $runq_hi_threads) {
                $runq_key = 'AbsRunQ_P90' if (!defined $runq_key);
                if (defined $vm_map_ref->{RunQMetrics} && ref($vm_map_ref->{RunQMetrics}) eq 'HASH') {
                    if (defined $vm_map_ref->{RunQMetrics}{$runq_key} && looks_like_number($vm_map_ref->{RunQMetrics}{$runq_key})) {
                        $runq_hi_threads = $vm_map_ref->{RunQMetrics}{$runq_key} + 0;
                    } elsif (defined $vm_map_ref->{RunQMetrics}{AbsRunQ_P90} && looks_like_number($vm_map_ref->{RunQMetrics}{AbsRunQ_P90})) {
                        $runq_key = 'AbsRunQ_P90';
                        $runq_hi_threads = $vm_map_ref->{RunQMetrics}{AbsRunQ_P90} + 0;
                    }
                }
            }

            # Tier-aware headroom: extract tier from profile name (e.g., "G3-50W15" -> "G3" -> tier 3).
            my $ms_tier_str = $p_name;
            $ms_tier_str =~ s/-.*//;
            my $rvcpu_tier_num = 3;  # Default baseline
            if ($ms_tier_str =~ /(\d)$/) {
                $rvcpu_tier_num = $1;
            } elsif ($ms_tier_str eq 'P') {
                $rvcpu_tier_num = 1;
            }
            my $rvcpu_tier_factor = $VCPU_TIER_HEADROOM_FACTORS{$rvcpu_tier_num} // 1.00;

            if (defined $rENT_P && $rENT_P > 0 && defined $smt_used && $smt_used > 0 && defined $runq_hi_threads && $runq_hi_threads > 0) {
                # Workload width to target headroom, tier-adjusted
                my $required_threads = ($runq_hi_threads / $RUNQ_TARGET_HEADROOM_FACTOR) * $rvcpu_tier_factor;
                my $required_cores = $required_threads / $smt_used;
                $rvcpu_raw = int($required_cores); $rvcpu_raw++ if ($required_cores > $rvcpu_raw);

                # Vendor envelope from rENT_P
                my $min_v = $rENT_P * $VCPU_RATIO_MIN;
                my $max_v = $rENT_P * $VCPU_RATIO_MAX;
                $rvcpu_min = int($min_v); $rvcpu_min++ if ($min_v > $rvcpu_min);
                $rvcpu_max = int($max_v); $rvcpu_max++ if ($max_v > $rvcpu_max);

                # Enforce minimum VP floor (evenisation removed)
                if (defined $rvcpu_min) {
                    $rvcpu_min = 1 if ($rvcpu_min < 1);
                }
                if (defined $rvcpu_max) {
                    $rvcpu_max = 1 if ($rvcpu_max < 1);
                    $rvcpu_max = $rvcpu_min if (defined $rvcpu_min && $rvcpu_max < $rvcpu_min);
                }

                $rvcpu_final = $rvcpu_raw;
                $rvcpu_final = $rvcpu_min if ($rvcpu_final < $rvcpu_min);
                $rvcpu_final = $rvcpu_max if ($rvcpu_final > $rvcpu_max);

                # Low-entitlement minimums
                $rvcpu_final = $VCPU_MIN_ABS if ($rvcpu_final < $VCPU_MIN_ABS);
                if ($rENT_P >= 0.5 && $rENT_P < 1.0 && $rvcpu_final < $VCPU_MIN_FOR_ENT_GE_0_5_LT_1) {
                    $rvcpu_final = $VCPU_MIN_FOR_ENT_GE_0_5_LT_1;
                }

                # Peak floor: PAE/AAE-driven headroom, tier-scaled.
                # Base is 1.00 (VCPUs must at minimum cover peak cores).
                # Headroom proportional to empirical above-entitlement evidence,
                # amplified by tier policy. Avoids multiplicative compounding.
                $p99w1_cores = $vm_map_ref->{CoreResults}{ProfileValues}{$MANDATORY_PEAK_PROFILE_FOR_HINT};
                if (defined $p99w1_cores && looks_like_number($p99w1_cores) && $p99w1_cores > 0) {
                    # RunQ credibility adjustment
                    $p99w1_adjusted = $p99w1_cores;
                    $cred_ratio = 1.00;

                    # Credibility uses a tail-to-tail comparison: P99W1 PhysC vs AbsRunQ_P99W1.
                    # Prefer the peak-helper-aligned W1 RunQ tail for credibility, so we
                    # compare like-for-like: P99W1 PhysC vs AbsRunQ_P99W1 (window=1).
                    my $runq_p99w1_for_cred =
                        $vm_map_ref->{RunQMetrics}{AbsRunQ_P99W1} //
                        $vm_map_ref->{RunQMetrics}{AbsRunQ_P99};

                    if (defined $runq_p99w1_for_cred && $runq_p99w1_for_cred > 0 && defined $smt_used && $smt_used > 0) {
                        my $runq_peak_cores = $runq_p99w1_for_cred / $smt_used;
                        $cred_ratio = $runq_peak_cores / $p99w1_cores;
                        my $credibility = $cred_ratio + $VCPU_PEAK_CRED_BIAS;
                        $credibility = $VCPU_PEAK_CRED_MIN if ($credibility < $VCPU_PEAK_CRED_MIN);
                        $credibility = 1.00 if ($credibility > 1.00);
                        $p99w1_adjusted = $p99w1_cores * $credibility;
                    }

                    my $pae_raw = 0.0;
                    my $aae_raw = 0.0;
                    if (defined $debug_info_ref && ref($debug_info_ref) eq 'HASH') {
                        $pae_raw = (defined $debug_info_ref->{'PhysC_Daily_PAE'} && looks_like_number($debug_info_ref->{'PhysC_Daily_PAE'}))
                                 ? ($debug_info_ref->{'PhysC_Daily_PAE'} + 0) : 0.0;
                        $aae_raw = (defined $debug_info_ref->{'PhysC_Daily_AAE'} && looks_like_number($debug_info_ref->{'PhysC_Daily_AAE'}))
                                 ? ($debug_info_ref->{'PhysC_Daily_AAE'} + 0) : 0.0;
                    }

                    my $pae_component = ($pae_raw > 0) ? (($pae_raw < 0.30 ? $pae_raw : 0.30) / 0.30) * 0.15 : 0.0;
                    my $aae_component = ($aae_raw > 0) ? (($aae_raw < 0.10 ? $aae_raw : 0.10) / 0.10) * 0.15 : 0.0;

                    my $headroom = ($pae_component + $aae_component) * $rvcpu_tier_factor;
                    $p99w1_factor_eff = 1.00 + $headroom;
                    $p99w1_mult = $headroom;  # Rationale: the headroom portion above 1.00

                    my $floor_v = ($p99w1_adjusted + 0) * $p99w1_factor_eff;

                    $p99w1_floor_vcpu = int($floor_v); $p99w1_floor_vcpu++ if ($floor_v > $p99w1_floor_vcpu);
                    # Enforce minimum floor (evenisation removed)
                    if (defined $p99w1_floor_vcpu) {
                        $p99w1_floor_vcpu = 1 if ($p99w1_floor_vcpu < 1);
                    }

                    if ($p99w1_floor_vcpu > $rvcpu_final) {
                        $rvcpu_final = $p99w1_floor_vcpu;
                        $rvcpu_final = $rvcpu_max if (defined $rvcpu_max && $rvcpu_final > $rvcpu_max);
                    }
                }

                # Enforce minimum (evenisation removed)
                $rvcpu_final = $VCPU_MIN_ABS if (defined $rvcpu_final && $rvcpu_final < $VCPU_MIN_ABS);
            }

            $vm_map_ref->{CoreResults}{Profile_rVCPU}{$p_name} = defined($rvcpu_final) ? $rvcpu_final : "";
            $vm_map_ref->{CoreResults}{Profile_rVCPU_Explain}{$p_name} = {
                Raw            => $rvcpu_raw,
                Min            => $rvcpu_min,
                Max            => $rvcpu_max,
                EntCores        => (defined $rENT_P ? sprintf("%.2f", $rENT_P) : undef),
                # Planning envelope and dispatch reserve: not computed in seasonal path
                # (seasonal model recalculates rVCPU from baseline without full modifier chain)
                EntCoresPlanning           => undef,
                EnvelopeExceedsOperational => 0,
                DispatchReserveVP          => 0,
                DispatchReserveReason      => '',
                DispatchReservePreFinal    => undef,
                TierNum        => $rvcpu_tier_num,
                TierFactor     => sprintf("%.2f", $rvcpu_tier_factor),
                RunQKeyUsed    => defined($runq_key) ? $runq_key : undef,
                RunQHiThreads  => (defined $runq_hi_threads ? sprintf("%.2f", $runq_hi_threads) : undef),
                P99W1Floor     => $p99w1_floor_vcpu,
                P99W1FactorEff => (defined $p99w1_factor_eff ? sprintf("%.3f", $p99w1_factor_eff) : undef),
                P99W1Mult      => (defined $p99w1_mult ? sprintf("%.3f", $p99w1_mult) : undef),
                P99W1CredRatio => sprintf("%.3f", $cred_ratio),
                P99W1AdjPeak   => sprintf("%.2f", $p99w1_adjusted),
                P99W1RawPeak   => sprintf("%.2f", $p99w1_cores),
                # Micro-partition override: not computed in seasonal path
                RunQEnvelopeOverride => "No",
                RunQEnvelopeBasis    => undef,
                OrigMin              => undef,
                OrigMax              => undef,
            };
            if (defined $debug_info_ref && ref($debug_info_ref) eq 'HASH') {
                # Persist the paired key/value used for rVCPU so rationale cannot drift.
                $debug_info_ref->{'rVCPU_RunQKeyUsed'}   = defined($runq_key) ? $runq_key : "N/A";
                $debug_info_ref->{'rVCPU_TierNum'}       = $rvcpu_tier_num;
                $debug_info_ref->{'rVCPU_TierFactor'}    = sprintf("%.2f", $rvcpu_tier_factor);
                $debug_info_ref->{'rVCPU_RunQHiThreads'} = defined($runq_hi_threads) ? sprintf("%.2f",$runq_hi_threads) : "N/A";
                $debug_info_ref->{'rVCPU_Raw'} = defined($rvcpu_raw) ? $rvcpu_raw : "N/A";
                $debug_info_ref->{'rVCPU_Min'} = defined($rvcpu_min) ? $rvcpu_min : "N/A";
                $debug_info_ref->{'rVCPU_Max'} = defined($rvcpu_max) ? $rvcpu_max : "N/A";
                $debug_info_ref->{'rVCPU_P99W1Floor'} = defined($p99w1_floor_vcpu) ? $p99w1_floor_vcpu : "N/A";
                $debug_info_ref->{'rVCPU_P99W1FactorEff'} = defined($p99w1_factor_eff) ? sprintf("%.3f",$p99w1_factor_eff) : "N/A";
                $debug_info_ref->{'rVCPU_P99W1Mult'} = defined($p99w1_mult) ? sprintf("%.3f",$p99w1_mult) : "N/A";
                $debug_info_ref->{'rVCPU_P99W1CredRatio'} = sprintf("%.3f", $cred_ratio);
                $debug_info_ref->{'rVCPU_P99W1AdjPeak'}   = sprintf("%.2f", $p99w1_adjusted);
                $debug_info_ref->{'rVCPU_P99W1RawPeak'}   = sprintf("%.2f", $p99w1_cores);
                $debug_info_ref->{'rVCPU_Final'} = defined($rvcpu_final) ? $rvcpu_final : "N/A";
            }

            if ($p_name eq $runq_source_profile_name) {
                $vm_map_ref->{CSVModifiers}{RunQ_Tactical}  = $debug_info_ref->{'FinalAdditive'};
                $vm_map_ref->{CSVModifiers}{RunQ_Strategic} = $debug_info_ref->{'RunQ_Strategic'};
                $vm_map_ref->{CSVModifiers}{RunQ_Potential} = $debug_info_ref->{'RunQ_Potential'};
                $vm_map_ref->{CSVModifiers}{RunQ_Source}    = $runq_source_profile_name;
            }
        }
    }

    # 7. Generate CSV (Rolling Numbering)
    # Phase 3.2: suppressed during aggregation iterations; anchor_bucket embedded for backfill
    if ($emit_csv) {
        my $date_str = localtime->strftime('%Y%m%d');
        my $anchor_bucket = $exec_ctx->{anchor_bucket} // '';

        # Prefix anchor_bucket to unique_suffix for backfill disambiguation
        my $unique_suffix;
        if ($anchor_bucket =~ /^\d{4}-\d{2}$/) {
            $unique_suffix = "$anchor_bucket.$date_str";
        } else {
            $unique_suffix = $date_str;
        }

        # Embed anchor_bucket in filename for backfill disambiguation
        my $base_name = "nfit-profile." . ($exec_ctx->{system_identifier}//'') . "." . $event_name;
        $base_name =~ s/[^a-zA-Z0-9_.-]//g;

        my $counter = 0;
        while (-e File::Spec->catfile($output_dir, "$base_name.$unique_suffix.csv")) {
            $counter++;
            if ($anchor_bucket =~ /^\d{4}-\d{2}$/) {
                $unique_suffix = "$anchor_bucket.$date_str-$counter";
            } else {
                $unique_suffix = "$date_str-$counter";
            }
        }

        _write_standard_csv_report(
            $assimilation_map,
            $event_name,
            $exec_ctx->{system_identifier},
            $unique_suffix,
            1, # is_multiplicative
            0,
            0,
            $exec_ctx->{adaptive_saturation_thresh},
            $exec_ctx->{adaptive_thresholds_href},
            $anchor_bucket  # Pass anchor_bucket for potential column inclusion
        );

        # Verbose Output Files
        if ($exec_ctx->{verbose}) {
            print STDERR "  ◆ Audit Trail\n";

            # Reconstruct baseline data map from debug info
            my %baseline_data_for_verbose;
            foreach my $vm (keys %seasonal_debug_info) {
                foreach my $prof (keys %{$seasonal_debug_info{$vm}}) {
                    $baseline_data_for_verbose{$vm}{$prof} = $seasonal_debug_info{$vm}{$prof}{baseline};
                }
            }

            write_seasonal_csv_output("current_baseline", $exec_ctx->{system_identifier}, $event_name, $unique_suffix, \%baseline_data_for_verbose);
            write_seasonal_csv_output("historic_snapshot", $exec_ctx->{system_identifier}, $event_name, $unique_suffix, $historic_data);
        }
   }

    # Log Rationale (Phase 3.2: suppressed during aggregation iterations)
    if ($emit_rationale && defined $open_log_files{$exec_ctx->{system_identifier}}) {
        my $ctx = {
            start       => $exec_ctx->{effective_start_date},
            end         => $exec_ctx->{effective_end_date},
            cache_start => $exec_ctx->{cache_start_date},
            cache_end   => $exec_ctx->{cache_end_date},
            interval    => $exec_ctx->{sampling_interval}
        };
        log_multiplicative_seasonal_rationale($open_log_files{$exec_ctx->{system_identifier}}, $effective_config, $ctx);
    }

    # Determine Forecast Horizon for Metadata (anchored to effective end date)
    my $asof_end = $exec_ctx->{effective_end_date} // $exec_ctx->{cache_end_date};
    my ($next_start, $next_end) = determine_event_period($effective_config, $asof_end, 'forecast');
    my $horizon_meta = undef;

    if ($next_end) {
        my $ref_time = (defined $asof_end && ref($asof_end) eq 'Time::Piece')
            ? $asof_end
            : Time::Piece->new();

        my $days_diff = int(($next_end->epoch - $ref_time->epoch) / 86400);
        $days_diff = 0 if $days_diff < 0;

        $horizon_meta = {
            target_date => $next_end->ymd,
            days        => $days_diff
        };
    }

    # Build the context object from existing args
    my $analysis_context = {
        analysis_start => time(),
        start       => $exec_ctx->{effective_start_date}, # Time::Piece object
        end         => $exec_ctx->{effective_end_date},   # Time::Piece object
        cache_start => $exec_ctx->{cache_start_date},
        cache_end   => $exec_ctx->{cache_end_date},
        interval    => $exec_ctx->{sampling_interval}, # Integer
        # PHASE 1: Include anchor bucket for deterministic storage
        anchor_bucket   => $exec_ctx->{anchor_bucket},
        anchor_source   => $exec_ctx->{anchor_source},
        # Phase 3.1: bucket provenance
        bucket_occurrence_count     => $exec_ctx->{bucket_occurrence_count},
        bucket_occurrence_end_dates => $exec_ctx->{bucket_occurrence_end_dates},
        # Phase 3.3: VM scope metadata (undef if no filtering applied)
        vm_scope => $vm_scope_meta,
    };

    # Store Forecast
    if ($store_history) {
        _store_model_forecast_to_history(
            $current_cache_path,
            $effective_event_name,
            'multiplicative_seasonal',
            $forecast_results,
            $effective_config,
            $horizon_meta,
            $analysis_context,
            $exec_ctx->{anchor_bucket},
            $exec_ctx->{_fingerprints}, # Phase 4: Idempotency fingerprints
            $exec_ctx->{_baseline_meta}  # Phase 6: Baseline metadata for staleness
        );
    }
    # For aggregated mode, return the raw forecast hashref to caller
    return $forecast_results;
}

# ==============================================================================
# SUBROUTINE: _execute_recency_decay
# PURPOSE:    Executes the Recency Decay model for a specific event.
# ==============================================================================
sub _execute_recency_decay {
    my ($event_name, $event_config, $current_cache_path, $profiles_aref, $args_href, $seasonality_config) = @_;

    # 1. Setup Logic
    $apply_seasonality_event = $event_name;

    # Recency decay cannot run with state-based decay
    if ($args_href->{decay_over_states}) {
        warn "    Skipping Recency Decay for '$event_name': Cannot run with --decay-over-states.\n";
        return;
    }

    my $asof_start = $args_href->{effective_start_date} // $args_href->{cache_start_date};
    my $asof_end = $args_href->{effective_end_date} // $args_href->{cache_end_date};

    # "Relative to our analysis end, when is the NEXT event?"
    my ($event_start_obj, $event_end_obj) = determine_event_period($event_config, $asof_end, 'forecast');

    unless ($event_start_obj && $event_end_obj) {
        warn "    Skipping Recency Decay for '$event_name': Could not determine event period.\n";
        return;
    }

    # --- PHASE 1: USE ORCHESTRATOR-PROVIDED ANCHOR ---
    # The anchor is resolved by _execute_automatic_seasonal_discovery and passed
    # via $args_href. No special handling needed here — all models receive the
    # same anchored context.

    my $anchor_date   = $args_href->{anchor_date};
    my $anchor_bucket = $args_href->{anchor_bucket} // '';
    my $anchor_source = $args_href->{anchor_source} // 'orchestrator';

    unless ($anchor_date) {
        warn "    [ERROR] No anchor context is available\n";
        return;
    }

    # Set the global for decay weight calculations (Time Zero reference)
    $nfit_analysis_reference_date_str = $anchor_date->ymd;

    # The effective_end_date is already clamped to anchor by the orchestrator
    my $run_end_date_obj = $args_href->{effective_end_date};
    my $run_end_date     = $run_end_date_obj->ymd;

    # 2. Prepare Exclusions
    my $exclusions = undef;
    if (defined $event_config->{exclude_dates}) {
        my $known_vms = _get_known_vms_from_cache($current_cache_path);
        $exclusions = _parse_exclusion_dates($event_config->{exclude_dates}, $known_vms);
    }

    # 3. Run Analysis
    # CRITICAL: The 'analysis_reference_date' defines "Time Zero" for decay weights.
    # It must be the Effective End Date (Current), NOT the Event Date (Future).
    my $parsed_results = run_single_pass_analysis(
        $current_cache_path,
        $profiles_aref,
        {
            %$args_href,
            enable_windowed_decay => 1,
            decay_over_states => 0,
            enable_growth_prediction => 1, # Required for valid decay stats

            analysis_reference_date => $nfit_analysis_reference_date_str,
            exclusions => $exclusions,

            # Use the calculated effective analysis dates (strings) from the main loop
            start_date => $asof_start->ymd,
            end_date => $run_end_date
        }
    );

    # 4. Assimilate
    my $assimilation_map = build_assimilation_map($parsed_results, $profiles_aref, $args_href->{adaptive_saturation_thresh});
    @vm_order = sort keys %{$assimilation_map};

    # 4a. Phase 3.3: Apply VM scope filter if configured (supports 'vms' and 'exclude_vms')
    my $vm_scope_filter = _parse_vm_scope_filter($event_config);
    my $vm_scope_meta = _apply_vm_scope_filter(
        $vm_scope_filter,
        \@vm_order,
        $event_name,
        $assimilation_map  # Use assimilation_map keys as cache VM set
    );

    # Early exit if all VMs filtered out
    if ($vm_scope_meta && scalar(@vm_order) == 0) {
        print STDERR "  [-] Skipping forecast storage for event '$event_name' (no matching VMs after scope filter)\n";
        return {};
    }

    # 5. Log Global Configuration (Once)
    my $log_fh = $open_log_files{$args_href->{system_identifier}};
    if ($log_fh) {
        my $ctx = {
            start       => $asof_start,
            end         => $run_end_date_obj,  # anchored end used for decay/time-zero
            cache_start => $args_href->{cache_start_date},
            cache_end   => $args_href->{cache_end_date},
            interval    => $args_href->{sampling_interval}
        };
        _log_seasonal_config_context($log_fh, $event_config, $ctx);
    }

    # 6. Enrichment, Logging, and Result Extraction
    my %recency_results_for_history;

    foreach my $vm_name (@vm_order) {
        my $vm_map_ref = $assimilation_map->{$vm_name};
        $vm_map_ref->{Configuration}{vm_name} = $vm_name;
        $vm_map_ref->{RunQMetrics} //= $per_profile_runq_metrics{$vm_name} || {};

        # Generate Hints
        my ($hint_type_tier, $hint_pattern_shape, $hint_pressure_bool, $pressure_detail_str, $rationale, $has_abs, $has_norm) =
            generate_sizing_hint(
                $vm_map_ref,
                $log_fh,
                $args_href->{adaptive_saturation_thresh}
            );

        # Store Hints & Globals
        $vm_map_ref->{Hinting}{AutoTier}       = $hint_type_tier;
        $vm_map_ref->{Hinting}{Pattern}        = $hint_pattern_shape;
        $vm_map_ref->{Hinting}{Pressure}       = $hint_pressure_bool;
        $vm_map_ref->{Hinting}{PressureDetail} = $pressure_detail_str // 'N/A';
        $hint_tier_for_csv{$vm_name}           = $hint_type_tier;
        $hint_pattern_for_csv{$vm_name}        = $hint_pattern_shape;
        $tier_override_for_csv{$vm_name}       = $vm_tier_overrides{$vm_name} // "";

        # --- Determine Source Profile for Standardised Modifiers ---
        my $user_tier = $vm_tier_overrides{$vm_name} // "";
        my $pattern_source = ($user_tier ne "") ? $user_tier : $hint_type_tier;
        my ($pattern_char) = ($pattern_source =~ /^([A-Z])/);
        $pattern_char //= 'G';
        my %pat_map = ('O'=>'O3-95W15', 'B'=>'B3-95W15', 'G'=>'G3-95W15', 'P'=>'G3-95W15');
        my $runq_source_profile_name = $pat_map{$pattern_char} // 'G3-95W15';

        # --- Pre-calculate CSVModifiers using the Source Profile ---
        # This must happen BEFORE the profile loop so all profiles can use RunQ_Potential.
        my ($runq_source_profile_obj) = grep { $_->{name} eq $runq_source_profile_name } @$profiles_aref;

        if ($runq_source_profile_obj) {
            my %pressure_flags_src = ( abs_pressure => $has_abs, norm_pressure => $has_norm );
            my %adaptive_thresholds_src = %{ $args_href->{adaptive_thresholds_href} // {} };
            $adaptive_thresholds_src{saturation}    //= $args_href->{adaptive_saturation_thresh};
            $adaptive_thresholds_src{target}        //= $args_href->{adaptive_target_norm_runq};
            $adaptive_thresholds_src{max_reduction} //= $args_href->{adaptive_max_efficiency_reduction};

            my (undef, $debug_info_ref_src) = calculate_runq_modified_physc(
                $vm_name, $vm_map_ref, $runq_source_profile_obj,
                \%pressure_flags_src, \%adaptive_thresholds_src
            );

            # Populate CSVModifiers for use by all profiles
            $vm_map_ref->{CSVModifiers}{RunQ_Tactical}  = $debug_info_ref_src->{'FinalAdditive'};
            $vm_map_ref->{CSVModifiers}{RunQ_Strategic} = $debug_info_ref_src->{'RunQ_Strategic'};
            $vm_map_ref->{CSVModifiers}{RunQ_Potential} = $debug_info_ref_src->{'RunQ_Potential'};
            $vm_map_ref->{CSVModifiers}{RunQ_Source}    = $runq_source_profile_name;
        }

        # --- Set Standardised Growth Metadata for Logging ---
        my $standard_growth_adj = $vm_map_ref->{Growth}{adjustments}{$runq_source_profile_name} // 0;
        $vm_map_ref->{Growth}{StandardAdjustment} = {
            Value  => $standard_growth_adj,
            Source => $runq_source_profile_name
        };

        # --- Apply Standardised Modifiers to All Profiles ---
        my $runq_potential = $vm_map_ref->{CSVModifiers}{RunQ_Potential} // 0;

        foreach my $profile (@$profiles_aref) {
            my $p_name = $profile->{name};

            # --- GUARD: Skip modifiers for P-99W1 (Peak Analysis Profile) ---
            if ($p_name eq $MANDATORY_PEAK_PROFILE_FOR_HINT) {
                if ($log_fh) {
                   my $base_physc = $vm_map_ref->{CoreResults}{ProfileValues}{$p_name};
                   log_peak_profile_rationale($log_fh, $vm_map_ref, $profile, $base_physc);
                }
                next;
            }

            # Get base value (pre-growth) and apply standardised adjustments
            my $base_val = $vm_map_ref->{Growth}{base_values}{$p_name};
            my $base_physc_for_log = $vm_map_ref->{CoreResults}{ProfileValues}{$p_name};

            if (defined $base_val && looks_like_number($base_val)) {
                # Final = Base + StandardGrowth + RunQ_Potential
                my $final_val = $base_val + $standard_growth_adj + $runq_potential;
                $vm_map_ref->{CoreResults}{ProfileValues}{$p_name} = sprintf("%.4f", $final_val);
            }

            my $final_csv_value = $vm_map_ref->{CoreResults}{ProfileValues}{$p_name} // "N/A";

            # Inline Rationale Logging (still call calculate_runq_modified_physc for debug_info)
            if ($log_fh) {
                my %pressure_flags = ( abs_pressure => $has_abs, norm_pressure => $has_norm );
                my %adaptive_thresholds = %{ $args_href->{adaptive_thresholds_href} // {} };
                $adaptive_thresholds{saturation}    //= $args_href->{adaptive_saturation_thresh};
                $adaptive_thresholds{target}        //= $args_href->{adaptive_target_norm_runq};
                $adaptive_thresholds{max_reduction} //= $args_href->{adaptive_max_efficiency_reduction};

                my (undef, $debug_info_ref) = calculate_runq_modified_physc(
                    $vm_name, $vm_map_ref, $profile, \%pressure_flags, \%adaptive_thresholds
                );

                my $raw_states_aref_for_log = $vm_map_ref->{RawNfitStates} || [];
                log_profile_rationale(
                    $log_fh, $vm_map_ref, $profile,
                    $base_physc_for_log, $final_csv_value, $debug_info_ref,
                    $raw_states_aref_for_log,
                    $args_href->{adaptive_saturation_thresh}
                );
            }
        }

        # Extract for History
        if (exists $vm_map_ref->{CoreResults}{ProfileValues}) {
             $recency_results_for_history{$vm_name} = { %{$vm_map_ref->{CoreResults}{ProfileValues}} };
        }
    }

    # 7. Generate CSV (Rolling Numbering)
    # Phase 3.2: anchor_bucket prefixed to unique_suffix for backfill disambiguation
    my $date_str = localtime->strftime('%Y%m%d');

    # Prefix anchor_bucket to unique_suffix for backfill disambiguation
    my $unique_suffix;
    if ($anchor_bucket =~ /^\d{4}-\d{2}$/) {
        $unique_suffix = "$anchor_bucket.$date_str";
    } else {
        $unique_suffix = $date_str;
    }

    my $base_name = "nfit-profile." . ($args_href->{system_identifier}//'') . "." . $event_name;
    $base_name =~ s/[^a-zA-Z0-9_.-]//g;

    my $counter = 0;
    while (-e File::Spec->catfile($output_dir, "$base_name.$unique_suffix.csv")) {
        $counter++;
        if ($anchor_bucket =~ /^\d{4}-\d{2}$/) {
            $unique_suffix = "$anchor_bucket.$date_str-$counter";
        } else {
            $unique_suffix = "$date_str-$counter";
        }
    }

    _write_standard_csv_report(
        $assimilation_map,
        $event_name,
        $args_href->{system_identifier},
        $unique_suffix,
        0,
        1, # is_recency_decay
        0,
        $args_href->{adaptive_saturation_thresh},
        $args_href->{adaptive_thresholds_href},
        $anchor_bucket  # Pass anchor_bucket for potential column inclusion
    );

    # 3. Horizon Metadata Calculation
    # We use the event_end_obj (e.g., May 31) derived earlier from the Event Config.
    # This ensures the metadata in history matches the "Time Machine" intent, not wall-clock time.

    my $horizon_meta = undef;

    if (defined $event_end_obj && defined $run_end_date_obj) {
        # Calculate days between Analysis Anchor (Apr 30) and Target (May 31)
        my $days_diff = int(($event_end_obj->epoch - $run_end_date_obj->epoch) / 86400);
        $days_diff = 0 if $days_diff < 0;

        $horizon_meta = {
            target_date => $event_end_obj->ymd,
            days        => $days_diff
        };
    }

    # Fallback (Safety only - should rarely be hit if determine_event_period works)
    # Note: We do NOT default to localtime() here to avoid pollution.
    # If we can't determine the target, it's better to have undef meta than wrong meta.

    # Build the context object from existing args
    my $analysis_context = {
        analysis_start  => time(),
        start       => $asof_start,         # Time::Piece object
        end         => $run_end_date_obj,   # Time::Piece object
        cache_start => $args_href->{cache_start_date},
        cache_end   => $args_href->{cache_end_date},
        interval    => $args_href->{sampling_interval}, # Integer
        # PHASE 1: Include anchor bucket for deterministic storage
        anchor_bucket   => $args_href->{anchor_bucket} // $anchor_bucket,
        anchor_source   => $args_href->{anchor_source} // $anchor_source,
        # Phase 3.1: bucket provenance
        bucket_occurrence_count     => $args_href->{bucket_occurrence_count},
        bucket_occurrence_end_dates => $args_href->{bucket_occurrence_end_dates},
        # Phase 3.3: VM scope metadata (undef if no filtering applied)
        vm_scope => $vm_scope_meta,
    };

    # 8. Store Forecast
    _store_model_forecast_to_history(
        $current_cache_path,
        $event_name,
        'recency_decay',
        \%recency_results_for_history,
        $event_config,
        $horizon_meta,
        $analysis_context,
        $args_href->{anchor_bucket},
        $args_href->{_fingerprints},  # Phase 4: Idempotency fingerprints
        $args_href->{_baseline_meta}  # Phase 6: Baseline metadata for staleness
    );

    # Phase 3.2: allow orchestration to aggregate results
    return \%recency_results_for_history;
}

# ==============================================================================
# SUBROUTINE: _execute_predictive_peak
# PURPOSE:    Executes the Predictive Peak model for a specific event.
# ==============================================================================
sub _execute_predictive_peak {
    my ($event_name, $event_config, $current_cache_path, $profiles_aref, $args_href, $seasonality_config) = @_;

    # 1. Determine analysis path
    my $effective_event_name = determine_seasonal_analysis_path(
        $event_config, $current_cache_path, $event_name
    );

    # Handle cold-start graceful skip
    unless (defined $effective_event_name) {
        print STDERR "  [-] Skipping predictive peak event '$event_name' (cold start - run --update-history first)\n";
        return;
    }

    my $effective_config = $event_config;
    if ($effective_event_name ne $event_name) {
        $effective_config = $seasonality_config->{$effective_event_name} // $event_config;
        print STDERR "  [INFO] Falling back to event '$effective_event_name' (due to history constraints)\n";
    }

    $is_predictive_peak_model_run = 1;
    $apply_seasonality_event = $event_name;

    # 2. Exclusions
    my $exclusions = undef;
    if (defined $effective_config->{exclude_dates}) {
        my $known_vms = _get_known_vms_from_cache($current_cache_path);
        $exclusions = _parse_exclusion_dates($effective_config->{exclude_dates}, $known_vms);
    }

    # 3. Run Analysis
    my $parsed_results = run_single_pass_analysis(
        $current_cache_path,
        $profiles_aref,
        {
            %$args_href,
            # Use the calculated effective analysis dates from the main loop
            start_date              => $args_href->{effective_start_date}->ymd,
            end_date                => $args_href->{effective_end_date}->ymd,
            enable_windowed_decay   => 1,  # Critical for trend detection
            decay_over_states       => 0,
            enable_growth_prediction => 1,
            exclusions => $exclusions,
        }
    );

    # 4. Assimilate
    my $assimilation_map = build_assimilation_map($parsed_results, $profiles_aref, $args_href->{adaptive_saturation_thresh});
    @vm_order = sort keys %{$assimilation_map};

    # 4a. Phase 3.3: Apply VM scope filter if configured (supports 'vms' and 'exclude_vms')
    my $vm_scope_filter = _parse_vm_scope_filter($event_config);
    my $vm_scope_meta = _apply_vm_scope_filter(
        $vm_scope_filter,
        \@vm_order,
        $event_name,
        $assimilation_map  # Use assimilation_map keys as cache VM set
    );

    # Early exit if all VMs filtered out
    if ($vm_scope_meta && scalar(@vm_order) == 0) {
        print STDERR "  [-] Skipping forecast storage for event '$event_name' (no matching VMs after scope filter)\n";
        return;
    }

    # Phase 6: Extract pre-computed history baseline (undef if --use-l1-baseline)
    my $precomputed_baselines = ($args_href->{_baseline_result})
        ? $args_href->{_baseline_result}{baselines} : undef;

    # 5. Calculate Forecast
    my $forecast_results = calculate_predictive_peak_forecast(
        $current_cache_path,
        $args_href->{system_identifier},
        $effective_event_name,
        $effective_config,
        $seasonality_config,
        $args_href->{adaptive_saturation_thresh},
        $exclusions,
        $args_href->{effective_start_date},
        $args_href->{effective_end_date},
        $precomputed_baselines     # Phase 6: history-based baseline (undef → L1 fallback)
    );

    # Merge Forecast
    foreach my $vm_name (@vm_order) {
        if (exists $forecast_results->{$vm_name}) {
             $assimilation_map->{$vm_name}{CoreResults}{ProfileValues} = $forecast_results->{$vm_name};
        }
    }

    # 6. Enrichment
    foreach my $vm_name (@vm_order) {
        my $vm_map_ref = $assimilation_map->{$vm_name};
        $vm_map_ref->{Configuration}{vm_name} = $vm_name;
        $vm_map_ref->{RunQMetrics} //= $per_profile_runq_metrics{$vm_name} || {};

        my ($hint_type_tier, $hint_pattern_shape, $hint_pressure_bool, $pressure_detail_str, $rationale, $has_abs, $has_norm) =
            generate_sizing_hint($vm_map_ref, undef, $args_href->{adaptive_saturation_thresh});

        $vm_map_ref->{Hinting}{AutoTier}       = $hint_type_tier;
        $vm_map_ref->{Hinting}{Pattern}        = $hint_pattern_shape;
        $vm_map_ref->{Hinting}{Pressure}       = $hint_pressure_bool;
        $vm_map_ref->{Hinting}{PressureDetail} = $pressure_detail_str // 'N/A';
        $hint_tier_for_csv{$vm_name}           = $hint_type_tier;
        $hint_pattern_for_csv{$vm_name}        = $hint_pattern_shape;
        $tier_override_for_csv{$vm_name}       = $vm_tier_overrides{$vm_name} // "";

        my $user_tier = $vm_tier_overrides{$vm_name} // "";
        my $pattern_source = ($user_tier ne "") ? $user_tier : $hint_type_tier;
        my ($pattern_char) = ($pattern_source =~ /^([A-Z])/);
        $pattern_char //= 'G';
        my %pat_map = ('O'=>'O3-95W15', 'B'=>'B3-95W15', 'G'=>'G3-95W15', 'P'=>'G3-95W15');
        my $runq_source_profile_name = $pat_map{$pattern_char} // 'G3-95W15';

        foreach my $profile (@$profiles_aref) {
            my $p_name = $profile->{name};

            # --- GUARD: Skip RunQ Modifiers for P-99W1 ---
            next if ($p_name eq $MANDATORY_PEAK_PROFILE_FOR_HINT);

            my %pressure_flags = ( abs_pressure => $has_abs, norm_pressure => $has_norm );
            my %adaptive_thresholds = %{ $args_href->{adaptive_thresholds_href} // {} };
            $adaptive_thresholds{saturation}    //= $args_href->{adaptive_saturation_thresh};
            $adaptive_thresholds{target}        //= $args_href->{adaptive_target_norm_runq};
            $adaptive_thresholds{max_reduction} //= $args_href->{adaptive_max_efficiency_reduction};

            my ($adjusted_physc, $debug_info_ref) = calculate_runq_modified_physc(
                $vm_name, $vm_map_ref, $profile, \%pressure_flags, \%adaptive_thresholds
            );
            if (looks_like_number($adjusted_physc)) {
                $vm_map_ref->{CoreResults}{ProfileValues}{$p_name} = sprintf("%.4f", $adjusted_physc);
            }

            if ($p_name eq $runq_source_profile_name) {
                $vm_map_ref->{CSVModifiers}{RunQ_Tactical}  = $debug_info_ref->{'FinalAdditive'};
                $vm_map_ref->{CSVModifiers}{RunQ_Strategic} = $debug_info_ref->{'RunQ_Strategic'};
                $vm_map_ref->{CSVModifiers}{RunQ_Potential} = $debug_info_ref->{'RunQ_Potential'};
                $vm_map_ref->{CSVModifiers}{RunQ_Source}    = $runq_source_profile_name;
            }
        }

        # 7. Peak Value logic
        # For multiplicative_seasonal: PeakValue was set by build_assimilation_map from
        # the raw peak tracker (peakPhyscFromLatestState or max of P-99W1 Peak metrics).
        # This is an immutable historical fact and must NOT be modified.
        # The 'TrueBaseline'/'PredictedPeak' fields are specific to predictive_peak model.
        #
        # No action needed here - CoreResults.PeakValue is already correct.
    }

    # 8. Generate CSV (Rolling Numbering)
    # Phase 3.2: anchor_bucket prefixed to unique_suffix for backfill disambiguation
    my $date_str = localtime->strftime('%Y%m%d');
    my $anchor_bucket = $args_href->{anchor_bucket} // '';

    # Prefix anchor_bucket to unique_suffix for backfill disambiguation
    my $unique_suffix;
    if ($anchor_bucket =~ /^\d{4}-\d{2}$/) {
        $unique_suffix = "$anchor_bucket.$date_str";
    } else {
        $unique_suffix = $date_str;
    }

    my $base_name = "nfit-profile." . ($args_href->{system_identifier}//'') . "." . $event_name;
    $base_name =~ s/[^a-zA-Z0-9_.-]//g;

    my $counter = 0;
    while (-e File::Spec->catfile($output_dir, "$base_name.$unique_suffix.csv")) {
        $counter++;
        if ($anchor_bucket =~ /^\d{4}-\d{2}$/) {
            $unique_suffix = "$anchor_bucket.$date_str-$counter";
        } else {
            $unique_suffix = "$date_str-$counter";
        }
    }

    # Log Rationale
    if (defined $open_log_files{$args_href->{system_identifier}}) {
        my $ctx = {
            start    => $args_href->{effective_start_date},
            end      => $args_href->{effective_end_date},
            cache_start    => $args_href->{cache_start_date},
            cache_end      => $args_href->{cache_end_date},
            interval => $args_href->{sampling_interval}
        };
        log_predictive_peak_rationale($open_log_files{$args_href->{system_identifier}}, $effective_config, $ctx);
    }

    _write_standard_csv_report(
        $assimilation_map,
        $event_name,
        $args_href->{system_identifier},
        $unique_suffix,
        0,
        0,
        1, # is_predictive_peak
        $args_href->{adaptive_saturation_thresh},
        $args_href->{adaptive_thresholds_href},
        $anchor_bucket  # Pass anchor_bucket for potential column inclusion
    );

    # Determine Forecast Horizon for Metadata (anchored to effective end date)
    my ($next_start, $next_end) = determine_event_period($effective_config, $args_href->{effective_end_date}, 'forecast');
    my $horizon_meta = undef;

    if ($next_end) {
        my $ref_time = (defined $args_href->{effective_end_date} && ref($args_href->{effective_end_date}) eq 'Time::Piece') ? $args_href->{effective_end_date} : Time::Piece->new();

        my $days_diff = int(($next_end->epoch - $ref_time->epoch) / 86400);
        $days_diff = 0 if $days_diff < 0;

        $horizon_meta = {
            target_date => $next_end->ymd,
            days        => $days_diff
        };
    }

    # Build the context object from existing args
    my $analysis_context = {
        analysis_start => time(),
        start       => $args_href->{effective_start_date}, # Must be Time::Piece object
        end         => $args_href->{effective_end_date},   # Must be Time::Piece object
        cache_start => $args_href->{cache_start_date},
        cache_end   => $args_href->{cache_end_date},
        interval    => $args_href->{sampling_interval}, # Integer
        # PHASE 1: Include anchor bucket for deterministic storage
        anchor_bucket   => $args_href->{anchor_bucket},
        anchor_source   => $args_href->{anchor_source},
        # Phase 3.1: bucket provenance (optional)
        bucket_occurrence_count     => $args_href->{bucket_occurrence_count},
        bucket_occurrence_end_dates => $args_href->{bucket_occurrence_end_dates},
        # Phase 3.3: VM scope metadata (undef if no filtering applied)
        vm_scope => $vm_scope_meta,
    };

    # 9. Store Forecast
    _store_model_forecast_to_history(
        $current_cache_path,
        $effective_event_name,
        'predictive_peak',
        $forecast_results,
        $effective_config,
        $horizon_meta,
        $analysis_context,
        $args_href->{anchor_bucket},
        $args_href->{_fingerprints},  # Phase 4: Idempotency fingerprints
        $args_href->{_baseline_meta}  # Phase 6: Baseline metadata for staleness
    );
}

# ==============================================================================
# SUBROUTINE: _resolve_effective_window
# PURPOSE: Determines the actual start and end dates for the analysis.
#          Clamps requested dates (CLI) to the available data (Cache).
# ==============================================================================
sub _resolve_effective_window {
    my ($cache_start, $cache_end, $cli_start, $cli_end) = @_;

    # 1. Default to cache bounds
    my $eff_start = $cache_start;
    my $eff_end   = $cache_end;

    # 2. Apply CLI Start (Clamp to Cache End if necessary, though validation handles that)
    if (defined $cli_start) {
        $eff_start = $cli_start;
        # If user asks for start before cache, clamp to cache start?
        # Ideally, we allow the engine to handle it, but for "effective" logic:
        if ($eff_start < $cache_start) {
            warn " [WARN] Requested --startdate $cli_start is before cache start (using " . $cache_start->ymd . ").\n";
            $eff_start = $cache_start;
        }
    }

    # 3. Apply CLI End
    if (defined $cli_end) {
        $eff_end = $cli_end;
        if ($eff_end > $cache_end) {
            warn " [WARN] Requested --enddate $cli_end is after cache end (using " . $cache_end->ymd . ").\n";
            $eff_end = $cache_end;
        }
    }

    # 4. Safety Check: Start > End
    if ($eff_start > $eff_end) {
        die "[ERROR] Effective start date (" . $eff_start->ymd . ") is after effective end date (" . $eff_end->ymd . "). Check your -s/-e flags against the cache range.\n";
    }

    return ($eff_start, $eff_end);
}
###############################################################################
# Phase 3.2 – Intra-Event Aggregation (Within-Bucket)
#
# Only multiplicative_seasonal is aggregated across multiple occurrences
# within a bucket. Other models remain "run once per bucket end".
#
# Adds:
#   - aggregation_method (median default; supports max/mean/p75)
#   - synthesis metadata (min/max/stddev/sample_size/method)
###############################################################################

sub _resolve_aggregation_method {
    my ($event_cfg, $seasonality_config) = @_;

    # Per-event override
    if (defined $event_cfg->{aggregation_method} && length $event_cfg->{aggregation_method}) {
        return lc $event_cfg->{aggregation_method};
    }

    # Optional global default (if your loader provides it)
    # this code is deprecated (inheritance is now performed early)
    if ($seasonality_config
        && ref($seasonality_config) eq 'HASH'
        && $seasonality_config->{Global}
        && ref($seasonality_config->{Global}) eq 'HASH'
        && defined $seasonality_config->{Global}{aggregation_method}
        && length $seasonality_config->{Global}{aggregation_method}) {
        return lc $seasonality_config->{Global}{aggregation_method};
    }

    return 'median';
}

sub _parse_vm_scope_filter {
    my ($event_config) = @_;

    return undef unless (defined $event_config && ref($event_config) eq 'HASH');

    # VM scope inheritance and resolution are performed in parse_seasonality_config.
    # At this stage we only return the already-resolved, already-parsed filter.
    my $vm_scope_filter = $event_config->{_vm_scope_filter};
    return undef unless ($vm_scope_filter && ref($vm_scope_filter) eq 'HASH');

    return $vm_scope_filter;
}

# ==============================================================================
# SUBROUTINE: _apply_vm_scope_filter
# PURPOSE:    Phase 3.3 - Applies VM scope filtering to @vm_order array.
#             Supports both inclusion ('vms') and exclusion ('exclude_vms').
#             Logs filter application and missing VM warnings.
#             Returns metadata hashref for AnalysisContext, or undef if no filter.
# ARGUMENTS:
#   $vm_scope_filter - hashref from _parse_vm_scope_filter (or undef)
#                      Structure: { include => {...}, exclude => {...} }
#   $vm_order_aref   - reference to @vm_order array (modified in place)
#   $event_name      - string, for logging context
#   $cache_vm_set    - hashref of all VMs present in cache (for missing VM detection)
# RETURNS:
#   hashref with vm_scope metadata, or undef if no filtering applied
# FILTER PRECEDENCE:
#   1. Start with all VMs from @vm_order
#   2. If 'include' is specified, filter DOWN to only those VMs
#   3. If 'exclude' is specified, SUBTRACT those VMs from the result
# ==============================================================================
sub _apply_vm_scope_filter {
    my ($vm_scope_filter, $vm_order_aref, $event_name, $cache_vm_set) = @_;

    # No filter configured - return undef (no metadata needed)
    return undef unless ($vm_scope_filter && ref($vm_scope_filter) eq 'HASH');

    my $include_filter = $vm_scope_filter->{include};
    my $exclude_filter = $vm_scope_filter->{exclude};

    # Both empty - no filtering
    return undef unless ($include_filter || $exclude_filter);

    my $before_count = scalar(@$vm_order_aref);
    my @filter_actions;

    # Step 1: Apply include filter (whitelist)
    if ($include_filter && ref($include_filter) eq 'HASH' && %$include_filter) {
        my $include_count = scalar(keys %$include_filter);

        # Check for configured include VMs not present in the cache
        if ($cache_vm_set && ref($cache_vm_set) eq 'HASH') {
            foreach my $configured_vm (sort keys %$include_filter) {
                unless ($cache_vm_set->{$configured_vm}) {
                    print STDERR "    [SCOPE] Warning: VM '$configured_vm' in 'vms' not found in cache for event '$event_name'\n";
                }
            }
        }

        @$vm_order_aref = grep { $include_filter->{$_} } @$vm_order_aref;
        push @filter_actions, "included $include_count";
    }

    # Step 2: Apply exclude filter (blacklist)
    if ($exclude_filter && ref($exclude_filter) eq 'HASH' && %$exclude_filter) {
        my $exclude_count = scalar(keys %$exclude_filter);
        my $excluded_actual = 0;

        my @filtered;
        foreach my $vm (@$vm_order_aref) {
            if ($exclude_filter->{$vm}) {
                $excluded_actual++;
            } else {
                push @filtered, $vm;
            }
        }
        @$vm_order_aref = @filtered;

        # Log warning for exclude VMs not in the current set (informational only)
        if ($cache_vm_set && ref($cache_vm_set) eq 'HASH') {
            foreach my $configured_vm (sort keys %$exclude_filter) {
                unless ($cache_vm_set->{$configured_vm}) {
                    print STDERR "    [SCOPE] Info: VM '$configured_vm' in 'exclude_vms' not present in cache for event '$event_name'\n";
                }
            }
        }

        push @filter_actions, "excluded $exclude_count ($excluded_actual matched)";
    }

    my $after_count = scalar(@$vm_order_aref);

    # Log filter application summary (include inheritance source if available)
    my $action_summary = join(', ', @filter_actions);

    # Append source info if available
    my @source_notes;
    if ($vm_scope_filter->{include_source}) {
        push @source_notes, "vms from " . $vm_scope_filter->{include_source};
    }
    if ($vm_scope_filter->{exclude_source}) {
        push @source_notes, "exclude_vms from " . $vm_scope_filter->{exclude_source};
    }
    my $source_info = @source_notes ? ' [' . join(', ', @source_notes) . ']' : '';

    if ($after_count == 0) {
        print STDERR "    [-] Warning: No VMs remain after filtering for event '$event_name' ($action_summary)$source_info - no forecasts will be generated\n";
    } else {
        print STDERR "    [-] Filtered to $after_count VM(s) for event '$event_name' ($action_summary, from $before_count)$source_info\n";
    }

    # Build metadata for AnalysisContext
    my $vm_scope_meta = {
        type             => ($include_filter && $exclude_filter) ? 'include_and_exclude'
                         : ($include_filter) ? 'explicit_include'
                         : 'explicit_exclude',
        include_configured => ($include_filter ? scalar(keys %$include_filter) : 0),
        exclude_configured => ($exclude_filter ? scalar(keys %$exclude_filter) : 0),
        matched_count      => $after_count,
        vms                => [ sort @$vm_order_aref ],
        # Store raw config for fingerprinting (downstream consumers can re-hash)
        include_list       => ($include_filter ? [ sort keys %$include_filter ] : undef),
        exclude_list       => ($exclude_filter ? [ sort keys %$exclude_filter ] : undef),
    };

    return $vm_scope_meta;
}

sub _aggregate_numeric {
    my ($vals_aref, $method) = @_;
    return undef unless ($vals_aref && ref($vals_aref) eq 'ARRAY');

    my @v = grep { defined($_) && looks_like_number($_) } @$vals_aref;
    return undef unless @v;

    $method = lc($method // 'median');

    if ($method eq 'max') {
        my $m = $v[0];
        for my $x (@v) { $m = $x if $x > $m; }
        return $m;
    }
    elsif ($method eq 'mean') {
        my $sum = 0;
        $sum += $_ for @v;
        return $sum / @v;
    }
    elsif ($method eq 'p75') {
        @v = sort { $a <=> $b } @v;
        my $idx = int(0.75 * (@v - 1) + 0.5);
        $idx = 0 if $idx < 0;
        $idx = $#v if $idx > $#v;
        return $v[$idx];
    }
    else { # median default
        @v = sort { $a <=> $b } @v;
        my $n = @v;
        return $v[int($n/2)] if ($n % 2 == 1);
        return ($v[$n/2 - 1] + $v[$n/2]) / 2.0;
    }
}

sub _stddev_sample {
    my ($vals_aref) = @_;
    return undef unless ($vals_aref && ref($vals_aref) eq 'ARRAY');

    my @v = grep { defined($_) && looks_like_number($_) } @$vals_aref;
    return undef unless @v >= 2;

    my $sum = 0;
    $sum += $_ for @v;
    my $mean = $sum / @v;

    my $ss = 0;
    for my $x (@v) {
        my $d = $x - $mean;
        $ss += ($d * $d);
    }
    return sqrt($ss / (@v - 1));
}

sub _synthesis_summary {
    my ($vals_aref, $method) = @_;
    my @v = grep { defined($_) && looks_like_number($_) } @{ $vals_aref // [] };
    return undef unless @v;

    my $n = scalar(@v);
    my $min = $v[0];
    my $max = $v[0];
    for my $x (@v) {
        $min = $x if $x < $min;
        $max = $x if $x > $max;
    }

    my $std = _stddev_sample(\@v);

    # Phase 3.2: Confidence quality flagging per design specification
    # Low confidence if: N < 2, or coefficient of variation > 0.5 (high dispersion)
    my $confidence = 'High';
    if ($n < 2) {
        $confidence = 'Low';  # Insufficient sample size for robust aggregation
    }
    elsif (defined $std && $std > 0) {
        # Calculate CV using the midpoint as a denominator proxy
        my $midpoint = ($min + $max) / 2;
        if ($midpoint > 0) {
            my $cv = $std / $midpoint;
            $confidence = 'Low' if ($cv > 0.5);  # High dispersion indicates unstable signal
        }
    }

    return {
        method       => $method,
        sample_size  => $n,
        min_observed => $min + 0,
        max_observed => $max + 0,
        confidence   => $confidence,
        (defined $std ? (std_dev => $std + 0) : ()),
    };
}

sub _execute_multiplicative_seasonal_bucket_aggregated {
    my ($event_name, $event_cfg, $current_cache_path, $profiles_aref, $bucket_exec_ctx, $seasonality_config, $bucket_occ_aref) = @_;

    # Aggregration of event per bucket (month)
    # "Intra-Month Aggregation" example -> if an event happens 4 times in a month (e.g., Weekly Batch)
    my $method = _resolve_aggregation_method($event_cfg, $seasonality_config);

    # Defensive: if only one occurrence, fall back to normal single execution.
    my $occ_n = ($bucket_occ_aref && ref($bucket_occ_aref) eq 'ARRAY') ? scalar(@$bucket_occ_aref) : 0;
    if ($occ_n <= 1) {
        _execute_multiplicative_seasonal($event_name, $event_cfg, $current_cache_path, $profiles_aref, $bucket_exec_ctx, $seasonality_config);
        return;
    }

    print STDERR "    Φ Multiplicative: aggregating $occ_n $event_name event occurrence(s) in " . ($bucket_exec_ctx->{anchor_bucket} // 'unknown') . " (method: $method)\n";

    # IMPORTANT: per-occurrence runs must NOT inherit the bucket-anchored start window.
    # Reset to cache bounds so _build_anchor_execution_context can clamp correctly per occurrence.
    my %bucket_base_ctx = %$bucket_exec_ctx;
    $bucket_base_ctx{effective_start_date} = $bucket_exec_ctx->{cache_start_date};
    $bucket_base_ctx{effective_end_date}   = $bucket_exec_ctx->{cache_end_date};
    my $bucket_base_ctx_ref = \%bucket_base_ctx;

    # Collect per-occurrence forecast values and multipliers
    # Phase 3.2: collect all component fields for complete history storage
    my %forecast_samples;       # vm -> profile -> [forecast, ...]
    my %multiplier_samples;     # vm -> profile -> [multiplier, ...]
    my %baseline_samples;       # vm -> profile -> [baseline, ...]
    my %trend_samples;          # vm -> profile -> [trend_factor, ...]
    my %volatility_samples;     # vm -> profile -> [volatility, ...]
    my %residual_samples;       # vm -> profile -> [forecasted_residual, ...]
    my %amplification_samples;  # vm -> profile -> [amplification_factor, ...]

    # We run the model once per occurrence end-date, but do NOT write history.
    # Performance: emit CSV only on the final occurrence to avoid redundant engine runs.
    my $occ_total = scalar(@$bucket_occ_aref);
    my $occ_idx = 0;

    foreach my $occ (@$bucket_occ_aref) {
        $occ_idx++;
        next unless ($occ && ref($occ) eq 'HASH');
        next unless ($occ->{end} && ref($occ->{end}) eq 'Time::Piece');

        my $occ_end = $occ->{end}->truncate(to => 'day');
        my $occ_start = ($occ->{start} && ref($occ->{start}) eq 'Time::Piece')
            ? scalar($occ->{start}->truncate(to => 'day'))
            : $occ_end;  # Fallback to end if start missing

        # Build a per-occurrence anchor_info that stays within the same bucket
        my $occ_anchor_info = {
            anchor_date    => $occ_end,
            anchor_bucket  => $bucket_exec_ctx->{anchor_bucket},
            anchor_source  => ($occ_idx == $occ_total) ? 'bucket_aggregated_csv' : 'occurrence_end',
            occurrence_start => $occ_start,
            occurrence_end => $occ_end,
            is_complete    => 1,
        };

        my $occ_exec_ctx = _build_anchor_execution_context($occ_anchor_info, $bucket_base_ctx_ref, $event_cfg);

        # Ensure per-occurrence run is isolated from previous run state.
        _reset_seasonal_state();
        %seasonal_debug_info = ();  # important: prevent leakage across runs

        # Final occurrence: enable CSV/rationale emission to avoid redundant engine run
        my $is_final_occurrence = ($occ_idx == $occ_total);

        my $forecast_href = _execute_multiplicative_seasonal(
            $event_name, $event_cfg, $current_cache_path, $profiles_aref, $occ_exec_ctx, $seasonality_config,
            { store_history => 0, emit_csv => $is_final_occurrence, emit_rationale => $is_final_occurrence }
        );

        next unless ($forecast_href && ref($forecast_href) eq 'HASH' && scalar(keys %$forecast_href) > 0);

        # Capture forecast values
        foreach my $vm (keys %$forecast_href) {
            next unless ref($forecast_href->{$vm}) eq 'HASH';
            foreach my $profile (keys %{ $forecast_href->{$vm} }) {
                my $val = $forecast_href->{$vm}{$profile};
                next unless defined($val) && looks_like_number($val);
                push @{ $forecast_samples{$vm}{$profile} }, ($val + 0);
            }
        }

        # Collect all component fields from seasonal_debug_info
        foreach my $vm (keys %seasonal_debug_info) {
            next unless ref($seasonal_debug_info{$vm}) eq 'HASH';
            foreach my $profile (keys %{ $seasonal_debug_info{$vm} }) {
                my $d = $seasonal_debug_info{$vm}{$profile};
                next unless (ref($d) eq 'HASH');

                # Collect multiplier
                my $m = $d->{multiplier};
                push @{ $multiplier_samples{$vm}{$profile} }, ($m + 0)
                    if (defined $m && looks_like_number($m));

                # Collect baseline
                my $b = $d->{baseline};
                push @{ $baseline_samples{$vm}{$profile} }, ($b + 0)
                    if (defined $b && looks_like_number($b));

                # Collect trend_factor
                my $t = $d->{trend_factor};
                push @{ $trend_samples{$vm}{$profile} }, ($t + 0)
                    if (defined $t && looks_like_number($t));

                # Collect volatility
                my $v = $d->{volatility};
                push @{ $volatility_samples{$vm}{$profile} }, ($v + 0)
                    if (defined $v && looks_like_number($v));

                # Collect forecasted_residual
                my $r = $d->{forecasted_residual};
                push @{ $residual_samples{$vm}{$profile} }, ($r + 0)
                    if (defined $r && looks_like_number($r));

                # Collect amplification_factor
                my $a = $d->{amplification_factor};
                push @{ $amplification_samples{$vm}{$profile} }, ($a + 0)
                    if (defined $a && looks_like_number($a));
            }
        }
    }

    # Aggregate into one forecast per VM/profile
    my %aggregated_forecast;
    foreach my $vm (keys %forecast_samples) {
        foreach my $profile (keys %{ $forecast_samples{$vm} }) {
            my $agg = _aggregate_numeric($forecast_samples{$vm}{$profile}, $method);
            next unless defined $agg;
            $aggregated_forecast{$vm}{$profile} = $agg + 0;
        }
    }

    # Phase 3.3: Apply VM scope filter to aggregated results (supports 'vms' and 'exclude_vms')
    # Filter is applied AFTER aggregation to ensure synthesis stats reflect all occurrences
    my $vm_scope_filter = _parse_vm_scope_filter($event_cfg);
    my $vm_scope_meta = undef;

    if ($vm_scope_filter) {
        my $include_filter = $vm_scope_filter->{include};
        my $exclude_filter = $vm_scope_filter->{exclude};

        if ($include_filter || $exclude_filter) {
            my @all_vms = sort keys %aggregated_forecast;
            my $before_count = scalar(@all_vms);
            my @filter_actions;

            # Step 1: Apply include filter (whitelist)
            my %working_set = %aggregated_forecast;
            if ($include_filter && ref($include_filter) eq 'HASH' && %$include_filter) {
                my $include_count = scalar(keys %$include_filter);

                # Check for configured include VMs not present
                foreach my $configured_vm (sort keys %$include_filter) {
                    unless (exists $aggregated_forecast{$configured_vm}) {
                        print STDERR "    [SCOPE] Warning: VM '$configured_vm' in 'vms' not found in aggregated results for event '$event_name'\n";
                    }
                }

                %working_set = ();
                foreach my $vm (keys %aggregated_forecast) {
                    $working_set{$vm} = $aggregated_forecast{$vm} if $include_filter->{$vm};
                }
                push @filter_actions, "included $include_count";
            }

            # Step 2: Apply exclude filter (blacklist)
            if ($exclude_filter && ref($exclude_filter) eq 'HASH' && %$exclude_filter) {
                my $exclude_count = scalar(keys %$exclude_filter);
                my $excluded_actual = 0;

                my %filtered;
                foreach my $vm (keys %working_set) {
                    if ($exclude_filter->{$vm}) {
                        $excluded_actual++;
                    } else {
                        $filtered{$vm} = $working_set{$vm};
                    }
                }
                %working_set = %filtered;
                push @filter_actions, "excluded $exclude_count ($excluded_actual matched)";
            }

            my $after_count = scalar(keys %working_set);
            my $action_summary = join(', ', @filter_actions);

            if ($after_count == 0) {
                print STDERR "    [SCOPE] Warning: No VMs remain after filtering aggregated results for event '$event_name' ($action_summary)\n";
            } else {
                print STDERR "    [SCOPE] Filtered aggregated results to $after_count VM(s) for event '$event_name' ($action_summary, from $before_count)\n";
            }

            %aggregated_forecast = %working_set;

            # Build metadata for AnalysisContext
            $vm_scope_meta = {
                type               => ($include_filter && $exclude_filter) ? 'include_and_exclude'
                                    : ($include_filter) ? 'explicit_include'
                                    : 'explicit_exclude',
                include_configured => ($include_filter ? scalar(keys %$include_filter) : 0),
                exclude_configured => ($exclude_filter ? scalar(keys %$exclude_filter) : 0),
                matched_count      => $after_count,
                vms                => [ sort keys %aggregated_forecast ],
                include_list       => ($include_filter ? [ sort keys %$include_filter ] : undef),
                exclude_list       => ($exclude_filter ? [ sort keys %$exclude_filter ] : undef),
            };
        }
    }

    # Early exit if all VMs filtered out
    unless (%aggregated_forecast) {
        print STDERR "  [-] Skipping history storage for event '$event_name' bucket " . ($bucket_exec_ctx->{anchor_bucket} // 'unknown') . " (no VMs after scope filter)\n";
        return;
    }

    # Stamp aggregated metadata into %seasonal_debug_info for history storage
    # Phase 3.2: include all component fields required by _store_model_forecast_to_history
    %seasonal_debug_info = (); # rebuild only with synthesis summaries
    foreach my $vm (keys %multiplier_samples) {
        foreach my $profile (keys %{ $multiplier_samples{$vm} }) {
            my $syn = _synthesis_summary($multiplier_samples{$vm}{$profile}, $method);
            next unless $syn;

            # Aggregate each component field
            # Multiplier uses the configured aggregation method; other fields use median for stability
            $seasonal_debug_info{$vm}{$profile}{synthesis} = $syn;
            my $agg_m = _aggregate_numeric($multiplier_samples{$vm}{$profile}, $method);
            my $agg_b = _aggregate_numeric($baseline_samples{$vm}{$profile} // [], 'median');
            my $agg_t = _aggregate_numeric($trend_samples{$vm}{$profile} // [], 'median');
            my $agg_v = _aggregate_numeric($volatility_samples{$vm}{$profile} // [], 'median');
            my $agg_r = _aggregate_numeric($residual_samples{$vm}{$profile} // [], 'median');
            my $agg_a = _aggregate_numeric($amplification_samples{$vm}{$profile} // [], 'median');

             $seasonal_debug_info{$vm}{$profile} = {
                multiplier           => (defined $agg_m ? ($agg_m + 0) : undef),
                baseline             => (defined $agg_b ? ($agg_b + 0) : undef),
                trend_factor         => (defined $agg_t ? ($agg_t + 0) : undef),
                volatility           => (defined $agg_v ? ($agg_v + 0) : undef),
                forecasted_residual  => (defined $agg_r ? ($agg_r + 0) : undef),
                amplification_factor => (defined $agg_a ? ($agg_a + 0) : undef),
                synthesis            => $syn,
            };
        }
    }

    # Build analysis context for storage (bucket-level)
    my $analysis_context = {
        analysis_start  => time(),
        start       => $bucket_exec_ctx->{effective_start_date},
        end         => $bucket_exec_ctx->{effective_end_date},
        cache_start => $bucket_exec_ctx->{cache_start_date},
        cache_end   => $bucket_exec_ctx->{cache_end_date},
        interval    => $bucket_exec_ctx->{sampling_interval},
        anchor_bucket => $bucket_exec_ctx->{anchor_bucket},
        anchor_source => $bucket_exec_ctx->{anchor_source},
        bucket_occurrence_count => $occ_n,
        bucket_occurrence_end_dates => [ map { $_->{end} ? $_->{end}->ymd : () } @$bucket_occ_aref ],
        # Phase 3.3: VM scope metadata (undef if no filtering applied)
        vm_scope => $vm_scope_meta,
    };

    # Horizon meta is optional for multiplicative; keep undef unless you already compute it
    my $horizon_meta = undef;

    _store_model_forecast_to_history(
        $current_cache_path,
        $event_name,
        'multiplicative_seasonal',
        \%aggregated_forecast,
        $event_cfg,
        $horizon_meta,
        $analysis_context,
        $bucket_exec_ctx->{anchor_bucket},
        $bucket_exec_ctx->{_fingerprints},  # Phase 4: Idempotency fingerprints
        $bucket_exec_ctx->{_baseline_meta}  # Phase 6: Baseline metadata for staleness
    );

    # Phase 3.2: CSV output for aggregated bucket
    # Note: CSV emission is now handled during the final occurrence iteration above,
    # eliminating the redundant engine run that previously occurred here.
    # The forecasts stored to history are the properly aggregated values.

    return;
}
