#!/usr/bin/env perl

# NAME     : nfit-profile
# AUTHOR   : NiÃ«l Lambrechts (https://github.com/niel-lambrechts)
# PURPOSE  : Runs 'nfit' multiple times with user-defined profiles.
#            Applies RunQ modifiers, generates hints, logs rationale, and aggregates to CSV.
# REQUIRES : Perl, nfit, Time::Piece, List::Util, IPC::Open3, version

use strict;
use warnings;
use Getopt::Long qw(GetOptions);
use Cwd qw(abs_path);
use File::Basename qw(dirname basename);
use File::Spec qw(catfile);
use File::Path qw(make_path);
use File::Temp qw(tempfile);
use Time::Piece;
use Time::Seconds;
use List::Util qw(sum sum0 min max uniq); # sum0 is available from List::Util 1.33+
use Scalar::Util qw(looks_like_number);
use IPC::Open3;
use IO::Select;
use version;
use JSON;
use Fcntl qw(:DEFAULT :flock);
use constant ONE_SECOND => 1;
use Data::Dumper;

# --- Store original ARGV for logging ---
my @original_argv = @ARGV;

# --- Capture nfit-profile.pl start time ---
my $PROFILE_SCRIPT_START_TIME_EPOCH = time();
my $PROFILE_SCRIPT_START_TIME_STR = localtime($PROFILE_SCRIPT_START_TIME_EPOCH)->strftime("%Y-%m-%d %H:%M:%S %Z");

# --- Version ---
my $VERSION = '5.25.199.0';

# --- Configuration ---
my $DEFAULT_ROUND_INCREMENT = 0.05;
my $DEFAULT_VM_CONFIG_FILE = "config-all.csv";
my $DEFAULT_PROFILES_CONFIG_FILE = "nfit.profiles.cfg";
my $DEFAULT_SMT_VALUE_PROFILE = 8;
my $DEFAULT_RUNQ_NORM_PERCS = "50,90"; # Global default for nfit-profile if not in profile's flags
my $DEFAULT_RUNQ_ABS_PERCS  = "90";    # Global default for nfit-profile if not in profile's flags
my $DEFAULT_NFIT_RUNQ_AVG_METHOD = "ema";

# This profile's CPU value is used for MaxCPU pressure checks,
# and its RunQ metrics will now be used for global RunQ pressure hints.
my $MANDATORY_PEAK_PROFILE_FOR_HINT = "P-99W1";

my $DEFAULT_PERCENTILE = 95;

# Windowed Decay Defaults (for when nfit-profile instructs nfit to use its internal decay)
my $DEFAULT_PROCESS_WINDOW_UNIT_FOR_NFIT = "weeks";
my $DEFAULT_PROCESS_WINDOW_SIZE_FOR_NFIT = 1;
my $DEFAULT_DECAY_HALF_LIFE_DAYS_FOR_NFIT = 30;

# Heuristic Thresholds (for sizing hints and RunQ modifiers)
my $PATTERN_RATIO_THRESHOLD = 2.0;     # For O vs B pattern determination
my $HIGH_PEAK_RATIO_THRESHOLD = 5.0;   # For "Very Peaky" shape
my $LOW_PEAK_RATIO_THRESHOLD  = 2.0;   # For "Moderately Peaky" shape
my $LIMIT_THRESHOLD_PERC = 0.98;       # P99W1 vs MaxCPU for pressure detection

# RunQ Modifier Thresholds (for calculate_runq_modified_physc)
my $WORKLOAD_PRESSURE_NORM_P90_TRIGGER_THRESHOLD = 2.0; # NormRunQ P90 above this may indicate workload pressure (NOT adaptive)
my $RUNQ_PRESSURE_P90_SATURATION_THRESHOLD = 1.5;       # AbsRunQ P90 / (MaxCPU * SMT) above this indicates RunQ pressure (ADAPTIVE)
my $RUNQ_ADDITIVE_TOLERANCE_FACTOR = 1.8;               # Tolerate AbsRunQ up to this Factor x Base_Profile_PhysC's LCPU capacity (NOT adaptive)
my $ENTITLEMENT_BURST_ALLOWANCE_FACTOR = 0.25;          # Allow uncapped VMs to burst 25% over their entitlement before pressure is assessed
my $BURST_ALLOWANCE_MIN_FACTOR = 0.0;
my $BURST_ALLOWANCE_MAX_FACTOR = 0.5;                   # Clamp at 50%

# Internal constants for growth heuristics (not user-configurable initially)
my $GROWTH_MIN_HISTORICAL_PERIODS       = 5;    # Min number of windowed periods to attempt trend
my $GROWTH_MAX_CV_THRESHOLD             = 0.50; # Max Coefficient of Variation (StdDev/Mean); if > this, data too volatile
my $GROWTH_MIN_POSITIVE_SLOPE_THRESHOLD = 0.01; # Min slope (units/period) to consider as actual growth for inflation
my $GROWTH_MAX_PROJECTION_HISTORY_RATIO = 2.0;  # Max ratio of projection duration to history duration used for trend
my $DEFAULT_GROWTH_PROJECTION_DAYS         = 90;
my $DEFAULT_MAX_GROWTH_INFLATION_PERCENT   = 25;

# Max_Additive_CPU Sliding Scale Thresholds (caps additive CPU based on current entitlement)
my $MAX_ADD_ENT_THRESH1 = 1.0; my $MAX_ADD_ABS_VAL1 = 1.0;    # If Ent < 1.0, max add = 1.0 core
my $MAX_ADD_ENT_THRESH2 = 2.0; my $MAX_ADD_PERC_VAL2 = 1.0;   # If Ent < 2.0, max add = 100% of Ent
my $MAX_ADD_ENT_THRESH3 = 4.0; my $MAX_ADD_PERC_VAL3 = 0.75;  # If Ent < 4.0, max add = 75% of Ent
my $MAX_ADD_PERC_VAL_ELSE = 0.5;                             # Else, max add = 50% of Ent

# RunQ Volatility Confidence Factor Thresholds & Values (adjusts additive CPU based on P90/P50 RunQ ratio)
my $VOLATILITY_SPIKY_THRESHOLD = 0.5;    my $VOLATILITY_SPIKY_FACTOR = 0.70;    # Very stable/spiky, less confidence in adding CPU
my $VOLATILITY_MODERATE_THRESHOLD = 0.8; my $VOLATILITY_MODERATE_FACTOR = 0.85; # Moderately stable
my $RUNQ_PRESSURE_SATURATION_CONFIDENCE_FACTOR = 1.0; # Full confidence if RunQ saturation is high

# --- Advanced Efficiency Adjustment Tunable Parameters ---
# Guard Rail for High Existing Constraint (CPU Downsizing Skip)
my $BASE_PHYSC_VS_MAXCPU_THRESH_FOR_CONSTRAINT_GUARD = 0.90; # If Base PhysC > 90% of MaxCPU
my $RUNQ_PRESSURE_FOR_CONSTRAINT_GUARD_FACTOR = 0.80;      # And RunQ Pressure > (this factor * saturation_threshold)

# Dynamic Blending Weights for Efficient Target (P_efficient_target_raw vs. BasePhysC)
my $NORM_P50_LOW_THRESH_FOR_BLEND1 = 0.25;       # If NormP50 < this, give more weight to raw target
my $BLEND_WEIGHT_BASE_FOR_LOW_P50_1 = 0.60;      #   60% Base / 40% Raw Target
my $NORM_P50_MODERATE_THRESH_FOR_BLEND2 = 0.40;  # If NormP50 < this (but >= BLEND1), moderate blend
my $BLEND_WEIGHT_BASE_FOR_LOW_P50_2 = 0.75;      #   75% Base / 25% Raw Target (original idea)
# If NormP50 >= MODERATE_THRESH_FOR_BLEND2 (but still low enough for efficiency consideration),
# lean more heavily on BasePhysC by default (e.g., 85% Base / 15% Target)
my $BLEND_WEIGHT_BASE_DEFAULT_LOW_P50 = 0.85;

# Volatility-Sensitive Cap for MAX_EFFICIENCY_REDUCTION_PERCENTAGE
# These thresholds are for Volatility Ratio (P90/P50)
my $VOLATILITY_MODERATE_LOW_CAP_THRESH = 1.2;  # Volatility above this starts to reduce the reduction cap
my $VOLATILITY_MODERATE_HIGH_CAP_THRESH = 1.8; # Volatility above this reduces cap more significantly
# Factors to scale down MAX_EFFICIENCY_REDUCTION_PERCENTAGE
my $REDUCTION_CAP_SCALE_FOR_MODERATE_VOLATILITY = 0.66; # e.g., 15% * 0.66 = ~10% max cut
my $REDUCTION_CAP_SCALE_FOR_MODERATE_HIGH_VOLATILITY = 0.33; # e.g., 15% * 0.33 = ~5% max cut

# --- Single-Thread-Dominated (STD) Workload Heuristics ---
# These constants are used to detect bursting workloads that are likely inefficient
# and should still be considered for strategic (but not tactical) downsizing.
my $STD_NORM_P90_THRESH = 0.5; # NormRunQ P90 must be below this to be considered non-concurrent.
my $STD_IQRC_THRESH     = 0.3; # Volatility must be below this (very steady).
my $STD_DAMPENING_FACTOR_FOR_UNCAPPED = 0.50; # Apply a 50% dampening factor to the strategic downsizing signal for these workloads.

# --- Enhanced Efficiency Factor Constants (for calculate_runq_modified_physc) ---
my $VOLATILITY_CAUTION_THRESHOLD = 2.5; # If NormRunQ P90/P50 ratio >= this, skip efficiency reduction
my $NORM_P50_THRESHOLD_FOR_EFFICIENCY_CONSIDERATION = 0.5; # NormRunQ P50 must be below this to consider efficiency
my $MAX_EFFICIENCY_REDUCTION_PERCENTAGE  = 0.15; # Max % a profile can be reduced by efficiency logic (ADAPTIVE)
my $MIN_P50_DENOMINATOR_FOR_VOLATILITY = 0.1;    # Min P50 value to avoid division by zero in volatility calc
my $DEFAULT_TARGET_NORM_RUNQ_FOR_EFFICIENCY_CALC = 0.75; # Base, SMT-dependent adjustments in calc sub (ADAPTIVE)

# --- Hot Thread Workload (HTW) Additive Dampening Heuristics ---
# These constants are used to detect and dampen additive CPU for workloads
# that appear constrained (e.g., single-threaded) despite high normalized RunQ.
my $HOT_THREAD_WL_ENT_FACTOR = 0.80;    # BasePhysC < Entitlement * this_factor
my $HOT_THREAD_WL_MAXCPU_FACTOR = 0.25;   # OR BasePhysC < MaxCPU * this_factor
my $HOT_THREAD_WL_HIGH_NORM_P50_THRESHOLD = 3.0; # NormRunQ P50 > this_threshold
# RUNQ_PRESSURE_P90_SATURATION_THRESHOLD (1.8) is an existing constant, also used here.
my $HOT_THREAD_WL_IQRC_THRESHOLD = 1.0;    # NormRunQ_IQRC > this_threshold (tune based on data)
my $HOT_THREAD_WL_DETECTION_MIN_CONDITIONS_MET = 4; # Minimum number of detection conditions to be met

# Dynamic Dampening Multipliers for HTW
my $HOT_THREAD_WL_BASE_DAMPENING_FACTOR = 0.25; # Base factor for the dynamic dampening calculation
my $HOT_THREAD_WL_MIN_DYNAMIC_DAMPENING = 0.05; # Floor for the final dynamic dampening factor
my $HOT_THREAD_WL_MAX_DYNAMIC_DAMPENING = 0.75; # Ceiling for the final dynamic dampening factor

# Enhanced Safety Caps for Additive CPU (applied AFTER all other factors)
my $ADDITIVE_CPU_SAFETY_CAP_FACTOR_OF_BASE = 2.0; # Max additive CPU as a multiple of BasePhysC
my $ADDITIVE_CPU_SAFETY_CAP_ABSOLUTE = 0.5;       # Absolute maximum additive CPU in cores

my $POOL_CONSTRAINT_CONFIDENCE_FACTOR = 0.80; # Reduction factor for additive CPU if VM is in a non-default pool

my $LOG_FILE_PATH = "/tmp/nfit-profile.log";        # Default log file path
my $log_file_path_for_run = $LOG_FILE_PATH;

my $CACHE_STATES_FILE = ".nfit.cache.states";
my $DATA_CACHE_FILE   = ".nfit.cache.data";
my $UNIFIED_HISTORY_FILE = ".nfit.history.json";    # Unified history file

# --- Profile Definitions (Loaded from file) ---
my @profiles;
my $PEAK_PROFILE_NAME = "Peak"; # Standardized name for the peak metric column in output
my @output_header_cols_csv;

# --- Argument Parsing ---
my $physc_data_file;
my $runq_data_file_arg;
my $vm_config_file_arg;
my $profiles_config_file_arg;
my $start_date_str;
my $target_vm_name;
my $round_arg;                     # For nfit's -r (round to nearest)
my $roundup_arg;                   # For nfit's -u (round up)
my $default_smt_arg = $DEFAULT_SMT_VALUE_PROFILE;
my $runq_norm_perc_list_str = $DEFAULT_RUNQ_NORM_PERCS; # Global default for nfit-profile itself
my $runq_abs_perc_list_str  = $DEFAULT_RUNQ_ABS_PERCS;  # Global default for nfit-profile itself
my $runq_perc_behavior_mode = 'fixed';                  # Default behavior: use fixed P90 for RunQ. Alternative: 'match'.
my $help = 0;
my $show_version = 0;
my $script_dir = dirname(abs_path($0));
my $nfit_script_path = "$script_dir/nfit"; # Default path to nfit script
my $nfit_enable_windowed_decay = 0;        # Flag to instruct nfit to use its internal decay
my $nfit_window_unit_str = $DEFAULT_PROCESS_WINDOW_UNIT_FOR_NFIT;
my $nfit_window_size_val = $DEFAULT_PROCESS_WINDOW_SIZE_FOR_NFIT;
my $nfit_decay_half_life_days_val = $DEFAULT_DECAY_HALF_LIFE_DAYS_FOR_NFIT;
my $nfit_analysis_reference_date_str;
my $nfit_runq_avg_method_str = $DEFAULT_NFIT_RUNQ_AVG_METHOD; # 'none', 'sma', 'ema' for nfit's RunQ processing
my $nfit_decay_over_states = 0;
my $nmon_dir;
my $excel_formulas_flag = "false";
my $mgsys_filter;
my $verbose = 0;

# Seasonality related variables
my $apply_seasonality_event;
my $update_history_flag = 0; # New unified history update flag
my $min_history_days_arg;    # New flag for partial month processing
my $reset_seasonal_cache = 0;
my $DEFAULT_SEASONALITY_CONFIG_FILE = "nfit.seasonality.cfg";

GetOptions(
    'nmondir=s'                  => \$nmon_dir,
    'config=s'                   => \$vm_config_file_arg,
    'profiles-config=s'          => \$profiles_config_file_arg,
    'startdate|s=s'              => \$start_date_str,
    'vm|lpar=s'                  => \$target_vm_name,
    'round|r:f'                  => \$round_arg,
    'roundup|u:f'                => \$roundup_arg,
    'default-smt|smt=i'          => \$default_smt_arg,
    'runq-norm-percentiles=s'    => \$runq_norm_perc_list_str,      # Global default list for NormRunQ
    'runq-abs-percentiles=s'     => \$runq_abs_perc_list_str,       # Global default list for AbsRunQ
    'runq-perc-behavior=s'       => \$runq_perc_behavior_mode,      # default: use 'AbsRunQ_P90' for all profiles ('match': match the RunQ percentile to the PhysC profile percentile)
    'help|h'                     => \$help,
    'nfit-path=s'                => \$nfit_script_path,
    'version|v'                  => \$show_version,
    'enable-windowed-decay'      => \$nfit_enable_windowed_decay,
    'process-window-unit=s'      => \$nfit_window_unit_str,
    'process-window-size=i'      => \$nfit_window_size_val,
    'decay-half-life-days=i'     => \$nfit_decay_half_life_days_val,
    'analysis-reference-date=s'  => \$nfit_analysis_reference_date_str,
    'runq-avg-method=s'          => \$nfit_runq_avg_method_str,
    'decay-over-states'          => \$nfit_decay_over_states,
    'excel-formulas=s'           => \$excel_formulas_flag,
    'mgsys|system|serial|host=s' => \$mgsys_filter,
    'apply-seasonality=s'        => \$apply_seasonality_event,
    'update-history'             => \$update_history_flag,
    'min-history-days=i'         => \$min_history_days_arg,
    'reset-seasonal-cache'       => \$reset_seasonal_cache,
    'verbose'                    => \$verbose,
) or die usage_wrapper();

# --- Validation ---
my $nfit_ver = "N/A"; # Store nfit version
# This block now runs every time to get the nfit version for logging
if (-x $nfit_script_path) {
	my $nfit_ver_output = `$nfit_script_path --version 2>&1`;
	my ($parsed_nfit_ver) = ($nfit_ver_output =~ /nfit version\s*([0-9.a-zA-Z-]+)/i);
	if (defined $parsed_nfit_ver) {
		$nfit_ver = $parsed_nfit_ver;
		# Check nfit version compatibility for certain features
		my $required_nfit_ver_for_windowing = "2.27.0";
		my $required_nfit_ver_for_runq_avg_and_decay = "2.28.0.4";

		if ($nfit_enable_windowed_decay && version->parse($nfit_ver) < version->parse($required_nfit_ver_for_windowing)) {
			print STDERR "Warning: --enable-windowed-decay requires nfit version $required_nfit_ver_for_windowing or higher. Your nfit version ($nfit_ver) may not support this.\n";
		}
		if (defined $nfit_runq_avg_method_str && $nfit_runq_avg_method_str ne 'none' && version->parse($nfit_ver) < version->parse($required_nfit_ver_for_runq_avg_and_decay)) {
			print STDERR "Warning: --runq-avg-method (sma/ema) may require nfit features from version $required_nfit_ver_for_runq_avg_and_decay or higher. Your nfit version ($nfit_ver) behavior might differ for RunQ processing, especially if --runq-decay is intended.\n";
		}
	} else {
		print STDERR "Could not determine nfit version from output: $nfit_ver_output\n";
		if ($nfit_enable_windowed_decay || (defined $nfit_runq_avg_method_str && $nfit_runq_avg_method_str ne 'none')) {
			print STDERR "Warning: Advanced nfit features are enabled but nfit version cannot be verified.\n";
		}
	}
} else {
    print STDERR "nfit script at '$nfit_script_path' not found or not executable.\n";
}

if ($show_version)
{
    print STDERR "nfit-profile version $VERSION\n";
    print STDERR "  nfit version $nfit_ver\n";
}

# --- Validation for runq-perc-behavior ---
$runq_perc_behavior_mode = lc($runq_perc_behavior_mode);
unless ($runq_perc_behavior_mode eq 'fixed' || $runq_perc_behavior_mode eq 'match')
{
    die "Error: Invalid value for --runq-perc-behavior. Must be 'fixed' or 'match' instead of '$runq_perc_behavior_mode'.\n";
}

# --- Data Source Validation ---
# The script now operates exclusively on pre-built cache directories.
my $DEFAULT_BASE_STAGE_DIR = File::Spec->catfile($script_dir, 'stage');

if ($help)
{
    print STDERR usage_wrapper();
    exit 0;
}

# A cache source must be specified via --nmondir or --mgsys.
if (!defined $nmon_dir && !defined $mgsys_filter)
{
    print STDERR usage_wrapper();
    die "Error: No data source cache specified. Please use --nmondir or --mgsys.\n";
}

# If --mgsys is provided without a base --nmondir, set --nmondir to the default.
# This allows the Smart Dispatcher to find the system-specific cache.
if (defined $mgsys_filter && !defined $nmon_dir)
{
    $nmon_dir = $DEFAULT_BASE_STAGE_DIR;
    # Only print the info message if it's a regular run, not a staging/snapshotting run.
    if (!$update_history_flag) {
        print STDERR "Cache Path (default): '$nmon_dir'\n";
    }
}

# Final check to ensure the base directory exists.
if (defined $nmon_dir && !-d $nmon_dir)
{
    die "Error: The specified cache directory (--nmondir) was not found: '$nmon_dir'\n";
}


if ($default_smt_arg <= 0)
{
    die "Error: --default-smt value must be a positive integer (e.g., 4, 8).\n";
}
if (! -x $nfit_script_path)
{
    die "Error: Cannot find or execute 'nfit' script at '$nfit_script_path'. Use --nfit-path.\n";
}
if (defined($round_arg) && defined($roundup_arg))
{
    die "Error: -round (-r) and -roundup (-u) options are mutually exclusive.\n";
}
if (defined $start_date_str && $start_date_str !~ /^\d{4}-\d{2}-\d{2}$/)
{
    die "Error: Invalid startdate (-s) format '$start_date_str'. Use YYYY-MM-DD.\n";
}

# Validations for nfit's windowed decay options, if enabled by nfit-profile
if ($nfit_enable_windowed_decay && $nfit_decay_over_states)
{
    die "Error: --enable-windowed-decay and --decay-over-states are mutually exclusive analysis modes.\n";
}
if ($nfit_enable_windowed_decay)
{
    if ($nfit_window_unit_str ne "days" && $nfit_window_unit_str ne "weeks")
    {
        die "Error: --process-window-unit must be 'days' or 'weeks'.\n";
    }
    if ($nfit_window_size_val < 1)
    {
        die "Error: --process-window-size must be at least 1.\n";
    }
    if ($nfit_decay_half_life_days_val < 1)
    {
        die "Error: --decay-half-life-days must be at least 1.\n";
    }
    if (defined $nfit_analysis_reference_date_str && $nfit_analysis_reference_date_str !~ /^\d{4}-\d{2}-\d{2}$/)
    {
        die "Error: Invalid --analysis-reference-date format. Use YYYY-MM-DD.\n";
    }
    print STDERR "nfit's internal windowed decay processing is enabled via nfit-profile.\n";
}

# Validate nfit's RunQ averaging method, if specified
if (defined $nfit_runq_avg_method_str)
{
    $nfit_runq_avg_method_str = lc($nfit_runq_avg_method_str);
    unless ($nfit_runq_avg_method_str eq 'none' || $nfit_runq_avg_method_str eq 'sma' || $nfit_runq_avg_method_str eq 'ema')
    {
        die "Error: --runq-avg-method must be 'none', 'sma', or 'ema'. Got '$nfit_runq_avg_method_str'.\n";
    }
    print STDERR "RunQ Averaging Method: $nfit_runq_avg_method_str.\n";
}

# --- Excel Formula Validation ---
my $add_excel_formulas = 1; # Default to on
if (defined $excel_formulas_flag)
{
    my $val = lc($excel_formulas_flag);
    if ($val eq 'true' || $val eq '1')
    {
        $add_excel_formulas = 1;
    }
    elsif ($val eq 'false' || $val eq '0')
    {
        $add_excel_formulas = 0;
    }
    else
    {
        die "Error: Invalid value for --excel-formulas. Must be 'true' or 'false'.\n";
    }
}

my $output_dir = File::Spec->catfile($script_dir, 'output');
make_path($output_dir);
if (! -d $output_dir) {
    die "Unable to create output directory '$output_dir': $!";
}
my @generated_files; # Global array to store names of all generated files

# --- Smart Dispatcher Logic ---
# This block determines which cache directories to process. It can handle
# being pointed at a single cache directory or a parent directory containing
# multiple system-specific caches.

my @target_systems_to_process;
my $base_cache_dir = $nmon_dir; # Assume the provided dir is the base by default.

# First, check if the provided --nmondir is ITSELF a valid cache directory.
if (-f File::Spec->catfile($nmon_dir, '.nfit_stage_id'))
{
    # This is a singular run targeting a specific cache directory.
    print STDERR "Single cache directory detected. Enabling singular analysis mode.\n";
    push @target_systems_to_process, undef; # 'undef' signals a single run.
    $base_cache_dir = dirname($nmon_dir); # The base is the parent of the cache dir.
}
else
{
    # If not, check if it's a PARENT directory containing multiple caches.
    opendir(my $dh, $nmon_dir) or die "Cannot open directory $nmon_dir: $!";
    my @subdirs = grep { -d File::Spec->catfile($nmon_dir, $_) && !/^\./ } readdir($dh);
    closedir($dh);

    my @found_serials;
    foreach my $subdir (@subdirs)
    {
        if (-f File::Spec->catfile($nmon_dir, $subdir, '.nfit_stage_id'))
        {
            push @found_serials, $subdir;
        }
    }

    if (@found_serials)
    {
        # This is a multi-system run.
        print STDERR "Managed system cache hierarchy detected. Enabling multi-system mode.\n";
        $base_cache_dir = $nmon_dir; # The provided dir is the base.

        # Apply --mgsys filter if provided, otherwise target all found systems.
        if (defined $mgsys_filter)
        {
            my %serials_from_args = map { $_ => 1 } split /,/, $mgsys_filter;
            @target_systems_to_process = grep { exists $serials_from_args{$_} } @found_serials;
        }
        else
        {
            @target_systems_to_process = @found_serials;
        }
    }
    else
    {
        # The directory is neither a cache itself, nor does it contain any caches.
        die "Error: The directory '$nmon_dir' is not a valid nFit cache and does not contain any cache subdirectories.\n";
    }
}

# If a --vm filter was provided, we must refine our list of target systems.
# This only makes sense in a multi-system context where we can check each one.
if (scalar(@target_systems_to_process) > 1 && defined $target_vm_name)
{
    my %vms_to_find = map { $_ => 1 } split /,/, $target_vm_name;
    my %systems_with_target_vms;

    foreach my $serial (@target_systems_to_process)
    {
        my $states_file = File::Spec->catfile($base_cache_dir, $serial, $CACHE_STATES_FILE);
        next unless -f $states_file;

        my $json_text = do { open my $fh, '<:encoding(utf8)', $states_file or next; local $/; my $content = <$fh>; close $fh; $content; };
        next unless defined $json_text;

        my $states = eval { decode_json($json_text) };
        if ($@) { warn "Warning: Could not decode JSON from '$states_file': $@. Skipping for VM discovery."; next; }

        foreach my $vm_in_state (keys %$states)
        {
            if (exists $vms_to_find{$vm_in_state})
            {
                $systems_with_target_vms{$serial} = 1;
                last; # Found a match for this system.
            }
        }
    }
    # The new target list is only the systems that contain the specified VMs.
    @target_systems_to_process = sort keys %systems_with_target_vms;
}

if (!@target_systems_to_process)
{
    die "Error: No target managed systems could be identified for processing based on the provided filters.\n";
}

print STDERR "Dispatcher will process " . scalar(@target_systems_to_process) . " managed system(s).\n";

# --- Load Profile Definitions ---
# The log file will now be created inside the main processing loop for each system.
print STDERR "Profile Definitions:\n";
my $profiles_config_path_to_load;
if (defined $profiles_config_file_arg)
{
    if (-f $profiles_config_file_arg)
    {
        $profiles_config_path_to_load = $profiles_config_file_arg;
    }
    else
    {
        die "Error: Specified profiles config (--profiles-config) not found: $profiles_config_file_arg\n";
    }
}
else # Attempt to find default profiles config
{
    $profiles_config_path_to_load = "$script_dir/etc/$DEFAULT_PROFILES_CONFIG_FILE";
    unless (-f $profiles_config_path_to_load)
    {
        $profiles_config_path_to_load = "$script_dir/$DEFAULT_PROFILES_CONFIG_FILE"; # Try in script's root
    }
    unless (-f $profiles_config_path_to_load)
    {
        die "Error: Default profiles config '$DEFAULT_PROFILES_CONFIG_FILE' not found in '$script_dir/etc/' or '$script_dir/'. Use --profiles-config.\n";
    }
}
print STDERR "  - Profile configurations: $profiles_config_path_to_load\n";
@profiles = load_profile_definitions($profiles_config_path_to_load);
if (scalar @profiles == 0)
{
    die "Error: No profiles loaded from '$profiles_config_path_to_load'.\n";
}

# --- Generate CSV header columns ---

# Define the order of columns for the CSV output
@output_header_cols_csv = (
    "VM", "TIER", "Hint", "Pattern", "Pressure", "PressureDetail", "SMT",
    "Serial", "SystemType", "Pool Name", "Pool ID", "RunQ_Modifier", "RunQ_Uncapped", "RunQ_Source", $PEAK_PROFILE_NAME
);

# Add profile names as column headers (these will contain the RunQ-modified PhysC values)
push @output_header_cols_csv, map { $_->{name} } @profiles;
# Add entitlement and formula placeholder columns
push @output_header_cols_csv, ("Current - ENT", "NFIT - ENT", "NETT", "NETT%");

# --- Ensure the Mandatory P-99W1 Profile is Defined ---
# This check occurs after profiles are loaded.
my $mandatory_profile_is_present = 0;
foreach my $profile_entry (@profiles)
{
    if (defined $profile_entry->{name} && $profile_entry->{name} eq $MANDATORY_PEAK_PROFILE_FOR_HINT)
    {
        $mandatory_profile_is_present = 1;
        last;
    }
}

unless ($mandatory_profile_is_present)
{
    die "ERROR: Mandatory profile \"$MANDATORY_PEAK_PROFILE_FOR_HINT\" is not defined in the profiles configuration file: '$profiles_config_path_to_load'.\n" .
    "       This profile is essential for core pressure detection logic in nfit-profile.\n" .
    "       Please add a profile named \"$MANDATORY_PEAK_PROFILE_FOR_HINT\" to your profiles configuration.\n";
}

print STDERR "  - Loaded " . scalar(@profiles) . " profiles.\n";

# --- Load Seasonality Definitions ---
my $seasonality_config_path;
my $seasonality_config_file_arg; # Assume this could be a future flag
my $seasonality_config;

if (defined $seasonality_config_file_arg && -f $seasonality_config_file_arg) {
    $seasonality_config_path = $seasonality_config_file_arg;
} else {
    my $default_path_etc = "$script_dir/etc/$DEFAULT_SEASONALITY_CONFIG_FILE";
    my $default_path_root = "$script_dir/$DEFAULT_SEASONALITY_CONFIG_FILE";
    if (-f $default_path_etc) {
        $seasonality_config_path = $default_path_etc;
    } elsif (-f $default_path_root) {
        $seasonality_config_path = $default_path_root;
    }
}

if ($seasonality_config_path) {
    print STDERR "Seasonality Definitions: $seasonality_config_path\n";
    # Replace Config::Tiny->read() with our custom, dependency-free parser.
    $seasonality_config = parse_seasonality_config($seasonality_config_path);
    unless (defined $seasonality_config) {
        # The custom sub will die on error, but this is a safeguard.
        die "Error: Could not parse seasonality config file '$seasonality_config_path'.\n";
    }
    my $event_count = scalar(keys %$seasonality_config);
    print STDERR "  - Loaded $event_count seasonal event definitions.\n";
} elsif ($apply_seasonality_event || $update_history_flag) {
    # It's an error to request seasonality if the config file doesn't exist.
    die "Error: Seasonality feature requested, but the configuration file '$DEFAULT_SEASONALITY_CONFIG_FILE' was not found in '$script_dir/etc/' or '$script_dir/'.\n";
}

# --- Determine Minimum Days for History Processing ---
my $min_days_for_history;
my $MIN_HISTORY_DAYS_DEFAULT = 28; # Hard-coded default

if (defined $min_history_days_arg) {
    $min_days_for_history = $min_history_days_arg; # Command line takes precedence
} elsif (defined $seasonality_config && exists $seasonality_config->{Global}{min_history_days}) {
    $min_days_for_history = $seasonality_config->{Global}{min_history_days}; # Config file is second
} else {
    $min_days_for_history = $MIN_HISTORY_DAYS_DEFAULT; # Fallback to default
}

# --- Validation for Seasonality and Decay Model Interaction ---
if (defined $apply_seasonality_event) {
    # Abort if the specified event doesn't exist in the loaded configuration.
    unless (exists $seasonality_config->{$apply_seasonality_event}) {
        die "Error: Seasonal event '$apply_seasonality_event' could not be found as a valid section in $seasonality_config_path\n";
    }

    my $event_config = $seasonality_config->{$apply_seasonality_event} // {};
    my $model_type = $event_config->{model} // '';

    # The multiplicative_seasonal model is a complete workflow and cannot be
    # combined with the top-level decay flags.
    if ($model_type eq 'multiplicative_seasonal' && ($nfit_enable_windowed_decay || $nfit_decay_over_states)) {
        die "Error: Incompatible arguments. The '--apply-seasonality' flag for a 'multiplicative_seasonal' event cannot be used with '--enable-windowed-decay' or '--decay-over-states'.\n";
    }
    # Automatically enable the correct decay model if needed
    elsif ($model_type eq 'recency_decay') {
        print STDERR "INFO: Enabling '--enable-windowed-decay' for 'recency_decay' model.\n";
        $nfit_enable_windowed_decay = 1; # Use the time-based windowed decay engine.
        $nfit_decay_over_states = 0;     # Ensure the state-based engine is disabled.
    }
}

# --- Locate and Load VM Configuration Data ---
# This data provides SMT, MaxCPU, Entitlement, etc., per VM.
my $vm_config_file_path = undef;
my $vm_config_found = 0;
my %vm_config_data;         # Stores parsed VM config: $vm_config_data{hostname}{key} = value
my %vm_config_col_idx;      # Maps column names (lowercase) to their index in the CSV
my $vm_config_header_count = 0; # Number of columns in VM config header

if (defined $vm_config_file_arg) # User specified a VM config file
{
    if (-f $vm_config_file_arg)
    {
        $vm_config_file_path = $vm_config_file_arg;
        $vm_config_found = 1;
        print STDERR "VM Configurations: $vm_config_file_path\n";
    }
    else
    {
        die "Error: Specified VM configuration file (-config) not found: $vm_config_file_arg\n";
    }
}
elsif (!defined $apply_seasonality_event)
{
    # No VM config file specified, try default locations
    my $dp_etc = "$script_dir/etc/$DEFAULT_VM_CONFIG_FILE";  # Default path: script_dir/etc/
    my $dp_root = "$script_dir/$DEFAULT_VM_CONFIG_FILE"; # Alternative path: script_dir/
    if (-f $dp_etc)
    {
        $vm_config_file_path = $dp_etc;
        $vm_config_found = 1;
        print STDERR "Found default VM configuration file: $vm_config_file_path\n";
    }
    elsif (-f $dp_root)
    {
        $vm_config_file_path = $dp_root;
        $vm_config_found = 1;
        print STDERR "Found default VM configuration file: $vm_config_file_path\n";
    }
    else
    {
        if ($nmon_dir) {
            # Message for the modern, self-sufficient --nmondir mode
            print STDERR "Info: Default VM configuration file '$DEFAULT_VM_CONFIG_FILE' not found. VM metadata columns 'SystemType' and 'Pool Name' will be blank. Dynamically sourcing SMT and MaxCPU from NMON.\n";
        } else {
            # Original message for the standard file source (--pc / --rq) modes
            print STDERR "Info: Default VM configuration file '$DEFAULT_VM_CONFIG_FILE' not found. VM config/SMT/MaxCPU columns will be blank/default. RunQPressure_P90 logic will be impacted.\n";
        }
    }
}

if ($vm_config_found)
{
#    print STDERR "Loading VM configuration data (manual parse)...\n";
    open my $cfg_fh, '<:encoding(utf8)', $vm_config_file_path or die "Error: Cannot open VM config file '$vm_config_file_path': $!\n";
    my $hdr = <$cfg_fh>; # Read header line
    unless (defined $hdr)
    {
        die "Error: Could not read header from VM config '$vm_config_file_path'\n";
    }
    chomp $hdr;
    $hdr =~ s/\r$//;        # Remove CR if present (Windows line endings)
    $hdr =~ s/^\x{FEFF}//;  # Remove BOM if present (UTF-8 Byte Order Mark)
    my @rhdrs = split /,/, $hdr;
    $vm_config_header_count = scalar @rhdrs;
    my %hmap; # Map lowercase header name to index
    for my $i (0 .. $#rhdrs)
    {
        my $cn = $rhdrs[$i];
        $cn =~ s/^\s*"?|"?\s*$//g; # Trim spaces and quotes from column name
        if ($cn ne '')
        {
            $hmap{lc($cn)} = $i; # Store lowercase column name
        }
    }

    # Check for required columns
    my @req_cols = qw(hostname serial systemtype procpool_name procpool_id entitledcpu maxcpu);
    my $has_smt_col = exists $hmap{'smt'}; # Check if SMT column exists
    unless (exists $hmap{'maxcpu'})
    {
        warn "  - 'maxcpu' column not found in VM configuration file. This can affect MaxCPU capping logic.\n";
    }
    if ($has_smt_col)
    {
        print STDERR "  - Found 'SMT' column in VM configuration file.\n";
    }
    else
    {
        print STDERR "  - Info: 'SMT' column not found in VM configuration file. Using default SMT: $default_smt_arg for RunQ calculations.\n";
    }

    foreach my $rc (@req_cols)
    {
        unless (exists $hmap{$rc})
        {
            die "Error: Required column '$rc' not found in VM config file header '$vm_config_file_path'\n";
        }
        $vm_config_col_idx{$rc} = $hmap{$rc};
    }
    if ($has_smt_col) # If SMT column exists, store its index
    {
        $vm_config_col_idx{'smt'} = $hmap{'smt'};
    }

    # Read data lines from VM config
    while (my $ln = <$cfg_fh>)
    {
        chomp $ln;
        $ln =~ s/\r$//;
        next if $ln =~ /^\s*$/; # Skip empty lines

        # Attempt to parse CSV with quoted fields (handles commas within quotes)
        my @rvals = ($ln =~ /"([^"]*)"/g);
        if (scalar @rvals != $vm_config_header_count)
        { # Fallback to simple comma split if quote parsing fails or count mismatches
            @rvals = split /,/, $ln;
            if (scalar @rvals != $vm_config_header_count)
            {
                warn "Warning: Mismatched field count on VM config line $. Skipping: $ln\n";
                next;
            }
            # Trim whitespace for values from simple split
            $_ =~ s/^\s+|\s+$//g for @rvals;
        }
        # Else, if quote parsing worked, values in @rvals are already unquoted and trimmed by regex.

        my $hn = $rvals[ $vm_config_col_idx{'hostname'} ]; # Get hostname
        if (defined $hn && $hn ne '')
        {
            my $smt_v = $default_smt_arg; # Default SMT value
            if ($has_smt_col && defined $rvals[$vm_config_col_idx{'smt'}] && $rvals[$vm_config_col_idx{'smt'}] ne '')
            {
                my $sf = $rvals[$vm_config_col_idx{'smt'}];
                if ($sf =~ /(\d+)$/) # Extract trailing digits for SMT value (e.g., "SMT4" -> 4)
                {
                    $smt_v = $1;
                    if ($smt_v <= 0) # Validate SMT
                    {
                        warn "Warning: Invalid SMT '$sf' for '$hn'. Using default $default_smt_arg.\n";
                        $smt_v = $default_smt_arg;
                    }
                }
                else
                {
                    warn "Warning: Could not parse SMT '$sf' for '$hn'. Using default $default_smt_arg.\n";
                }
            }

            my $max_cpu_val = $rvals[$vm_config_col_idx{'maxcpu'}];
            # Validate MaxCPU value
            unless (defined $max_cpu_val && $max_cpu_val =~ /^[0-9.]+$/ && ($max_cpu_val+0) > 0)
            {
                $max_cpu_val = 0; # Default to 0 if invalid, meaning no effective MaxCPU cap from config
            }

            # Store parsed VM config data
            $vm_config_data{$hn} = {
                serial      => $rvals[$vm_config_col_idx{'serial'}],
                systemtype  => $rvals[$vm_config_col_idx{'systemtype'}],
                pool_name   => $rvals[$vm_config_col_idx{'procpool_name'}],
                pool_id     => $rvals[$vm_config_col_idx{'procpool_id'}],
                entitlement => $rvals[$vm_config_col_idx{'entitledcpu'}],
                maxcpu      => ($max_cpu_val + 0), # Store as number
                smt         => $smt_v,
            };
        }
        else # Hostname missing or empty
        {
            warn "Warning: Missing hostname on VM config line $. Skipping.\n";
        }
    }
    close $cfg_fh;
    print STDERR "  - Loaded configurations for " . scalar(keys %vm_config_data) . " VMs.\n\n";
}
else # VM config file not found or not specified
{
    print STDERR "Warning: VM configuration file not loaded. MaxCPU capping logic will be affected, and SMT will use default.\n";
}

# --- Construct Common Flags for nfit ---
# These flags are common to ALL nfit runs initiated by nfit-profile.
# Note: RunQ percentile flags (--runq-norm-perc, --runq-abs-perc) are now handled PER PROFILE run.
my $common_nfit_flags_base = "-q";
if ($nmon_dir)
{
    $common_nfit_flags_base .= " -k --nmondir \"$nmon_dir\"";
    # When using --nmondir, runq data comes from within the NMON files, so --runq-data is not used.
    # However, we still need to pass the RunQ averaging method to nfit if specified.
    if (defined $nfit_runq_avg_method_str)
    {
        $common_nfit_flags_base .= " --runq-avg-method \"$nfit_runq_avg_method_str\"";
    }
}
else # The original path using --physc-data
{
    $common_nfit_flags_base .= " -k --physc-data \"$physc_data_file\"";
    if (defined $runq_data_file_arg)
    {
        $common_nfit_flags_base .= " --runq-data \"$runq_data_file_arg\"";
        if (defined $nfit_runq_avg_method_str)
        {
            $common_nfit_flags_base .= " --runq-avg-method \"$nfit_runq_avg_method_str\"";
        }
    }
}
if (defined $start_date_str) # Global start date for all nfit runs
{
    $common_nfit_flags_base .= " -s $start_date_str";
}

# Common rounding flags (passed to nfit for its output formatting)
my $rounding_flags_for_nfit = ""; # These are applied by nfit itself
if (defined $round_arg)
{
    $rounding_flags_for_nfit .= " -r";
    if (length $round_arg && $round_arg !~ /^\s*$/) # Check if round_arg has a value (e.g. -r=0.1)
    {
        $rounding_flags_for_nfit .= "=$round_arg";
    }
}
elsif (defined $roundup_arg)
{
    $rounding_flags_for_nfit .= " -u";
    if (length $roundup_arg && $roundup_arg !~ /^\s*$/) # Check if roundup_arg has a value (e.g. -u=0.1)
    {
        $rounding_flags_for_nfit .= "=$roundup_arg";
    }
}
$common_nfit_flags_base .= $rounding_flags_for_nfit; # Add rounding to common flags if specified

# --- Main Logic: Run nfit Profiles ---
my %results_table; # Stores PhysC values from nfit for each profile: $results_table{vm_name}{profile_name}
my %runq_modifier_values;
my %nfit_growth_adjustments;
my %runq_uncapped_values;
my %hint_tier_for_csv;
my %hint_pattern_for_csv;
my %hint_pressure_for_csv;
my @vm_order;      # To maintain CSV output order consistent with first nfit run that reports VMs
my %vm_seen;       # Tracks VMs seen to populate @vm_order correctly
my %primary_runq_metrics_captured_for_vm; # Tracks if global P50/P90 RunQ metrics captured for hints
my %source_profile_for_global_runq; # Which profile's output sourced the global RunQ P50/P90 for hints
my %per_profile_runq_metrics; # Stores ALL RunQ metrics (e.g. AbsRunQ_P80, AbsRunQ_P98) from EACH profile's nfit run
# Structure: $per_profile_runq_metrics{vm_name}{profile_name}{runq_metric_key}
my %per_profile_nfit_raw_results;
my %pressure_details_for_csv;
my %seasonal_debug_info;
my %outlier_warnings;

my %parsed_growth_adj_values;      # Stores GrowthAdj from nfit output
my %parsed_growth_adj_abs_values;  # Stores GrowthAdjAbs from nfit output
my $FLOAT_EPSILON = 1e-9;

print STDERR "nfit-profile version $VERSION\n";

my $LOG_FH;
my %open_log_files;

my $is_seasonal_run = $apply_seasonality_event || $update_history_flag;

# --- Main Processing Loop (iterates through systems for InfluxDB cache, runs once for standard file source modes) ---
foreach my $system_serial (@target_systems_to_process)
{

    # Flag for the multiplicative seasonal model
    my $is_multiplicative_forecast_run = 0;
    my $is_predictive_peak_model_run = 0;

    # First, determine the full path to the cache we are processing in this iteration.
    my $current_cache_path = defined($system_serial) ? File::Spec->catfile($base_cache_dir, $system_serial) : $nmon_dir;

    # Now, robustly determine the system identifier for logging.
    # Default to the directory name, but prefer the canonical name from the ID file.
    my $system_identifier = basename($current_cache_path); # Tier 2 Fallback identifier
    my $id_file_path = File::Spec->catfile($current_cache_path, '.nfit_stage_id');

    if (-f $id_file_path) {
        eval {
            open my $id_fh, '<:encoding(utf8)', $id_file_path;
            my $id_content = <$id_fh>;
            close $id_fh;

            # Tier 1 Attempt: Extract the system name from the file.
            if ($id_content && $id_content =~ /for system\s+(.+)/) {
                my $candidate_name = $1;
                $candidate_name =~ s/^\s+|\s+$//g; # Trim leading/trailing whitespace

                # --> ADDED: Validate the extracted name.
                # It must not be empty and must not contain illegal filename characters.
                if ($candidate_name ne '' && $candidate_name !~ /[\\\/:\*\?"<>\|]/) {
                    # Validation passed. Use the canonical name.
                    $system_identifier = $candidate_name;
                } else {
                    # Validation failed. The fallback (directory name) will be used.
                    warn "Warning: Unusable system identifier ('$candidate_name') found in '$id_file_path'. Reverting to directory name.";
                }
            }
        };
        if ($@) {
            # This catches errors during file open/read. The fallback will be used.
            warn "Warning: Could not read or parse '$id_file_path'. Using directory name for log. Error: $@";
        }
    }

    my $report_type_for_log = 'state-based'; # Default for the "no flags" forensic model
    if (defined $apply_seasonality_event && $apply_seasonality_event ne '') {
        $report_type_for_log = $apply_seasonality_event;
    } elsif ($nfit_decay_over_states) {
        $report_type_for_log = 'hybrid-state-decay';
    } elsif ($nfit_enable_windowed_decay) {
        $report_type_for_log = 'windowed-decay';
    }

    if ($system_identifier && $system_identifier ne '.')
    {
        print STDERR "\n--- Processing Managed System: $system_identifier ---\n";

        # Efficiently report the data cache's time-span for user context.
        my $data_cache_file = File::Spec->catfile($current_cache_path, '.nfit.cache.data');
        my ($start_date, $end_date) = _get_cache_date_range($data_cache_file);
        if ($start_date && $end_date) {
            print STDERR "Data Cache Timespan: " . $start_date->strftime('%Y-%m-%d') . " to " . $end_date->strftime('%Y-%m-%d') . "\n";
        }

        # Determine and print the analysis type for clarity
        my $analysis_type_desc = "Standard Profile Analysis";
        if ($apply_seasonality_event) {
            my $event_config = $seasonality_config->{$apply_seasonality_event} // {};
            my $model_type = $event_config->{model} // '';
            if ($model_type eq 'multiplicative_seasonal') {
                $analysis_type_desc = "Seasonal Forecast (Multiplicative Model for event '$apply_seasonality_event')";
            } elsif ($model_type eq 'recency_decay') {
                 $analysis_type_desc = "Seasonal Analysis (Recency-Decay Model for event '$apply_seasonality_event')";
            }
        } elsif ($nfit_decay_over_states) {
            $analysis_type_desc = "Hybrid State-Time Decay Analysis (--decay-over-states)";
        } elsif ($nfit_enable_windowed_decay) {
            $analysis_type_desc = "Time-Based Windowed Decay Analysis (--enable-windowed-decay)";
        }
        print STDERR "Analysis Type: $analysis_type_desc\n";

        # Generate a timestamp string e.g., "20250723-084500"
        my $timestamp_str = localtime->strftime('%Y%m%d-%H%M%S');
        # Get the current Process ID (PID) for uniqueness
        my $pid = $$;

        # Sanitise system identifiers for use in the filename.
        my $log_suffix_system = $system_identifier;
        $log_suffix_system =~ s/[^a-zA-Z0-9_.-]//g;
        my $log_suffix_report = $report_type_for_log;
        $log_suffix_report =~ s/[^a-zA-Z0-9_.-]//g;

        $log_file_path_for_run = File::Spec->catfile($output_dir, "nfit-profile.$log_suffix_system.$log_suffix_report.$timestamp_str.$pid.log");
    }
    else
    {

# Fallback to a default log path inside the output directory for single/non-serial runs.
        my $timestamp_str = localtime->strftime('%Y%m%d-%H%M%S');
        my $pid = $$;
        $log_file_path_for_run = File::Spec->catfile($output_dir, "nfit-profile.default_system.$report_type_for_log.$timestamp_str.$pid.log");
    }

    # --- Open Log File for this specific system ---
    my $log_fh_for_system = IO::File->new($log_file_path_for_run, '>')
        or warn "Error: Cannot open rationale log for '$system_serial' at '$log_file_path_for_run': $!. Rationale logging will be skipped for this system.\n";

    if ($log_fh_for_system)
    {
        $open_log_files{$system_identifier} = $log_fh_for_system;
        my $LOG_FH = $log_fh_for_system; # Use a lexical variable for printing the header

        $LOG_FH->autoflush(1); # Ensure immediate writing to log
        print {$LOG_FH} "======================================================================\n";
        print {$LOG_FH} "nFit Profile Rationale Log\n";
        print {$LOG_FH} "======================================================================\n";
        print {$LOG_FH} "nfit-profile.pl Run Started: $PROFILE_SCRIPT_START_TIME_STR\n";
        print {$LOG_FH} "nfit-profile.pl Version  : $VERSION\n";
        print {$LOG_FH} "nfit Version Used        : $nfit_ver\n";
        print {$LOG_FH} "----------------------------------------------------------------------\n";
        my @quoted_original_argv_log = map { $_ =~ /\s/ ? qq/"$_"/ : $_ } @original_argv;
        print {$LOG_FH} "Invocation: $0 " . join(" ", @quoted_original_argv_log) . "\n";
        print {$LOG_FH} "----------------------------------------------------------------------\n";
        print {$LOG_FH} "Key Global Settings for System: " . ($system_identifier // 'N/A') . "\n";
        print {$LOG_FH} "  - Profiles Config File       : $profiles_config_path_to_load\n";
        print {$LOG_FH} "  - VM Config File             : " . ($vm_config_file_path // "Not Provided/Default Attempted") . "\n";
        print {$LOG_FH} "  - Common Flags               : -q, -k, rounding, smt, runq-avg-method\n";
        print {$LOG_FH} "  - Dynamic Flags              : Date filters, RunQ percentiles, and Decay models (dynamic).\n";
        print {$LOG_FH} "  - RunQ Avg Method            : $nfit_runq_avg_method_str\n";
        print {$LOG_FH} "  - Default SMT for Profile    : $default_smt_arg\n";
        print {$LOG_FH} "======================================================================\n\n";
    }

    # --- Adaptive Threshold Initialisation for this system ---
    # This block is self-contained and does not mutate global variables.
    # It detects the interval and gets the appropriate thresholds for this system's run.
    my $data_cache_for_interval_detection = File::Spec->catfile($current_cache_path, $DATA_CACHE_FILE);
    my $detected_interval_secs = detect_sampling_interval($data_cache_for_interval_detection);

    # The set_adaptive_thresholds function returns the new values, which are stored
    # in local variables. This avoids modifying the global defaults.
    my ($adaptive_runq_saturation_thresh, $adaptive_target_norm_runq, $adaptive_max_efficiency_reduction) = set_adaptive_thresholds($detected_interval_secs, $log_fh_for_system);

    # --- Seasonality Engine: Main Controller ---
    # This block determines if a seasonal model should be applied and orchestrates the analysis.
    # This block handles all three modes:
    # 1. Updating a snapshot.
    # 2. Applying a recency_decay forecast.
    # 3. Applying a multiplicative_seasonal forecast.

    if ($update_history_flag) {
        # --- Mode 1: Update the Unified Monthly History Cache ---
        # This is a special run mode. Its only purpose is to analyse completed
        # months in the data cache and save the results to the new history file.
        # It does not produce a CSV output.
        # Call the new subroutine to handle the history generation.
        update_monthly_history($current_cache_path, $system_identifier, $seasonality_config, $min_days_for_history);

        # After updating the history, we skip the rest of the processing for this system.
        print STDERR "--- History update complete for system: $system_identifier ---\n";
        next; # Proceed to the next system in the loop.
    } elsif ($apply_seasonality_event) {
        # --- Apply a Seasonal Forecast ---
        my $is_seasonal_run = 1;

        print STDERR "\n--- Applying Seasonality Model for Event: '$apply_seasonality_event' ---\n";
        # Get the configuration for the event the user requested.
        my $requested_event_config = $seasonality_config->{$apply_seasonality_event} // {};

        # check for unsupported model types
        # Prevent this script from incorrectly running models handled by other tools.
        my $requested_model_type = $requested_event_config->{model} // '';
        if ($requested_model_type eq 'adaptive_peak_forecasting') {
            die "ERROR: The 'adaptive_peak_forecasting' model must be run using the 'nfit-forecast.py' script.\n" .
                "       Please use the command: ./nfit-forecast.py --nmondir $current_cache_path --apply-seasonality $apply_seasonality_event\n";
        }

        # Determine the correct analysis path. This function checks for sufficient
        # history for multiplicative models and returns the effective event name to run.
        my $effective_event_name = determine_seasonal_analysis_path(
            $requested_event_config,
            $current_cache_path,
            $apply_seasonality_event
        );

        # Get the configuration for the event we are actually going to run.
        my $effective_config = $seasonality_config->{$effective_event_name} // {};
        my $effective_model_type = $effective_config->{model} // '';

        # Now, set up the analysis based on the EFFECTIVE model type.
        if ($effective_model_type eq 'recency_decay') {
            # This model uses --enable-windowed-decay, making it mutually exclusive
            # with the state-based decay model.
            if ($nfit_decay_over_states) {
                die "FATAL: The 'recency_decay' model (for event '$effective_event_name') cannot be used with the --decay-over-states flag.\n";
            }

            my ($event_start_obj, $event_end_obj) = determine_event_period($effective_config);

            if ($event_start_obj && $event_end_obj) {
                # DO NOT set a start date. This ensures nfit analyses the entire cache.
                # The model's behavior is controlled by the analysis reference date only.
                # $start_date_str = $event_start_obj->strftime('%Y-%m-%d');
                $nfit_analysis_reference_date_str = $event_end_obj->strftime('%Y-%m-%d');
                print STDERR "Recency-Decay Model: Anchoring analysis to reference date: $nfit_analysis_reference_date_str\n";
                # Programmatically enable the correct decay model for the main profile loop.
                $nfit_decay_over_states = 1;
            } else {
                 warn "Warning: Could not determine a valid period for the '$effective_event_name' model. Analysis will proceed without date anchoring.\n";
            }

        } elsif ($effective_model_type eq 'multiplicative_seasonal') {
            # The check passed. Set the flag to run the multiplicative forecast AFTER
            # the main profile loop has gathered the Peak value.
            $is_multiplicative_forecast_run = 1;
        } elsif ($effective_model_type eq 'predictive_peak') {
            $is_predictive_peak_model_run = 1;
            # This is the new model. It's a self-contained forecast that runs
            # instead of the standard profile loop.
            my $forecast_results = calculate_predictive_peak_forecast(
                $current_cache_path,
                $system_identifier,
                $effective_event_name,
                $effective_config,
                $seasonality_config,
                $adaptive_runq_saturation_thresh
            );
            # Overwrite the main results table with the forecast
            %results_table = %$forecast_results;

            # Populate the global vm_order array so the reporter knows which VMs to process.
            @vm_order = sort keys %results_table;

            # Generate and store the necessary hint/pressure data for each VM for the CSV.
            foreach my $vm_name (@vm_order) {
                my $cfg_for_hint = $vm_config_data{$vm_name};
                my $smt_for_hint = (defined $cfg_for_hint && defined $cfg_for_hint->{smt}) ? $cfg_for_hint->{smt} : $default_smt_arg;
                my $maxcpu_for_hint = (defined $cfg_for_hint && defined $cfg_for_hint->{maxcpu}) ? $cfg_for_hint->{maxcpu} : 0;

                # We don't have raw nfit state data, so we call the hint generator with what we have.
                # The pressure part will be less accurate, but Hint/Pattern will work.
                my ($hint_type_tier, $hint_pattern_shape, $hint_pressure_bool, $pressure_detail_str) =
                generate_sizing_hint(
                    'log_fh_ref'  => undef, # No log rationale needed here
                    'results_ref' => \%results_table,
                    'vm'          => $vm_name,
                    'config_ref'  => $cfg_for_hint,
                    'max_cpu_for_vm_numeric' => $maxcpu_for_hint,
                    'smt_used_for_vm_numeric' => $smt_for_hint,
                    'per_profile_runq_metrics_ref' => {}, # No RunQ metrics available in this path
                    'adaptive_saturation_thresh' => $adaptive_runq_saturation_thresh
                );

                $hint_tier_for_csv{$vm_name}      = $hint_type_tier;
                $hint_pattern_for_csv{$vm_name}   = $hint_pattern_shape;
                $hint_pressure_for_csv{$vm_name}  = $hint_pressure_bool;
                $pressure_details_for_csv{$vm_name} = $pressure_detail_str // 'N/A';
                # The final peak is the higher of the baseline and prediction for the P-99W1 profile.
                my $baseline_peak = $seasonal_debug_info{$vm_name}{'P-99W1'}{'TrueBaseline'} // 0;
                my $predicted_peak = $seasonal_debug_info{$vm_name}{'P-99W1'}{'PredictedPeak'} // 0;
                $results_table{$vm_name}{$PEAK_PROFILE_NAME} = ($baseline_peak > $predicted_peak) ? $baseline_peak : $predicted_peak;

            }
        }
    }

    # --- Capture the start time for this specific system's analysis ---
    my $system_analysis_start_time = time();

    # The standard analysis loop runs nfit for each profile. This is the default
    # path for all models EXCEPT `predictive_peak`, which performs its own
    # analysis and populates the results table directly.
    my $skip_standard_profile_loop = 0;
    if ($is_seasonal_run) {
        my $model_type = $seasonality_config->{$apply_seasonality_event}{model} // '';
        if ($model_type eq 'predictive_peak') {
            $skip_standard_profile_loop = 1;
        }
    }

    # This inner loop iterates through the profiles from the config file.
    foreach my $profile (@profiles)
    {
        # --- DYNAMIC AbsRunQ Collation for P-99W1 (Merge & Replace Strategy) ---
        if ($profile->{name} eq $MANDATORY_PEAK_PROFILE_FOR_HINT
            && $runq_perc_behavior_mode eq 'match') {

            my %need;

            # Step 1: Pre-scan other profiles (skip P-99W1) for PhysC -p/--percentile
            foreach my $p_inner (@profiles) {
                next if $p_inner->{name} eq $MANDATORY_PEAK_PROFILE_FOR_HINT;
                if ($p_inner->{flags} =~ /(?:^|\s)(?:-p|--percentile)\s+([0-9]+(?:\.[0-9]+)?)/) {
                    my $p = int($1 + 0.0001); # normalise to integer key
                    $need{$p} = 1 if $p;
                }
            }

            # Step 2: Merge any existing AbsRunQ percs from P-99W1 (handles quoted or unquoted)
            if ($profile->{flags} =~ /--runq-abs-perc\s+["']?([0-9.,]+)["']?/i) {
                for my $tok (split /,/, $1) {
                    my $p = int($tok + 0.0001);
                    $need{$p} = 1 if $p;
                }
            }

            # Step 3: Add a sensible safety superset
            $need{90} = 1; # legacy default
            $need{95} = 1; # frequently matched
            $need{99} = 1; # frequently matched

            # Step 4: Replace any existing --runq-abs-perc (quoted or not), then append argv-safe
            $profile->{flags} =~ s/\s--runq-abs-perc\s+["']?[0-9.,]+["']?//ig;

            my @final = sort { $a <=> $b } keys %need;
            $profile->{flags} .= " --runq-abs-perc " . join(",", @final);
        }

        last if $skip_standard_profile_loop;
        my $profile_name = $profile->{name};
        my $profile_runq_behavior = $profile->{runq_behavior} // 'default'; # Default RunQ modifier behavior if not in config

        # --- Seasonal Model Flag Override ---
        # Create a local copy of the profile flags that we can safely modify.
        my $profile_specific_flags_for_nfit_run = $profile->{flags};

        # If a seasonal decay model is active, we must ensure it does not conflict
        # with standard decay flags that may be present in the profile.
        if ($is_multiplicative_forecast_run) {
            # For a multiplicative run, this main profile loop's only purpose is to
            # gather the absolute Peak value. The other profile results will be
            # recalculated later for the baseline. To prevent warnings from nfit,
            # we disable growth prediction for this initial data-gathering run.
            if ($profile_specific_flags_for_nfit_run =~ /--enable-growth-prediction/) {
                print STDERR "  - INFO: Disabling growth prediction for initial Peak gathering run.\n" if ($verbose);
                $profile_specific_flags_for_nfit_run =~ s/--enable-growth-prediction\s*//g;
            }
        }
        elsif ($nfit_decay_over_states) {
            # The --decay-over-states model is incompatible with the standard --decay
            # and --runq-decay flags, which control intra-window EMA smoothing.
            # To prevent nfit from receiving conflicting instructions, we strip them here.
            # The --enable-growth-prediction flag is compatible and is intentionally left in place.
            if ($profile_specific_flags_for_nfit_run =~ /--decay\s/ || $profile_specific_flags_for_nfit_run =~ /--runq-decay\s/) {
                print STDERR "  - INFO: Overriding profile's --decay/--runq-decay flags for seasonal recency_decay model.\n" if ($verbose);
                # Use a more robust regex to handle decay levels with hyphens (e.g., 'very-high').
                $profile_specific_flags_for_nfit_run =~ s/--decay\s+[\w-]+\s*//g;
                $profile_specific_flags_for_nfit_run =~ s/--runq-decay\s+[\w-]+\s*//g;
            }
        }

        # Determine Profile's PhysC Percentile (e.g., value X from -p X in profile's flags)
        # This is used for:
        # 1. Extracting the correct PhysC metric from nfit's output (e.g., P98=value).
        # 2. Optionally matching the AbsRunQ percentile if --match-runq-perc-to-profile is set.
        my $profile_physc_perc_val_num;
        if ($profile->{flags} =~ /(?:-p|--percentile)\s+([0-9.]+)/)
        {
            $profile_physc_perc_val_num = $1 + 0; # Store as number
        }

        # --- Construct nfit command with dynamically determined RunQ percentile flags for THIS profile run ---

        my $nfit_runq_related_flags_for_command = ""; # This will hold the final --runq-...-perc flags for this nfit call
        my $has_runq_source = defined($runq_data_file_arg) || defined($nmon_dir);

        # Check if any RunQ data source is available
        if ($has_runq_source)
        {
            # --- Handle Absolute RunQ Percentiles ---
            my @abs_percs_to_collate;
            if ($profile_specific_flags_for_nfit_run =~ /--runq-abs-perc\s+([^\s\'\"]+)/)
            {
                push @abs_percs_to_collate, split(/,/, $1);
            }
            push @abs_percs_to_collate, split(/,/, $runq_abs_perc_list_str);
            push @abs_percs_to_collate, 90;
            if ($runq_perc_behavior_mode eq 'match' && defined $profile_physc_perc_val_num)
            {
                push @abs_percs_to_collate, $profile_physc_perc_val_num;
            }
            $profile_specific_flags_for_nfit_run =~ s/--runq-abs-perc\s+[^\s\'\"]+//g;
            my %seen_abs;
            my @unique_abs_percs = sort {$a <=> $b} grep { looks_like_number($_) && !$seen_abs{$_}++ } @abs_percs_to_collate;
            if (@unique_abs_percs)
            {
                $nfit_runq_related_flags_for_command .= " --runq-abs-perc '" . join(",", @unique_abs_percs) . "'";
            }

            # --- Handle Normalized RunQ Percentiles ---
            my @norm_percs_to_collate;
            if ($profile_specific_flags_for_nfit_run =~ /--runq-norm-perc\s+([^\s\'\"]+)/)
            {
                push @norm_percs_to_collate, split(/,/, $1);
            }
            push @norm_percs_to_collate, split(/,/, $runq_norm_perc_list_str);
            push @norm_percs_to_collate, (25, 50, 75, 90);
            $profile_specific_flags_for_nfit_run =~ s/--runq-norm-perc\s+[^\s\'\"]+//g;
            my %seen_norm;
            my @unique_norm_percs = sort {$a <=> $b} grep { looks_like_number($_) && !$seen_norm{$_}++ } @norm_percs_to_collate;
            if (@unique_norm_percs)
            {
                $nfit_runq_related_flags_for_command .= " --runq-norm-perc '" . join(",", @unique_norm_percs) . "'";
            }
        }

        # Common nfit windowing/decay flags (if nfit-profile is instructing nfit to use its internal decay)
        my $nfit_windowing_decay_flags = "";
        if ($nfit_enable_windowed_decay)
        {
            $nfit_windowing_decay_flags .= " --enable-windowed-decay";
            $nfit_windowing_decay_flags .= " --process-window-unit \"$nfit_window_unit_str\"";
            $nfit_windowing_decay_flags .= " --process-window-size $nfit_window_size_val";
            $nfit_windowing_decay_flags .= " --decay-half-life-days $nfit_decay_half_life_days_val";
            $nfit_windowing_decay_flags .= " --analysis-reference-date \"$nfit_analysis_reference_date_str\"" if (defined $nfit_analysis_reference_date_str);
        }
        elsif ($nfit_decay_over_states)
        {
            # Pass through the flag for the new Hybrid State-Time Decay Model.
            $nfit_windowing_decay_flags .= " --decay-over-states";
            # Also pass through the recency parameters it requires.
            $nfit_windowing_decay_flags .= " --decay-half-life-days $nfit_decay_half_life_days_val";
            $nfit_windowing_decay_flags .= " --analysis-reference-date \"$nfit_analysis_reference_date_str\"" if (defined $nfit_analysis_reference_date_str);
        }

        # Display the formatted flags for the upcoming run.
        if ($is_multiplicative_forecast_run) {
            print STDERR "Running profile: $profile_name (for Peak gathering)\n";
        } else {
            print STDERR "Running profile: $profile_name\n";
        }
        my $formatted_run_info = format_nfit_flags_for_display($profile_name, $profile_specific_flags_for_nfit_run, $nfit_runq_related_flags_for_command, $profile_runq_behavior);
        print STDERR $formatted_run_info . "\n" if ($formatted_run_info && $verbose);

        # --- Assemble and Execute the nfit Command ---
        my $target_cache_for_nfit_run;
        if (defined $system_serial) {
            # This is a multi-system run; point to the specific serial's cache.
            $target_cache_for_nfit_run = File::Spec->catfile($base_cache_dir, $system_serial);
        } else {
            # This is a single run on a cache directory directly specified by the user.
            $target_cache_for_nfit_run = $nmon_dir;
        }

        # The base flags for nfit are now much simpler.
        # It always gets --nmondir. All other flags are for analysis control.
        my $base_flags = "-q -k --nmondir \"$target_cache_for_nfit_run\" $rounding_flags_for_nfit";
        if (defined $start_date_str) { $base_flags .= " -s $start_date_str"; }
        if (defined $nfit_runq_avg_method_str) { $base_flags .= " --runq-avg-method \"$nfit_runq_avg_method_str\""; }

        # The VM filter is still passed through if specified.
        my $vm_filter_arg = defined($target_vm_name) ? " -vm \"$target_vm_name\"" : "";
        my $smt_flag = "--smt $default_smt_arg";

        # Assemble the final, complete command.
        my $command = "$nfit_script_path $base_flags $nfit_windowing_decay_flags $vm_filter_arg $smt_flag --profile-label '$profile_name' $profile_specific_flags_for_nfit_run $nfit_runq_related_flags_for_command";

        # Execute and process the command
        my $raw_nfit_output = '';
        # Use open3 to capture STDOUT, but let STDERR pass through to the terminal
        # This avoids ambiguity with undef being a "false" value.
        my $stderr_arg = ">&=" . fileno(STDERR);
        my $pid = open3(undef, my $stdout_nfit, $stderr_arg, "$command --show-progress");
        while(my $line = <$stdout_nfit>) {
             $raw_nfit_output .= $line;
        }
        waitpid($pid, 0);

        my $exit_status = $? >> 8;

        if ($exit_status != 0)
        {
            warn "Warning: '$nfit_script_path' cmd failed for profile '$profile_name' (exit $exit_status). Command was: $command. Output: $raw_nfit_output. Skip.\n";
            if ($LOG_FH)
            {
                print {$LOG_FH} "ERROR: nfit execution FAILED for profile '$profile_name' on VM(s) '". ($target_vm_name // "All") ."'.\n";
                print {$LOG_FH} "  Command: $command\n";
                print {$LOG_FH} "  Exit Status: $exit_status\n";
                my $short_output = substr($raw_nfit_output, 0, 1000) . (length($raw_nfit_output) > 1000 ? "..." : ""); # Log first 1KB of output
                print {$LOG_FH} "  Output (first 1000 chars): $short_output\n";
            }
            next; # Skip to the next profile in the loop
        }

        # --- ASSIMILATION LOGIC ---
        my $parsed_nfit_results = parse_nfit_json_output($raw_nfit_output);

        foreach my $vm_name (sort keys %{$parsed_nfit_results})
        {
            unless ($vm_seen{$vm_name}++) {
                push @vm_order, $vm_name;
            }

            my @states_for_vm = @{$parsed_nfit_results->{$vm_name}};
            $per_profile_nfit_raw_results{$vm_name}{$profile_name} = \@states_for_vm; # Store for logging

            next unless @states_for_vm; # Skip if no data for this VM

            my $p_metric_key = "P" . clean_perc_label($profile_physc_perc_val_num // $DEFAULT_PERCENTILE);
            my @valid_p_values;
            foreach my $state (@states_for_vm) {
                my $metric_val = $state->{metrics}{physc}{$p_metric_key};
                my $growth_adj = $state->{metrics}{growth}{adjustment};

                # Store the growth adjustment for this specific profile run
                if (defined $growth_adj && looks_like_number($growth_adj)) {
                    $nfit_growth_adjustments{$vm_name}{$profile_name} = $growth_adj;
                }

                # The base value for the RunQ modifier logic must be the value *before* growth.
                if (defined $metric_val && looks_like_number($metric_val)) {
                    my $pre_growth_val = $metric_val - ($growth_adj // 0);
                    push @valid_p_values, $pre_growth_val;
                }
                elsif (defined $metric_val && looks_like_number($metric_val)) {
                    push @valid_p_values, $metric_val;
                }
            }

            # This is the PRE-GROWTH base value
            my $base_physc_for_profile = "N/A";
            if (@valid_p_values) {
                # Simple, unweighted average as per design for multi-state results
                $base_physc_for_profile = sum0(@valid_p_values) / scalar(@valid_p_values);
            }

            $results_table{$vm_name}{$profile_name} = $base_physc_for_profile;

            # --- Data Capture for Sizing Hints and Modifiers ---
            # For hints, we always use the MOST RECENT state from the P-99W1 profile run.
            # ==============================================================================
            # This version correctly captures the Peak value from both standard and
            # aggregated decay model outputs.
            # ==============================================================================

            # The last (or only) state in the time-sorted list
            my $most_recent_state = $states_for_vm[-1];

            # For hints, we always use the MOST RECENT state from the P-99W1 profile run
            if ($profile_name eq $MANDATORY_PEAK_PROFILE_FOR_HINT) {
                my $metrics = $most_recent_state->{metrics};
                my $runq_norm = $metrics->{runq}{normalized} || {};
                my $runq_abs = $metrics->{runq}{absolute} || {};

                # Store all parsed metrics from the most recent state for later use
                $per_profile_runq_metrics{$vm_name}{$profile_name} = $most_recent_state;

                # Capture RunQ metrics from the most recent state for global hints
                my %first_capture_target_keys = ( 'NormRunQ_P50' => 1, 'NormRunQ_P90' => 1, 'AbsRunQ_P90'  => 1 );
                foreach my $key (keys %first_capture_target_keys) {
                    if (!$primary_runq_metrics_captured_for_vm{$vm_name}{$key}) {
                        my $value;
                        if ($key =~ /NormRunQ_P(\d+)/) { $value = $runq_norm->{"P$1"}; }
                        elsif ($key =~ /AbsRunQ_P(\d+)/) { $value = $runq_abs->{"P$1"}; }
                        $results_table{$vm_name}{$key} = defined($value) ? $value : "N/A";
                        $primary_runq_metrics_captured_for_vm{$vm_name}{$key} = 1;
                        $source_profile_for_global_runq{$vm_name} //= $profile_name;
                    }
                }
            }
            else {
                # For other profiles, we still need to store their RunQ metrics for their own modifications
                $per_profile_runq_metrics{$vm_name}{$profile_name} = $most_recent_state;
            }

            # --- Peak Value Capture ---
            # CRITICAL: Only capture the Peak value from the mandatory, unfiltered P-99W1
            # profile run. This prevents it from being overwritten by subsequent,
            # time-filtered profiles (e.g., -online, -batch), or influence by a user defined profile.
            if ($profile_name eq $MANDATORY_PEAK_PROFILE_FOR_HINT) {
                my $peak_val = $most_recent_state->{metrics}{physc}{'Peak'};
                $results_table{$vm_name}{$PEAK_PROFILE_NAME} = $peak_val if defined $peak_val;
            }
        }

    } # End foreach profile

    # --- Apply Multiplicative Seasonal Forecast (if applicable) ---
    # This block runs AFTER the standard analysis has populated the results_table
    # with baseline values and, crucially, the 'Peak' metric.
    my $historic_data_for_csv_href; # To hold data for verbose reporting
    if ($is_multiplicative_forecast_run) {
        my $event_config = $seasonality_config->{$apply_seasonality_event} // {};

        # Call the refactored forecast function, passing the baseline results
        # and capturing the historical data for verbose reporting.
        my ($forecast_results_href, $historic_data_href) = calculate_multiplicative_forecast(
            $current_cache_path,
            $system_identifier,
            $apply_seasonality_event,
            $event_config,
            $seasonality_config,
            \%results_table # Pass the pre-populated results table as the baseline
        );
        $historic_data_for_csv_href = $historic_data_href;

        # Overwrite the profile values in the main results table with the forecasted values
        foreach my $vm_name (keys %$forecast_results_href) {
            foreach my $p_name (keys %{$forecast_results_href->{$vm_name}}) {
                # The forecast result hash may contain other keys; only overwrite profile values.
                next if ($p_name =~ /^_/); # Skip internal keys like _report_data
                $results_table{$vm_name}{$p_name} = $forecast_results_href->{$vm_name}{$p_name};
            }
        }
        print STDERR "--- Multiplicative forecast applied successfully ---\n";
    }

    # --- Rationale Logging and Hint Generation ---
    # This block now uses a conditional to call the correct logging subroutine
    # based on the analysis model that was run. It also now captures all hint
    # components for later use in the CSV report, improving efficiency
    if ($is_multiplicative_forecast_run) {
        # Path for the multiplicative seasonal model, which needs a unique log format.
        # First, generate and store the hint data needed for the CSV report.
        foreach my $vm_name (@vm_order) {
            my $cfg_for_hint = $vm_config_data{$vm_name};
            my $smt_for_hint = $default_smt_arg;
            my $maxcpu_for_hint = 0;
            my $ent_for_hint = 0;
            my $p99w1_results_aref = $per_profile_nfit_raw_results{$vm_name}{$MANDATORY_PEAK_PROFILE_FOR_HINT};

            if (ref($p99w1_results_aref) eq 'ARRAY' && @$p99w1_results_aref) {
                my $last_state_data = $p99w1_results_aref->[-1];
                my $config = $last_state_data->{metadata}{configuration} || {};

				if (defined $config->{maxCpu} && looks_like_number($config->{maxCpu}) && $config->{maxCpu} > 0) {
                    $maxcpu_for_hint = $config->{maxCpu};
                }
				if (defined $config->{smt} && looks_like_number($config->{smt}) && $config->{smt} > 0) {
                    $smt_for_hint = $config->{smt};
                }
				if (defined $config->{entitlement} && looks_like_number($config->{entitlement})) {
                    $ent_for_hint = $config->{entitlement};
                }
            }

            my ($hint_type_tier, $hint_pattern_shape, $hint_pressure_bool, $pressure_detail_str, undef, undef, undef) =
                generate_sizing_hint(
                    'log_fh_ref' => undef, # We don't need the rationale text here, just the CSV values.
                    'results_ref' => \%results_table,
                    'vm' => $vm_name,
                    'config_ref' => $cfg_for_hint,
                    'max_cpu_for_vm_numeric' => $maxcpu_for_hint,
                    'smt_used_for_vm_numeric' => $smt_for_hint,
                    'per_profile_runq_metrics_ref' => \%per_profile_runq_metrics,
                    'adaptive_saturation_thresh' => $adaptive_runq_saturation_thresh # Pass the adaptive value

                );
            $hint_tier_for_csv{$vm_name} = $hint_type_tier;
            $hint_pattern_for_csv{$vm_name} = $hint_pattern_shape;
            $hint_pressure_for_csv{$vm_name} = $hint_pressure_bool;
            $pressure_details_for_csv{$vm_name} = $pressure_detail_str // 'N/A';
        }
        log_multiplicative_seasonal_rationale($open_log_files{$system_identifier});
    } elsif ($is_predictive_peak_model_run) {
        # Path for the new predictive peak model
        log_predictive_peak_rationale($open_log_files{$system_identifier});
    } else {
        foreach my $vm_name (@vm_order) {

			my $total_states_for_vm = scalar(@{$per_profile_nfit_raw_results{$vm_name}{$MANDATORY_PEAK_PROFILE_FOR_HINT} || []});

            # *** Print a clear, top-level header for each VM ***
            if ($log_fh_for_system) {
                print {$log_fh_for_system} "\n######################################################################\n";
                print {$log_fh_for_system} "# Rationale for VM: $vm_name\n";
                print {$log_fh_for_system} "######################################################################\n\n";
            }

            # --- Generate Sizing Hint Rationale (once per VM) ---
            my $cfg_for_hint = $vm_config_data{$vm_name};

            # This logic is identical to the one in _write_standard_csv_report to get consistent SMT/MaxCPU
			my $smt_for_hint = $default_smt_arg;
			my $maxcpu_for_hint = 0;
			my $ent_for_hint = 0; # Initialize entitlement
			my $p99w1_results_aref = $per_profile_nfit_raw_results{$vm_name}{$MANDATORY_PEAK_PROFILE_FOR_HINT};

			if (ref($p99w1_results_aref) eq 'ARRAY' && @$p99w1_results_aref) {
				my $last_state_data = $p99w1_results_aref->[-1];
                my $config = $last_state_data->{metadata}{configuration} || {};

				if (defined $config->{maxCpu} && looks_like_number($config->{maxCpu}) && $config->{maxCpu} > 0) {
                    $maxcpu_for_hint = $config->{maxCpu};
                }
				if (defined $config->{smt} && looks_like_number($config->{smt}) && $config->{smt} > 0) {
                    $smt_for_hint = $config->{smt};
                }
				if (defined $config->{entitlement} && looks_like_number($config->{entitlement})) {
                    $ent_for_hint = $config->{entitlement};
                }
			}

			if ($maxcpu_for_hint == 0 && defined $cfg_for_hint && defined $cfg_for_hint->{maxcpu} && $cfg_for_hint->{maxcpu} > 0) { $maxcpu_for_hint = $cfg_for_hint->{maxcpu}; }
			if ($smt_for_hint == $default_smt_arg && defined $cfg_for_hint && defined $cfg_for_hint->{smt}) { $smt_for_hint = $cfg_for_hint->{smt}; }
			if ($ent_for_hint == 0 && defined $cfg_for_hint && defined $cfg_for_hint->{entitlement}) { $ent_for_hint = $cfg_for_hint->{entitlement}; }

			my ($hint_type_tier, $hint_pattern_shape, $hint_pressure_bool, $pressure_detail_str, $pressure_rationale_text, $p99w1_has_abs_pressure, $p99w1_has_norm_pressure) =
                generate_sizing_hint(
                    'log_fh_ref' => $log_fh_for_system,
                    'results_ref' => \%results_table,
                    'vm' => $vm_name,
                    'config_ref' => $cfg_for_hint,
                    'max_cpu_for_vm_numeric' => $maxcpu_for_hint,
                    'smt_used_for_vm_numeric' => $smt_for_hint,
                    'per_profile_runq_metrics_ref' => \%per_profile_runq_metrics,
                    'adaptive_saturation_thresh' => $adaptive_runq_saturation_thresh
                );

            # Store all hint components in global hashes for the final report.
			$hint_tier_for_csv{$vm_name} = $hint_type_tier;
            $hint_pattern_for_csv{$vm_name} = $hint_pattern_shape;
            $hint_pressure_for_csv{$vm_name} = $hint_pressure_bool;
			$pressure_details_for_csv{$vm_name} = $pressure_detail_str // 'N/A';

            # This now prints the "Section G" block under the new VM header
            print {$log_fh_for_system} $pressure_rationale_text . "\n" if ($log_fh_for_system && $pressure_rationale_text);

            # --- Apply Modifiers and Log Rationale (per profile) ---
            foreach my $profile (@profiles) {
                my $profile_name = $profile->{name};
                my $base_physc = $results_table{$vm_name}{$profile_name} // "N/A";

                my ($adjusted_physc, $debug_info_ref, $final_val_for_csv);

                # Determine if this is a forecasting model. Modifiers are not applied to forecasts.
                my $is_forecasting_model = $is_multiplicative_forecast_run || $is_predictive_peak_model_run;

                if ($is_forecasting_model) {
                    # For forecasting models, the value from the forecast engine is the final value.
                    $adjusted_physc = $base_physc;
                    $debug_info_ref = { ReasonForNoModification => "RunQ modifiers are not applied to forecasting models." };
                    $runq_modifier_values{$vm_name}{$profile_name} = 0;
                    $runq_uncapped_values{$vm_name}{$profile_name} = 0;
                } else {
                    # For all standard and trending models, apply the full modifier logic.
                    my $profile_runq_behavior = $profile->{runq_behavior} // 'default';
                    my $abs_runq_key_to_use = ($runq_perc_behavior_mode eq 'match' && $profile->{flags} =~ /-p\s+([0-9.]+)/)
                    ? "AbsRunQ_P" . clean_perc_label($1)
                    : "AbsRunQ_P90";
                    my $abs_runq_perc_label = ($abs_runq_key_to_use =~ /P(.*)/) ? "P$1" : undef;

                    my $all_runq_metrics_for_vm = $per_profile_runq_metrics{$vm_name} || {};
                    my $runq_source_profile_name = $source_profile_for_global_runq{$vm_name} // $MANDATORY_PEAK_PROFILE_FOR_HINT;

                    my $runq_source_metrics = $all_runq_metrics_for_vm->{$runq_source_profile_name}{metrics}{runq} || {};
                    my $norm_metrics = $runq_source_metrics->{normalized} || {};
                    my $abs_metrics = $runq_source_metrics->{absolute} || {};

                    ($adjusted_physc, $debug_info_ref) = calculate_runq_modified_physc(
                        $base_physc,
                        $norm_metrics->{'P25'} // "N/A",
                        $norm_metrics->{'P50'} // "N/A",
                        $norm_metrics->{'P75'} // "N/A",
                        $norm_metrics->{'P90'} // "N/A",
                        $abs_metrics->{$abs_runq_perc_label} // "N/A",
                        $abs_runq_key_to_use,
                        $smt_for_hint,
                        $ent_for_hint,
                        $maxcpu_for_hint,
                        (defined $cfg_for_hint->{pool_id} && $cfg_for_hint->{pool_id} != 0),
                        $profile_runq_behavior,
                        $p99w1_has_abs_pressure,
                        $p99w1_has_norm_pressure,
                        # Pass the adaptive thresholds
                        $adaptive_runq_saturation_thresh,
                        $adaptive_target_norm_runq,
                        $adaptive_max_efficiency_reduction
                    );

                    # Calculate and store the final net modifier (additive or reductive) for this specific profile.
                    # The RunQ modifier is the change applied to the PRE-GROWTH base.
                    my $base_physc_num = looks_like_number($base_physc) ? $base_physc + 0 : undef;
                    my $adjusted_physc_num = looks_like_number($adjusted_physc) ? $adjusted_physc + 0 : undef;
                    $runq_modifier_values{$vm_name}{$profile_name} = (defined $base_physc_num && defined $adjusted_physc_num) ? ($adjusted_physc_num - $base_physc_num) : 0;

                    # Retrieve the nfit growth adjustment calculated earlier.
                    my $growth_adj_for_final_calc = $nfit_growth_adjustments{$vm_name}{$profile_name} // 0;

                    # The final value is the RunQ-modified value PLUS the nfit growth adjustment.
                    # Safely add the growth adjustment, treating non-numeric values as zero.
                    my $adjusted_physc_num_for_final = looks_like_number($adjusted_physc) ? $adjusted_physc : 0;
                    my $growth_adj_num_for_final = looks_like_number($growth_adj_for_final_calc) ? $growth_adj_for_final_calc : 0;
                    my $final_calculated_value = $adjusted_physc_num_for_final + $growth_adj_num_for_final;

                    $adjusted_physc = $final_calculated_value; # This is now the final value

                    # Store the uncapped potential reduction directly from the debug hash, which now handles the STD logic.
                    $runq_uncapped_values{$vm_name}{$profile_name} = $debug_info_ref->{'RunQ_Uncapped'} // 0;
                }

                my $nfit_output_dp = get_nfit_output_dp_from_flags($profile->{flags} . " " . $rounding_flags_for_nfit);
                $final_val_for_csv = looks_like_number($adjusted_physc) ? sprintf("%.${nfit_output_dp}f", $adjusted_physc) : "N/A";

                # CRITICAL: Overwrite the results_table with the FINAL fully calculated value for hints/CSV.
                $results_table{$vm_name}{$profile_name} = $final_val_for_csv;

                # Now, call the logger with all the necessary data
                log_profile_rationale(
                    $log_fh_for_system,
                    $vm_name,
                    $profile,
                    $base_physc,
                    $per_profile_nfit_raw_results{$vm_name}{$profile_name},
					$total_states_for_vm,
                    $source_profile_for_global_runq{$vm_name} // $MANDATORY_PEAK_PROFILE_FOR_HINT,
                    $debug_info_ref->{AbsRunQValueUsedForCalc} // 'N/A', # Pass the used value
                    $debug_info_ref->{NormRunQ_P50_Val} // 'N/A',
                    $debug_info_ref->{NormRunQ_P90_Val} // 'N/A',
                    $cfg_for_hint,
                    $smt_for_hint,
                    $maxcpu_for_hint,
                    $ent_for_hint,
                    $profile->{runq_behavior} // 'default',
                    $debug_info_ref,
                    $final_val_for_csv,
                    $nfit_growth_adjustments{$vm_name}{$profile_name} // 0, # Pass the growth adj
                    $nfit_growth_adjustments{$vm_name}{$profile_name} // 0, # Pass it again for Abs
                    $adaptive_runq_saturation_thresh # Pass the adaptive value
                );
            }
        }
    }

    # -- Reporting and Reset Block --
    # This block executes after all profiles have been run for the current system.

    my $is_recency_decay_run = 0;
    my $report_type_for_filename = 'state-based';
    if ($nfit_decay_over_states) {
        $report_type_for_filename = 'hybrid-state-decay';
    } elsif ($nfit_enable_windowed_decay) {
        $report_type_for_filename = 'windowed-decay';
    }

    if ($is_seasonal_run && !$is_multiplicative_forecast_run) {
        my $event_config = $seasonality_config->{$apply_seasonality_event} // {};
        $is_recency_decay_run = (($event_config->{model} // '') eq 'recency_decay');
    }

    my $ts = gmtime()->strftime('%Y%m%d-%H%M%S');
    if ($is_multiplicative_forecast_run) {
        # This is the new path for the multiplicative model, which now uses the standard reporter.
        if (@vm_order) {
            _write_standard_csv_report($apply_seasonality_event, $system_identifier, $ts, 1, 0); # Multiplicative=true, Recency=false
            # If verbose mode is on, generate the extra audit trail files.
            if ($verbose) {
                print "  - Verbose mode: Generating additional audit trail files...\n";
                # The baseline results are the final values *before* the forecast was applied.
                # The %results_table was overwritten, so we can't re-use it.
                # However, the baseline is available inside the seasonal_debug_info hash.
                my %baseline_data_for_verbose;
                foreach my $vm (keys %seasonal_debug_info) {
                    foreach my $prof (keys %{$seasonal_debug_info{$vm}}) {
                        $baseline_data_for_verbose{$vm}{$prof} = $seasonal_debug_info{$vm}{$prof}{baseline};
                    }
                }
                # Use $apply_seasonality_event as it's in scope here.
                write_seasonal_csv_output("current_baseline", $system_identifier, $apply_seasonality_event, $ts, \%baseline_data_for_verbose);
                write_seasonal_csv_output("historic_snapshot", $system_identifier, $apply_seasonality_event, $ts, $historic_data_for_csv_href);
            }
        }
    }
    elsif ($is_seasonal_run) {
        # This path is for all non-multiplicative seasonal models (recency_decay, predictive_peak).
        my $event_config = $seasonality_config->{$apply_seasonality_event} // {};
        my $model_type = $event_config->{model} // '';

        if ($model_type eq 'recency_decay' || $model_type eq 'predictive_peak') {
            if (@vm_order) {
                # For recency_decay, pass a true flag to add its specific columns.
                # For predictive_peak, pass a false flag to generate a standard report.
                my $is_recency_flag = ($model_type eq 'recency_decay') ? 1 : 0;
                my $is_predictive_flag = ($model_type eq 'predictive_peak') ? 1 : 0;
                _write_standard_csv_report($apply_seasonality_event, $system_identifier, $ts, 0, $is_recency_flag, $is_predictive_flag);
            } else {
                print STDERR "  - INFO: No VM output data was generated for seasonal event '$apply_seasonality_event'.\n";
            }
        }
    } else {
        # This is the standard, non-seasonal run path. It now generates one report
        # per system before proceeding to the next.
        if (@vm_order) {
            _write_standard_csv_report($report_type_for_filename, $system_identifier, $ts, 0, 0, 0); # Multiplicative=false, Recency=false
        }
    }

    # CRITICAL: Reset global data structures before processing the next system.
    @vm_order = ();
    %vm_seen = ();
    %results_table = ();
    %per_profile_runq_metrics = ();
    %primary_runq_metrics_captured_for_vm = ();
    %per_profile_nfit_raw_results = ();
    %seasonal_debug_info = ();
    # Reset date filters to avoid them leaking into a subsequent standard run.
    $start_date_str = undef;
    $nfit_analysis_reference_date_str = undef;

    # Report the duration for the completed system's analysis
    my $system_analysis_duration = time() - $system_analysis_start_time;
    print STDERR "--- System processing complete for: $system_identifier [DONE: " . format_duration($system_analysis_duration) . "] ---\n";


} # End foreach system

print STDERR "\nnFit profiling completed.\n";

# --- Collect unique serials that were part of the output ---
my %serials_in_output_map;
if (%vm_config_data && @vm_order) { # Ensure vm_config_data was loaded and there are VMs to process
    foreach my $vm_name_in_order (@vm_order) {
        if (exists $vm_config_data{$vm_name_in_order} &&
            defined $vm_config_data{$vm_name_in_order}{serial} &&
            $vm_config_data{$vm_name_in_order}{serial} ne '') {
            $serials_in_output_map{$vm_config_data{$vm_name_in_order}{serial}} = 1;
        }
    }
}
my @sorted_unique_serials_list = sort keys %serials_in_output_map;
my $excel_row_num_counter = 1; # Excel rows are 1-based; header is row 1, so first data row is 2.

# --- Final Report Generation ---
# This block now only handles the default, non-seasonal run.
# All seasonal models now handle their own output from within the main system loop.

# --- Script Footer ---
my $final_message = "\n";
if (@generated_files) {
    # Use List::Util::uniqstr if available, otherwise a simple hash works.
    my %seen;
    my @unique_files = grep { !$seen{$_}++ } @generated_files;

    $final_message .= "\nOutput:\n";
    foreach my $file (@unique_files) {
        $final_message .= "  - $file\n";
    }
} elsif ($update_history_flag) {
    $final_message .= " Unified monthly history update completed.";
} else {
    $final_message .= " No output files were generated.";
}
$final_message .= "Rationale Log:\n  - $log_file_path_for_run\n";
print STDERR "$final_message\n";

# --- Report final script duration summary to STDERR ---
my $PROFILE_SCRIPT_END_TIME_EPOCH = time();
my $PROFILE_SCRIPT_DURATION = $PROFILE_SCRIPT_END_TIME_EPOCH - $PROFILE_SCRIPT_START_TIME_EPOCH;
print STDERR "Total execution time: " . format_duration($PROFILE_SCRIPT_DURATION) . "\n";

# --- Finalise and close all rationale log files ---
my $final_end_time_str = localtime($PROFILE_SCRIPT_END_TIME_EPOCH)->strftime("%Y-%m-%d %H:%M:%S %Z");
foreach my $system_id (keys %open_log_files) {
    my $log_fh = $open_log_files{$system_id};
    if ($log_fh) {
        print {$log_fh} "\n----------------------------------------------------------------------\n";
        print {$log_fh} "Analysis for System '$system_id' completed at: " . $final_end_time_str . "\n";
        # Note: A per-system duration would require storing start times in a hash as well.
        # This is sufficient to close the log with a final timestamp.
        print {$log_fh} "======================================================================\n";
        close $log_fh;
    }
}

exit 0;

# ==============================================================================
# Subroutines
# ==============================================================================

# --- get_excel_col_name ---
# Converts a 1-based column index to an Excel column name (e.g., 1 -> A, 27 -> AA).
sub get_excel_col_name {
    my ($idx) = @_;
    my $name = '';
    die "Column index must be positive" if (!defined $idx || $idx <= 0);
    while ($idx > 0) {
        my $mod = ($idx - 1) % 26;
        $name = chr(65 + $mod) . $name;
        $idx = int(($idx - $mod - 1) / 26); # Corrected logic for 1-based index progression
    }
    return $name;
}

# --- generate_nfit_ent_formula ---
# Generates the dynamic Excel formula for the "NFIT_ENT_UserFormula" column.
sub generate_nfit_ent_formula {
    my ($excel_row_num, $num_profiles, $column_offset) = @_;

	$column_offset //= 0;

    # First profile column is M (13th column).
    # Peak (L) is the 12th column. Profiles start after Peak.
    my $first_profile_excel_col_letter = get_excel_col_name(12 + 1);
    my $last_profile_excel_col_letter = get_excel_col_name(12 + $num_profiles);

    # The fixed array string for the MATCH function, as provided by the user.
    my $tier_match_array_str_for_formula = '{"P","G1","G2","G3","G4","O1","O2","O3","O4","B1","B2","B3","B4"}';

    # Dynamic column index for 'NFIT - Ent'.
    # 13 fixed leading columns (A-M) + num_profiles columns + 1 (for "NFIT - Ent" itself).
    my $entitlement_column_index = 13 + $num_profiles + 1 + $column_offset;

    # Using "A:AZ" as the VLOOKUP range as requested for stability.
    my $vlookup_range_for_peer_ent = "A:AZ";

    my $formula_body = sprintf(
        'IF(ISNUMBER(SEARCH("PowerHA Standby", I%d)),VLOOKUP(VLOOKUP(A%d, PowerHA!A:B, 2, FALSE),%s, %d, FALSE) * $L$258, CEILING(INDEX(%s%d:%s%d, MATCH(B%d, %s, 0)), 0.05))',
        $excel_row_num,                            # For I%d (SystemType)
        $excel_row_num,                            # For A%d (VM Name for inner VLOOKUP)
        $vlookup_range_for_peer_ent,               # Range for outer VLOOKUP (e.g., A:AZ)
        $entitlement_column_index,                 # Dynamic column index for Current_ENT of the peer
        $first_profile_excel_col_letter, $excel_row_num, # For M%d (start of profile data range)
        $last_profile_excel_col_letter,  $excel_row_num, # For e.g. Y%d (end of profile data range)
        $excel_row_num,                            # For C%d (Hint column, containing the tier string like "G3")
        $tier_match_array_str_for_formula          # For {"P","G1",...} array
    );
    return "=" . $formula_body; # Excel formulas start with "="
}

# --- print_csv_footer ---
# Prints the summary footer section with labels and Excel formulas.
# Make sure Time::Piece is used if not already at the top of your script for strftime
# use Time::Piece; # Already in the full script you provided.
# use List::Util qw(sum min max); # Already in the full script.
# Ensure get_excel_col_name and quote_csv are defined elsewhere or within this sub's scope.
# It accepts an offset to correctly calculate column letters when extra
# columns (like SeasonalMultiplier) are present in the report.
sub print_csv_footer {
    my ($fh, $last_data_row, $nmon_physc_file, $num_profiles, $sorted_unique_serials_list_ref, $col_offset) = @_;
    $col_offset //= 0; # Default to 0 if not provided

    my @sorted_unique_serials = @$sorted_unique_serials_list_ref;
    my $count_of_unique_serials = scalar(@sorted_unique_serials);
    my $loop_count_for_serials = ($count_of_unique_serials == 0) ? 1 : $count_of_unique_serials;

    # --- Calculate dynamic column letters based on script's output structure ---
    my $col_serial_letter = get_excel_col_name(8);
    my $col_system_type_letter = get_excel_col_name(9);

    # Apply the offset to all subsequent column calculations
    my $idx_current_ent = 12 + $num_profiles + 1 + $col_offset;
    my $col_current_ent_letter = get_excel_col_name($idx_current_ent);
    my $idx_nfit_ent_user_formula = 12 + $num_profiles + 2 + $col_offset;
    my $col_nfit_ent_user_formula_letter = get_excel_col_name($idx_nfit_ent_user_formula);

    # --- Get NMON physc data file modification timestamp ---
    my $file_timestamp_str = "N/A";
    if (defined $nmon_physc_file && -f $nmon_physc_file) {
        my $mtime_epoch = (stat($nmon_physc_file))[9];
        if (defined $mtime_epoch) {
            $file_timestamp_str = localtime($mtime_epoch)->strftime("%Y-%m-%d %H:%M:%S");
        } else {
            $file_timestamp_str = "Timestamp N/A (stat fetch failed for $nmon_physc_file)";
        }
    } else {
        $file_timestamp_str = "Timestamp N/A (File not found or not provided)";
    }

    # --- Define starting row for footer elements ---
    print $fh "\n"; # Blank line after main data
    my $footer_start_row = $last_data_row + 2;

    my $row_data_age = $footer_start_row;
    my $row_timestamp = $footer_start_row + 1;
    my $row_as_is_nfit_labels = $footer_start_row + 3;
    my $row_ent_col_headers = $footer_start_row + 4;
    my $row_unique_serials_start = $footer_start_row + 5;

    my $empty = "";
    my @csv_row;

    # --- Row 1 of Footer: Data Age ---
    my $col_letter_data_age_sum_current_incl = get_excel_col_name(23 + $col_offset);
    my $col_letter_data_age_sum_nfit_incl = get_excel_col_name(24 + $col_offset);
    my $col_letter_data_age_delta_incl = get_excel_col_name(25 + $col_offset);
    my $col_letter_data_age_perc_incl = get_excel_col_name(26 + $col_offset);

    my $formula_sum_current_ent_incl_sby = sprintf("=SUM(%s2:%s%d)", $col_current_ent_letter, $col_current_ent_letter, $last_data_row);
    my $formula_sum_nfit_ent_incl_sby = sprintf("=SUM(%s2:%s%d)", $col_nfit_ent_user_formula_letter, $col_nfit_ent_user_formula_letter, $last_data_row);
    my $formula_delta_incl_sby = sprintf("=%s%d-%s%d", $col_letter_data_age_sum_nfit_incl, $row_data_age, $col_letter_data_age_sum_current_incl, $row_data_age);
    my $formula_perc_incl_sby = sprintf("=IFERROR(%s%d/%s%d,\"\")", $col_letter_data_age_delta_incl, $row_data_age, $col_letter_data_age_sum_current_incl, $row_data_age);

    @csv_row = ($empty) x (22 + $col_offset);
    $csv_row[0] = "Data Age";
    $csv_row[21 + $col_offset] = "Incl. SBY";
    push @csv_row, $formula_sum_current_ent_incl_sby, $formula_sum_nfit_ent_incl_sby, $formula_delta_incl_sby, $formula_perc_incl_sby;
    print $fh join(",", map { quote_csv($_) } @csv_row) . "\n";

    # --- Row 2 of Footer: Timestamp ---
    my $col_letter_ts_sum_current_excl = $col_letter_data_age_sum_current_incl;
    my $col_letter_ts_sum_nfit_excl    = $col_letter_data_age_sum_nfit_incl;
    my $col_letter_ts_delta_excl       = $col_letter_data_age_delta_incl;
    my $col_letter_ts_perc_excl        = $col_letter_data_age_perc_incl;

    my $formula_sum_current_ent_excl_sby = sprintf("=SUMIFS(%s\$2:%s\$%d, %s\$2:%s\$%d, \"<>*PowerHA Standby*\")", $col_current_ent_letter, $col_current_ent_letter, $last_data_row, $col_system_type_letter, $col_system_type_letter, $last_data_row);
    my $formula_sum_nfit_ent_excl_sby = sprintf("=SUMIFS(%s\$2:%s\$%d, %s\$2:%s\$%d, \"<>*PowerHA Standby*\")", $col_nfit_ent_user_formula_letter, $col_nfit_ent_user_formula_letter, $last_data_row, $col_system_type_letter, $col_system_type_letter, $last_data_row);
    my $formula_delta_excl_sby = sprintf("=%s%d-%s%d", $col_letter_ts_sum_nfit_excl, $row_timestamp, $col_letter_ts_sum_current_excl, $row_timestamp);
    my $formula_perc_excl_sby = sprintf("=IFERROR(%s%d/%s%d,\"\")", $col_letter_ts_delta_excl, $row_timestamp, $col_letter_ts_sum_current_excl, $row_timestamp);

    @csv_row = ($empty) x (22 + $col_offset);
    $csv_row[0] = $file_timestamp_str;
    $csv_row[21 + $col_offset] = "Excl. SBY";
    push @csv_row, $formula_sum_current_ent_excl_sby, $formula_sum_nfit_ent_excl_sby, $formula_delta_excl_sby, $formula_perc_excl_sby;
    print $fh join(",", map { quote_csv($_) } @csv_row) . "\n";

    print $fh "\n";

    # --- Row: AS-IS NFIT Labels ---
    @csv_row = ($empty) x (23 + $col_offset);
    $csv_row[17 + $col_offset] = "AS-IS";
    $csv_row[18 + $col_offset] = "NFIT";
    $csv_row[20 + $col_offset] = "AS-IS";
    $csv_row[21 + $col_offset] = "NFIT";
    print $fh join(",", map { quote_csv($_) } @csv_row) . "\n";

    # --- Row: ENT Column Headers and other labels ---
    @csv_row = ($empty) x (22 + $col_offset);
    $csv_row[0] = "ENT"; $csv_row[1] = "ENT-NOVIO"; $csv_row[2] = "ENT-HA"; $csv_row[3] = "ENT-NFIT";
    $csv_row[4] = "NFIT-ENT-NO-VIO"; $csv_row[5] = "NFIT-ENT-NO-POWERHA-STANDBY";
    $csv_row[6] = "NFIT-ENT-NO-POWERHA-SBY-NO-VIO"; $csv_row[7] = "NFIT-ENT-POWERHA-SBY-AS-IS";
    $csv_row[8] = "NFIT-ENT-POWERHA-SBY-AS-IS-NOVIO";
    $csv_row[12] = "PowerHA SBY% TGT"; $csv_row[13] = "0.25";

    my $largest_frame_formula_as_is = sprintf("=LET(sys,%s\$2:%s\$%d,type,%s\$2:%s\$%d,ent,%s\$2:%s\$%d,rows,FILTER(HSTACK(sys,ent),NOT(type=\"VIO Server\")),uniqSys,UNIQUE(INDEX(rows,,1)),sums,BYROW(uniqSys,LAMBDA(s,SUM(FILTER(INDEX(rows,,2),INDEX(rows,,1)=s)))),XLOOKUP(MAX(sums),sums,uniqSys))", $col_serial_letter, $col_serial_letter, $last_data_row, $col_system_type_letter, $col_system_type_letter, $last_data_row, $col_current_ent_letter, $col_current_ent_letter, $last_data_row);
    my $largest_frame_formula_nfit = sprintf("=LET(sys,%s\$2:%s\$%d,type,%s\$2:%s\$%d,ent,%s\$2:%s\$%d,rows,FILTER(HSTACK(sys,ent),NOT(type=\"VIO Server\")),uniqSys,UNIQUE(INDEX(rows,,1)),sums,BYROW(uniqSys,LAMBDA(s,SUM(FILTER(INDEX(rows,,2),INDEX(rows,,1)=s)))),XLOOKUP(MAX(sums),sums,uniqSys))", $col_serial_letter, $col_serial_letter, $last_data_row, $col_system_type_letter, $col_system_type_letter, $last_data_row, $col_nfit_ent_user_formula_letter, $col_nfit_ent_user_formula_letter, $last_data_row);
    my $largest_powerha_formula_as_is = sprintf("=LET(sys,%s\$2:%s\$%d,type,%s\$2:%s\$%d,ent,%s\$2:%s\$%d,pharows,FILTER(HSTACK(sys,ent),ISNUMBER(SEARCH(\"PowerHA Primary\",type))),uniqSysPHA,UNIQUE(INDEX(pharows,,1)),sumsPHA,BYROW(uniqSysPHA,LAMBDA(s,SUM(FILTER(INDEX(pharows,,2),INDEX(pharows,,1)=s)))),XLOOKUP(MAX(sumsPHA),sumsPHA,uniqSysPHA,\"\"))", $col_serial_letter, $col_serial_letter, $last_data_row, $col_system_type_letter, $col_system_type_letter, $last_data_row, $col_current_ent_letter, $col_current_ent_letter, $last_data_row);
    my $largest_powerha_formula_nfit = sprintf("=LET(sys,%s\$2:%s\$%d,type,%s\$2:%s\$%d,ent,%s\$2:%s\$%d,pharows,FILTER(HSTACK(sys,ent),ISNUMBER(SEARCH(\"PowerHA Primary\",type))),uniqSysPHA,UNIQUE(INDEX(pharows,,1)),sumsPHA,BYROW(uniqSysPHA,LAMBDA(s,SUM(FILTER(INDEX(pharows,,2),INDEX(pharows,,1)=s)))),XLOOKUP(MAX(sumsPHA),sumsPHA,uniqSysPHA,\"\"))", $col_serial_letter, $col_serial_letter, $last_data_row, $col_system_type_letter, $col_system_type_letter, $last_data_row, $col_nfit_ent_user_formula_letter, $col_nfit_ent_user_formula_letter, $last_data_row);

    $csv_row[16 + $col_offset] = "Largest Frame";
    $csv_row[17 + $col_offset] = $largest_frame_formula_as_is;
    $csv_row[18 + $col_offset] = $largest_frame_formula_nfit;
    $csv_row[19 + $col_offset] = "Largest PowerHA";
    $csv_row[20 + $col_offset] = $largest_powerha_formula_as_is;
    $csv_row[21 + $col_offset] = $largest_powerha_formula_nfit;
    print $fh join(",", map { quote_csv($_) } @csv_row) . "\n";

    # --- Per-Serial Summary Rows ---
    for (my $i = 0; $i < $loop_count_for_serials; $i++) {
        my $current_formula_row = $row_unique_serials_start + $i;
        my @csv_row_serial_summary;

        if ($i == 0) {
            my $formula_unique_serials = sprintf("=UNIQUE(%s\$2:%s\$%d)", $col_serial_letter, $col_serial_letter, $last_data_row);
            push @csv_row_serial_summary, $formula_unique_serials;
        } else {
            push @csv_row_serial_summary, $empty;
        }

        # Formulas for columns B-J
        push @csv_row_serial_summary, sprintf("=IFERROR(SUMIFS(\$%s\$2:\$%s\$%d, \$%s\$2:\$%s\$%d, A%d),\"\")", $col_current_ent_letter, $col_current_ent_letter, $last_data_row, $col_serial_letter, $col_serial_letter, $last_data_row, $current_formula_row);
        push @csv_row_serial_summary, sprintf("=IFERROR(SUMIFS(\$%s\$2:\$%s\$%d, \$%s\$2:\$%s\$%d, A%d, \$%s\$2:\$%s\$%d, \"<>VIO Server\"),\"\")", $col_current_ent_letter, $col_current_ent_letter, $last_data_row, $col_serial_letter, $col_serial_letter, $last_data_row, $current_formula_row, $col_system_type_letter, $col_system_type_letter, $last_data_row);
        push @csv_row_serial_summary, sprintf("=IFERROR(SUMIFS(\$%s\$2:\$%s\$%d, \$%s\$2:\$%s\$%d, A%d, \$%s\$2:\$%s\$%d, \"*PowerHA Primary*\"),\"\")", $col_current_ent_letter, $col_current_ent_letter, $last_data_row, $col_serial_letter, $col_serial_letter, $last_data_row, $current_formula_row, $col_system_type_letter, $col_system_type_letter, $last_data_row);
        push @csv_row_serial_summary, sprintf("=IFERROR(SUMIFS(\$%s\$2:\$%s\$%d, \$%s\$2:\$%s\$%d, A%d),\"\")", $col_nfit_ent_user_formula_letter, $col_nfit_ent_user_formula_letter, $last_data_row, $col_serial_letter, $col_serial_letter, $last_data_row, $current_formula_row);
        push @csv_row_serial_summary, sprintf("=IFERROR(SUMIFS(\$%s\$2:\$%s\$%d, \$%s\$2:\$%s\$%d, A%d, \$%s\$2:\$%s\$%d, \"<>VIO Server\"),\"\")", $col_nfit_ent_user_formula_letter, $col_nfit_ent_user_formula_letter, $last_data_row, $col_serial_letter, $col_serial_letter, $last_data_row, $current_formula_row, $col_system_type_letter, $col_system_type_letter, $last_data_row);
        push @csv_row_serial_summary, sprintf("=IFERROR(SUMIFS(\$%s\$2:\$%s\$%d, \$%s\$2:\$%s\$%d, A%d, \$%s\$2:\$%s\$%d, \"<>*PowerHA Standby*\"),\"\")", $col_nfit_ent_user_formula_letter, $col_nfit_ent_user_formula_letter, $last_data_row, $col_serial_letter, $col_serial_letter, $last_data_row, $current_formula_row, $col_system_type_letter, $col_system_type_letter, $last_data_row);
        push @csv_row_serial_summary, sprintf("=IFERROR(SUMIFS(\$%s\$2:\$%s\$%d, \$%s\$2:\$%s\$%d, A%d, \$%s\$2:\$%s\$%d, \"<>*PowerHA Standby*\", \$%s\$2:\$%s\$%d, \"<>VIO Server\"),\"\")", $col_nfit_ent_user_formula_letter, $col_nfit_ent_user_formula_letter, $last_data_row, $col_serial_letter, $col_serial_letter, $last_data_row, $current_formula_row, $col_system_type_letter, $col_system_type_letter, $last_data_row, $col_system_type_letter, $col_system_type_letter, $last_data_row);
        push @csv_row_serial_summary, sprintf("=IFERROR(SUMPRODUCT((\$%s\$2:\$%s\$%d=A%d)*IF(ISNUMBER(SEARCH(\"PowerHA Standby\",\$%s\$2:\$%s\$%d)),\$%s\$2:\$%s\$%d,\$%s\$2:\$%s\$%d)),\"\")", $col_serial_letter, $col_serial_letter, $last_data_row, $current_formula_row, $col_system_type_letter, $col_system_type_letter, $last_data_row, $col_current_ent_letter, $col_current_ent_letter, $last_data_row, $col_nfit_ent_user_formula_letter, $col_nfit_ent_user_formula_letter, $last_data_row);
        push @csv_row_serial_summary, sprintf("=IFERROR(SUMPRODUCT((\$%s\$2:\$%s\$%d=A%d)*(\$%s\$2:\$%s\$%d<>\"VIO Server\")*IF(ISNUMBER(SEARCH(\"PowerHA Standby\",\$%s\$2:\$%s\$%d)),\$%s\$2:\$%s\$%d,\$%s\$2:\$%s\$%d)),\"\")", $col_serial_letter, $col_serial_letter, $last_data_row, $current_formula_row, $col_system_type_letter, $col_system_type_letter, $last_data_row, $col_system_type_letter, $col_system_type_letter, $last_data_row, $col_current_ent_letter, $col_current_ent_letter, $last_data_row, $col_nfit_ent_user_formula_letter, $col_nfit_ent_user_formula_letter, $last_data_row);

        if ($i == 0) {
            my $num_main_formulas = scalar(@csv_row_serial_summary);
            my $padding_needed = (16 + $col_offset) - $num_main_formulas;
            push @csv_row_serial_summary, ($empty) x $padding_needed if $padding_needed > 0;

            my $col_R_header_cell = get_excel_col_name(18 + $col_offset) . $row_ent_col_headers;
            my $col_S_header_cell = get_excel_col_name(19 + $col_offset) . $row_ent_col_headers;
            my $col_U_header_cell = get_excel_col_name(21 + $col_offset) . $row_ent_col_headers;

            push @csv_row_serial_summary, "Largest Frame ENT (Excl. VIO)";
            push @csv_row_serial_summary, sprintf("=IFERROR(SUMIFS(\$%s\$2:\$%s\$%d, \$%s\$2:\$%s\$%d, %s, \$%s\$2:\$%s\$%d, \"<>VIO Server\"),\"\")", $col_current_ent_letter, $col_current_ent_letter, $last_data_row, $col_serial_letter, $col_serial_letter, $last_data_row, $col_R_header_cell, $col_system_type_letter, $col_system_type_letter, $last_data_row);
            push @csv_row_serial_summary, sprintf("=IFERROR(SUMIFS(\$%s\$2:\$%s\$%d, \$%s\$2:\$%s\$%d, %s, \$%s\$2:\$%s\$%d, \"<>VIO Server\"),\"\")", $col_nfit_ent_user_formula_letter, $col_nfit_ent_user_formula_letter, $last_data_row, $col_serial_letter, $col_serial_letter, $last_data_row, $col_S_header_cell, $col_system_type_letter, $col_system_type_letter, $last_data_row);
            push @csv_row_serial_summary, "Largest PowerHA ENT";
            push @csv_row_serial_summary, sprintf("=IFERROR(SUMIFS(\$%s\$2:\$%s\$%d, \$%s\$2:\$%s\$%d, %s, \$%s\$2:\$%s\$%d, \"*PowerHA Primary*\"),\"\")", $col_current_ent_letter, $col_current_ent_letter, $last_data_row, $col_serial_letter, $col_serial_letter, $last_data_row, $col_U_header_cell, $col_system_type_letter, $col_system_type_letter, $last_data_row);
            push @csv_row_serial_summary, sprintf("=IFERROR(SUMIFS(\$%s\$2:\$%s\$%d, \$%s\$2:\$%s\$%d, %s, \$%s\$2:\$%s\$%d, \"*PowerHA Primary*\"),\"\")", $col_nfit_ent_user_formula_letter, $col_nfit_ent_user_formula_letter, $last_data_row, $col_serial_letter, $col_serial_letter, $last_data_row, $col_U_header_cell, $col_system_type_letter, $col_system_type_letter, $last_data_row);
        }
        print $fh join(",", map { quote_csv($_) } @csv_row_serial_summary) . "\n";
    }

    # --- Rows after per-serial summary (Frame Evac, etc.) ---
    my $actual_row_unique_serials_end = $row_unique_serials_start + $loop_count_for_serials - 1;
    my $row_after_serials_block = $row_unique_serials_start + $loop_count_for_serials;

    my $cell_largest_frame_asis_val = get_excel_col_name(18 + $col_offset) . $row_ent_col_headers;
    my $cell_largest_frame_nfit_val = get_excel_col_name(19 + $col_offset) . $row_ent_col_headers;
    my $cell_largest_pha_asis_val   = get_excel_col_name(21 + $col_offset) . $row_ent_col_headers;
    my $cell_largest_pha_nfit_val   = get_excel_col_name(22 + $col_offset) . $row_ent_col_headers;

    # Row: Frame Evac - Max Required
    @csv_row = ($empty) x (22 + $col_offset);
    $csv_row[16 + $col_offset] = "Frame Evac - Max Required";
    $csv_row[17 + $col_offset] = sprintf("=MAX(%s,%s)", $cell_largest_frame_asis_val, $cell_largest_pha_asis_val);
    $csv_row[21 + $col_offset] = sprintf("=MAX(%s,%s)", $cell_largest_frame_nfit_val, $cell_largest_pha_nfit_val);
    print $fh join(",", map { quote_csv($_) } @csv_row) . "\n";

    # Row: Frame Evac - Required per frame
    my $current_print_row_for_evac_max = $row_after_serials_block;
    my $cell_max_req_as_is_val = get_excel_col_name(18 + $col_offset) . $current_print_row_for_evac_max;
    my $cell_max_req_nfit_val  = get_excel_col_name(22 + $col_offset) . $current_print_row_for_evac_max;

    @csv_row = ($empty) x (22 + $col_offset);
    $csv_row[16 + $col_offset] = "Frame Evac - Required per frame";
    $csv_row[17 + $col_offset] = sprintf("=IFERROR(%s/COUNTA(UNIQUE(\$%s\$2:\$%s\$%d)),\"N/A\")", $cell_max_req_as_is_val, $col_serial_letter, $col_serial_letter, $last_data_row);
    $csv_row[21 + $col_offset] = sprintf("=IFERROR(%s/COUNTA(UNIQUE(\$%s\$2:\$%s\$%d)),\"N/A\")", $cell_max_req_nfit_val, $col_serial_letter, $col_serial_letter, $last_data_row);
    print $fh join(",", map { quote_csv($_) } @csv_row) . "\n";

    # --- Total Row for Per-Serial Summary ---
    @csv_row = ();
    push @csv_row, "Total";
    for my $col_idx (2..10) {
        my $col_letter = get_excel_col_name($col_idx);
        if ($count_of_unique_serials > 0) {
            push @csv_row, sprintf("=SUM(%s%d:%s%d)", $col_letter, $row_unique_serials_start, $col_letter, $actual_row_unique_serials_end);
        } else {
            push @csv_row, "0";
        }
    }
    my $current_cols = scalar(@csv_row);
    push @csv_row, ($empty) x ((22 + $col_offset) - $current_cols) if (22 + $col_offset) > $current_cols;
    print $fh join(",", map { quote_csv($_) } @csv_row) . "\n";
}


# ==============================================================================
# Subroutine to format nfit flags for display
# ==============================================================================
sub format_nfit_flags_for_display {
    my ($profile_name, $profile_specific_flags, $runq_perc_flags, $runq_behavior) = @_;
    my @output_lines;

    my $temp_profile_flags = $profile_specific_flags; # Work on a copy

    my @core_fit_parts;
    my @decay_parts;
    my @growth_parts;
    my @other_parts; # For flags not specifically categorized

    # Helper sub-subroutine to extract and remove a flag pattern
    # Arguments:
    #   1. Regex for the flag and its potential value (e.g., qr/-p\s+[^\s]+/)
    #   2. Array reference to store the extracted flag string
    #   3. Scalar reference to the string of flags to be processed (will be modified)
    sub _extract_flag {
        my ($flag_regex, $parts_array_ref, $flags_string_ref) = @_;
        if ($$flags_string_ref =~ s/($flag_regex)//) {
            my $extracted_part = $1;
            $extracted_part =~ s/^\s+|\s+$//g; # Trim whitespace
            push @$parts_array_ref, $extracted_part if $extracted_part;
        }
    }

    # --- Core Fit Parameters ---
    _extract_flag(qr/--percentile\s+(?:\"[^\"]*\"|\'[^\']*\'|[^\s\"]+)|-p\s+(?:\"[^\"]*\"|\'[^\']*\'|[^\s\"]+)/, \@core_fit_parts, \$temp_profile_flags);
    _extract_flag(qr/--process-window-size\s+(?:\"[^\"]*\"|\'[^\']*\'|[^\s\"]+)|-w\s+(?:\"[^\"]*\"|\'[^\']*\'|[^\s\"]+)/, \@core_fit_parts, \$temp_profile_flags);
    _extract_flag(qr/--filter-above-perc\s+(?:\"[^\"]*\"|\'[^\']*\'|[^\s\"]+)/, \@core_fit_parts, \$temp_profile_flags);
    # Add more related flags here if needed, e.g.:
    # _extract_flag(qr/--filter-metric\s+[^\s]+/, \@core_fit_parts, \$temp_profile_flags);
    # _extract_flag(qr/--filter-limit\s+[^\s]+/, \@core_fit_parts, \$temp_profile_flags);

    # --- Decay Options ---
    _extract_flag(qr/--decay\s+(?:\"[^\"]*\"|\'[^\']*\'|[^\s\"]+)/, \@decay_parts, \$temp_profile_flags);
    _extract_flag(qr/--runq-decay\s+(?:\"[^\"]*\"|\'[^\']*\'|[^\s\"]+)/, \@decay_parts, \$temp_profile_flags);
    # Add more related flags here, e.g.:
    # _extract_flag(qr/--ema-period\s+[^\s]+/, \@decay_parts, \$temp_profile_flags);
    # _extract_flag(qr/--sma-period\s+[^\s]+/, \@decay_parts, \$temp_profile_flags);

    # --- Growth Prediction ---
    _extract_flag(qr/--enable-growth-prediction\b/, \@growth_parts, \$temp_profile_flags); # \b for word boundary
    _extract_flag(qr/--max-growth-inflation-percent\s+(?:\"[^\"]*\"|\'[^\']*\'|[^\s\"]+)/, \@growth_parts, \$temp_profile_flags);
    _extract_flag(qr/--growth-period-days\s+(?:\"[^\"]*\"|\'[^\']*\'|[^\s\"]+)/, \@growth_parts, \$temp_profile_flags);

    if (@core_fit_parts) {
        push @output_lines, "  Core         : " . join(" ", @core_fit_parts);
    }
    if (@decay_parts) {
        push @output_lines, "  Decay        : " . join(" ", @decay_parts);
    }
    if (@growth_parts) {
        push @output_lines, "  Growth       : " . join(" ", @growth_parts);
    }

    # --- RunQ Percentiles (these are from the already processed $runq_perc_flags) ---
    my $trimmed_runq_perc_flags = $runq_perc_flags;
    $trimmed_runq_perc_flags =~ s/^\s+|\s+$//g; # Trim
    if ($trimmed_runq_perc_flags) {
        push @output_lines, "  RunQ Percs   : " . $trimmed_runq_perc_flags;
    }

    # --- Other/Remaining Profile Flags ---
    # Any flags left in $temp_profile_flags are considered "Other"
    $temp_profile_flags =~ s/^\s+|\s+$//g; # Trim remaining
    my @remaining_flags = split(/\s+/, $temp_profile_flags); # Split remaining by space
    @remaining_flags = grep { $_ ne "" } @remaining_flags; # Filter out empty strings
    if (@remaining_flags) {
        # Reconstruct to handle flags that might have been split from their values if not perfectly matched above
        # This simplistic split might not be perfect if un-extracted flags had quoted spaces.
        # For robust handling of complex "Other" flags, more sophisticated parsing of $temp_profile_flags would be needed.
        # However, ideally, most common flags are explicitly extracted above.
        push @output_lines, "  Other Args   : " . join(" ", @remaining_flags);
    }

    # --- RunQ Behavior ---
    # This comes directly from the profile config, not from the flag strings
    if (defined $runq_behavior && $runq_behavior ne 'default') {
        push @output_lines, "  RunQBehavior : $runq_behavior";
    }

    return join("\n", @output_lines);
}

# --- parse_profile_name_for_log ---
# Parses common nfit-profile profile name patterns for a more descriptive log output.
# Adheres to Allman style and includes comments.
sub parse_profile_name_for_log
{
    my ($profile_name_str) = @_;

    my $description = $profile_name_str; # Default to original name if no pattern matches
    my @parts;

    # Regex to capture common patterns like O3-95W15, P-99W1, G2-BatchSpecial etc.
    # This regex looks for: TypeChar [TierNum] - Percentile W WindowNum [SuffixLetters]
    if ($profile_name_str =~ /^([OBGP])(?:-?(\d+))?-?(\d{2,3})(?:W(\d+))?([A-Z]*)?$/i)
    {
        my $type_char   = uc($1);
        my $tier_num    = $2; # Optional
        my $perc_val    = $3;
        my $win_val     = $4; # Optional
        my $suffix_char = $5; # Optional

        my $type_desc = "Unknown Type"; # Default for safety
        if ($type_char eq 'O')
        {
            $type_desc = "Online";
        }
        elsif ($type_char eq 'B')
        {
            $type_desc = "Batch";
        }
        elsif ($type_char eq 'G')
        {
            $type_desc = "General";
        }
        elsif ($type_char eq 'P')
        {
            $type_desc = "Peak";
        }

        if (defined $tier_num && $tier_num ne "")
        {
            $type_desc .= " (Tier $tier_num)";
        }
        push @parts, $type_desc;

        if (defined $perc_val)
        {
            push @parts, "$perc_val" . "th Percentile";
        }
        if (defined $win_val)
        {
            push @parts, "$win_val-minute Window";
        }
        if (defined $suffix_char && $suffix_char ne "")
        {
            push @parts, "Variant '$suffix_char'";
        }

        $description = join(", ", @parts);
    }
    elsif (lc($profile_name_str) eq "peak") # Handle specific "Peak" profile name
    {
        $description = "Absolute Peak Value";
    }
    # Add more 'elsif' blocks here for other distinct profile naming conventions if needed.

    return "$profile_name_str ($description)";
}

# --- log_profile_rationale ---
# Logs the detailed rationale for how a profile's PhysC value was adjusted.
# Incorporates a summary-first approach and clearer narrative for planners.
# Adheres to Allman style and includes comments.
sub log_profile_rationale
{
    my ($fh,
        $vm_name,
        $profile_obj,
        $base_physc_for_profile, # This is NOW the aggregated Base PhysC or direct value
        $raw_nfit_states_aref,   # NEW: Array ref of all state result hashes from nfit
		$total_states_for_vm,
        $runq_metrics_source_profile_name_for_this_calc,
        $abs_runq_value_used_for_calc,
        $normP50_for_this_calc,
        $normP90_for_this_calc,
        $vm_cfg_ref,
        $smt_val,
        $lpar_max_cpu_cfg_val_from_config,
        $entitlement_val,
        $profile_rq_behavior,
        $calc_debug_info_ref,
        $final_csv_value_for_profile,
        $nfit_growth_adj_for_log,
        $nfit_growth_adj_abs_for_log,
        $adaptive_runq_saturation_thresh
    ) = @_;

    # Declare and initialise variables for STD rationale block
    my $eff_p_base_numeric = (defined $base_physc_for_profile && looks_like_number($base_physc_for_profile)) ? ($base_physc_for_profile + 0) : undef;
    my $curr_ent_numeric = (defined $entitlement_val && looks_like_number($entitlement_val)) ? ($entitlement_val + 0) : undef;

    # Ensure script doesn't die if log handle isn't valid
    return unless $fh;

    # --- Determine if a seasonal run occurred to select the correct logging path ---
    my $event_config = defined($apply_seasonality_event) ? ($seasonality_config->{$apply_seasonality_event} // {}) : {};

    # --- Get profile name and metric key from the profile object ---
	my $model_type = $event_config->{model} // '';
	my $profile_being_adjusted = $profile_obj->{name}; # Get name from the object
	my $profile_desc = parse_profile_name_for_log($profile_being_adjusted);
	my $profile_physc_perc_val_num;
	if ($profile_obj->{flags} =~ /(?:-p|--percentile)\s+([0-9.]+)/) {
		$profile_physc_perc_val_num = $1 + 0;
	}
	my $p_metric_key = "P" . clean_perc_label($profile_physc_perc_val_num // $DEFAULT_PERCENTILE);

    # --- PATH A: Multiplicative Seasonal Model has its own log format ---
    if ($model_type eq 'multiplicative_seasonal' && exists $seasonal_debug_info{$vm_name}{$profile_being_adjusted}) {
        my $s_data = $seasonal_debug_info{$vm_name}{$profile_being_adjusted};
        print {$fh} "\n======================================================================\n";
        printf {$fh} "VM Name                                : %s\n", $vm_name;
        printf {$fh} "Profile Processed                      : %s\n", $profile_desc;
        print {$fh} "----------------------------------------------------------------------\n";
        print {$fh} "CPU Sizing Path: Multiplicative Seasonal Forecast\n\n";
        printf {$fh} "  - Current Baseline Value      : %.4f cores\n", $s_data->{baseline};
        printf {$fh} "  - Historical Multiplier       : %.4f\n", $s_data->{multiplier};
        printf {$fh} "  - Volatility Buffer           : %.4f\n", $s_data->{volatility};
        print {$fh} "  - Calculation                 : Baseline * Multiplier * Volatility\n";
        printf {$fh} "  - Final Forecasted Value      : %.4f cores\n", $s_data->{forecast};
        print {$fh} "======================================================================\n\n";
        return;
    }

    # --- PATH B: Standard Rationale (now also used by recency_decay model) ---
    my $na = 'N/A'; # Consistent N/A string for display
    my $abs_runq_key_reported_in_log = $calc_debug_info_ref->{AbsRunQKeyUsed} // 'AbsRunQ_P90 (default)';

    # --- Top Summary Block ---
    my $profile_description_log = parse_profile_name_for_log($profile_being_adjusted);

    # Use the unrounded final value from debug_info for precise change calculation
    my $final_recommendation_unrounded_str = $calc_debug_info_ref->{'FinalAdjustedPhysC'} // $na;
    my $final_recommendation_unrounded_num = ($final_recommendation_unrounded_str ne $na && $final_recommendation_unrounded_str =~ /^-?[0-9.]+$/)
    ? ($final_recommendation_unrounded_str + 0) : undef;

    my $base_physc_val_num = looks_like_number($base_physc_for_profile) ? $base_physc_for_profile + 0 : undef;

    my $net_change_str = $na;
    if (defined $base_physc_val_num && defined $final_recommendation_unrounded_num) {
        my $delta = $final_recommendation_unrounded_num - $base_physc_val_num;
        my $perc_change_str = (abs($base_physc_val_num) > $FLOAT_EPSILON) ? sprintf(" (Change: %s%.1f%%)", ($delta >=0 ? "+" : ""), ($delta / $base_physc_val_num) * 100) : "";
        $net_change_str = sprintf("%s%.4f cores%s", ($delta >=0 ? "+" : ""), abs($delta), $perc_change_str);
    }

    print {$fh} "\n======================================================================\n";
    printf {$fh} "VM Name                                     : %s\n", $vm_name;
    printf {$fh} "Profile Processed                           : %s\n", $profile_description_log;
    print {$fh} "----------------------------------------------------------------------\n";

	if ($model_type eq 'recency_decay') {
		# --- PATH B: Log the Recency Decay Rationale ---
		my $profile_desc = parse_profile_name_for_log($profile_being_adjusted);

		# Extract the growth adjustment value from the nfit results
		my $growth_adj = 0;
		if (ref($raw_nfit_states_aref) eq 'ARRAY' && @$raw_nfit_states_aref) {
			# For a decay run, nfit returns a single aggregated result line
			my $result_line = $raw_nfit_states_aref->[0];
			if (defined $result_line->{GrowthAdj} && looks_like_number($result_line->{GrowthAdj})) {
				$growth_adj = $result_line->{GrowthAdj};
			}
		}

		my $base_val_unrounded = (looks_like_number($base_physc_for_profile))
		? $base_physc_for_profile - $growth_adj
		: "N/A";

		print {$fh} "======================================================================\n";
		printf {$fh} "VM Name                         : %s\n", $vm_name;
		printf {$fh} "Profile Processed               : %s\n", $profile_desc;
		print {$fh} "----------------------------------------------------------------------\n";
		print {$fh} "CPU Sizing Path: Recency-Anchored Decay (Seasonal: '$apply_seasonality_event')\n\n";
		print {$fh} "  - This model solves the 'Start-of-Month' problem and includes nfit's\n";
		print {$fh} "    standard growth prediction.\n\n";
		printf {$fh} "  - Analysis Reference Date       : %s\n", ($nfit_analysis_reference_date_str // "N/A");
		printf {$fh} "  - Base Value (Recency-Anchored) : %.4f cores\n", $base_val_unrounded if (looks_like_number($base_val_unrounded));
		printf {$fh} "  - Growth Adjustment             : +%.4f cores\n", $growth_adj;
		print {$fh} "  --------------------------------------------------------------------\n";
		printf {$fh} "  - Final nfit Value (Unrounded)  : %s cores\n", ($base_physc_for_profile // "N/A");
		print {$fh} "======================================================================\n\n";
		return;
	}

    # --- Section A: nfit Raw State Analysis ---
    if (ref($raw_nfit_states_aref) eq 'ARRAY' && @$raw_nfit_states_aref) {
        print {$fh} "Section A: nfit Raw State Analysis & Base Value Calculation\n";
        printf {$fh} "  - nfit reported the following configuration states for this profile:\n";

        my $first_result = $raw_nfit_states_aref->[0];
        my $is_aggregated = ($first_result->{analysisType} || '') =~ /aggregated/;

        if ($is_aggregated) {
            my $config = $first_result->{metadata}{configuration} || {};
            my $metric_val = $first_result->{metrics}{physc}{$p_metric_key};
            # Special logging for aggregated decay-model results
            printf {$fh} "    - %-38s: Ent=%.2f, MaxCPU=%.2f, SMT=%d, %s=%.4f\n",
                "Aggregated Result",
                $config->{entitlement} // 0,
                $config->{maxCpu} // 0,
                $config->{smt} // 0,
                $p_metric_key,
                defined($metric_val) ? $metric_val : 0;
			my $state_count = $first_result->{state}{stateCount} // 1;
            printf {$fh} "    - Aggregation Method                    : Time-weighted decay model applied across %d configuration states.\n", $state_count;
        } else {
            # Standard logging for non-aggregated results
            foreach my $state_res (@$raw_nfit_states_aref) {
                my $state_id_str = $state_res->{state}{id} // 'N/A';
                my $config = $state_res->{metadata}{configuration} || {};
                my $metric_val = $state_res->{metrics}{physc}{$p_metric_key};

                printf {$fh} "    - %-26s: Ent=%.2f, MaxCPU=%.2f, SMT=%d, %s=%s\n",
                    $state_id_str,
                    $config->{entitlement} // 0,
                    $config->{maxCpu} // 0,
                    $config->{smt} // 0,
                    $p_metric_key,
                    defined($metric_val) ? sprintf("%.4f", $metric_val) : $na;
            }
            if (@$raw_nfit_states_aref > 1) {
                printf {$fh} "    - Aggregation Method                     : Simple Average of %d states was used.\n", scalar(@$raw_nfit_states_aref);
            } else {
                printf {$fh} "    - Aggregation Method                     : Direct value from a single state was used.\n";
            }
        }
    }

    my $nfit_growth_adj_to_display = "0.0000";
    my $first_nfit_state = (ref($raw_nfit_states_aref) eq 'ARRAY' && @$raw_nfit_states_aref) ? $raw_nfit_states_aref->[0] : {};
    if (defined $first_nfit_state->{metrics}{growth}{adjustment} && looks_like_number($first_nfit_state->{metrics}{growth}{adjustment})) {
        $nfit_growth_adj_to_display = sprintf("%.4f", $first_nfit_state->{metrics}{growth}{adjustment});
    }

    printf {$fh} "Initial Base PhysC for Profile              : %s cores (Aggregated value from nfit)\n", ($base_physc_for_profile // $na);

    # Check if any GrowthDebug keys exist to print the rationale block
    if ($first_nfit_state->{debug}{growthRationale}) {
        my $gd = $first_nfit_state->{debug}{growthRationale};

        # Helper for consistent boolean/skipped formatting
        my $format_check_result = sub {
            my $val = shift;
            return "Skipped" if (!defined $val || $val eq 'Skipped');
            return $val ? "Passed" : "Failed";
        };

        print {$fh} "  - nfit GrowthAdj Applied : +$nfit_growth_adj_to_display cores\n";
        print {$fh} "  - Rationale for GrowthAdj (from nfit):\n";
        printf {$fh} "    1. Data Eligibility : %s historical periods found (Min: %d).\n", $gd->{num_hist_periods}, $GROWTH_MIN_HISTORICAL_PERIODS; # Note: You may need to pass this constant or hardcode it
        printf {$fh} "    2. Volatility Check : Coefficient of Variation (CV) was %s. Trend analysis proceeds if CV < %.2f. [Result: %s]\n", $gd->{stats_cv}, $GROWTH_MAX_CV_THRESHOLD, ($gd->{cv_check_passed} ? "Passed" : "Failed");
        printf {$fh} "        3. Trend Analysis   : Linear regression slope was %s. Trend is considered positive if slope > %.2f. [Result: %s]\n", ($gd->{slope} // 'N/A'), $GROWTH_MIN_POSITIVE_SLOPE_THRESHOLD, $format_check_result->($gd->{slope_check_passed} // 'Skipped');

        # Use string comparison 'eq' for the check, as the value can be "Skipped", 0, or 1.
        if (defined $gd->{slope_check_passed} && $gd->{slope_check_passed} eq '1') {
            # Safely format projected value, printing "N/A" as a string if it's not numeric.
            my $proj_val_str = looks_like_number($gd->{projected_val}) ? sprintf("%.4f", $gd->{projected_val}) : "N/A";
            printf {$fh} "        4. Projection       : Trend projected to %s cores over %d days.\n", $proj_val_str, $DEFAULT_GROWTH_PROJECTION_DAYS;

            # Safely format inflation percentage and handle the ternary operator for capping status.
            my $inflation_str = looks_like_number($gd->{inflation_perc}) ? sprintf("%.2f", $gd->{inflation_perc}) : "N/A";
            my $capping_msg = (defined $gd->{was_capped} && $gd->{was_capped} eq '1') ? "CAPPED" : "not capped";
            printf {$fh} "        5. Inflation        : Calculated inflation is %s%%. This was %s to the max of %d%%.\n", $inflation_str, $capping_msg, $DEFAULT_MAX_GROWTH_INFLATION_PERCENT;
        }
        print {$fh} "\n";
    }

    printf {$fh} "Final nfit-profile Recommendation           : %s cores (Unrounded: %s)\n", ($final_csv_value_for_profile // $na), $final_recommendation_unrounded_str;
    printf {$fh} "Net Adjustment by nfit-profile              : %s\n", $net_change_str;
    print {$fh} "======================================================================\n\n";

    # --- Section B: Key Inputs & Configuration ---

    print {$fh} "Section B: Key Inputs & Configuration for Modifier Logic\n";
    printf {$fh} "  1. Key RunQ Metrics (source: %s, state: %s):\n",
    $runq_metrics_source_profile_name_for_this_calc,
    "Most Recent";
    printf {$fh} "     - AbsRunQ for Upsizing (%s)   : %s threads\n", $abs_runq_key_reported_in_log, ($abs_runq_value_used_for_calc // $na);
    printf {$fh} "     - NormRunQ P25                         : %s\n", ($calc_debug_info_ref->{'NormRunQ_P25_Val'} // $na);
    printf {$fh} "     - NormRunQ P50                         : %s\n", ($normP50_for_this_calc // ($calc_debug_info_ref->{'NormRunQ_P50_Val'} // $na) );
    printf {$fh} "     - NormRunQ P75                         : %s\n", ($calc_debug_info_ref->{'NormRunQ_P75_Val'} // $na);
    printf {$fh} "     - NormRunQ P90                         : %s\n", ($normP90_for_this_calc // $na);

    my $iqrc_val_for_log_A_sec = $calc_debug_info_ref->{'NormRunQ_IQRC_Val'} // $na;
    printf {$fh} "     - NormRunQ IQRC (Volatility)           : %s", $iqrc_val_for_log_A_sec;
    my $iqrc_interpretation_log_A_sec = $na;
    if ($iqrc_val_for_log_A_sec ne $na && $iqrc_val_for_log_A_sec =~ /^-?[0-9.]+$/)
    {
        my $iqrc_num_A_sec = $iqrc_val_for_log_A_sec + 0;
        if    ($iqrc_num_A_sec < 0.3)  { $iqrc_interpretation_log_A_sec = "Very steady"; }
        elsif ($iqrc_num_A_sec <= 0.6) { $iqrc_interpretation_log_A_sec = "Moderate variability"; }
        elsif ($iqrc_num_A_sec <= 1.0) { $iqrc_interpretation_log_A_sec = "High variability"; }
        else                           { $iqrc_interpretation_log_A_sec = "Very bursty/erratic"; }
        printf {$fh} " (%s)\n", $iqrc_interpretation_log_A_sec;
    } else {
        print {$fh} "\n";
    }

    my $is_runq_pressure_C_log_sec = ($calc_debug_info_ref->{'IsRunQPressure'} // "False") eq "True";
    my $is_workload_pressure_C_log_sec = ($calc_debug_info_ref->{'IsWorkloadPressure'} // "False") eq "True";
    printf {$fh} "  2. VM Configuration & Profile Behavior:\n";
    printf {$fh} "     - SMT                                  : %s\n", ($smt_val // $na);
	my $entitlement_display_A_log_sec = (defined $entitlement_val && looks_like_number($entitlement_val)) ? $entitlement_val : $na;
    printf {$fh} "     - Current Entitlement                  : %s cores\n", $entitlement_display_A_log_sec;
    my $lpar_max_cpu_display_A_log_sec = ($lpar_max_cpu_cfg_val_from_config > 0) ? sprintf("%.2f", $lpar_max_cpu_cfg_val_from_config) : $na;
    printf {$fh} "     - LPAR MaxCPU                          : %s cores\n", $lpar_max_cpu_display_A_log_sec;
    printf {$fh} "     - Profile RunQ Behaviour               : %s\n", ($profile_rq_behavior // $na);
	printf {$fh} "  3. Pressure Assessment Summary:\n";
    # Enhanced line for Overall LPAR RunQ Pressure
    my $abs_runq_source_str = "$runq_metrics_source_profile_name_for_this_calc " . ($calc_debug_info_ref->{AbsRunQKeyUsed} // '');

    # Correctly retrieve the rationale string from the debug info hash
    my $pressure_basis_str = $calc_debug_info_ref->{'PressureBasisRationale'} // "MaxCPU";
    my $lpar_pressure_reason;

    if (($calc_debug_info_ref->{'IsRunQPressure'} // "False") eq "True") {
        $lpar_pressure_reason = sprintf("Ratio %.2f > %.2f (Pressure detected)", ($calc_debug_info_ref->{'RunQPressure_P90_Val'} // 0), $adaptive_runq_saturation_thresh);
    } else {
        $lpar_pressure_reason = sprintf("Ratio %.2f <= %.2f (No pressure detected)", ($calc_debug_info_ref->{'RunQPressure_P90_Val'} // 0), $adaptive_runq_saturation_thresh);
    }
    printf {$fh} "     - Overall LPAR RunQ Pressure           : %s (Source: %s)\n", (($calc_debug_info_ref->{'IsRunQPressure'} // "False") eq "True" ? "True" : "False"), $abs_runq_source_str;
    printf {$fh} "         Basis for Pressure Calc            : %s\n", $pressure_basis_str;
    printf {$fh} "         Reason for Pressure Flag           : %s\n", $lpar_pressure_reason;

    # Enhanced line for Normalised Workload Pressure
    my $norm_runq_source_str = "$runq_metrics_source_profile_name_for_this_calc";
    printf {$fh} "     - Normalised Workload Pressure         : %s (Source: %s; Reason: %s)\n\n",
        (($calc_debug_info_ref->{'IsWorkloadPressure'} // "False") eq "True" ? "True" : "False"),
        $norm_runq_source_str,
        ($calc_debug_info_ref->{'WorkloadPressureReason'} // "N/A");

    if (defined $calc_debug_info_ref->{'ReasonForNoModification'} && $calc_debug_info_ref->{'ReasonForNoModification'} ne '')
    {
        printf {$fh} "CPU Modification Path Skipped: %s\n", $calc_debug_info_ref->{'ReasonForNoModification'};
    }
    else
    {
        # --- Section C: CPU Downsizing (Efficiency Assessment) ---
        print {$fh} "Section C: CPU Downsizing (Efficiency Assessment)\n";
        my $downsizing_reason_B_log = $calc_debug_info_ref->{'DownsizingReason'} // "Not calculated or N/A."; # Use renamed key
        my $downsizing_factor_B_log = $calc_debug_info_ref->{'DownsizingFactor'} // "1.00"; # Use renamed key
		my $physc_after_downsizing_B_log = $calc_debug_info_ref->{'DownsizedPhysC'} // $na;

        if ($downsizing_reason_B_log =~ /Single-Threaded Dominant \(STD\) Workload Pattern Detected/) {
            # Custom, detailed logging for the STD heuristic path.
            printf {$fh} "  - Overall Status                          : Tactical Downsizing Skipped (Entitlement Floor Guard)\n";
            printf {$fh} "  - Guardrail Rationale                     : Base PhysC (%.4f) > Entitlement (%.2f)\n", ($eff_p_base_numeric // 0), ($curr_ent_numeric // 0);
            printf {$fh} "  - Heuristic Override                      : Single-Thread-Dominated (STD) workload pattern was detected.\n";
            printf {$fh} "      - Condition Met (NormP90 < %.1f)       : %s\n", $STD_NORM_P90_THRESH, ($normP90_for_this_calc // $na);
            printf {$fh} "      - Condition Met (IQRC < %.1f)          : %s\n", $STD_IQRC_THRESH, ($calc_debug_info_ref->{'NormRunQ_IQRC_Val'} // $na);
            print  {$fh} "  - Strategic Signal Calculation (RunQ_Uncapped):\n";
            my $actual_reduction = $calc_debug_info_ref->{'EffActualReductionCores'} // 'N/A';
            my $runq_uncapped_val = $calc_debug_info_ref->{'RunQ_Uncapped'} // 'N/A';
            printf {$fh} "      - Potential Downsizing                : %.4f cores\n", (looks_like_number($actual_reduction) ? $actual_reduction : 0);
            printf {$fh} "      - Dampening Factor Applied            : %.0f%%\n", ($STD_DAMPENING_FACTOR_FOR_UNCAPPED * 100);
            printf {$fh} "      - Final RunQ_Uncapped Value           : %.4f cores\n", (looks_like_number($runq_uncapped_val) ? $runq_uncapped_val : 0);
        } else {
            # Standard logging for all other downsizing scenarios.
            printf {$fh} "  - Overall Status                          : %s\n", $downsizing_reason_B_log;
        }
        printf {$fh} "  - Final Downsizing Factor                 : %s\n", $downsizing_factor_B_log;

        # Conditionally print detailed analytical breakdown for downsizing
        if ($downsizing_reason_B_log =~ /^Analytical/ &&
            defined $calc_debug_info_ref->{'EffPEfficientTarget'} && # Internal keys can remain Eff...
            defined $calc_debug_info_ref->{'EffCondNormP50Met'} &&
            defined $calc_debug_info_ref->{'EffCondVolatilityMet'})
        {
            printf {$fh} "  - Detailed Analytical Path for Downsizing:\n";
            printf {$fh} "     a. Initial Condition Checks for Downsizing Path:\n";
            my $eff_cond_norm_p50_met_str_B = $calc_debug_info_ref->{'EffCondNormP50Met'} ? "YES (Low P50)" : "NO (P50 not low enough)";
            printf {$fh} "        - NormRunQ P50                      : %-5s (Condition: < %.2f for consideration? %s)\n",
            ($normP50_for_this_calc // $na),
            $NORM_P50_THRESHOLD_FOR_EFFICIENCY_CONSIDERATION,
            $eff_cond_norm_p50_met_str_B;

            my $eff_cond_volatility_met_str_B = $calc_debug_info_ref->{'EffCondVolatilityMet'} ? "YES (Not excessively volatile)" : "NO (Too volatile)";
            printf {$fh} "        - Workload Volatility               : %-5s (NormP90 %.2f / NormP50 %.2f. Condition: < %.2f to proceed? %s)\n",
            ($calc_debug_info_ref->{'EffVolatilityRatio'} // $na),
            ($normP90_for_this_calc ne $na ? ($normP90_for_this_calc+0):0), # Ensure numeric for sprintf
            ($normP50_for_this_calc ne $na ? ($normP50_for_this_calc+0):0),
            $VOLATILITY_CAUTION_THRESHOLD,
            $eff_cond_volatility_met_str_B;
            print {$fh} "\n";

            printf {$fh} "     b. Calculating Raw Efficient PhysC Target (Theoretical Minimum if RunQ was at Target Norm):\n";
            printf {$fh} "        - Base PhysC for Profile            : %s cores\n", ($calc_debug_info_ref->{'EffPBase'} // $na);
            printf {$fh} "        - AbsRunQ Metric Used               : %s (value: %s threads)\n", $abs_runq_key_reported_in_log, ($abs_runq_value_used_for_calc // $na);
            printf {$fh} "        - SMT Value                         : %s\n", ($calc_debug_info_ref->{'EffSMTValue'} // $na);
            my $smt_txt = defined $calc_debug_info_ref->{'EffSMTValue'}
                          ? $calc_debug_info_ref->{'EffSMTValue'} : $na;
            my $tgt_val = $calc_debug_info_ref->{'EffTargetNormRunQ'};
            my $tgt_txt = (defined $tgt_val && looks_like_number($tgt_val))
                          ? sprintf('%.2f', $tgt_val) : $na;
            printf {$fh} "        - Target NormRunQ for SMT%-11s: %s (internal heuristic for optimal queue/LCPU)\n", $smt_txt, $tgt_txt;
            printf {$fh} "        - Raw Efficient PhysC Target        : AbsRunQ / (SMT * Target NormRunQ)\n";
            printf {$fh} "                                              %s / (%s * %.2f) = %s cores\n",
            ($abs_runq_value_used_for_calc // $na),
            ($calc_debug_info_ref->{'EffSMTValue'} // $na),
            ($calc_debug_info_ref->{'EffTargetNormRunQ'} // $na),
            ($calc_debug_info_ref->{'EffPEfficientTargetRaw'} // $na);
            print {$fh} "\n";

            printf {$fh} "     c. Blending Raw Target with Observed Base PhysC (Applying Confidence):\n";
            printf {$fh} "        - Blending Weights                  : %.0f%% Base PhysC / %.0f%% Raw Target\n",
            defined $calc_debug_info_ref->{'EffBlendWeightBase'} ? (($calc_debug_info_ref->{'EffBlendWeightBase'} // 0) * 100) : 0,
            defined $calc_debug_info_ref->{'EffBlendWeightTarget'} ? (($calc_debug_info_ref->{'EffBlendWeightTarget'} // 0) * 100) : 0;
            printf {$fh} "        - Blending Rationale                : %s\n", ($calc_debug_info_ref->{'EffBlendReason'} // $na);
            printf {$fh} "        - Blended Efficient Target          : (Base PhysC * Weight) + (Raw Target * Weight)\n";
            printf {$fh} "                                              (%s * %.2f) + (%s * %.2f) = %s cores\n",
            ($calc_debug_info_ref->{'EffPBase'} // $na),
            ($calc_debug_info_ref->{'EffBlendWeightBase'} // 0.0),
            ($calc_debug_info_ref->{'EffPEfficientTargetRaw'} // $na),
            ($calc_debug_info_ref->{'EffBlendWeightTarget'} // 0.0),
            ($calc_debug_info_ref->{'EffPEfficientTarget'} // $na);
            print {$fh} "\n";

            printf {$fh} "     d. Determining Potential CPU Downsizing (Based on Blended Target):\n";
            my $eff_comp_base_vs_target_met_str_B = defined($calc_debug_info_ref->{'EffComparisonBaseVsTargetMet'})
            ? ($calc_debug_info_ref->{'EffComparisonBaseVsTargetMet'} ? "YES" : "NO") : $na;
            printf {$fh} "        - Comparison                        : Base PhysC (%s) > Blended Efficient Target (%s)? %s\n",
            ($calc_debug_info_ref->{'EffPBase'} // $na),
            ($calc_debug_info_ref->{'EffPEfficientTarget'} // $na),
            $eff_comp_base_vs_target_met_str_B;

            if (defined $calc_debug_info_ref->{'EffComparisonBaseVsTargetMet'} && $calc_debug_info_ref->{'EffComparisonBaseVsTargetMet'})
            {
                printf {$fh} "        - Potential CPU Downsize            : %s - %s = %s cores\n",
                ($calc_debug_info_ref->{'EffPBase'} // $na),
                ($calc_debug_info_ref->{'EffPEfficientTarget'} // $na),
                ($calc_debug_info_ref->{'EffPotentialReduction'} // $na);
                printf {$fh} "        - Max Downsize Cap %%                : %.1f%% (Reason: %s)\n",
                ($calc_debug_info_ref->{'EffMaxAllowableReductionPerc'} eq $na ? ($MAX_EFFICIENCY_REDUCTION_PERCENTAGE*100) : ($calc_debug_info_ref->{'EffMaxAllowableReductionPerc'} +0) ),
                ($calc_debug_info_ref->{'EffReductionCapReason'} // $na);
                printf {$fh} "        - Max Allowable Downsize            : %s cores (Base PhysC * Max Downsize Cap %%)\n",
                ($calc_debug_info_ref->{'EffMaxAllowableReductionCores'} // $na);
                printf {$fh} "        - Actual CPU Downsized By           : %s cores (min of Potential and Max Allowable)\n",
                ($calc_debug_info_ref->{'EffActualReductionCores'} // $na);
            }
            else
            {
                printf {$fh} "        - No potential for downsizing based on Blended Target, or reduction was zero.\n";
            }
            print {$fh} "\n";

            printf {$fh} "     e. Final Downsizing Factor Calculation:\n";
            printf {$fh} "        - Calculated Factor                 : (Base PhysC - Actual Reduction) / Base PhysC\n";
            my $eff_p_base_val_for_div_B = ($calc_debug_info_ref->{'EffPBase'} ne $na && ($calc_debug_info_ref->{'EffPBase'} + 0) != 0)
            ? ($calc_debug_info_ref->{'EffPBase'} + 0) : 1.0;
            my $eff_p_base_display_for_div_B = ($eff_p_base_val_for_div_B == 1.0 && ($calc_debug_info_ref->{'EffPBase'} eq $na || ($calc_debug_info_ref->{'EffPBase'} + 0) == 0))
            ? "$eff_p_base_val_for_div_B (adj for display)" : ($calc_debug_info_ref->{'EffPBase'} // $na);
            printf {$fh} "                                              (%s - %s) / %s = %s\n",
            ($calc_debug_info_ref->{'EffPBase'} // $na),
            ($calc_debug_info_ref->{'EffActualReductionCores'} // "0.0000"),
            $eff_p_base_display_for_div_B,
            ($calc_debug_info_ref->{'EffCalculatedFactor'} // $na);
        }
        printf {$fh} "  => PhysC after Downsizing                 : %s cores\n\n", $physc_after_downsizing_B_log;

        # --- Section D: CPU Upsizing (Additive CPU) ---
        print {$fh} "Section D: CPU Upsizing (Additive CPU)\n";
        my $apply_additive_C_log_sec = $is_runq_pressure_C_log_sec || $is_workload_pressure_C_log_sec;

		printf {$fh} "  - Additive Logic Triggered                : %s\n", ($apply_additive_C_log_sec ? "Yes" : "No");

        if ($apply_additive_C_log_sec)
        {
            printf {$fh} "    Details of Upsizing Calculation:\n";
            printf {$fh} "    - Base for Upsizing                     : %s cores (PhysC after Downsizing)\n", $physc_after_downsizing_B_log;
            printf {$fh} "    - Effective LCPUs at Base               : %s threads\n", ($calc_debug_info_ref->{'EffectiveLCPUsAtBase'} // $na);
            printf {$fh} "    - Excess Threads Calculated             : %s threads\n", ($calc_debug_info_ref->{'ExcessThreads'} // $na);
            printf {$fh} "    - Raw Additive CPU                      : %s cores\n", ($calc_debug_info_ref->{'RawAdditive'} // $na);
            printf {$fh} "    - Entitlement-Based Cap                 : %s cores (Max Additive Cap: %s)\n",
            (defined $calc_debug_info_ref->{'HotThreadWLDampenedAdditiveFrom'} && $calc_debug_info_ref->{'HotThreadWLDampeningApplied'} eq "True"
                ? $calc_debug_info_ref->{'HotThreadWLDampenedAdditiveFrom'}
                : ($calc_debug_info_ref->{'CappedRawAdditive'} // $na) # Show original value before HTW if HTW was applied
            ), ($calc_debug_info_ref->{'MaxAdditiveCap'} // $na);

            printf {$fh} "    Details of LCPU Calculations:\n";
            # Log enhanced burst and small entitlement handling details
            printf {$fh} "  Pressure Basis Rationale                   : %s\n", ($calc_debug_info_ref->{'PressureBasisRationale'} // "N/A");
            printf {$fh} "  Burst Allowance Used                       : %s\n", ($calc_debug_info_ref->{'BurstAllowanceUsed'} // "N/A");
            printf {$fh} "  Small Entitlement Handler Active           : %s\n", ($calc_debug_info_ref->{'SmallEntitlementHandler'} // "No");
            printf {$fh} "  Effective LCPUs for Pressure Calc          : %s\n", ($calc_debug_info_ref->{'EffectiveLCPUsForPressure'} // "N/A");

            if (defined $calc_debug_info_ref->{'HotThreadWLDampeningApplied'} && $calc_debug_info_ref->{'HotThreadWLDampeningApplied'} eq "True")
            {
				printf {$fh} "    - Hot Thread Dampening                         : Applied\n";
                printf {$fh} "        Conditions Summary                         : %s\n", ($calc_debug_info_ref->{'HotThreadWLConditionsString'} // $na);
                printf {$fh} "        Dynamic Dampen Factor                      : %s\n", ($calc_debug_info_ref->{'HotThreadWLDynamicFactor'} // $na);
                printf {$fh} "        Additive (Before HTW)                      : %s cores -> (After HTW): %s cores\n",
                ($calc_debug_info_ref->{'HotThreadWLDampenedAdditiveFrom'} // $na),
                ($calc_debug_info_ref->{'HotThreadWLDampenedAdditiveTo'} // $na);
            }
            elsif (defined $calc_debug_info_ref->{'HotThreadWLDampeningApplied'}) # Checked but not applied
            {
                printf {$fh} "    - Hot Thread Dampening                  : Not Applied (Details: %s)\n", ($calc_debug_info_ref->{'HotThreadWLConditionsString'} // "Conditions not met");
            }

            printf {$fh} "    - Volatility Factor Applied             : %s (Reason: %s)\n", ($calc_debug_info_ref->{'VoltFactor'} // $na), ($calc_debug_info_ref->{'VoltFactorReason'} // $na);
            printf {$fh} "    - Pool Factor Applied                   : %s\n", ($calc_debug_info_ref->{'PoolFactor'} // $na);

            if (defined $calc_debug_info_ref->{'AdditiveSafetyCapApplied'} && $calc_debug_info_ref->{'AdditiveSafetyCapApplied'} =~ /^True/)
            {
                printf {$fh} "    - Final Additive Safety Cap             : %s\n", $calc_debug_info_ref->{'AdditiveSafetyCapApplied'};
            }
        }
		printf {$fh} "  => Final Additive CPU                     : %s cores\n", ($calc_debug_info_ref->{'FinalAdditive'} // "0.0000");
		printf {$fh} "  => PhysC after Upsizing                   : %s cores\n\n", ($calc_debug_info_ref->{'PreMaxCpuCapRec'} // $na);


        # --- Section E: Maximum CPU Sizing Sanity Checks ---
        print {$fh} "Section E: Maximum CPU Sizing Sanity Checks\n";
        printf {$fh} "  - Recommendation before Max Sanity Check  : %s cores\n", ($calc_debug_info_ref->{'PreMaxCpuCapRec'} // $na);
        printf {$fh} "  - LPAR MaxCPU (from VM config)            : %s cores\n", $lpar_max_cpu_display_A_log_sec;
        printf {$fh} "  - Entitlement (for forecast multiplier)   : %s cores\n", $entitlement_display_A_log_sec;
        printf {$fh} "  - Forecast Multiplier Used                : %s\n", ($calc_debug_info_ref->{'ForecastMultiplier'} // $na);
        printf {$fh} "  - Effective MaxCPU Sanity Limit           : %s cores\n", ($calc_debug_info_ref->{'EffectiveMaxCPUCap'} // $na);
        printf {$fh} "  - Limited by MaxCPU Sanity Check?         : %s\n", ($calc_debug_info_ref->{'CappedByMaxCPU'} // $na);
        printf {$fh} "  => Recommendation after Max Sanity Check  : %s cores\n\n", ($final_recommendation_unrounded_str // $na);
    } # End else for ReasonForNoModification (main calculation block)

    # --- Footer Block (Bottom Summary) ---
    print {$fh} "----------------------------------------------------------------------\n";
    printf {$fh} "Overall Summary for Profile: %s\n", $profile_being_adjusted;
    printf {$fh} "  Final Recommended Value : %s cores (Unrounded: %s)\n",
    ($final_csv_value_for_profile // $na), # This is rounded for CSV
    $final_recommendation_unrounded_str;    # This is unrounded from calc
    print  {$fh} "  Key Decision Path & Justification:\n";

    # --- Source nfit's GrowthAdj and print detailed rationale if available ---
    my $growth_metrics = $first_nfit_state->{metrics}{growth} || {};
    if (defined $growth_metrics->{adjustment} && looks_like_number($growth_metrics->{adjustment})) {
        $nfit_growth_adj_to_display = sprintf("%.4f", $growth_metrics->{adjustment});
    }

    printf {$fh} "    - Initial Base PhysC                    : %s cores (Aggregated value from nfit)\n", ($base_physc_for_profile // $na);

    if ($first_nfit_state->{debug}{growthRationale}) {
        my $gd = $first_nfit_state->{debug}{growthRationale} || {};
        my $gp = $first_nfit_state->{debug}{growthParams} || {};

        my $format_check_result = sub {
            my $val = shift;
            return "Skipped" if (!defined $val || $val eq 'Skipped');
            return $val ? "Passed" : "Failed";
        };

        printf {$fh} "    - nfit GrowthAdj Applied                : +%s cores\n", $nfit_growth_adj_to_display;
        printf {$fh} "    - Rationale for GrowthAdj (from nfit):\n";
        # Use growth parameters reported by nfit if available, otherwise use this script's defaults.
        my $proj_days = $gp->{projection_days} // $DEFAULT_GROWTH_PROJECTION_DAYS;
        my $max_inf   = $gp->{max_inflation_perc} // $DEFAULT_MAX_GROWTH_INFLATION_PERCENT;

        printf {$fh} "        1. Data Eligibility : %s historical periods found (Min: %d).\n", $gd->{num_hist_periods}, $GROWTH_MIN_HISTORICAL_PERIODS;
        printf {$fh} "        2. Volatility Check : Coefficient of Variation (CV) was %s. Trend analysis proceeds if CV < %.2f. [Result: %s]\n", $gd->{stats_cv}, $GROWTH_MAX_CV_THRESHOLD, $format_check_result->($gd->{cv_check_passed});
        printf {$fh} "        3. Trend Analysis   : Linear regression slope was %s. Trend is considered positive if slope > %.2f. [Result: %s]\n", ($gd->{slope} // 'N/A'), $GROWTH_MIN_POSITIVE_SLOPE_THRESHOLD, $format_check_result->($gd->{slope_check_passed} // 'Skipped');

        if (defined $gd->{slope_check_passed} && $gd->{slope_check_passed} ne 'N/A') {
            printf {$fh} "        4. Projection       : Trend projected to %s cores over %d days.\n", $gd->{projected_val}, $proj_days;
            my $capping_msg = ($gd->{was_capped} eq '1') ? "CAPPED" : "not capped";
            printf {$fh} "        5. Inflation        : Calculated inflation is %s%%. This was %s to the max of %d%%.\n", $gd->{inflation_perc}, $capping_msg, $max_inf;
        }
    }

    my $downsizing_summary_reason_footer = $calc_debug_info_ref->{'DownsizingReason'} // "N/A";
# Let us not abbreviate this, at least not for now.
#    if (length($downsizing_summary_reason_footer) > 70) { $downsizing_summary_reason_footer = substr($downsizing_summary_reason_footer, 0, 67) . "..."; }
	printf {$fh} "    - CPU Downsizing                        : Factor %s -> PhysC became %s. (Summary: %s)\n",
    ($calc_debug_info_ref->{'DownsizingFactor'} // $na),
    ($calc_debug_info_ref->{'DownsizedPhysC'} // $na),
    $downsizing_summary_reason_footer;

    my $additive_final_val_footer = $calc_debug_info_ref->{'FinalAdditive'} // "0.0000";
    my $additive_reason_summary_footer;

    if ($calc_debug_info_ref->{'IsRunQPressure'} eq "True" || $calc_debug_info_ref->{'IsWorkloadPressure'} eq "True") {
        $additive_reason_summary_footer = "Pressure detected";
        my @pressure_types_footer;
        if ($calc_debug_info_ref->{'IsRunQPressure'} eq "True") { push @pressure_types_footer, "Overall LPAR"; }
        if ($calc_debug_info_ref->{'IsWorkloadPressure'} eq "True") { push @pressure_types_footer, "Normalized Workload"; }
        if (@pressure_types_footer) { $additive_reason_summary_footer .= " (" . join(", ", @pressure_types_footer) . ")";}

        if (defined $calc_debug_info_ref->{'HotThreadWLDampeningApplied'} && $calc_debug_info_ref->{'HotThreadWLDampeningApplied'} eq "True")
        {
            $additive_reason_summary_footer .= "; HTW Dampened";
        }
        if (defined $calc_debug_info_ref->{'AdditiveSafetyCapApplied'} && $calc_debug_info_ref->{'AdditiveSafetyCapApplied'} =~ /^True/) {
            $additive_reason_summary_footer .= "; Safety Capped";
        }
        # Add note if additive ended up zero despite pressure
        if (abs(($additive_final_val_footer // 0)+0) < $FLOAT_EPSILON && ($calc_debug_info_ref->{'IsRunQPressure'} eq "True" || $calc_debug_info_ref->{'IsWorkloadPressure'} eq "True")) {
            $additive_reason_summary_footer .= "; Final Additive Zero (due to caps/dampening)";
        }
    }
    else {
        # This path is taken only when no pressure flags were set.
        if (defined $calc_debug_info_ref->{'ExcessThreads'} && $calc_debug_info_ref->{'ExcessThreads'} =~ /No excess threads/i) {
             $additive_reason_summary_footer = "No excess threads calculated for additive.";
         }
        else {
             $additive_reason_summary_footer = "No significant pressure detected.";
        }
    }
    printf {$fh} "    - CPU Upsizing (Additive)               : %s cores. (Reason: %s)\n",
    $additive_final_val_footer, $additive_reason_summary_footer;

    my $lpar_cap_applied_summary_footer = "Not applied or N/A";
    if (defined $calc_debug_info_ref->{'CappedByMaxCPU'})
    {
        $lpar_cap_applied_summary_footer = $calc_debug_info_ref->{'CappedByMaxCPU'} eq "True"
        ? "Applied (Effective Limit: " . ($calc_debug_info_ref->{'EffectiveMaxCPUCap'} // $na) . ")"
        : "Not Applied (Effective Limit: " . ($calc_debug_info_ref->{'EffectiveMaxCPUCap'} // $na) . ")";
    }
    printf {$fh} "    - Max CPU Sanity Check                  : %s\n", $lpar_cap_applied_summary_footer;
    print {$fh} "======================================================================\n\n";

}
# End of log_profile_rationale

# --- Helper subroutine to parse and collate percentile lists for nfit calls ---
# Takes an existing list of percentiles (as a comma-separated string) and an array
# of numeric percentiles that must be ensured to be present in the final list.
# Returns a sorted, unique array of percentile strings, formatted for nfit.
sub parse_and_collate_percentiles
{
    my ($existing_perc_list_str, @ensure_these_percs_numeric) = @_;
    my %final_percs_map; # Use a hash to store unique percentiles to avoid duplicates

    # Add percentiles from the existing string (e.g., from profile flags or global nfit-profile default)
    if (defined $existing_perc_list_str && $existing_perc_list_str ne '')
    {
        my @raw_list = split /,\s*/, $existing_perc_list_str;
        foreach my $p_str (@raw_list)
        {
            next if $p_str eq ''; # Skip empty strings that might result from split (e.g. "80,,90")

            # Validate if it looks like a percentile number, then format it consistently for nfit
            if ($p_str =~ /^[0-9.]+$/ && $p_str >= 0 && $p_str <= 100)
            {
                my $p_label = sprintf("%.2f", $p_str + 0); # Normalize format (e.g., "90.00")
                $p_label =~ s/\.?0+$//;                    # Clean trailing ".00" (e.g., "90")
                $p_label = "0" if $p_label eq "" && abs(($p_str+0) - 0) < 0.001; # Handle case of "0.00" -> "0"
                $final_percs_map{$p_label} = 1; # Add to hash (value 1 is arbitrary, key is what matters)
            }
            else
            {
                # If it's not a simple number, it might be an invalid value.
                # nfit will ultimately validate it. For now, include as is.
                # Alternatively, one could issue a warning here:
                # warn "Warning: Non-standard percentile string '$p_str' found in list '$existing_perc_list_str'. Passing to nfit as is.\n";
                $final_percs_map{$p_str} = 1;
            }
        }
    }

    # Add percentiles that must be ensured (e.g., P90 for AbsRunQ, P50/P90 for NormRunQ)
    foreach my $p_num (@ensure_these_percs_numeric)
    {
        my $p_label = sprintf("%.2f", $p_num); # Format, e.g., 90 -> "90.00", 98.5 -> "98.50"
        $p_label =~ s/\.?0+$//;                # Clean to "90", "98.5"
        $p_label = "0" if $p_label eq "" && abs($p_num - 0) < 0.001; # "0.00" -> "0"
        $final_percs_map{$p_label} = 1; # Add/overwrite in hash to ensure it's present
    }

    # Return a numerically sorted list of unique percentile strings
    my @sorted_keys = sort {
        # Robust sort: treat as numbers if possible, otherwise string compare
        my $is_a_num = ($a =~ /^[0-9.]+$/); # Check if $a looks like a number
        my $is_b_num = ($b =~ /^[0-9.]+$/); # Check if $b looks like a number
        if ($is_a_num && $is_b_num) { return ($a+0) <=> ($b+0); } # Both are numbers, numeric sort
        elsif ($is_a_num) { return -1; } # Numbers come before non-numbers
        elsif ($is_b_num) { return 1;  } # Non-numbers come after numbers
        else { return $a cmp $b; }       # Both are non-numbers (e.g. invalid values), string compare
    } keys %final_percs_map; # Get unique keys from hash and sort them

    return @sorted_keys;
}

# --- quote_csv ---
# Ensures a string is properly quoted for CSV output, escaping internal double quotes.
sub quote_csv {
    my ($field) = @_;
    if (!defined $field) # Handle undefined fields as empty strings
    {
        $field = '';
    }
    $field =~ s/"/""/g; # Escape any double quotes within the field by doubling them
    return qq/"$field"/; # Enclose the entire field in double quotes
}

# --- load_profile_definitions ---
# Loads profile configurations from the specified INI-like file.
# Each section [Profile Name] defines a profile.
# Keys: nfit_flags (mandatory), runq_modifier_behavior (optional, default: 'default').
sub load_profile_definitions
{
    my ($filepath) = @_;
    my @loaded_profiles_list;
    my $current_section_name = undef; # Name of the current [Profile] being parsed
    my $line_number = 0;

    open my $fh, '<:encoding(utf8)', $filepath
        or die "Error: Cannot open profiles config file '$filepath': $!\n";

    while (my $line = <$fh>)
    {
        $line_number++;
        chomp $line;
        $line =~ s/\s*#.*//;    # Remove comments starting with #
        $line =~ s/\s*;.*//;    # Remove comments starting with ; (alternative comment)
        $line =~ s/^\s+|\s+$//g; # Trim leading/trailing whitespace
        next if $line eq '';    # Skip empty lines or lines that became empty after comment removal

        if ($line =~ /^\s*\[\s*([^\]]+?)\s*\]\s*$/) # Matches section header like [Profile Name]
        {
            # Before starting a new section, check if the previous one was potentially incomplete
            if (defined $current_section_name && @loaded_profiles_list &&
                $loaded_profiles_list[-1]{name} eq $current_section_name &&
                !defined $loaded_profiles_list[-1]{flags} && # Flags are mandatory
                (!defined $loaded_profiles_list[-1]{runq_behavior} || $loaded_profiles_list[-1]{runq_behavior} eq 'default') )
            {
                my $is_incomplete = 1;
                # A profile needs flags to be valid. If flags were somehow defined but this check missed it,
                # this might be overly cautious, but missing flags is the primary concern.
                if (defined $loaded_profiles_list[-1]{flags})
                {
                    $is_incomplete = 0;
                }
                if ($is_incomplete)
                {
                    warn "Warning: Profile section '[$current_section_name]' in '$filepath' (ending before line $line_number) appears incomplete (missing nfit_flags). Skipping this potentially malformed entry.\n";
                    pop @loaded_profiles_list; # Remove the incomplete profile
                }
            }

            $current_section_name = $1; # Capture profile name from section header
            $current_section_name =~ s/^\s+|\s+$//g; # Trim spaces from profile name
            if ($current_section_name eq '')
            {
                warn "Warning: Empty section name in '$filepath' at line $line_number. Skipping.\n";
                $current_section_name = undef; # Reset current section context
                next;
            }
            # Initialize the new profile with its name and default runq_behavior. Flags added later.
            push @loaded_profiles_list, { name => $current_section_name, flags => undef, runq_behavior => 'default' };
        }
        elsif (defined $current_section_name && $line =~ /^\s*([^=]+?)\s*=\s*(.+)$/) # Matches key-value pair like nfit_flags = ...
        {
            my $key = lc($1); # Key name (lowercase for case-insensitivity)
            my $value = $2;   # Value
            $key =~ s/^\s+|\s+$//g;   # Trim spaces from key
            $value =~ s/^\s+|\s+$//g; # Trim spaces from value

            # Ensure we are adding to the currently parsed profile
            if (@loaded_profiles_list && $loaded_profiles_list[-1]{name} eq $current_section_name)
            {
                if ($key eq 'nfit_flags')
                {
                    if (defined $loaded_profiles_list[-1]{flags}) # Check for duplicate flags
                    {
                        warn "Warning: Duplicate 'nfit_flags' for profile '[$current_section_name]' in '$filepath' (around line $line_number). Using last one defined.\n";
                    }
                    $loaded_profiles_list[-1]{flags} = $value;
                }
                elsif ($key eq 'runq_modifier_behavior')
                {
                    if (defined $loaded_profiles_list[-1]{runq_behavior} && $loaded_profiles_list[-1]{runq_behavior} ne 'default')
                    { # Check for duplicate runq_behavior definition
                        warn "Warning: Duplicate 'runq_modifier_behavior' for profile '[$current_section_name]' in '$filepath' (around line $line_number). Using last one defined.\n";
                    }
                    # Validate the value for runq_modifier_behavior
                    if ($value eq 'additive_only' || $value eq 'default')
                    {
                        $loaded_profiles_list[-1]{runq_behavior} = $value;
                    }
                    else
                    {
                        warn "Warning: Invalid value '$value' for 'runq_modifier_behavior' in profile '[$current_section_name]' (line $line_number). Using default. Allowed: additive_only, default.\n";
                        $loaded_profiles_list[-1]{runq_behavior} = 'default'; # Fallback to default if invalid value
                    }
                }
                # Add other potential profile-specific keys here if needed in the future
                # else { warn "Warning: Unknown key '$key' in profile '[$current_section_name]' ..."; }
            }
            else # Key-value pair found outside a valid section or for a mismatched section
            {
                warn "Warning: Key-value pair '$key=$value' found for invalid or unexpected section '$current_section_name' in '$filepath' (line $line_number). Ensure section was defined correctly. Skipping.\n";
            }
        }
        elsif ($line ne '') # Non-empty line that doesn't match section or key-value format
        {
            warn "Warning: Unparseable line $line_number in profiles config '$filepath': $line\n";
        }
    } # End while loop reading file
    close $fh;

    # Final validation of loaded profiles: ensure all valid profiles have 'nfit_flags'
    my @valid_profiles;
    foreach my $p_ref (@loaded_profiles_list)
    {
        if (defined $p_ref->{flags} && $p_ref->{flags} ne '')
        {
            # --- Mandate that the special P-99W1 profile is unfiltered ---
            if ($p_ref->{name} eq $MANDATORY_PEAK_PROFILE_FOR_HINT) {
                # Check for any time-filtering flags. The regex looks for the flags as distinct words.
                if ($p_ref->{flags} =~ /\s-(?:online|batch|no-weekends)\b/) {
                    die "ERROR: The mandatory profile '[$MANDATORY_PEAK_PROFILE_FOR_HINT]' in '$filepath' must not contain any time-filtering flags " .
                        "(-online, -batch, -no-weekends).\n" .
                        "       This profile is used to determine the true, unfiltered Peak value for the entire dataset.\n" .
                        "       Please remove the time-filtering flag from this specific profile definition.\n";
                }
            }

            # runq_behavior is already defaulted or set during parsing
            push @valid_profiles, $p_ref;
        }
        else # Profile is missing the mandatory 'nfit_flags'
        {
            warn "Warning: Profile '[" . $p_ref->{name} . "]' in '$filepath' is missing mandatory 'nfit_flags' definition. Removing this profile from processing.\n";
        }
    }
    return @valid_profiles;
}

# --- calculate_graduated_burst ---
# Returns graduated burst allowance based on entitlement size
# Small entitlements get higher burst allowance to reflect real-world burst capability
sub calculate_graduated_burst {
    my ($entitlement) = @_;

    if ($entitlement < 0.25) {
        return 0.50;  # 50% burst for micro-partitions
    } elsif ($entitlement < 0.50) {
        return 0.40;  # 40% burst for very small
    } elsif ($entitlement < 1.00) {
        return 0.30;  # 30% burst for small
    } else {
        return 0.25;  # 25% standard burst
    }
}

# --- calculate_structural_availability_factor ---
# For small entitlements, estimates the effective availability of vCPU threads
# based on entitlement size. Scales from 0.5 at Ent=0.1 to 0.95 at Ent=0.9
sub calculate_structural_availability_factor {
    my ($entitlement) = @_;

    # Ensure entitlement is in valid range
    return 0.5 if $entitlement <= 0.1;
    return 0.95 if $entitlement >= 0.9;

    # Linear scaling: 0.5 + (0.45 * entitlement)
    return 0.5 + (0.45 * $entitlement);
}

# --- apply_minimum_lcpu_floor ---
# Applies a minimum LCPU floor based on SMT level
# Prevents unrealistic near-zero LCPU calculations for micro-partitions
sub apply_minimum_lcpu_floor {
    my ($effective_lcpus, $smt_used) = @_;

    # Ensure at least 1 full hardware thread for micro-partitions
    # Scale slightly with SMT to be platform-aware
    my $min_floor = max(1, int($smt_used / 4));  # At SMT=8, floor=2; at SMT=4, floor=1

    return max($min_floor, $effective_lcpus);
}

# --- calculate_runq_modified_physc (with enhanced efficiency logic and detailed debug output) ---
# Calculates the final PhysC value for a profile after applying efficiency factors
# and RunQ-driven additive CPU adjustments.
# Takes the raw PhysC from nfit, RunQ metrics, SMT, entitlement, MaxCPU, etc.
# Returns the adjusted PhysC value and a hash of debug information for logging.
# --- calculate_runq_modified_physc (refactor: decoupled C & D, no early-returns, adds RunQ_Uncapped) ---
sub calculate_runq_modified_physc
{
    my (
        $selected_tier_physc_value_str,
        $norm_runq_p25_str,
        $norm_runq_p50_str,
        $norm_runq_p75_str,
        $norm_runq_p90_str,
        $abs_runq_p_value_str,
        $abs_runq_key_for_debug,
        $smt_used,
        $current_entitlement_str,
        $max_cpu_config_str,
        $is_in_non_default_pool,
        $profile_runq_behavior_setting,
        $p99w1_overall_vm_has_abs_runq_pressure,
        $p99w1_overall_vm_has_norm_runq_pressure,
        # NEW: Adaptive threshold parameters
        $adaptive_runq_saturation_thresh,
        $adaptive_target_norm_runq,
        $adaptive_max_efficiency_reduction
    ) = @_;

    my %debug_info;
    my $na_str = "N/A";

    # --- Initialize all debug fields to sensible defaults or N/A ---
    $debug_info{'AbsRunQKeyUsed'} = $abs_runq_key_for_debug // 'N/A (key not provided)';
    $debug_info{'AbsRunQValueUsedForCalc'} = $abs_runq_p_value_str;
    $debug_info{'BasePhysC'} = $selected_tier_physc_value_str // $na_str;
    $profile_runq_behavior_setting //= 'default';

    my $base_physc = ($selected_tier_physc_value_str ne $na_str && $selected_tier_physc_value_str =~ /^-?[0-9.]+$/)
        ? ($selected_tier_physc_value_str + 0)
        : undef;

    # Efficiency related fields - meticulously initialized
    $debug_info{'DownsizingReason'} = "Efficiency calculation not initiated or skipped by initial guards.";
    $debug_info{'DownsizingFactor'} = "1.00"; # final sprintf form later
    $debug_info{'EffCondNormP50Met'} = undef;
    $debug_info{'EffCondVolatilityMet'} = undef;
    $debug_info{'EffVolatilityRatio'} = $na_str;
    $debug_info{'EffPBase'} = (defined $base_physc) ? sprintf("%.4f", $base_physc) : $na_str;
    $debug_info{'EffSMTValue'} = $smt_used // $na_str;
    $debug_info{'EffTargetNormRunQ'} = $na_str;
    $debug_info{'EffPEfficientTargetRaw'} = $na_str;
    $debug_info{'EffBlendReason'} = "Blending not applied or not applicable.";
    $debug_info{'EffBlendWeightBase'} = $na_str;
    $debug_info{'EffBlendWeightTarget'} = $na_str;
    $debug_info{'EffPEfficientTarget'} = $na_str;
    $debug_info{'EffComparisonBaseVsTargetMet'} = undef;
    $debug_info{'EffPotentialReduction'} = $na_str;
    $debug_info{'EffMaxAllowableReductionPerc'} = $adaptive_max_efficiency_reduction * 100;
    $debug_info{'EffReductionCapReason'} = "Default reduction cap applied.";
    $debug_info{'EffMaxAllowableReductionCores'} = $na_str;
    $debug_info{'EffActualReductionCores'} = $na_str;
    $debug_info{'EffCalculatedFactor'} = "1.0000";
    $debug_info{'EffFinalFactorApplied'} = "1.00";

    # General fields
    $debug_info{'DownsizedPhysC'} = defined($base_physc) ? sprintf("%.4f", $base_physc) : $na_str;
    $debug_info{'RunQ_Uncapped'} = "0.0000"; # <-- new downsizing-only strategic field
    $debug_info{'RunQPressure_P90_Val'} = 0; $debug_info{'IsRunQPressure'} = "False";
    $debug_info{'IsWorkloadPressure'} = "False"; $debug_info{'WorkloadPressureReason'} = "Conditions not met or N/A inputs";
    $debug_info{'EffectiveLCPUsAtBase'} = $na_str; $debug_info{'ExcessThreads'} = $na_str;
    $debug_info{'RawAdditive'} = "0.0000"; $debug_info{'MaxAdditiveCap'} = "0.0000"; $debug_info{'CappedRawAdditive'} = "0.0000";
    $debug_info{'VoltFactorReason'} = "Default (no overriding condition met or additive not applied)";
    $debug_info{'VoltFactor'} = "1.00";
    $debug_info{'PoolFactor'} = "1.00";
    $debug_info{'FinalAdditive'} = "0.0000";
    $debug_info{'PreMaxCpuCapRec'} = $debug_info{'DownsizedPhysC'};
    $debug_info{'LPARMaxCPUConfig'} = ($max_cpu_config_str ne "" && $max_cpu_config_str =~ /^[0-9.]+$/ && ($max_cpu_config_str+0) > 0)
        ? ($max_cpu_config_str+0) : $na_str;
    $debug_info{'EntitlementForForecast'} = (defined $current_entitlement_str && $current_entitlement_str ne "" && $current_entitlement_str =~ /^-?[0-9.]+$/)
        ? ($current_entitlement_str + 0) : 0;
    $debug_info{'ForecastMultiplier'} = $na_str; $debug_info{'EffectiveMaxCPUCap'} = $na_str;
    $debug_info{'CappedByMaxCPU'} = $na_str;
    $debug_info{'FinalAdjustedPhysC'} = $debug_info{'BasePhysC'};
    $debug_info{'ReasonForNoModification'} = "";

    my $curr_ent_numeric = $debug_info{'EntitlementForForecast'};
    my $eff_p_base_numeric = defined($base_physc) ? $base_physc : undef;
    $debug_info{'EffPBase'} = defined($eff_p_base_numeric) ? sprintf("%.4f", $eff_p_base_numeric) : $na_str;

    unless (defined $base_physc)
    {
        $debug_info{'ReasonForNoModification'} = "BasePhysC for profile not numeric or N/A";
        $debug_info{'FinalAdjustedPhysC'} = $selected_tier_physc_value_str // $na_str;
        return ($selected_tier_physc_value_str // $na_str, \%debug_info);
    }

    my $norm_p50_numeric = ($norm_runq_p50_str ne $na_str && $norm_runq_p50_str =~ /^-?[0-9.]+$/) ? ($norm_runq_p50_str + 0) : undef;
    my $norm_p90_numeric = ($norm_runq_p90_str ne $na_str && $norm_runq_p90_str =~ /^-?[0-9.]+$/) ? ($norm_runq_p90_str + 0) : undef;
    my $abs_runq_p_numeric  = ($abs_runq_p_value_str  ne $na_str && $abs_runq_p_value_str  =~ /^-?[0-9.]+$/) ? ($abs_runq_p_value_str + 0)  : undef;
    $debug_info{'NormRunQ_P90_Val'} = (defined $norm_p90_numeric) ? sprintf("%.2f", $norm_p90_numeric) : $na_str;

    my $max_cpu_for_lpar_numeric = ($debug_info{'LPARMaxCPUConfig'} ne $na_str) ? $debug_info{'LPARMaxCPUConfig'} : 0;
    my $norm_p25_numeric = ($norm_runq_p25_str ne $na_str && $norm_runq_p25_str =~ /^-?[0-9.]+$/) ? ($norm_runq_p25_str + 0) : undef;
    my $norm_p75_numeric = ($norm_runq_p75_str ne $na_str && $norm_runq_p75_str =~ /^-?[0-9.]+$/) ? ($norm_runq_p75_str + 0) : undef;

    my $normrunq_iqrc_val = undef;
    if (defined $norm_p25_numeric && defined $norm_p50_numeric && defined $norm_p75_numeric)
    {
        my $p50_denominator_for_iqrc = (abs($norm_p50_numeric) > $MIN_P50_DENOMINATOR_FOR_VOLATILITY)
            ? $norm_p50_numeric
            : (($norm_p50_numeric >= 0) ? $MIN_P50_DENOMINATOR_FOR_VOLATILITY : -$MIN_P50_DENOMINATOR_FOR_VOLATILITY);
        if (abs($p50_denominator_for_iqrc) > $FLOAT_EPSILON)
        {
            $normrunq_iqrc_val = ($norm_p75_numeric - $norm_p25_numeric) / $p50_denominator_for_iqrc;
        }
        elsif (abs($norm_p75_numeric - $norm_p25_numeric) < $FLOAT_EPSILON)
        {
            $normrunq_iqrc_val = 0.0;
        }
        else
        {
            $normrunq_iqrc_val = 999.0;
        }
    }
    $debug_info{'NormRunQ_P25_Val'} = (defined $norm_p25_numeric) ? sprintf("%.2f", $norm_p25_numeric) : $na_str;
    $debug_info{'NormRunQ_P50_Val'} = (defined $norm_p50_numeric) ? sprintf("%.2f", $norm_p50_numeric) : $na_str;
    $debug_info{'NormRunQ_P75_Val'} = (defined $norm_p75_numeric) ? sprintf("%.2f", $norm_p75_numeric) : $na_str;
    $debug_info{'NormRunQ_IQRC_Val'} = (defined $normrunq_iqrc_val) ? sprintf("%.3f", $normrunq_iqrc_val) : $na_str;

    # =========================
    # Section C: Strategic Efficiency / Downsizing (ALWAYS RUNS)
    # =========================
    my $efficiency_factor_numeric = 1.00;
    $debug_info{'DownsizingFactor'} = sprintf("%.2f", $efficiency_factor_numeric);
    my $base_adjusted_physc = (defined $eff_p_base_numeric ? $eff_p_base_numeric : 0) * $efficiency_factor_numeric;
    $debug_info{'DownsizedPhysC'} = sprintf("%.4f", $base_adjusted_physc);
    $debug_info{'DownsizingReason'} = "Default (No CPU downsizing applied or eligible).";

    # We now always attempt the analytical path to ensure its results are available for the guardrails.
    if (defined $norm_p50_numeric && defined $norm_p90_numeric && defined $abs_runq_p_numeric && $smt_used > 0 && defined $eff_p_base_numeric)
    {
        $debug_info{'EffCondNormP50Met'} = ($norm_p50_numeric < $NORM_P50_THRESHOLD_FOR_EFFICIENCY_CONSIDERATION);

        my $effective_p50_for_volatility = ($norm_p50_numeric > 0.0001)
            ? max($norm_p50_numeric, $MIN_P50_DENOMINATOR_FOR_VOLATILITY)
            : $MIN_P50_DENOMINATOR_FOR_VOLATILITY;

        my $volatility_ratio = (defined $norm_p90_numeric && $effective_p50_for_volatility > 0.0001)
            ? ($norm_p90_numeric / $effective_p50_for_volatility)
            : 1.0;

        $debug_info{'EffVolatilityRatio'} = sprintf("%.2f", $volatility_ratio);
        $debug_info{'EffCondVolatilityMet'} = ($volatility_ratio < $VOLATILITY_CAUTION_THRESHOLD);

        if (!$debug_info{'EffCondVolatilityMet'}) {
            $debug_info{'DownsizingReason'} = sprintf("Skipped CPU Downsizing: Workload volatile (NormP90/P50 ratio %.2f >= %.2f). No analytical reduction.", $volatility_ratio, $VOLATILITY_CAUTION_THRESHOLD);
        }
        elsif (!$debug_info{'EffCondNormP50Met'}) {
            $debug_info{'DownsizingReason'} = sprintf("Skipped CPU Downsizing: NormRunQ P50 (%.2f) not below threshold (%.2f) for efficiency consideration. No analytical reduction.",
                $norm_p50_numeric, $NORM_P50_THRESHOLD_FOR_EFFICIENCY_CONSIDERATION);
        }
        else
        {
            my $target_norm_runq_eff_calc = $adaptive_target_norm_runq;
            my $p_efficient_target_raw = ($smt_used * $target_norm_runq_eff_calc > 0.0001)
                ? ($abs_runq_p_numeric / ($smt_used * $target_norm_runq_eff_calc))
                : $eff_p_base_numeric + 1;
            $debug_info{'EffPEfficientTargetRaw'} = sprintf("%.4f", $p_efficient_target_raw);
            $debug_info{'EffTargetNormRunQ'} = sprintf("%.2f", $target_norm_runq_eff_calc);

            my $base_physc_weight = $BLEND_WEIGHT_BASE_DEFAULT_LOW_P50;
            my $efficient_target_weight = 1.0 - $base_physc_weight;
            my $blending_details_str = sprintf("Default low P50 blend (%.0f%% Base / %.0f%% Target).", $base_physc_weight*100, $efficient_target_weight*100);

            if ($norm_p50_numeric < $NORM_P50_LOW_THRESH_FOR_BLEND1) {
                $base_physc_weight = $BLEND_WEIGHT_BASE_FOR_LOW_P50_1;
                $efficient_target_weight = 1.0 - $base_physc_weight;
                $blending_details_str = sprintf("NormP50 (%.2f) < %.2f, using more aggressive blend (%.0f%% Base / %.0f%% Target).",
                    $norm_p50_numeric, $NORM_P50_LOW_THRESH_FOR_BLEND1, $base_physc_weight*100, $efficient_target_weight*100);
            } elsif ($norm_p50_numeric < $NORM_P50_MODERATE_THRESH_FOR_BLEND2) {
                $base_physc_weight = $BLEND_WEIGHT_BASE_FOR_LOW_P50_2;
                $efficient_target_weight = 1.0 - $base_physc_weight;
                $blending_details_str = sprintf("NormP50 (%.2f) < %.2f, using moderate blend (%.0f%% Base / %.0f%% Target).",
                    $norm_p50_numeric, $NORM_P50_MODERATE_THRESH_FOR_BLEND2, $base_physc_weight*100, $efficient_target_weight*100);
            }
            $debug_info{'EffBlendWeightBase'}   = sprintf("%.2f", $base_physc_weight);
            $debug_info{'EffBlendWeightTarget'} = sprintf("%.2f", $efficient_target_weight);
            $debug_info{'EffBlendReason'}       = $blending_details_str;

            my $blended_efficient_target = ($eff_p_base_numeric * $base_physc_weight) + ($p_efficient_target_raw * $efficient_target_weight);
            $debug_info{'EffPEfficientTarget'}  = sprintf("%.4f", $blended_efficient_target);
            $debug_info{'EffComparisonBaseVsTargetMet'} = ($eff_p_base_numeric > $blended_efficient_target);

            if ($debug_info{'EffComparisonBaseVsTargetMet'})
            {
                my $potential_reduction_cores = $eff_p_base_numeric - $blended_efficient_target;
                $debug_info{'EffPotentialReduction'} = sprintf("%.4f", $potential_reduction_cores);

                my $current_max_reduction_perc_val = $adaptive_max_efficiency_reduction;
                my $reduction_cap_reason_template = "Volatility (%.2f) low, using full reduction cap of %.1f%%.";

                if ($volatility_ratio > $VOLATILITY_MODERATE_HIGH_CAP_THRESH) {
                    $current_max_reduction_perc_val = $adaptive_max_efficiency_reduction * $REDUCTION_CAP_SCALE_FOR_MODERATE_HIGH_VOLATILITY;
                    $reduction_cap_reason_template = "Volatility (%.2f) moderately high (>%.2f), reduction cap limited to %.1f%%.";
                    $debug_info{'EffReductionCapReason'} = sprintf($reduction_cap_reason_template, $volatility_ratio, $VOLATILITY_MODERATE_HIGH_CAP_THRESH, $current_max_reduction_perc_val * 100);
                } elsif ($volatility_ratio > $VOLATILITY_MODERATE_LOW_CAP_THRESH) {
                    $current_max_reduction_perc_val = $adaptive_max_efficiency_reduction * $REDUCTION_CAP_SCALE_FOR_MODERATE_VOLATILITY;
                    $reduction_cap_reason_template = "Volatility (%.2f) moderate (>%.2f), reduction cap limited to %.1f%%.";
                    $debug_info{'EffReductionCapReason'} = sprintf($reduction_cap_reason_template, $volatility_ratio, $VOLATILITY_MODERATE_LOW_CAP_THRESH, $current_max_reduction_perc_val * 100);
                } else {
                    $debug_info{'EffReductionCapReason'} = sprintf($reduction_cap_reason_template, $volatility_ratio, $current_max_reduction_perc_val * 100);
                }

                $debug_info{'EffMaxAllowableReductionPerc'} = $current_max_reduction_perc_val * 100;
                my $max_allowable_reduction_cores = $eff_p_base_numeric * $current_max_reduction_perc_val;
                $debug_info{'EffMaxAllowableReductionCores'} = sprintf("%.4f", $max_allowable_reduction_cores);

                my $actual_reduction_cores = min($potential_reduction_cores, $max_allowable_reduction_cores);
                $actual_reduction_cores = max(0, $actual_reduction_cores);
                $debug_info{'EffActualReductionCores'} = sprintf("%.4f", $actual_reduction_cores);

                # ---- NEW: strategic downsizing-only output for CSV ----
                $debug_info{'RunQ_Uncapped'} = sprintf("%.4f", -1 * ($actual_reduction_cores+0));

                if ($actual_reduction_cores > 0.0001)
                {
                    my $new_physc_after_reduction = $eff_p_base_numeric - $actual_reduction_cores;
                    $efficiency_factor_numeric = ($eff_p_base_numeric > 0.0001) ? ($new_physc_after_reduction / $eff_p_base_numeric) : 1.00;

                    my $min_expected_eff_factor = 1 - $current_max_reduction_perc_val;
                    if ($efficiency_factor_numeric < ($min_expected_eff_factor - 0.001) ) { $efficiency_factor_numeric = $min_expected_eff_factor; }
                    $efficiency_factor_numeric = 1.00 if $efficiency_factor_numeric > 1.00;
                    $efficiency_factor_numeric = max(0, $efficiency_factor_numeric);

                    $debug_info{'EffCalculatedFactor'}   = sprintf("%.4f", $efficiency_factor_numeric);
                    $debug_info{'EffFinalFactorApplied'} = sprintf("%.2f",  $efficiency_factor_numeric);
                    $debug_info{'DownsizingReason'}      = sprintf("Analytical CPU Downsizing (using blended target & dynamic cap): Reduction of %.4f cores applied.", $actual_reduction_cores);
                }
                else
                {
                    $debug_info{'EffFinalFactorApplied'} = "1.00";
                    $debug_info{'EffCalculatedFactor'}   = "1.0000";
                    $debug_info{'DownsizingReason'} = sprintf("Analytical CPU Downsizing (using blended target & dynamic cap): Base_PhysC %.4f, Blended_Target %.4f. Calculated reduction (%.4f) negligible or zero. No adjustment from this path.",
                        $eff_p_base_numeric, $blended_efficient_target, $actual_reduction_cores // 0.0);
                }
            }
            else
            {
                $debug_info{'EffFinalFactorApplied'} = "1.00";
                $debug_info{'EffCalculatedFactor'}   = "1.0000";
                $debug_info{'DownsizingReason'} = sprintf("Analytical CPU Downsizing: Base_PhysC %.4f not greater than Blended_Efficient_Target_PhysC %.4f. No reduction. Blending Reason: %s",
                    $eff_p_base_numeric, $blended_efficient_target, $blending_details_str);
            }
        }
    } else {
        $debug_info{'DownsizingReason'} = "Key metrics (NormP50/P90, AbsRunQ) N/A for full analytical CPU downsizing check. No efficiency adjustment applied.";
    }

    # --- CONSOLIDATED GUARDRAIL AND DECISION BLOCK ---
    my $skip_downsizing_reason;
    if (
        (defined $p99w1_overall_vm_has_abs_runq_pressure && $p99w1_overall_vm_has_abs_runq_pressure) ||
        (defined $p99w1_overall_vm_has_norm_runq_pressure && $p99w1_overall_vm_has_norm_runq_pressure)
    ) {
        $skip_downsizing_reason = sprintf("VM's %s profile shows RunQ pressure.", $MANDATORY_PEAK_PROFILE_FOR_HINT);
    } elsif ($profile_runq_behavior_setting eq 'additive_only') {
       $skip_downsizing_reason = "Profile runq_behavior=additive_only.";
    } elsif (defined $curr_ent_numeric && $curr_ent_numeric > 0 && defined $eff_p_base_numeric && ($eff_p_base_numeric > $curr_ent_numeric)) {
        # --- Entitlement Floor Guard with Single Threaded Dominated (STD) Workload Heuristic Check ---
        # Condition: VM is bursting above its entitlement.

        my $is_std_pattern = (defined $norm_p90_numeric && $norm_p90_numeric < $STD_NORM_P90_THRESH) &&
                             (defined $normrunq_iqrc_val && $normrunq_iqrc_val < $STD_IQRC_THRESH);

        if ($is_std_pattern) {
            # PATTERN MATCHED: This is likely an inefficient workload.
            # Do NOT apply tactical downsizing, but DO populate a dampened strategic signal.
            my $actual_reduction = $debug_info{'EffActualReductionCores'};

            # note: handle potential "N/A" string
            my $actual_reduction_num = (defined $actual_reduction && looks_like_number($actual_reduction)) ? ($actual_reduction + 0) : 0;
            my $dampened_reduction = $actual_reduction_num * $STD_DAMPENING_FACTOR_FOR_UNCAPPED;

            $debug_info{'RunQ_Uncapped'} = sprintf("%.4f", -$dampened_reduction);
            $skip_downsizing_reason = sprintf(
                "Single-Threaded Dominant (STD) Workload Pattern Detected (NormP90<%.1f, IQRC<%.1f). Skipping tactical downsizing, updating strategic RunQ_Uncapped with %.0f%% dampening.",
                $STD_NORM_P90_THRESH, $STD_IQRC_THRESH, $STD_DAMPENING_FACTOR_FOR_UNCAPPED * 100
            );
        } else {
            # PATTERN NOT MATCHED: This is a genuine, volatile burst. Block all downsizing.
            $skip_downsizing_reason = sprintf("Base PhysC (%.4f) > Entitlement (%.2f) and workload pattern does not suggest inefficiency.", $eff_p_base_numeric, $curr_ent_numeric);
            # Ensure RunQ_Uncapped is zero for these legitimate bursts.
            $debug_info{'RunQ_Uncapped'} = "0.0000";
        }
    }

    if (defined $skip_downsizing_reason) {
        $base_adjusted_physc = $eff_p_base_numeric;
        # If downsizing is skipped for any reason, the effective factor is 1.00.
        $efficiency_factor_numeric = 1.0;
        $debug_info{'DownsizingReason'} = "Skipped Applying Downsizing: " . $skip_downsizing_reason;
    } else {
        $base_adjusted_physc = $eff_p_base_numeric * $efficiency_factor_numeric;
        if ($efficiency_factor_numeric < 1 - 1e-4) {
            $debug_info{'DownsizingReason'} = "Analytical CPU Downsizing applied.";
        }
    }

    # Now, definitively set the final factor that was actually used.
    $debug_info{'DownsizingFactor'} = sprintf("%.2f", $efficiency_factor_numeric);
    $debug_info{'DownsizedPhysC'} = sprintf("%.4f", $base_adjusted_physc);

    # =========================
    # Section D: Tactical Additive CPU (ALWAYS RUNS; no early return)
    # =========================
    my $runq_pressure_p_val = 0;
    my $effective_lcpus_for_pressure_calc = 0;
    my $pressure_basis_rationale = "N/A";

    # --- Calculate graduated burst allowance based on entitlement size ---
    my $burst_allowance_raw;
    if ($curr_ent_numeric > 0 && $curr_ent_numeric < 1.0 && $max_cpu_for_lpar_numeric > $curr_ent_numeric) {
        # Use graduated burst for small uncapped LPARs
        $burst_allowance_raw = calculate_graduated_burst($curr_ent_numeric);
    } else {
        # Use standard burst factor for larger entitlements or capped LPARs
        $burst_allowance_raw = $ENTITLEMENT_BURST_ALLOWANCE_FACTOR;
    }
    my $burst_allowance_clamped = max($BURST_ALLOWANCE_MIN_FACTOR, min($BURST_ALLOWANCE_MAX_FACTOR, $burst_allowance_raw));
    my $burst_was_clamped = (abs($burst_allowance_raw - $burst_allowance_clamped) > 1e-9);

    if ($max_cpu_for_lpar_numeric > 0 && $smt_used > 0)
    {
        # 1. Calculate the MaxCPU-based capacity candidate
        my $l_conf = $max_cpu_for_lpar_numeric * $smt_used;

        # 2. For uncapped VMs (where MaxCPU > Entitlement), use enhanced calculation
        if ($curr_ent_numeric > 0 && $max_cpu_for_lpar_numeric > $curr_ent_numeric) {

            # Special handling for small entitlements < 1
            if ($curr_ent_numeric < 1.0) {
                # Calculate structural availability based on hardware thread allocation behaviour
                my $structural_factor = calculate_structural_availability_factor($curr_ent_numeric);
                my $l_structural = 1.0 * $smt_used * $structural_factor;

                # Calculate burst-tolerant entitlement with graduated burst
                my $l_ent_burst = ($curr_ent_numeric * (1 + $burst_allowance_clamped)) * $smt_used;

                # Use the maximum of structural and burst calculations for small entitlements
                # This acknowledges hardware thread availability whilst remaining conservative
                my $l_small_ent = max($l_structural, $l_ent_burst);

                # Choose the minimum of MaxCPU capacity and small entitlement calculation
                $effective_lcpus_for_pressure_calc = min($l_conf, $l_small_ent);

                # Apply minimum floor for safety
                my $pre_floor_value = $effective_lcpus_for_pressure_calc;
                $effective_lcpus_for_pressure_calc = apply_minimum_lcpu_floor($effective_lcpus_for_pressure_calc, $smt_used);

                # Build detailed rationale string
                $pressure_basis_rationale = sprintf(
                    "Small Entitlement Handler (Ent=%.2f): max(Structural: 1vCPUÃ%d SMTÃ%.2f factor=%.1f, Burst: %.2fÃ(1+%.2f)Ã%d SMT=%.1f) capped by MaxCPUÃSMT=%.0f â %.0f LCPUs",
                    $curr_ent_numeric,
                    $smt_used, $structural_factor, $l_structural,
                    $curr_ent_numeric, $burst_allowance_clamped, $smt_used, $l_ent_burst,
                    $l_conf,
                    $effective_lcpus_for_pressure_calc
                );

                if ($pre_floor_value < $effective_lcpus_for_pressure_calc) {
                    $pressure_basis_rationale .= sprintf(" [Floor applied: raised from %.1f to %d LCPUs]",
                        $pre_floor_value, $effective_lcpus_for_pressure_calc);
                }

                if ($burst_was_clamped) {
                    $pressure_basis_rationale .= sprintf(" [Burst %.2fâ%.2f]", $burst_allowance_raw, $burst_allowance_clamped);
                }

            } else {
                # Standard calculation for entitlements >= 1
                my $l_ent_burst = ($curr_ent_numeric * (1 + $burst_allowance_clamped)) * $smt_used;

                # Choose the minimum of the two candidates
                $effective_lcpus_for_pressure_calc = min($l_conf, $l_ent_burst);

                # Build the detailed rationale string showing the comparison
                $pressure_basis_rationale = sprintf("Burst-Tolerant Entitlement (min(%.2f vCPUÃSMT=%d=%.0f, %.2f ENTÃ(1+%.2f burst)ÃSMT=%d=%.0f) â %.0f LCPUs)",
                    $max_cpu_for_lpar_numeric, $smt_used, $l_conf,
                    $curr_ent_numeric, $burst_allowance_clamped, $smt_used, $l_ent_burst,
                    $effective_lcpus_for_pressure_calc
                );
                if ($burst_was_clamped) {
                    $pressure_basis_rationale .= sprintf(" [Note: Original burst factor %.2f was clamped to %.2f]", $burst_allowance_raw, $burst_allowance_clamped);
                }
            }
        } else {
            # For capped VMs, the basis is simply MaxCPU-based capacity
            if ($max_cpu_for_lpar_numeric > 0) {
                $effective_lcpus_for_pressure_calc = $l_conf;
                $pressure_basis_rationale = sprintf("MaxCPU basis (%.2f vCPU * %d SMT = %.0f LCPUs)",
                    $max_cpu_for_lpar_numeric, $smt_used, $effective_lcpus_for_pressure_calc);
            } elsif ($curr_ent_numeric > 0) {
                $effective_lcpus_for_pressure_calc = $curr_ent_numeric * $smt_used;
                $pressure_basis_rationale = sprintf("Entitlement fallback (%.2f ENT * %d SMT = %.0f LCPUs)",
                    $curr_ent_numeric, $smt_used, $effective_lcpus_for_pressure_calc);
            } else {
                $effective_lcpus_for_pressure_calc = 0;
                $pressure_basis_rationale = "No valid capacity basis (MaxCPU/Entitlement missing)";
            }
        }

        if ($effective_lcpus_for_pressure_calc > 0) {
            my $absq = (defined $abs_runq_p_numeric && looks_like_number($abs_runq_p_numeric)) ? $abs_runq_p_numeric : 0;
            $runq_pressure_p_val = $absq / $effective_lcpus_for_pressure_calc;
        } else {
            $runq_pressure_p_val = 0; # Safe default if no capacity basis is found
            $pressure_basis_rationale = "No valid capacity basis found (MaxCPU/Entitlement missing or zero)";
        }
    }
    $debug_info{'RunQPressure_P90_Val'} = sprintf("%.4f", $runq_pressure_p_val);
    my $is_runq_pressure = ($runq_pressure_p_val > $adaptive_runq_saturation_thresh);
    $debug_info{'PressureBasisRationale'} = $pressure_basis_rationale;
    $debug_info{'BurstAllowanceUsed'} = sprintf("%.2f%% (graduated for Ent=%.2f)" ,
        $burst_allowance_clamped * 100, $curr_ent_numeric);
    $debug_info{'SmallEntitlementHandler'} = ($curr_ent_numeric < 1.0 && $max_cpu_for_lpar_numeric > $curr_ent_numeric)
        ? "Yes" : "No";
    $debug_info{'IsRunQPressure'} = $is_runq_pressure ? "True" : "False";
    # Log the enhanced burst and small entitlement details
    $debug_info{'EffectiveLCPUsForPressure'} = sprintf("%.1f", $effective_lcpus_for_pressure_calc);

    my $is_workload_pressure_calc = 0;
    my $workload_pressure_reason_str_calc = "Workload pressure conditions not met or inputs N/A.";
    my $min_absrunq_for_workload_pressure_check = $smt_used;

    if (defined $norm_p90_numeric)
    {
        if ($norm_p90_numeric > $WORKLOAD_PRESSURE_NORM_P90_TRIGGER_THRESHOLD)
        {
            if (defined $abs_runq_p_numeric && $abs_runq_p_numeric >= $min_absrunq_for_workload_pressure_check)
            {
                $is_workload_pressure_calc = 1;
                $workload_pressure_reason_str_calc = sprintf("NormRunQ P90 (%.2f) > threshold (%.2f) AND AbsRunQ (%s=%.2f) >= SMT-based min threshold (%.2f)",
                    $norm_p90_numeric, $WORKLOAD_PRESSURE_NORM_P90_TRIGGER_THRESHOLD,
                    $debug_info{AbsRunQKeyUsed}, $abs_runq_p_numeric,
                    $min_absrunq_for_workload_pressure_check);
            }
            else
            {
                $workload_pressure_reason_str_calc = sprintf("NormRunQ P90 (%.2f) > threshold (%.2f), BUT AbsRunQ (%s=%.2f) < SMT-based min threshold (%.2f). Workload Pressure NOT flagged.",
                    $norm_p90_numeric, $WORKLOAD_PRESSURE_NORM_P90_TRIGGER_THRESHOLD,
                    $debug_info{AbsRunQKeyUsed}, $abs_runq_p_numeric // $na_str,
                    $min_absrunq_for_workload_pressure_check);
            }
        }
        else
        {
            $workload_pressure_reason_str_calc = sprintf("NormRunQ P90 (%.2f) <= threshold (%.2f)",
                $norm_p90_numeric, $WORKLOAD_PRESSURE_NORM_P90_TRIGGER_THRESHOLD);
        }
    }
    else
    {
        $workload_pressure_reason_str_calc = "NormRunQ P90 N/A";
    }
    $debug_info{'IsWorkloadPressure'} = $is_workload_pressure_calc ? "True" : "False";
    $debug_info{'WorkloadPressureReason'} = $workload_pressure_reason_str_calc;

    my $apply_additive_logic = ($is_runq_pressure || $is_workload_pressure_calc);
    my $additive_cpu = 0.0;
    my $raw_additive_cpu = 0.0;
    my $max_additive_cap_sliding = 0.0;
    my $capped_raw_additive_val = 0.0;
    my $volatility_confidence_factor = 1.0;
    my $pool_confidence_factor = 1.0;

    if ($apply_additive_logic && defined $smt_used && $smt_used > 0)
    {
        # The tolerance calculation for excess threads must use the same entitlement-aware
        # LCPU basis that was used to detect the pressure in the first place.
        $debug_info{'EffectiveLCPUsAtBase'} = sprintf("%.4f", $effective_lcpus_for_pressure_calc); # Log the correct basis
        my $capacity_threshold_for_excess = $RUNQ_ADDITIVE_TOLERANCE_FACTOR * $effective_lcpus_for_pressure_calc;
        my $excess_threads = (defined $abs_runq_p_numeric ? $abs_runq_p_numeric : 0) - $capacity_threshold_for_excess;

        if ($excess_threads > 0)
        {
            $debug_info{'ExcessThreads'} = sprintf("%.4f (AbsRunQ %s %.2f > Tolerated Capacity %.2f based on %.1f x %.4f LCPUs from %s)",
                $excess_threads,
                $debug_info{AbsRunQKeyUsed}, ($abs_runq_p_numeric // 0.0),
                $capacity_threshold_for_excess,
                $RUNQ_ADDITIVE_TOLERANCE_FACTOR,
                $effective_lcpus_for_pressure_calc,
                ($pressure_basis_rationale =~ /Burst-Tolerant/ ? "Entitlement+Burst" : "MaxCPU"));
            $raw_additive_cpu = $excess_threads / $smt_used if $smt_used > 0;

            if ($curr_ent_numeric < $MAX_ADD_ENT_THRESH1)       { $max_additive_cap_sliding = $MAX_ADD_ABS_VAL1; }
            elsif ($curr_ent_numeric < $MAX_ADD_ENT_THRESH2)    { $max_additive_cap_sliding = $curr_ent_numeric * $MAX_ADD_PERC_VAL2; }
            elsif ($curr_ent_numeric < $MAX_ADD_ENT_THRESH3)    { $max_additive_cap_sliding = $curr_ent_numeric * $MAX_ADD_PERC_VAL3; }
            else                                                { $max_additive_cap_sliding = $curr_ent_numeric * $MAX_ADD_PERC_VAL_ELSE; }
            if ($curr_ent_numeric == 0 && $max_additive_cap_sliding == 0) { $max_additive_cap_sliding = $MAX_ADD_ABS_VAL1; }

            $capped_raw_additive_val = min($raw_additive_cpu, $max_additive_cap_sliding);

            # --- Hot-thread workload dampening (unchanged maths) ---
            my $apply_hot_thread_wl_dampening = 0;
            my $hot_thread_wl_conditions_met_str = "";
            my $final_dynamic_dampening_factor = 1.0;

            $debug_info{'HotThreadWLDampeningApplied'} = "False";
            $debug_info{'HotThreadWLConditionsString'} = "N/A";
            $debug_info{'HotThreadWLDynamicFactor'} = "N/A";
            $debug_info{'HotThreadWLDampenedAdditiveFrom'} = "N/A";
            $debug_info{'HotThreadWLDampenedAdditiveTo'} = "N/A";

            if ($capped_raw_additive_val > $FLOAT_EPSILON)
            {
                my @htw_conditions_met_details;
                my $htw_condition_count = 0;
                if ($is_workload_pressure_calc) {
                    push @htw_conditions_met_details, "HighNormP90"; $htw_condition_count++;
                }
                my $cond2_underutilized = 0;
                if (defined $base_physc && $base_physc > $FLOAT_EPSILON)
                {
                    my $underutilized_vs_ent    = (defined $curr_ent_numeric && $curr_ent_numeric > $FLOAT_EPSILON && $base_physc < ($curr_ent_numeric * $HOT_THREAD_WL_ENT_FACTOR));
                    my $underutilized_vs_maxcpu = (defined $max_cpu_for_lpar_numeric && $max_cpu_for_lpar_numeric > $FLOAT_EPSILON && $base_physc < ($max_cpu_for_lpar_numeric * $HOT_THREAD_WL_MAXCPU_FACTOR));
                    if ($underutilized_vs_ent || $underutilized_vs_maxcpu)
                    {
                        $cond2_underutilized = 1;
                        my $detail_ent_str = $underutilized_vs_ent ? sprintf("BaseP(%.2f)<Ent(%.2f)*%.1f", $base_physc, $curr_ent_numeric // 0, $HOT_THREAD_WL_ENT_FACTOR) : "";
                        my $detail_max_str = $underutilized_vs_maxcpu ? sprintf("BaseP(%.2f)<MaxP(%.2f)*%.1f", $base_physc, $max_cpu_for_lpar_numeric // 0, $HOT_THREAD_WL_MAXCPU_FACTOR) : "";
                        push @htw_conditions_met_details, "UnderutilizedCap(" . join(" or ", grep { $_ ne "" } $detail_ent_str, $detail_max_str) . ")";
                        $htw_condition_count++;
                    }
                }
                if (defined $norm_p50_numeric && $norm_p50_numeric > $HOT_THREAD_WL_HIGH_NORM_P50_THRESHOLD) {
                    push @htw_conditions_met_details, sprintf("HighNormP50(%.2f>%.1f)", $norm_p50_numeric, $HOT_THREAD_WL_HIGH_NORM_P50_THRESHOLD);
                    $htw_condition_count++;
                }
                if (!$is_runq_pressure && defined $debug_info{'RunQPressure_P90_Val'} && $debug_info{'RunQPressure_P90_Val'} ne $na_str) {
                    push @htw_conditions_met_details, sprintf("NoLPARRunQSat(AbsPVal:%.2f<%.1f)", ($debug_info{'RunQPressure_P90_Val'} + 0), $RUNQ_PRESSURE_P90_SATURATION_THRESHOLD);
                    $htw_condition_count++;
                }
                if (defined $normrunq_iqrc_val && abs($normrunq_iqrc_val) > $HOT_THREAD_WL_IQRC_THRESHOLD) {
                    push @htw_conditions_met_details, sprintf("HighIQRC(%.2f>%.1f)", $normrunq_iqrc_val, $HOT_THREAD_WL_IQRC_THRESHOLD);
                    $htw_condition_count++;
                }
                $hot_thread_wl_conditions_met_str = @htw_conditions_met_details ? join("; ", @htw_conditions_met_details) : "No specific conditions met";

                if ($htw_condition_count >= $HOT_THREAD_WL_DETECTION_MIN_CONDITIONS_MET)
                {
                    $apply_hot_thread_wl_dampening = 1;
                    my $util_ratio = (defined $max_cpu_for_lpar_numeric && $max_cpu_for_lpar_numeric > $FLOAT_EPSILON && defined $base_physc)
                        ? ($base_physc / $max_cpu_for_lpar_numeric) : 1.0;
                    my $util_damp_multiplier = max(0.1, min(1.0, $util_ratio));
                    my $iqrc_damp_multiplier = (defined $normrunq_iqrc_val)
                        ? min(1.0, 1.0 / (1.0 + abs($normrunq_iqrc_val)))
                        : 1.0;
                    my $base_physc_severity_multiplier = (defined $base_physc && $base_physc > $FLOAT_EPSILON)
                        ? min(1.0, $base_physc / 1.0)
                        : 0.1;
                    my $calculated_dynamic_damp_factor = $HOT_THREAD_WL_BASE_DAMPENING_FACTOR *
                        $util_damp_multiplier *
                        $iqrc_damp_multiplier *
                        $base_physc_severity_multiplier;

                    my $final_dynamic_dampening_factor_calc = max($HOT_THREAD_WL_MIN_DYNAMIC_DAMPENING,
                        min($HOT_THREAD_WL_MAX_DYNAMIC_DAMPENING, $calculated_dynamic_damp_factor));
                    my $original_additive_val = $capped_raw_additive_val;
                    $capped_raw_additive_val *= $final_dynamic_dampening_factor_calc;

                    $debug_info{'HotThreadWLDampeningApplied'} = "True";
                    $debug_info{'HotThreadWLConditionsString'} = $hot_thread_wl_conditions_met_str;
                    $debug_info{'HotThreadWLDynamicFactor'} = sprintf("%.4f (Base:%.2f UtilM:%.2f IqrcM:%.2f SevM:%.2f -> RawCalc:%.4f)",
                        $final_dynamic_dampening_factor_calc,
                        $HOT_THREAD_WL_BASE_DAMPENING_FACTOR,
                        $util_damp_multiplier,
                        $iqrc_damp_multiplier,
                        $base_physc_severity_multiplier,
                        $calculated_dynamic_damp_factor);
                    $debug_info{'HotThreadWLDampenedAdditiveFrom'} = sprintf("%.4f", $original_additive_val);
                    $debug_info{'HotThreadWLDampenedAdditiveTo'}   = sprintf("%.4f", $capped_raw_additive_val);
                    $debug_info{'CappedRawAdditive'} = sprintf("%.4f", $capped_raw_additive_val);
                }
                else
                {
                    $debug_info{'HotThreadWLDampeningApplied'} = "False";
                    $debug_info{'HotThreadWLConditionsString'} = sprintf("Conditions not met for HTW dampening (%d/%d met: %s).",
                        $htw_condition_count, 5, $hot_thread_wl_conditions_met_str);
                }
            }

            # Volatility / pool confidence factors (original behaviour)
            if ($is_runq_pressure)
            {
                $volatility_confidence_factor = $RUNQ_PRESSURE_SATURATION_CONFIDENCE_FACTOR;
                $debug_info{'VoltFactorReason'} = sprintf("RunQPressure Saturation (Factor set to %.2f)", $RUNQ_PRESSURE_SATURATION_CONFIDENCE_FACTOR);
            }
            elsif ($is_workload_pressure_calc && defined $norm_p50_numeric && defined $norm_p90_numeric && $norm_p90_numeric > 0.01)
            {
                my $volatility_ratio_for_factor = ($norm_p50_numeric > 0.01)
                    ? ($norm_p90_numeric / $norm_p50_numeric)
                    : 999;
                if ($volatility_ratio_for_factor < $VOLATILITY_SPIKY_THRESHOLD)      { $volatility_confidence_factor = $VOLATILITY_SPIKY_FACTOR; }
                elsif ($volatility_ratio_for_factor < $VOLATILITY_MODERATE_THRESHOLD){ $volatility_confidence_factor = $VOLATILITY_MODERATE_FACTOR; }
                else                                                                 { $volatility_confidence_factor = 1.0; }
                $debug_info{'VoltFactorReason'} = sprintf("Calculated (NormRQ P90/P50 ratio %.2f for WorkloadPressure -> Factor %.2f)", $volatility_ratio_for_factor, $volatility_confidence_factor);
            }
            else
            {
                $debug_info{'VoltFactorReason'} = "Additive logic applied, but conditions for specific Volatility Factor adjustment not met (e.g., WorkloadPressure False or P50/P90 N/A for ratio). Using default factor.";
            }

            $additive_cpu = $capped_raw_additive_val * $volatility_confidence_factor;
            if ($is_in_non_default_pool && $additive_cpu > 0)
            {
                $pool_confidence_factor = $POOL_CONSTRAINT_CONFIDENCE_FACTOR;
                $additive_cpu *= $pool_confidence_factor;
            }
        }
        else
        {
            $debug_info{'ExcessThreads'} = sprintf("0.0000 (No excess above tolerated capacity of %.2f from %s; AbsRunQ %s was %.2f)",
                $capacity_threshold_for_excess,
                ($pressure_basis_rationale =~ /Burst-Tolerant/ ? "Entitlement+Burst" : "MaxCPU"),
                $debug_info{AbsRunQKeyUsed}, ($abs_runq_p_numeric // $na_str));
            $raw_additive_cpu = 0.0;
            $capped_raw_additive_val = 0.0;
            $additive_cpu = 0.0;
            $debug_info{'VoltFactorReason'} = "No excess threads, so no additive CPU calculated.";
        }
    }
    else
    {
        $debug_info{'ExcessThreads'} = "N/A (Additive logic not applied as no significant pressure detected or missing inputs)";
        $debug_info{'VoltFactorReason'} = "Additive logic not applied.";
    }
    $debug_info{'RawAdditive'}       = sprintf("%.4f", $raw_additive_cpu);
    $debug_info{'MaxAdditiveCap'}    = sprintf("%.4f", $max_additive_cap_sliding);
    $debug_info{'CappedRawAdditive'} = sprintf("%.4f", $capped_raw_additive_val);
    $debug_info{'VoltFactor'}        = sprintf("%.2f", $volatility_confidence_factor);
    $debug_info{'PoolFactor'}        = sprintf("%.2f", $pool_confidence_factor);

    # --- Safety hard-cap for additive (unchanged logic) ---
    my $original_additive_before_safety_cap = $additive_cpu;
    my $additive_safety_cap_applied_reason = "Not applied";
    my $safety_cap_from_base = (defined $eff_p_base_numeric && $eff_p_base_numeric > $FLOAT_EPSILON)
        ? ($eff_p_base_numeric * $ADDITIVE_CPU_SAFETY_CAP_FACTOR_OF_BASE)
        : $ADDITIVE_CPU_SAFETY_CAP_ABSOLUTE;
    my $final_safety_cap_value = min($ADDITIVE_CPU_SAFETY_CAP_ABSOLUTE, $safety_cap_from_base);
    $final_safety_cap_value = max(0.0, $final_safety_cap_value);
    if ($additive_cpu > $final_safety_cap_value)
    {
        $additive_cpu = $final_safety_cap_value;
        $additive_safety_cap_applied_reason = sprintf("True: Additive CPU hard-capped to %.4f (min(%.2f abs_cap, %.2f * BaseP=%.4f))",
            $additive_cpu,
            $ADDITIVE_CPU_SAFETY_CAP_ABSOLUTE,
            $ADDITIVE_CPU_SAFETY_CAP_FACTOR_OF_BASE,
            $eff_p_base_numeric // 0.0);
    }
    else
    {
        $additive_safety_cap_applied_reason = sprintf("False (Additive %.4f within hard cap of %.4f)",
            $original_additive_before_safety_cap,
            $final_safety_cap_value);
    }
    $debug_info{'AdditiveSafetyCapApplied'} = $additive_safety_cap_applied_reason;
    $debug_info{'FinalAdditive'} = sprintf("%.4f", $additive_cpu);

    # =========================
    # Final Synthesis + MaxCPU (same behaviour)
    # =========================
    my $calculated_demand = $base_adjusted_physc + $additive_cpu;
    my $runq_modified_rec = $calculated_demand;
    $debug_info{'PreMaxCpuCapRec'} = sprintf("%.4f", $runq_modified_rec);

    my $forecast_multiplier_val = 1.25;
    if ($curr_ent_numeric < 0.5)    { $forecast_multiplier_val = 2.5; }
    elsif ($curr_ent_numeric < 1.0) { $forecast_multiplier_val = 2.0; }
    elsif ($curr_ent_numeric < 2.0) { $forecast_multiplier_val = 1.75; }
    elsif ($curr_ent_numeric < 4.0) { $forecast_multiplier_val = 1.5; }
    $debug_info{'ForecastMultiplier'} = $forecast_multiplier_val;

    my $effective_max_cpu_cap_val = ($max_cpu_for_lpar_numeric > 0) ? ($max_cpu_for_lpar_numeric * $forecast_multiplier_val) : undef;
    $debug_info{'EffectiveMaxCPUCap'} = defined($effective_max_cpu_cap_val) ? sprintf("%.4f", $effective_max_cpu_cap_val) : $na_str;

    if (defined $effective_max_cpu_cap_val && $calculated_demand > $effective_max_cpu_cap_val)
    {
        $runq_modified_rec = $effective_max_cpu_cap_val;
        $debug_info{'CappedByMaxCPU'} = "True";
    }
    else
    {
        $debug_info{'CappedByMaxCPU'} = (defined $effective_max_cpu_cap_val) ? "False" : "N/A (No LPAR MaxCPU for cap check or MaxCPU not exceeded)";
    }

    if ($runq_modified_rec < 0) { $runq_modified_rec = 0; }
    $debug_info{'FinalAdjustedPhysC'} = sprintf("%.4f", $runq_modified_rec);

    return ($runq_modified_rec, \%debug_info);
}

# --- generate_sizing_hint (Unified Global Pressure Detection with Logging Rationale) ---
# Generates a sizing tier hint, pattern, and overall pressure indication for a VM.
# Also returns a detailed rationale string for its pressure assessment, AND
# specific boolean flags for P-99W1's RunQ pressure conditions.
sub generate_sizing_hint
{
    my %args = @_;
    # Unpack arguments, including the log file handle reference.
    my $LOG_FH                       = $args{'log_fh_ref'};
    my $results_ref                  = $args{'results_ref'};
    my $vm                           = $args{'vm'};
    my $config_ref                   = $args{'config_ref'};
    my $max_cpu_for_vm_numeric       = $args{'max_cpu_for_vm_numeric'} // 0;
    my $smt_used_for_vm_numeric      = $args{'smt_used_for_vm_numeric'} // 0;
    my $per_profile_runq_metrics_ref = $args{'per_profile_runq_metrics_ref'};
    my $adaptive_saturation_thresh   = $args{'adaptive_saturation_thresh'};

    # $MANDATORY_PEAK_PROFILE_FOR_HINT should be globally available or passed.
    # Example: my $MANDATORY_PEAK_PROFILE_FOR_HINT = "P-99W1"; (defined outside)

    my $na_str_hint = "N/A";
    my @global_pressure_rationale_lines;

    # ... (initial part of rationale logging: Inputs like MaxCPU, SMT etc. - as in previous version) ...
    push @global_pressure_rationale_lines, sprintf("  Input LPAR MaxCPU            : %.2f cores", $max_cpu_for_vm_numeric);
    push @global_pressure_rationale_lines, sprintf("  Input SMT for VM             : %d", $smt_used_for_vm_numeric);
    # Determine if small entitlement handler would be active
    my $vm_entitlement = $config_ref->{'entitlement'} // 0;
    my $is_small_ent_handler = ($vm_entitlement > 0 && $vm_entitlement < 1.0 &&
                                 $max_cpu_for_vm_numeric > $vm_entitlement) ? "Yes" : "No";
    my $burst_used = ($vm_entitlement < 1.0 && $is_small_ent_handler eq "Yes") ?
                     calculate_graduated_burst($vm_entitlement) : 0.25;

    push @global_pressure_rationale_lines, sprintf("  Small Entitlement Handler    : %s", $is_small_ent_handler);
    push @global_pressure_rationale_lines, sprintf("  Burst Allowance Factor       : %.0f%% (Ent=%.2f)",
                                                   $burst_used * 100, $vm_entitlement);

    # --- VIO Server Check ---
    my $is_vio_server = 0;
    if (defined $config_ref &&
        defined $config_ref->{'systemtype'} &&
        $config_ref->{'systemtype'} =~ /VIO Server/i)
    {
        $is_vio_server = 1;
    }

    # --- Profile Value Parsing (Pattern/Peakiness & P-99W1 PhysC) ---
    my $o3_val_str = $results_ref->{$vm}{'O3-95W15'} // "0";
    my $o3_val_num = ($o3_val_str ne $na_str_hint && $o3_val_str =~ /^-?[0-9.]+\z/) ? ($o3_val_str + 0) : 0;
    my $b3_val_str = $results_ref->{$vm}{'B3-95W15'} // "0";
    my $b3_val_num = ($b3_val_str ne $na_str_hint && $b3_val_str =~ /^-?[0-9.]+\z/) ? ($b3_val_str + 0) : 0;
    my $g3_val_str = $results_ref->{$vm}{'G3-95W15'} // "0";
    my $g3_val_num = ($g3_val_str ne $na_str_hint && $g3_val_str =~ /^-?[0-9.]+\z/) ? ($g3_val_str + 0) : 0;
    my $p99w1_physc_val_str = $results_ref->{$vm}{$MANDATORY_PEAK_PROFILE_FOR_HINT} // $na_str_hint;
    my $p99w1_physc_val_num = ($p99w1_physc_val_str ne $na_str_hint && $p99w1_physc_val_str =~ /^-?[0-9.]+\z/)
    ? ($p99w1_physc_val_str + 0)
    : 0;
    push @global_pressure_rationale_lines, sprintf("  Input %s PhysC Value     : %s (from nfit output for %s)", $MANDATORY_PEAK_PROFILE_FOR_HINT, $p99w1_physc_val_str, $MANDATORY_PEAK_PROFILE_FOR_HINT);

    # (Pattern and Peakiness logic - unchanged)
    my $suggested_pattern = "G";
    if ($b3_val_num > 0.01 && $o3_val_num > ($b3_val_num * $PATTERN_RATIO_THRESHOLD)) { $suggested_pattern = "O"; }
    elsif ($o3_val_num > 0.01 && $b3_val_num > ($o3_val_num * $PATTERN_RATIO_THRESHOLD)) { $suggested_pattern = "B"; }
    my $peakiness_ratio = ($g3_val_num > 0.001) ? ($p99w1_physc_val_num / $g3_val_num) : 0;
    my $shape_descriptor = "Steady";
    if ($peakiness_ratio >= $HIGH_PEAK_RATIO_THRESHOLD) { $shape_descriptor = "Very Peaky"; }
    elsif ($peakiness_ratio >= $LOW_PEAK_RATIO_THRESHOLD) { $shape_descriptor = "Moderately Peaky"; }


    # --- Unified Pressure Detection ---
    my $pressure_detected_maxcpu_limit = 0;
    # Specific P-99W1 RunQ pressure flags to be returned
    my $p99w1_has_absolute_runq_pressure = 0;
    my $p99w1_has_normalized_runq_pressure = 0;
    my @pressure_points;

    # Fetch P-99W1's specific RunQ metrics
    my $p99w1_abs_runq_p90_val_str = $na_str_hint;
    my $p99w1_norm_runq_p90_val_str = $na_str_hint;
    # ... (logic to fetch $p99w1_abs_runq_p90_val_str and $p99w1_norm_runq_p90_val_str as before) ...
    # ... (logging of these input RunQ values to @global_pressure_rationale_lines as before) ...
    if (defined $per_profile_runq_metrics_ref &&
        exists $per_profile_runq_metrics_ref->{$vm} &&
        exists $per_profile_runq_metrics_ref->{$vm}{$MANDATORY_PEAK_PROFILE_FOR_HINT})
    {
        my $p99w1_metrics_block = $per_profile_runq_metrics_ref->{$vm}{$MANDATORY_PEAK_PROFILE_FOR_HINT}->{metrics};

        if (defined $p99w1_metrics_block && ref($p99w1_metrics_block) eq 'HASH') {
            my $abs_runq_metrics = $p99w1_metrics_block->{runq}{absolute} || {};
            my $norm_runq_metrics = $p99w1_metrics_block->{runq}{normalized} || {};
            $p99w1_abs_runq_p90_val_str = $abs_runq_metrics->{'P90'} // $na_str_hint;
            $p99w1_norm_runq_p90_val_str = $norm_runq_metrics->{'P90'} // $na_str_hint;
        }
    }
    push @global_pressure_rationale_lines, sprintf("  Input %s AbsRunQ P90     : %s threads (from nfit output for %s)", $MANDATORY_PEAK_PROFILE_FOR_HINT, $p99w1_abs_runq_p90_val_str, $MANDATORY_PEAK_PROFILE_FOR_HINT);
    push @global_pressure_rationale_lines, sprintf("  Input %s NormRunQ P90    : %s (from nfit output for %s)", $MANDATORY_PEAK_PROFILE_FOR_HINT, $p99w1_norm_runq_p90_val_str, $MANDATORY_PEAK_PROFILE_FOR_HINT);
    push @global_pressure_rationale_lines, "";


    # 1. MaxCPU Limit Pressure
    # ... (logic as before, sets $pressure_detected_maxcpu_limit, logs to @global_pressure_rationale_lines) ...
    # ... (ensure push @pressure_points, "MaxCPU"; happens if $pressure_detected_maxcpu_limit = 1;) ...
    push @global_pressure_rationale_lines, sprintf("  1. MaxCPU Limit Pressure Check (%s PhysC vs LPAR MaxCPU):", $MANDATORY_PEAK_PROFILE_FOR_HINT);
    # ... (detailed logging lines for MaxCPU check)
    my $maxcpu_limit_calc_threshold = ($max_cpu_for_vm_numeric > 0) ? ($max_cpu_for_vm_numeric * $LIMIT_THRESHOLD_PERC) : 0;
    my $maxcpu_condition_met_str = "FALSE";
    if ($max_cpu_for_vm_numeric > 0 && $p99w1_physc_val_num >= $maxcpu_limit_calc_threshold)
    {
        $pressure_detected_maxcpu_limit = 1;
        $maxcpu_condition_met_str = "TRUE";
        push @pressure_points, "MaxCPU";
    }
    push @global_pressure_rationale_lines, sprintf("     - %s PhysC Value      : %.2f", $MANDATORY_PEAK_PROFILE_FOR_HINT, $p99w1_physc_val_num);
    push @global_pressure_rationale_lines, sprintf("     - LPAR MaxCPU             : %.2f", $max_cpu_for_vm_numeric);
    push @global_pressure_rationale_lines, sprintf("     - Threshold (>= %.0f%%)      : %.2f cores", $LIMIT_THRESHOLD_PERC * 100, $maxcpu_limit_calc_threshold);
    push @global_pressure_rationale_lines, sprintf("     - Condition Met           : (%.2f >= %.2f) -> %s", $p99w1_physc_val_num, $maxcpu_limit_calc_threshold, $maxcpu_condition_met_str);
    push @global_pressure_rationale_lines, sprintf("     - MaxCPU Pressure Flag    : %s", $maxcpu_condition_met_str);
    push @global_pressure_rationale_lines, "";


    # 2. Absolute RunQ Pressure (using P-99W1's AbsRunQ_P90)
    # ... (logic as before, sets $p99w1_has_absolute_runq_pressure, logs to @global_pressure_rationale_lines) ...
    # ... (ensure push @pressure_points, ... happens if $p99w1_has_absolute_runq_pressure = 1;) ...
    my $p99w1_abs_runq_p90_num = ($p99w1_abs_runq_p90_val_str ne $na_str_hint && $p99w1_abs_runq_p90_val_str =~ /^-?[0-9.]+$/) ? ($p99w1_abs_runq_p90_val_str + 0) : undef;
    my $calculated_abs_runq_pressure_ratio = 0;
    my $lpar_max_lcpu_capacity = ($max_cpu_for_vm_numeric > 0 && $smt_used_for_vm_numeric > 0) ? ($max_cpu_for_vm_numeric * $smt_used_for_vm_numeric) : 0;
    push @global_pressure_rationale_lines, sprintf("  2. Absolute RunQ Pressure Check (%s AbsRunQ P90 vs LPAR Capacity):", $MANDATORY_PEAK_PROFILE_FOR_HINT);
    # ... (detailed logging lines for AbsRunQ check)
    if (defined $p99w1_abs_runq_p90_num && $lpar_max_lcpu_capacity > 0)
    {
        $calculated_abs_runq_pressure_ratio = $p99w1_abs_runq_p90_num / $lpar_max_lcpu_capacity;
    }
    my $absrunq_cond_met_str = "FALSE";
    if ($calculated_abs_runq_pressure_ratio > $adaptive_saturation_thresh)
    {
        $p99w1_has_absolute_runq_pressure = 1; # Set the specific flag
        $absrunq_cond_met_str = "TRUE";
        push @pressure_points, sprintf("RunQAbs_%s(P90=%.2f)", $MANDATORY_PEAK_PROFILE_FOR_HINT, $calculated_abs_runq_pressure_ratio);
    }
    push @global_pressure_rationale_lines, sprintf("     - %s AbsRunQ P90      : %s threads", $MANDATORY_PEAK_PROFILE_FOR_HINT, $p99w1_abs_runq_p90_val_str);
    push @global_pressure_rationale_lines, sprintf("     - LPAR Max LCPU Capacity  : (%.2f MaxCPU * %d SMT) = %.2f threads", $max_cpu_for_vm_numeric, $smt_used_for_vm_numeric, $lpar_max_lcpu_capacity);
    push @global_pressure_rationale_lines, sprintf("     - Calculated Ratio        : %.4f", $calculated_abs_runq_pressure_ratio);
    push @global_pressure_rationale_lines, sprintf("     - Threshold               : > %.2f", $adaptive_saturation_thresh);
    push @global_pressure_rationale_lines, sprintf("     - Condition Met           : (%.4f > %.2f) -> %s", $calculated_abs_runq_pressure_ratio, $adaptive_saturation_thresh, $absrunq_cond_met_str);
    push @global_pressure_rationale_lines, sprintf("     - %s Specific Absolute RunQ Pressure Flag: %s", $MANDATORY_PEAK_PROFILE_FOR_HINT, $absrunq_cond_met_str);
    push @global_pressure_rationale_lines, "";


    # 3. Normalised Workload Pressure (using P-99W1's NormRunQ_P90)
    # ... (logic as before, sets $p99w1_has_normalized_runq_pressure, logs to @global_pressure_rationale_lines) ...
    # ... (ensure push @pressure_points, ... happens if $p99w1_has_normalized_runq_pressure = 1;) ...
    my $p99w1_norm_runq_p90_num = ($p99w1_norm_runq_p90_val_str ne $na_str_hint && $p99w1_norm_runq_p90_val_str =~ /^-?[0-9.]+$/) ? ($p99w1_norm_runq_p90_val_str + 0) : undef;
    my $min_abs_runq_for_norm_check = $smt_used_for_vm_numeric > 0 ? $smt_used_for_vm_numeric : 1.0;
    push @global_pressure_rationale_lines, sprintf("  3. Normalised Workload Pressure Check (%s NormRunQ P90):", $MANDATORY_PEAK_PROFILE_FOR_HINT);
    # ... (detailed logging lines for NormRunQ check)
    my $normrunq_cond1_met_str = (defined $p99w1_norm_runq_p90_num && $p99w1_norm_runq_p90_num > $WORKLOAD_PRESSURE_NORM_P90_TRIGGER_THRESHOLD) ? "TRUE" : "FALSE";
    my $normrunq_cond2_met_str = (defined $p99w1_abs_runq_p90_num && $p99w1_abs_runq_p90_num >= $min_abs_runq_for_norm_check) ? "TRUE" : "FALSE";
    my $normrunq_overall_cond_met_str = "FALSE";
    if ($normrunq_cond1_met_str eq "TRUE" && $normrunq_cond2_met_str eq "TRUE")
    {
        $p99w1_has_normalized_runq_pressure = 1; # Set the specific flag
        $normrunq_overall_cond_met_str = "TRUE";
        push @pressure_points, sprintf("RunQNorm_%s(P90=%.2f)", $MANDATORY_PEAK_PROFILE_FOR_HINT, defined $p99w1_norm_runq_p90_num ? $p99w1_norm_runq_p90_num : 0);
    }
    push @global_pressure_rationale_lines, sprintf("     - %s NormRunQ P90     : %s", $MANDATORY_PEAK_PROFILE_FOR_HINT, $p99w1_norm_runq_p90_val_str);
    push @global_pressure_rationale_lines, sprintf("     - Threshold               : > %.2f", $WORKLOAD_PRESSURE_NORM_P90_TRIGGER_THRESHOLD);
    push @global_pressure_rationale_lines, sprintf("     - Condition Met (Norm)    : (%s > %.2f) -> %s", $p99w1_norm_runq_p90_val_str, $WORKLOAD_PRESSURE_NORM_P90_TRIGGER_THRESHOLD, $normrunq_cond1_met_str);
    push @global_pressure_rationale_lines, sprintf("     - %s AbsRunQ P90      : %s (for magnitude check)", $MANDATORY_PEAK_PROFILE_FOR_HINT, $p99w1_abs_runq_p90_val_str);
    push @global_pressure_rationale_lines, sprintf("     - Min AbsRunQ Threshold   : %.2f (typically SMT)", $min_abs_runq_for_norm_check);
    push @global_pressure_rationale_lines, sprintf("     - Condition Met (Mag)     : (%s >= %.2f) -> %s", $p99w1_abs_runq_p90_val_str, $min_abs_runq_for_norm_check, $normrunq_cond2_met_str);
    push @global_pressure_rationale_lines, sprintf("     - %s Specific Normalised Workload Pressure Flag: %s", $MANDATORY_PEAK_PROFILE_FOR_HINT, $normrunq_overall_cond_met_str);
    push @global_pressure_rationale_lines, "";


    # --- Combine pressure flags & Pool Context ---
    my $overall_pressure_detected_for_csv = $pressure_detected_maxcpu_limit ||
    $p99w1_has_absolute_runq_pressure || # Use the specific P-99W1 flags here for overall CSV flag
    $p99w1_has_normalized_runq_pressure;
    # Pool context logic, using @pressure_points
    push @global_pressure_rationale_lines, "  4. Pool Context:";

    # Get Pool ID from nfit's dynamic data, and Pool Name from the optional config file
    my $pool_id_from_nmon = $per_profile_runq_metrics_ref->{$vm}{$MANDATORY_PEAK_PROFILE_FOR_HINT}->{metadata}{poolId} // 0;
    my $pool_name_from_config = (defined $config_ref && defined $config_ref->{pool_name}) ? $config_ref->{pool_name} : undef;

    # A non-default pool is one where the ID is not 0.
    my $is_non_default_pool = (looks_like_number($pool_id_from_nmon) && $pool_id_from_nmon != 0);

    # Pool ID is now directly from the JSON output of nfit
    push @global_pressure_rationale_lines, sprintf("     - Pool ID                 : %s", $pool_id_from_nmon);
    push @global_pressure_rationale_lines, sprintf("     - Pool Name (from config) : %s", $pool_name_from_config // "N/A");
    push @global_pressure_rationale_lines, sprintf("     - Is Non-Default Pool     : %s", $is_non_default_pool ? "TRUE" : "FALSE");

    if ($is_non_default_pool && $overall_pressure_detected_for_csv) {
        # If we have a pool name, use it. Otherwise, fall back to the ID.
        my $pool_identifier = defined($pool_name_from_config) && $pool_name_from_config ne '' ? $pool_name_from_config : "ID:$pool_id_from_nmon";
        push @pressure_points, "Pool($pool_identifier)";
    }
    push @global_pressure_rationale_lines, "";

    my $overall_pressure_detected_for_csv_str = $overall_pressure_detected_for_csv ? "TRUE" : "FALSE";
    my $pressure_detail_str = @pressure_points ? join(", ", @pressure_points) : "None";
    push @global_pressure_rationale_lines, sprintf("  5. Overall Global Hint Pressure Flag (for CSV): %s", $overall_pressure_detected_for_csv_str);
    push @global_pressure_rationale_lines, sprintf("  6. Final PressureDetail string for CSV     : \"%s\"", $pressure_detail_str);

    my $global_pressure_rationale_text = "Section G: Global Sizing Hint Pressure Assessment (source: generate_sizing_hint)\n" .
    join("\n", @global_pressure_rationale_lines);


    # --- Tiering logic (remains unchanged) ---
    my $initial_tier_range_str = "3/4";
    # --- Tiering logic (with added safety checks for numeric values) ---
    my $p99w1_val_for_tier = $results_ref->{$vm}{'P-99W1'};
    my $g3_val_for_tier    = $results_ref->{$vm}{'G3-95W15'};

    if (looks_like_number($p99w1_val_for_tier) && looks_like_number($g3_val_for_tier) && $g3_val_for_tier > 0) {
        if ($p99w1_val_for_tier > $g3_val_for_tier * $HIGH_PEAK_RATIO_THRESHOLD) {
            $shape_descriptor = "Very Peaky";
        }
        elsif ($p99w1_val_for_tier > $g3_val_for_tier * $LOW_PEAK_RATIO_THRESHOLD) {
            $shape_descriptor = "Moderately Peaky";
        }
    }

    if ($shape_descriptor eq "Very Peaky") { $initial_tier_range_str = "1/2"; }
    elsif ($shape_descriptor eq "Moderately Peaky") { $initial_tier_range_str = "2/3"; }
    my $adjusted_tier_str = $initial_tier_range_str;
    if ($overall_pressure_detected_for_csv)
    {
        if ($initial_tier_range_str eq "3/4") { $adjusted_tier_str = "3"; }
        elsif ($initial_tier_range_str eq "2/3") { $adjusted_tier_str = "2"; }
        elsif ($initial_tier_range_str eq "1/2") { $adjusted_tier_str = "1"; }
    }
    my $pattern_tier_string = $suggested_pattern . $adjusted_tier_str;

    if ($is_vio_server)
    {
        # For VIOs, we return the *actual* calculated pressure flags and details,
        # but override the final tier/pattern hint for safety and clarity.
        return (
            "P",    # Override Hint to "P" for Peak/Manual
            $shape_descriptor, # Override Pattern
            $overall_pressure_detected_for_csv, # Return ACTUAL calculated pressure
            $pressure_detail_str,               # Return ACTUAL pressure details
            $global_pressure_rationale_text,
            $p99w1_has_absolute_runq_pressure,
            $p99w1_has_normalized_runq_pressure
        );
    }
    else
    {
        # For non-VIOs, return the standard, calculated hint and pattern.
        return (
            $pattern_tier_string,
            $shape_descriptor,
            $overall_pressure_detected_for_csv,
            $pressure_detail_str,
            $global_pressure_rationale_text,
            $p99w1_has_absolute_runq_pressure,
            $p99w1_has_normalized_runq_pressure
        );
    }
}
# end of generate_sizing_hint

# --- parse_percentile_list_for_header ---
# This sub is used by the OLD global RunQ metric collection logic (which is now superseded
# by per-profile RunQ metrics). It might still be called if that logic path is hit,
# or could be refactored/removed if that path is fully deprecated.
# For now, keeping it as it might be used by initial population of $results_table{$vm_name}{$rq_metric_name}.
# It prepares percentile numbers for use as metric name suffixes.
sub parse_percentile_list_for_header
{
    my ($perc_str, $clean_zeros) = @_;
    $clean_zeros = 1 if !defined $clean_zeros; # Default to cleaning "X.00" to "X"
    my @percentiles_cleaned;
    if (defined $perc_str && $perc_str ne '')
    {
        my @raw_percentiles = split /,\s*/, $perc_str;
        foreach my $p (@raw_percentiles)
        {
            if ($p =~ /^[0-9]+(?:\.[0-9]+)?$/ && $p >= 0 && $p <= 100) # Validate numeric and range
            {
                my $p_label = $p;
                if ($clean_zeros)
                {
                    $p_label = sprintf("%.2f", $p); # Format to two decimal places
                    $p_label =~ s/\.?0+$//;         # Remove trailing ".00" or ".0"
                    $p_label = "0" if $p_label eq "" && ($p eq "0" || $p eq "0.00"); # Handle "0.00" -> "0"
                }
                push @percentiles_cleaned, $p_label;
            }
            else # Invalid percentile value
            {
                die "Error: Invalid percentile value '$p' found in list '$perc_str'. Must be numeric between 0 and 100.\n";
            }
        }
    }
    return \@percentiles_cleaned; # Return reference to array of cleaned percentile labels
}

# --- ensure_percentiles_requested ---
# Checks if a list of required percentiles are present in a given percentile string.
# Potentially used for validating if nfit was asked to calculate necessary percentiles
# for the old global RunQ metric collection. May be less relevant with per-profile logic.
sub ensure_percentiles_requested
{
    my ($perc_list_str, @required_percs) = @_; # perc_list_str is comma-separated, required_percs are numbers
    return 1 unless defined $perc_list_str && $perc_list_str ne ''; # If no list provided, assume not applicable or handled elsewhere

    # Parse the provided list string into a map for easy lookup
    my $parsed_percs_ref = parse_percentile_list_for_header($perc_list_str, 0); # Get raw numbers, no zero cleaning for comparison
    my %present_map = map { $_ => 1 } @{$parsed_percs_ref};

    foreach my $req_p_num (@required_percs) # Iterate through numerically required percentiles
    {
        # Check if the numeric value (or its string representation) exists in the parsed list
        my $req_p_str = "$req_p_num"; # Simple string conversion
        my $req_p_str_formatted = sprintf("%.2f", $req_p_num); # e.g. 90.00
        my $req_p_str_cleaned = $req_p_str_formatted;
        $req_p_str_cleaned =~ s/\.?0+$//;
        $req_p_str_cleaned = "0" if $req_p_str_cleaned eq "" && abs($req_p_num -0) < 0.001;


        unless (exists $present_map{$req_p_str} ||
            exists $present_map{$req_p_str_formatted} ||
            exists $present_map{$req_p_str_cleaned} )
        {
            # Check common string representations due to potential formatting differences
            my $found = 0;
            foreach my $key (keys %present_map) {
                if (abs($key - $req_p_num) < 0.001) { # Floating point comparison
                    $found = 1;
                    last;
                }
            }
            return 0 unless $found; # Required percentile not found
        }
    }
    return 1; # All required percentiles found
}

# --- get_nfit_output_dp_from_flags ---
# Determines the number of decimal places nfit is expected to use for a profile's output,
# based on the rounding flags (-r or -u) passed to nfit for that profile.
# This helps nfit-profile format its *own* adjusted values consistently.
sub get_nfit_output_dp_from_flags
{
    my ($nfit_flags_str_for_this_run) = @_; # Combined global and profile-specific flags for nfit

    # Regex to find -r[=increment] or -u[=increment]
    # It captures the increment value if provided.
    if ($nfit_flags_str_for_this_run =~ /-r(?:=(\d*\.\d+))?|-u(?:=(\d*\.\d+))?/)
    {
        my $increment_val_str = $1 // $2; # $1 for -r=val, $2 for -u=val

        # If -r or -u is present but no increment value, nfit uses its default increment.
        if (!(defined $increment_val_str && $increment_val_str ne ""))
        {
            # nfit's default increment is $DEFAULT_ROUND_INCREMENT (from nfit, assumed here to be same as nfit-profile's)
            # For robustness, it's better if nfit-profile knows nfit's default or this is coordinated.
            # Using nfit-profile's default as a proxy.
            return get_decimal_places($DEFAULT_ROUND_INCREMENT);
        }
        else # Increment value was specified
        {
            return get_decimal_places($increment_val_str);
        }
    }
    # If no -r or -u flag, nfit typically outputs with more precision (e.g., 4 decimal places by default internally).
    # nfit version 2.28.0.4 defaults to 4 DP if no rounding.
    return 4;
}

# --- get_decimal_places ---
# Calculates the number of decimal places in a given number string.
sub get_decimal_places
{
    my ($number_str) = @_;
    # Handle scientific notation by converting to fixed point string first
    $number_str = sprintf("%.15f", $number_str) if ($number_str =~ /e/i);

    if ($number_str =~ /\.(\d+)$/) # If there's a decimal part
    {
        return length($1); # Length of the digits after decimal point
    }
    else # No decimal part
    {
        return 0;
    }
}

# Helper to format a percentile number into a clean string for metric keys.
sub clean_perc_label
{
    my ($p) = @_;
    my $label = sprintf("%.2f", $p);
    $label =~ s/\.?0+$//;
    $label = "0" if $label eq "" && abs($p-0)<0.001;
    return $label;
}

# --- parse_nfit_json_output ---
# Parses the multi-line JSON output from nfit. Each line is a distinct JSON object.
# Returns a hash where keys are VM names and values are arrays of the parsed JSON objects (as Perl hashes).
sub parse_nfit_json_output
{
    my ($raw_output) = @_;
    my %parsed_data;
    my $json_decoder = JSON->new->utf8;
    my @lines = split /\n/, $raw_output;

    foreach my $line (@lines)
    {
        next if $line =~ /^\s*$/; # Skip empty lines

        my $decoded_hash = eval { $json_decoder->decode($line) };
        if ($@ || !ref($decoded_hash) eq 'HASH') {
            warn "Warning: Could not decode JSON line from nfit: $line. Error: $@";
            next;
        }

        my $vm_name = $decoded_hash->{vmName};
        if ($vm_name) {
            push @{$parsed_data{$vm_name}}, $decoded_hash;
        }

    }

    return \%parsed_data;
}

# ==============================================================================
# Subroutine to determine the start and end date of a seasonal event period.
# It reads the event's configuration and calculates the absolute date range
# for the analysis.
# It accepts a base_date_obj to correctly calculate periods for
# historical months during an --update-history run.
# ==============================================================================
sub determine_event_period {
    my ($event_config, $base_date_obj_for_recurring) = @_;

    my $model_type = $event_config->{model} // '';
    # Use the provided base date for historical calculations, or default to today for forecasts.
    my $base_date = (defined $base_date_obj_for_recurring) ? $base_date_obj_for_recurring : gmtime()->truncate(to => 'day');

    # --- Path for events defined by fixed 'dates' ---
    if (defined $event_config->{dates}) {
        my @date_ranges = split /\s*,\s*/, $event_config->{dates};
        my ($next_event_start, $next_event_end);

        foreach my $range (@date_ranges) {
            if ($range =~ /(\d{4}-\d{2}-\d{2}):(\d{4}-\d{2}-\d{2})/) {
                my $start_obj = Time::Piece->strptime($1, '%Y-%m-%d')->truncate(to => 'day');
                my $end_obj   = Time::Piece->strptime($2, '%Y-%m-%d')->truncate(to => 'day');

                if (!defined $base_date_obj_for_recurring) {
                    # FORECAST CONTEXT: Find the *next* event period relative to the current date.
                    if ($end_obj >= $base_date) {
                        if (!defined $next_event_start || $start_obj < $next_event_start) {
                            $next_event_start = $start_obj;
                            $next_event_end   = $end_obj;
                        }
                    }
                } else {
                    # HISTORY CONTEXT: Check if this specific fixed-date event falls
                    # within the historical month being processed. The base_date is the
                    # first day of that historical month.
                    my $month_end = Time::Piece->new($base_date->epoch)->add_months(1) - ONE_DAY;
                    if ($start_obj >= $base_date && $end_obj <= $month_end) {
                        return ($start_obj, $end_obj);
                    }
                }
            }
        }
        # In a forecast context, we return the closest future event found.
        return ($next_event_start, $next_event_end) if (defined $next_event_start && !defined $base_date_obj_for_recurring);
    }
    # --- Path for events defined by a recurring 'period' ---
    elsif (defined $event_config->{period} && lc($event_config->{period}) eq 'monthly') {
        my $day_of_period = $event_config->{day_of_period} // -1;
        my $duration_days = $event_config->{duration_days} // 7;

        # FORECAST CONTEXT for 'recency_decay' model: Find the *last completed* peak.
        if ($model_type eq 'recency_decay' && !defined $base_date_obj_for_recurring) {
            my ($current_month_start, $current_month_end) = _get_recurring_monthly_period($base_date, $day_of_period, $duration_days);
            if ($current_month_end > $base_date) {
                my $last_month_base = add_months($base_date, -1);
                return _get_recurring_monthly_period($last_month_base, $day_of_period, $duration_days);
            } else {
                return ($current_month_start, $current_month_end);
            }
        }
        # FORECAST CONTEXT for other models: Find the *next upcoming* peak.
        elsif (!defined $base_date_obj_for_recurring) {
            my ($current_month_start, $current_month_end) = _get_recurring_monthly_period($base_date, $day_of_period, $duration_days);
            if ($current_month_end >= $base_date) {
                return ($current_month_start, $current_month_end);
            } else {
                my $next_month_base = add_months($base_date, 1);
                return _get_recurring_monthly_period($next_month_base, $day_of_period, $duration_days);
            }
        }
        # HISTORY CONTEXT for any recurring model: Calculate the period for the *specific historical month* provided.
        else {
            return _get_recurring_monthly_period($base_date, $day_of_period, $duration_days);
        }
    }

    # Fallback: return nothing if no valid period is found.
    return;
}

# ==============================================================================
# Subroutine to parse a simple INI-style configuration file.
# This replaces the need for the external Config::Tiny module.
#
# Takes:
# 1. The path to the configuration file.
#
# Returns:
# - A hash reference representing the parsed INI data.
# - Dies on file open error.
# ==============================================================================
sub parse_seasonality_config {
    my ($filepath) = @_;
    my %config_data;
    my $current_section = '';

    open my $fh, '<:encoding(utf8)', $filepath
        or die "Error: Cannot open seasonality config file '$filepath': $!";

    while (my $line = <$fh>) {
        chomp $line;
        $line =~ s/^\s+|\s+$//g; # Trim whitespace
        $line =~ s/\s*[#;].*//;   # Remove comments

        next if $line eq ''; # Skip empty or comment-only lines

        if ($line =~ /^\s*\[\s*([^\]]+?)\s*\]\s*$/) {
            # This is a section header
            $current_section = $1;
        } elsif ($current_section ne '' && $line =~ /^\s*([^=]+?)\s*=\s*(.+)$/) {
            # This is a key-value pair within a section
            my $key = $1;
            my $value = $2;
            $key =~ s/^\s+|\s+$//g;
            $value =~ s/^\s+|\s+$//g;
            $config_data{$current_section}{$key} = $value;
        }
    }
    close $fh;

    return \%config_data;
}

# ==============================================================================
# SUBROUTINE: calculate_multiplicative_forecast (NEW ROBUST DESIGN)
# PURPOSE:    Calculates a forecast using the multiplicative seasonal model.
#             This version implements a robust, multi-stage forecast. It
#             intelligently determines the most appropriate recent baseline and
#             applies a recency-weighted seasonal multiplier derived from
#             historical, saturation-corrected peak data. It also includes
#             logic for volatility, residual peak forecasting, and compounding.
# RETURNS:
#   - A hash reference containing the final forecasted results.
#   - A hash reference containing historical data for verbose reporting.
# ==============================================================================
sub calculate_multiplicative_forecast {
    my ($system_cache_dir, $system_identifier, $event_name, $event_config, $full_seasonality_config, $initial_results_table_href) = @_;

    # This subroutine implements a robust, multi-stage forecast for multiplicative seasonal events.
    # It intelligently determines the most appropriate recent baseline and applies a recency-weighted
    # seasonal multiplier derived from historical, saturation-corrected peak data.

    print STDERR "\n--- Applying Multiplicative Seasonal Forecast ---\n";
    print STDERR "Executing robust forecast for event '$event_name' on system '$system_identifier'.\n";

    # --- Configuration Constants ---
    # The number of days of stable, non-peak activity to use for the baseline.
    my $baseline_days_config = $event_config->{baseline_period_days} // 16;
    # A baseline must have at least this many clean days to be considered statistically valid.
    my $MIN_BASELINE_DAYS = 7;
    # Placeholder for future enhancement: A trend adjustment factor could be applied here.
    my $trend_adjustment_factor = 1.0;

    # --- Step 1: Get Key Dates and Read Unified History ---
    my $data_cache_file = File::Spec->catfile($system_cache_dir, '.nfit.cache.data');
    my (undef, $cache_end_date_obj) = _get_cache_date_range($data_cache_file);

    unless ($cache_end_date_obj) {
        warn "Warning: Could not determine data cache end date. Cannot proceed with forecast.\n";
        return ({}, {});
    }

    my $unified_history = read_unified_history($system_cache_dir);
    my @event_history_initial;
    my ($most_recent_peak_snapshot, $most_recent_peak_end_obj);

    # Find all historical snapshots for this specific event from the unified history.
    foreach my $month_key (sort { $b cmp $a } keys %$unified_history) {
        my $month_data = $unified_history->{$month_key};
        if (exists $month_data->{SeasonalEventSnapshots}{$event_name}) {
            my $snapshot = $month_data->{SeasonalEventSnapshots}{$event_name};
            # Enrich the snapshot with the month key for date context.
            $snapshot->{_month_key} = $month_key;
            push @event_history_initial, $snapshot;
        }
    }

    # --- Step 2: Two-Tiered Baseline Strategy ---
    my %current_baseline_results;
    my $baseline_source_log = "N/A";
    my @event_history_final = @event_history_initial; # Start with all available history.

    # --- Path A (Ideal): Attempt to calculate a "Post-Peak" baseline ---
    # The ideal baseline is the N days of data ending on the last day of the cache.
    my $post_peak_baseline_start_obj = $cache_end_date_obj->truncate(to => 'day') - (($baseline_days_config - 1) * 86400);

    # Check if this ideal baseline period is contaminated by the most recent historical peak.
    my $is_contaminated = 0;
    if ($most_recent_peak_end_obj) {
        $is_contaminated = ($post_peak_baseline_start_obj <= $most_recent_peak_end_obj);
    }
    # Also check if the clean period is long enough to be statistically valid.
    my $clean_days_available = $is_contaminated ?
    ($cache_end_date_obj->epoch - $most_recent_peak_end_obj->epoch) / 86400 - 1 :
    $baseline_days_config;

    if (!$is_contaminated && $clean_days_available >= $MIN_BASELINE_DAYS) {
        $baseline_source_log = "Post-Peak (calculated from " . $post_peak_baseline_start_obj->date . " to " . $cache_end_date_obj->date . ")";
        print STDERR "  - Baseline Strategy: Using ideal Post-Peak baseline.\n";

        my $start_str = $post_peak_baseline_start_obj->strftime('%Y-%m-%d');
        my $end_str   = $cache_end_date_obj->strftime('%Y-%m-%d');

        my $profile_count = scalar(@profiles);
        my $profile_num = 0;
        foreach my $profile (@profiles) {
            $profile_num++;
            my $profile_name = $profile->{name};
            print STDERR "    - Analysing post-peak baseline $profile_num/$profile_count: $profile_name\n";
            # This is a contextual baseline, so is_generic is 0.
            my $nfit_cmd = _build_nfit_baseline_command($profile->{flags}, $start_str, $end_str, $system_cache_dir, 0, 0, undef, $profile->{name});
            my $nfit_output = '';
            my $stderr_arg = ">&=" . fileno(STDERR);
            my $pid_nfit = open3(undef, my $stdout_nfit, $stderr_arg, $nfit_cmd);
            while (my $line = <$stdout_nfit>) { $nfit_output .= $line; }
            waitpid($pid_nfit, 0);
            next if $?; # Skip if nfit command failed

            my $parsed = parse_nfit_json_output($nfit_output);
            my $p_key  = "P" . clean_perc_label(($profile->{flags} =~ /-p\s+([0-9.]+)/) ? $1 : $DEFAULT_PERCENTILE);

            foreach my $vm (keys %$parsed) {
                my @vals = map { $_->{metrics}{physc}{$p_key} } @{$parsed->{$vm}};
                my @valid_vals = grep { defined $_ && looks_like_number($_) } @vals;
                if (@valid_vals) {
                    $current_baseline_results{$vm}{$profile->{name}} = sum0(@valid_vals) / scalar(@valid_vals);
                }
            }
        }
    }
    # --- Path B (Fallback): Re-use the "Pre-Peak" baseline from the most recent snapshot ---
    elsif ($most_recent_peak_snapshot) {
        $baseline_source_log = "Pre-Peak (re-used from snapshot ending " . $most_recent_peak_snapshot->{periodEndDate} . ")";
        print STDERR "  - Baseline Strategy: Post-Peak baseline contaminated or too short. Falling back to re-use Pre-Peak baseline.\n";
        %current_baseline_results = %{$most_recent_peak_snapshot->{results}{HistoricBaseline} || {}};
        # If we re-use this baseline, we must exclude its corresponding peak from the multiplier calculation.
        shift @event_history_final;
    }

    unless (%current_baseline_results) {
        warn "Warning: Could not determine a valid baseline. Cannot proceed with forecast.\n";
        return ({}, {});
    }

    # --- Step 3: Detect Active Events for Compounding ---
    my $today_for_recency = gmtime();
    my ($next_event_start, $next_event_end) = determine_event_period($event_config);
    my $forecast_date_obj = (defined $next_event_start && ref($next_event_start) eq 'Time::Piece') ? $next_event_start : $today_for_recency;
    my @active_events = detect_active_events($forecast_date_obj, $full_seasonality_config);

    # --- Step 4: Calculate Recency-Weighted Multiplier & Final Forecast ---
    my %final_forecasts;
    my %historic_data_for_csv; # For verbose reporting

    if (!@active_events) {
        print STDERR "  - No active seasonal events found for the forecast period. Using baseline as forecast.\n";
        # If no events are active, the forecast is simply the baseline.
        foreach my $vm_name (keys %current_baseline_results) {
            foreach my $profile (@profiles) {
                $final_forecasts{$vm_name}{$profile->{name}} = $current_baseline_results{$vm_name}{$profile->{name}};
            }
        }
        return (\%final_forecasts, \%historic_data_for_csv);
    }

    my $is_compound_run = (scalar(@active_events) > 1);
    my @dampening_factors;
    print "  - Detected " . scalar(@active_events) . " active event(s): " . join(", ", map { $_->{_eventName} } @active_events) . "\n";
    if ($is_compound_run) { print "  - Compounding mode enabled.\n"; }

    # Main processing loop: iterate through each VM that has a valid baseline.
    foreach my $vm_name (sort keys %current_baseline_results) {
        my $vm_forecasts = {};
        my %vm_historic_multipliers;
        my %vm_historic_residuals;

        # Gather historical data for all active events for this VM.
        foreach my $active_event_cfg (@active_events) {
            my $active_event_name = $active_event_cfg->{_eventName};

            my @history_for_this_event;
            foreach my $hist_event (@event_history_final) {
                if ($hist_event->{eventName} eq $active_event_name) {
                    push @history_for_this_event, $hist_event;
                }
            }

            # Only proceed to calculate multipliers if history exists for this event.
            if (@history_for_this_event) {
                if (defined $active_event_cfg->{interaction_dampening_factor}) {
                    push @dampening_factors, $active_event_cfg->{interaction_dampening_factor};
                }
                foreach my $profile (@profiles) {
                    my $p_name = $profile->{name};
                    my @multipliers;
                    my @residuals;
                    foreach my $hist_event (@history_for_this_event) {
                        my $hist_results = $hist_event->{results};
                        my $hist_peak = $hist_results->{HistoricPeak}{$vm_name}{$p_name};
                        if (defined $hist_results->{ClippingInfo}{$vm_name}{$p_name}{unclippedPeakEstimate} && looks_like_number($hist_results->{ClippingInfo}{$vm_name}{$p_name}{unclippedPeakEstimate})) {
                            $hist_peak = $hist_results->{ClippingInfo}{$vm_name}{$p_name}{unclippedPeakEstimate};
                        }
                        my $hist_base = $hist_results->{HistoricBaseline}{$vm_name}{$p_name};
                        my $hist_residual = $hist_results->{PeakResidual}{$vm_name}{$p_name};

                        if (defined $hist_peak && defined $hist_base && $hist_base > 0.001) {
                            push @multipliers, { value => $hist_peak / $hist_base, date => Time::Piece->strptime($hist_event->{periodEndDate}, '%Y-%m-%d') };
                        }
                        if (defined $hist_residual && looks_like_number($hist_residual)) {
                            push @residuals, { value => $hist_residual, date => Time::Piece->strptime($hist_event->{periodEndDate}, '%Y-%m-%d') };
                        }
                    }
                    next unless @multipliers;

                    my $event_multiplier = calculate_recency_weighted_average(\@multipliers, $today_for_recency, 365);
                    $vm_historic_multipliers{$p_name}{$active_event_name} = { 'multiplier' => $event_multiplier, 'history' => \@multipliers };
                    $vm_historic_residuals{$p_name}{$active_event_name} = \@residuals;

                }
            }
        }

        # Now, synthesize the final forecast for each profile.
        foreach my $profile (@profiles) {
            my $p_name = $profile->{name};
            my $current_baseline_val = $current_baseline_results{$vm_name}{$p_name};
            next unless (defined $current_baseline_val && looks_like_number($current_baseline_val));
            next unless (exists $vm_historic_multipliers{$p_name});

            my $final_multiplier = 1.0;
            foreach my $event_data (values %{$vm_historic_multipliers{$p_name}}) {
                $final_multiplier *= $event_data->{'multiplier'};
            }

            if ($is_compound_run && @dampening_factors) {
                my $dampening_to_apply = (sort { $a <=> $b } @dampening_factors)[0];
                $final_multiplier = 1 + (($final_multiplier - 1) * $dampening_to_apply);
            }

            my $volatility_buffer = 1.0;
            my $primary_event_cfg = (grep { $_->{_eventName} eq $event_name } @active_events)[0] || $active_events[0];
            if (($primary_event_cfg->{volatility_adjustment} // 0) == 1 && exists $vm_historic_multipliers{$p_name}{$primary_event_cfg->{_eventName}}) {
                my $primary_event_history_ref = $vm_historic_multipliers{$p_name}{$primary_event_cfg->{_eventName}}{'history'};
                my $confidence = $primary_event_cfg->{seasonal_confidence_level} // '0.95';
                $volatility_buffer = calculate_volatility_buffer($primary_event_history_ref, $confidence);
            }

            my $forecasted_residual = 0;
            my @all_residuals_for_profile;
            if (exists $vm_historic_residuals{$p_name}) {
                foreach my $event_name (keys %{$vm_historic_residuals{$p_name}}) {
                    push @all_residuals_for_profile, @{$vm_historic_residuals{$p_name}{$event_name}};
                }
            }
            if (@all_residuals_for_profile) {
                $forecasted_residual = calculate_recency_weighted_average(\@all_residuals_for_profile, $today_for_recency, 365) // 0;
            }

            my $trend_adjusted_baseline = $current_baseline_val * $trend_adjustment_factor;
            my $primary_forecast = $trend_adjusted_baseline * $final_multiplier * $volatility_buffer;
            my $combined_forecast = $primary_forecast + $forecasted_residual;
            my $amplification = $event_config->{peak_amplification_factor} // 1.0;
            my $final_recommendation = $combined_forecast * $amplification;

            $vm_forecasts->{$p_name} = $final_recommendation;

            $seasonal_debug_info{$vm_name}{$p_name} = {
                historical_multipliers => $vm_historic_multipliers{$p_name}{$event_name}{'history'},
                baseline             => $current_baseline_val,
                baseline_source      => $baseline_source_log,
                trend_factor         => $trend_adjustment_factor,
                multiplier           => $final_multiplier,
                volatility           => $volatility_buffer,
                forecasted_residual  => $forecasted_residual,
                amplification_factor => $amplification,
                forecast             => $final_recommendation,
                OutlierWarning       => _get_warning_for_vm_forecast($vm_name, \%outlier_warnings)
            };
        }
        $final_forecasts{$vm_name} = $vm_forecasts;
    }

    # Populate data for the verbose historic_snapshot.csv file.
    if ($most_recent_peak_snapshot) {
        foreach my $vm_name (keys %{$most_recent_peak_snapshot->{results}{HistoricPeak}}) {
            $historic_data_for_csv{$vm_name}{'HistoricPeak'} = $most_recent_peak_snapshot->{results}{HistoricPeak}{$vm_name} || {};
            $historic_data_for_csv{$vm_name}{'HistoricBaseline'} = $most_recent_peak_snapshot->{results}{HistoricBaseline}{$vm_name} || {};
        }
    }

    # Return both the forecasts and the historical data for verbose reporting.
    return (\%final_forecasts, \%historic_data_for_csv);
}

# ==============================================================================
# Helper subroutine to write the multi-file CSV output for seasonal forecasts.
# ==============================================================================
# This version safely enhances the seasonal forecast output:
# - It adds a new, rich format ONLY for the 'final_forecast.csv'.
# - The logic for 'current_baseline' and 'historic_snapshot' is UNCHANGED,
#   ensuring no regressions.
# - It is fully commented and robust.
# ==============================================================================
sub write_seasonal_csv_output {
    my ($type, $system_id, $event_name, $timestamp, $data_href) = @_;

    # Prevent creation of empty files
    if (!defined $data_href || !%{$data_href} || !scalar(keys %{$data_href})) {
        print STDERR "  - INFO: No data available for seasonal output type '$type'. Skipping file generation.\n";
        return;
    }

    # Sanitise identifiers for use in filenames.
    my $s_id_safe = $system_id;
    my $e_name_safe = $event_name;
    $s_id_safe =~ s/[^a-zA-Z0-9_.-]//g;
    $e_name_safe =~ s/[^a-zA-Z0-9_.-]//g;

    my $filename = File::Spec->catfile($output_dir, "nfit-profile.$s_id_safe.$e_name_safe.$type.$timestamp.csv");
    # Store filename for final notification
    push @generated_files, $filename;

    print "  - Generating output file: $filename\n";

    open my $fh, '>', $filename or do {
        warn "Warning: Could not open output file '$filename': $!";
        return;
    };

    # --- NEW: Check if this is the rich forecast report ---
    if ($type eq 'final_forecast') {
        # --- PATH A: Generate the new, rich final_forecast.csv ---

        # Build the enhanced header with all standard columns plus the new one.
        my @header = (
            "VM", "TIER", "Hint", "Pattern", "Pressure", "PressureDetail", "SMT",
            "Serial", "SystemType", "Pool Name", "Pool ID", $PEAK_PROFILE_NAME
        );
        push @header, (map { $_->{name} } @profiles);
        # Add the new SeasonalMultiplier column before the entitlement columns.
        push @header, ("SeasonalMultiplier", "Current - ENT");
        print $fh join(",", map { quote_csv($_) } @header) . "\n";

        foreach my $vm_name (sort keys %$data_href) {
            my $vm_data = $data_href->{$vm_name};
            my $report_data = $vm_data->{_report_data} || {};
            my $cfg_csv = $vm_config_data{$vm_name};

            # Gather standard fields for the row
            my $smt_out = $cfg_csv->{smt} // $default_smt_arg;
            my ($serial_out, $systype_out, $poolname_out, $poolid_out, $ent_out) = ('', '', '', '', '');
            if (defined $cfg_csv) {
                $serial_out = $cfg_csv->{serial} // ''; $systype_out = $cfg_csv->{systemtype} // '';
                $poolname_out = $cfg_csv->{pool_name} // ''; $poolid_out = $cfg_csv->{pool_id} // '';
                $ent_out = $cfg_csv->{entitlement} // '';
            }

            my @row = (
                $vm_name, "", $report_data->{hint} // "", $report_data->{pattern} // "",
                $report_data->{pressure} // "", $report_data->{pressure_detail} // "", $smt_out,
                $serial_out, $systype_out, $poolname_out, $poolid_out, "" # Peak is not applicable for a forecast
            );

            my $seasonal_multiplier_for_row = "N/A";
            foreach my $profile (@profiles) {
                my $p_name = $profile->{name};
                my $value = $vm_data->{$p_name} // '';
                push @row, (looks_like_number($value) ? sprintf("%.4f", $value) : $value);

                # Use the multiplier from the first available profile for the summary column
                if ($seasonal_multiplier_for_row eq 'N/A' && exists $report_data->{seasonal_debug}{$p_name}) {
                    $seasonal_multiplier_for_row = sprintf("%.2f", $report_data->{seasonal_debug}{$p_name}{multiplier});
                }
            }

            # Add the new column value and the final entitlement value
            push @row, $seasonal_multiplier_for_row;
            push @row, (looks_like_number($ent_out) ? sprintf("%.2f", $ent_out) : $ent_out);

            print $fh join(",", map { quote_csv($_) } @row) . "\n";
        }

    } else {
        # --- PATH B: Original logic for other file types (UNCHANGED) ---

        # --- Build header ---
        my @header;
        if ($type eq 'historic_snapshot') {
            # Specialised header for the detailed historic report.
            @header = ("VM", "MetricType");
        } else {
            # Standard header for the simple current_baseline report.
            @header = ("VM");
        }
        push @header, map { $_->{name} } @profiles;
        print $fh join(",", map { quote_csv($_) } @header) . "\n";

        # --- Write data rows ---
        foreach my $vm_name (sort keys %$data_href) {
            if ($type eq 'historic_snapshot') {
                # --- Special handling for the historic snapshot file ---
                my $hist_data = $data_href->{$vm_name} || {};
                my $peak_data = $hist_data->{HistoricPeak} || {};
                my $base_data = $hist_data->{HistoricBaseline} || {};
                my @peak_row = ($vm_name, "HistoricPeak");
                foreach my $profile (@profiles) {
                    my $value = $peak_data->{$profile->{name}} // '';
                    push @peak_row, (looks_like_number($value) ? sprintf("%.4f", $value) : $value);
                }
                print $fh join(",", map { quote_csv($_) } @peak_row) . "\n";
                my @base_row = ($vm_name, "HistoricBaseline");
                foreach my $profile (@profiles) {
                    my $value = $base_data->{$profile->{name}} // '';
                    push @base_row, (looks_like_number($value) ? sprintf("%.4f", $value) : $value);
                }
                print $fh join(",", map { quote_csv($_) } @base_row) . "\n";
            } else {
                # --- Standard handling for 'current_baseline' file ---
                my @row = ($vm_name);
                my $vm_data = $data_href->{$vm_name};
                foreach my $profile (@profiles) {
                    my $value = $vm_data->{$profile->{name}} // '';
                    push @row, (looks_like_number($value) ? sprintf("%.4f", $value) : $value);
                }
                print $fh join(",", map { quote_csv($_) } @row) . "\n";
            }
        }
    }
    close $fh;
}

# ==============================================================================
# Subroutine to calculate a recency-weighted average.
# This is a key statistical function used by the multiplicative model.
# ==============================================================================
sub calculate_recency_weighted_average {
    my ($data_points_aref, $reference_date_obj, $half_life_days) = @_;

    my $sum_weighted_values = 0;
    my $sum_weights = 0;
    my $lambda = log(2) / $half_life_days;

    foreach my $dp (@$data_points_aref) {
        my $value = $dp->{value};
        my $date_obj = $dp->{date};

        my $days_diff = ($reference_date_obj->epoch - $date_obj->epoch) / ONE_DAY;
        $days_diff = 0 if $days_diff < 0;

        my $weight = exp(-$lambda * $days_diff);
        $sum_weighted_values += $value * $weight;
        $sum_weights += $weight;
    }

    if ($sum_weights > 1e-9) {
        return $sum_weighted_values / $sum_weights;
    } else {
        # Fallback: if all weights are zero (e.g., very old data), return a simple average.
        my @values = map { $_->{value} } @$data_points_aref;
        return @values ? (sum0(@values) / scalar(@values)) : undef;
    }
}

# ==============================================================================
# Subroutine to calculate a statistical volatility buffer.
# This increases the forecast based on the standard deviation of historical
# seasonal multipliers to account for year-over-year variance.
# ==============================================================================
sub calculate_volatility_buffer {
    my ($multipliers_aref, $confidence_level) = @_;

    my @values = map { $_->{value} } @$multipliers_aref;

    # Standard deviation requires at least 2 data points.
    return 1.0 if scalar(@values) < 2;

    # --- Calculate Mean and Standard Deviation ---
    my $sum = sum0(@values);
    my $mean = $sum / scalar(@values);
    return 1.0 if $mean == 0; # Avoid division by zero if mean is zero.

    my $sum_sq_diff = 0;
    foreach my $val (@values) {
        $sum_sq_diff += ($val - $mean)**2;
    }
    my $std_dev = sqrt($sum_sq_diff / (scalar(@values) - 1));

    # --- Z-score for common confidence levels ---
    # This lookup table provides the one-sided Z-score for a given confidence level.
    my %z_scores = (
        '0.90' => 1.645,
        '0.95' => 1.960,
        '0.98' => 2.326,
        '0.99' => 2.576,
    );
    my $z_score = $z_scores{$confidence_level} // 1.960; # Default to 95%

    # The buffer is 1 + (a fraction of the coefficient of variation).
    # This adds a percentage uplift proportional to the historical volatility.
    my $volatility_buffer = 1.0 + ($z_score * ($std_dev / $mean));

    # Sanity check: don't let the buffer be less than 1 (non-reducing).
    return $volatility_buffer > 1.0 ? $volatility_buffer : 1.0;
}

# ==============================================================================
# Subroutine to detect all active, compoundable seasonal events for a given date.
# Returns:
# - An array of event configuration hashes, sorted by priority (desc).
# Notes:
# - CORRECTED to handle both fixed-date and recurring-period definitions.
# ==============================================================================
sub detect_active_events {
    my ($analysis_date_obj, $seasonality_config_href) = @_;

    my @active_events;
    return @active_events unless (defined $analysis_date_obj && ref($analysis_date_obj) eq 'Time::Piece');

    # *** FIX: Use UTC for consistent comparisons ***
    my $analysis_date_utc = gmtime($analysis_date_obj->epoch)->truncate(to => 'day');

    foreach my $event_name (keys %{$seasonality_config_href}) {
        my $event_config = $seasonality_config_href->{$event_name};
        next unless (($event_config->{allow_compounding} // '') =~ /^(true|1)$/i);

        # Determine the period for this event relative to the analysis date.
        my ($start_obj, $end_obj) = determine_event_period($event_config, $analysis_date_utc);

        my $is_active = 0;

        if (defined $event_config->{dates}) {
            my @date_ranges = split /\s*,\s*/, $event_config->{dates};
            foreach my $range (@date_ranges) {
                if ($range =~ /(\d{4}-\d{2}-\d{2}):(\d{4}-\d{2}-\d{2})/) {
                    my $start_obj = Time::Piece->strptime($1, '%Y-%m-%d')->truncate(to => 'day');
                    my $end_obj   = Time::Piece->strptime($2, '%Y-%m-%d')->truncate(to => 'day');
                    if ($analysis_date_utc >= $start_obj && $analysis_date_utc <= $end_obj) {
                        $is_active = 1;
                        last;
                    }
                }
            }
        }
        elsif (defined $event_config->{period} && lc($event_config->{period}) eq 'monthly') {
            # *** FIX: Use the robust helper for consistent logic ***
            my $day_of_period = $event_config->{day_of_period} // -1;
            my $duration_days = $event_config->{duration_days} // 7;
            my ($event_start_obj, $event_end_obj) = _get_recurring_monthly_period($analysis_date_utc, $day_of_period, $duration_days);

            # The check is now simpler because determine_event_period already did the work.
            if (defined $start_obj && $analysis_date_utc >= $start_obj && $analysis_date_utc <= $end_obj) {
                $is_active = 1;
            }
        }

        if ($is_active) {
            $event_config->{_eventName} = $event_name;
            push @active_events, $event_config;
        }
    }

    return sort { ($b->{priority} // 0) <=> ($a->{priority} // 0) } @active_events;
}

# ==============================================================================
# Helper function to validate data line format
# ==============================================================================
sub is_valid_data_line {
    my ($line) = @_;
    # Simply check if line starts with your timestamp format
    return $line && $line =~ /^\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2},/;
}

# ==============================================================================
# Efficiently gets the start and end timestamps from a large cache file
# by reading only the first and last data lines.
# ==============================================================================
sub _get_cache_date_range {
    my ($data_cache_file) = @_;
    return (undef, undef) unless (-f $data_cache_file && -s $data_cache_file);

    open my $fh, '<', $data_cache_file or die "Could not open $data_cache_file: $!";
    my ($start_ts_str, $end_ts_str);

    # Get first data line (line 2) - validate it's a proper data line
    my $header = <$fh>;  # Skip header
    while (my $line = <$fh>) {
        chomp $line;
        if ($line && is_valid_data_line($line)) {
            ($start_ts_str) = split ',', $line, 2;
            last;
        }
    }

    # Get last line by reading backwards line by line
    my $file_size = -s $data_cache_file;
    my $pos = $file_size;
    my $line_buffer = '';
    my $last_valid_line = '';

    # Read backwards in small chunks to build lines
    while ($pos > 0 && !$last_valid_line) {
        my $chunk_size = ($pos < 1024) ? $pos : 1024;
        $pos -= $chunk_size;

        seek $fh, $pos, 0;
        my $chunk;
        read $fh, $chunk, $chunk_size;

        # Prepend to our buffer
        $line_buffer = $chunk . $line_buffer;

        # Process complete lines from the end
        my @lines = split /\n/, $line_buffer;

        # If we're not at the start, the first line might be partial
        if ($pos > 0) {
            $line_buffer = shift @lines;  # Keep partial line for next iteration
        } else {
            $line_buffer = '';
        }

        # Check lines from end to start
        for my $line (reverse @lines) {
            if ($line && is_valid_data_line($line)) {
                $last_valid_line = $line;
                last;
            }
        }
    }

    if ($last_valid_line) {
        ($end_ts_str) = split ',', $last_valid_line, 2;
    }

    close $fh;

    # Parse timestamps
    my ($start_obj, $end_obj);
    eval { $start_obj = Time::Piece->strptime($start_ts_str, "%Y-%m-%d %H:%M:%S") if $start_ts_str; };
    if ($@) {
        warn "Warning: Could not parse start timestamp '$start_ts_str' from cache.";
        return (undef, undef);
    }
    eval { $end_obj = Time::Piece->strptime($end_ts_str, "%Y-%m-%d %H:%M:%S") if $end_ts_str; };
    if ($@) {
        warn "Warning: Could not parse end timestamp '$end_ts_str' from cache.";
        return (undef, undef);
    }

    return ($start_obj, $end_obj);
}

sub _get_warning_for_vm_forecast {
    my ($vm_name, $warnings_href) = @_;
    # This finds the most recent warning for a given VM, as the hash is keyed by snapshot date.
    foreach my $key (sort { $b cmp $a } keys %$warnings_href) {
        if (exists $warnings_href->{$key}{$vm_name}) {
            return $warnings_href->{$key}{$vm_name};
        }
    }
    return undef;
}

# ==============================================================================
# Encapsulates the entire CSV generation process, from header to footer.
# It relies on globally scoped variables (@vm_order, %results_table, etc.)
# being populated before it is called.
# It applies consistent, 3-decimal-place formatting for all numeric results.
# ==============================================================================
sub _write_standard_csv_report {
    my ($report_type, $system_id, $file_timestamp, $is_multiplicative_run_flag, $is_recency_decay_run_flag, $is_predictive_peak_run_flag) = @_;

    # Ensure we have data to process before creating a file.
    return unless (@vm_order);

    # Sanitise identifiers for use in filenames.
    my $system_id_for_filename = $system_id || 'standard';
    $system_id_for_filename =~ s/[^a-zA-Z0-9_.-]//g;
    my $report_type_for_filename = $report_type;
    $report_type_for_filename =~ s/[^a-zA-Z0-9_.-]//g;

    my $output_filename = File::Spec->catfile($output_dir, "nfit-profile.$system_id_for_filename.$report_type_for_filename.$file_timestamp.csv");
    push @generated_files, $output_filename;

    open my $out_fh, '>', $output_filename or die "FATAL: Cannot open output file '$output_filename': $!";
#    print STDERR "Analysis complete. Writing '$report_type' report to: $output_filename\n";

    # Create a local copy of the header to modify for this specific report.
    my @header_for_this_report = @output_header_cols_csv;
    my $formula_col_offset = 0;

    # Conditionally remove RunQ_Modifier for multiplicative forecasts
    if ($is_multiplicative_run_flag) {
        @header_for_this_report = grep { $_ ne 'RunQ_Modifier' && $_ ne 'RunQ_Uncapped' && $_ ne 'RunQ_Source' } @header_for_this_report;
    }

    # Find the index of the 'Current - ENT' column to insert new columns before it.
    my ($ent_idx) = grep { $header_for_this_report[$_] eq 'Current - ENT' } 0..$#header_for_this_report;
    $ent_idx //= scalar(@header_for_this_report); # Fallback to appending if not found

    # Conditionally add new columns for the specific seasonal models.
    if ($is_multiplicative_run_flag) {
        # Add columns for the Multiplicative model
        my @new_cols = ("SeasonalMultiplier", "Baseline_PhysC");
        splice @header_for_this_report, $ent_idx, 0, @new_cols;
        $formula_col_offset = scalar(@new_cols);
    } elsif ($is_recency_decay_run_flag) {
        # Add columns for the Recency-Decay model
        my @new_cols = ("GrowthAdj_Min", "GrowthAdj_Max");
        splice @header_for_this_report, $ent_idx, 0, @new_cols;
        $formula_col_offset = scalar(@new_cols);
    } elsif ($nfit_decay_over_states) {
        # Also add columns for the standard (non-seasonal) Hybrid State-Time Decay model
        my @new_cols = ("GrowthAdj_Min", "GrowthAdj_Max");
        splice @header_for_this_report, $ent_idx, 0, @new_cols;
        $formula_col_offset = scalar(@new_cols);
    } elsif ($is_predictive_peak_run_flag) {
        # Add columns for the Predictive Peak model
        my @new_cols = ("TrueBaseline_P99W1", "PredictedPeak_P99W1", "ForecastSource");
        splice @header_for_this_report, $ent_idx, 0, @new_cols;
        $formula_col_offset = scalar(@new_cols);
    }

    # Print the CSV header to the file.
    print {$out_fh} join(",", map { quote_csv($_) } @header_for_this_report) . "\n";

    my $excel_row_num_counter = 1;
    my %runq_modifier_source_for_csv; # To store the dynamically chosen source profile

    # Iterate through VMs in the order they were first seen to maintain consistency
    foreach my $vm_name (sort @vm_order) {
        $excel_row_num_counter++;
        my @data_row_csv;
        my $cfg_csv = $vm_config_data{$vm_name};

        my $is_vio_server = (defined $cfg_csv && defined $cfg_csv->{'systemtype'} && $cfg_csv->{'systemtype'} =~ /VIO Server/i);

        my $smt_used_for_vm_csv = $default_smt_arg;
        if (defined $cfg_csv && defined $cfg_csv->{smt}) {
            $smt_used_for_vm_csv = $cfg_csv->{smt};
        }

        # --- Use pre-calculated hint data ---
        my $hint_type_tier_csv = $hint_tier_for_csv{$vm_name} // '';
        my $hint_pattern_shape_csv = $hint_pattern_for_csv{$vm_name} // '';
        my $pressure_bool_str_csv = $hint_pressure_for_csv{$vm_name} ? "True" : "False";
        my $pressure_detail_str_csv = $pressure_details_for_csv{$vm_name} // '';

		push @data_row_csv, $vm_name, "", $hint_type_tier_csv, $hint_pattern_shape_csv, $pressure_bool_str_csv, $pressure_detail_str_csv, $smt_used_for_vm_csv;

        my ($serial_out, $systype_out, $poolname_out, $poolid_out, $ent_out) = ('', '', '', '', '');
        if (defined $cfg_csv) {
            $serial_out = $cfg_csv->{serial} // '';
            $systype_out = $cfg_csv->{systemtype} // '';
            $poolname_out = $cfg_csv->{pool_name} // '';
            $poolid_out = $cfg_csv->{pool_id} // '';
            $ent_out = $cfg_csv->{entitlement} // '';
        } else {
            my $last_state_data_ref = ($per_profile_nfit_raw_results{$vm_name}{$MANDATORY_PEAK_PROFILE_FOR_HINT} // [])->[-1];
            if ($last_state_data_ref && ref($last_state_data_ref) eq 'HASH') {
                my $metadata = $last_state_data_ref->{metadata} || {};
                my $config = $metadata->{configuration} || {};
                $serial_out = $metadata->{serialNumber} // '';
                $poolid_out = $metadata->{poolId} // '';
                $ent_out = $config->{entitlement} // '';
            }
        }
        push @data_row_csv, $serial_out, $systype_out, $poolname_out, $poolid_out;

        # --- Dynamic RunQ Modifier source logic ---
        unless ($is_multiplicative_run_flag || $is_predictive_peak_run_flag) {
            # Determine the source profile based on the hint
            my $hint = $hint_tier_for_csv{$vm_name} // "G";
            my ($pattern) = ($hint =~ /^([A-Z])/);
            my %pattern_to_profile_map = (
                'O' => 'O3-95W15',
                'B' => 'B3-95W15',
                'G' => 'G3-95W15',
                'P' => 'G3-95W15' # Fallback for Peak/VIO
            );
            my $source_profile = $pattern_to_profile_map{$pattern} // 'G3-95W15';

            # If the dynamically chosen profile has no data, fall back to G3
            unless (exists $runq_modifier_values{$vm_name}{$source_profile}) {
                $source_profile = 'G3-95W15';
            }
            $runq_modifier_source_for_csv{$vm_name} = $source_profile;

            # Get modifier values from the dynamically determined source profile
            my $modifier_val = $runq_modifier_values{$vm_name}{$source_profile} // 0;
            my $modifier_val_formatted = sprintf("%+.3f", $modifier_val); # Use %+f to show sign
            push @data_row_csv, $modifier_val_formatted;

            my $uncapped_modifier_val = $runq_uncapped_values{$vm_name}{$source_profile} // 0;
            my $uncapped_modifier_formatted = sprintf("%.3f", $uncapped_modifier_val);
            push @data_row_csv, $uncapped_modifier_formatted;

            # Add the source profile name to the CSV
            push @data_row_csv, $source_profile;
        }

        # --- Apply consistent 3-decimal-place formatting to all numeric results ---
        my $peak_val = $results_table{$vm_name}{$PEAK_PROFILE_NAME} // "";
        my $peak_val_formatted = (looks_like_number($peak_val)) ? sprintf("%.3f", $peak_val) : $peak_val;
        push @data_row_csv, $peak_val_formatted;

        foreach my $profile (@profiles) {
            my $prof_val = $results_table{$vm_name}{$profile->{name}} // "";
            my $prof_val_formatted = (looks_like_number($prof_val)) ? sprintf("%.3f", $prof_val) : $prof_val;
            push @data_row_csv, $prof_val_formatted;
        }

        # Conditionally add data for the new seasonal columns.
        if ($is_multiplicative_run_flag) {
            my $seasonal_multiplier_for_row = "N/A";
            my $baseline_physc_for_row = "N/A";
            if (exists $seasonal_debug_info{$vm_name}) {
                # Use the P-99W1 profile's debug info as the representative value for the row.
                my $debug_info_for_vm = $seasonal_debug_info{$vm_name}{$MANDATORY_PEAK_PROFILE_FOR_HINT} || {};
                if (defined $debug_info_for_vm->{'multiplier'}) {
                    $seasonal_multiplier_for_row = sprintf("%.2f", $debug_info_for_vm->{'multiplier'});
                }
                if (defined $debug_info_for_vm->{'baseline'}) {
                    $baseline_physc_for_row = sprintf("%.3f", $debug_info_for_vm->{'baseline'});
                }
            }
            push @data_row_csv, $seasonal_multiplier_for_row, $baseline_physc_for_row;

        } elsif ($is_recency_decay_run_flag || $nfit_decay_over_states) {
            # --- Calculate Min/Max GrowthAdj across all profiles for this VM ---
            my @growth_adjs;
            foreach my $profile (@profiles) {
                my $raw_results_ref = $per_profile_nfit_raw_results{$vm_name}{$profile->{name}};
                if (ref($raw_results_ref) eq 'ARRAY' && @$raw_results_ref && $raw_results_ref->[0]{metrics}{growth}) {
                    my $growth_val = $raw_results_ref->[0]{metrics}{growth}{adjustment};
                    if (defined $growth_val && looks_like_number($growth_val)) {
                       push @growth_adjs, $growth_val if $growth_val > 0.001;
                    }
                }
            }

            my $min_adj_str = @growth_adjs ? sprintf("%.3f", min(@growth_adjs)) : "0.000";
            my $max_adj_str = @growth_adjs ? sprintf("%.3f", max(@growth_adjs)) : "0.000";

            push @data_row_csv, $min_adj_str, $max_adj_str;
        } elsif ($is_predictive_peak_run_flag) {
            # Retrieve the debug info for the specific VM being processed in the loop.
            my $s_data = $seasonal_debug_info{$vm_name}{'P-99W1'} || {};

            my $baseline_val = $s_data->{'TrueBaseline'};
            my $predicted_val = $s_data->{'PredictedPeak'};
            my $source = $s_data->{'FinalSource'} || 'N/A';

            # Use our finalized, user-friendly labels for the CSV output.
            if ($source eq 'PeakPrediction') {
                $source = 'PredictedPeak';
            } elsif ($source eq 'TrueBaseline') {
                $source = 'BaselineIsHigher';
            } elsif ($source eq 'BaselineOnly (NoPrediction)') {
                $source = 'BaselineOnly (NoPrediction)';
            }

            my $baseline_str = (defined $baseline_val && looks_like_number($baseline_val)) ? sprintf("%.3f", $baseline_val) : "N/A";
            my $predicted_str = (defined $predicted_val && looks_like_number($predicted_val)) ? sprintf("%.3f", $predicted_val) : "N/A";

            push @data_row_csv, $baseline_str, $predicted_str, $source;
        }

        my $current_ent_display = (looks_like_number($ent_out)) ? sprintf("%.2f", $ent_out) : $ent_out;

        # Pass the calculated offset to the formula generator
        my $nfit_ent_formula_str = generate_nfit_ent_formula($excel_row_num_counter, scalar(@profiles), $formula_col_offset);

        # Dynamically calculate column letters for NETT formulas to handle the offset.
        my $col_nfit_ent_letter = get_excel_col_name(12 + scalar(@profiles) + 2 + $formula_col_offset);
        my $col_curr_ent_letter = get_excel_col_name(12 + scalar(@profiles) + 1 + $formula_col_offset);

        my $nett_user_formula_str = sprintf("=(%s%d-%s%d)", $col_nfit_ent_letter, $excel_row_num_counter, $col_curr_ent_letter, $excel_row_num_counter);
        my $nett_perc_user_formula_str = sprintf("=IFERROR((%s%d-%s%d)/%s%d,\"\")", $col_nfit_ent_letter, $excel_row_num_counter, $col_curr_ent_letter, $excel_row_num_counter, $col_curr_ent_letter, $excel_row_num_counter);

        push @data_row_csv, $current_ent_display, $nfit_ent_formula_str, $nett_user_formula_str, $nett_perc_user_formula_str;
        print {$out_fh} join(",", map { quote_csv($_) } @data_row_csv) . "\n";
    }

    if ($add_excel_formulas) {
        my %serials_map;
        foreach my $vm (@vm_order) {
            if (exists $vm_config_data{$vm} && defined $vm_config_data{$vm}{serial} && $vm_config_data{$vm}{serial} ne '') {
                $serials_map{$vm_config_data{$vm}{serial}} = 1;
            }
        }
        my @sorted_serials = sort keys %serials_map;
        print_csv_footer($out_fh, $excel_row_num_counter, $physc_data_file, scalar(@profiles), \@sorted_serials, $formula_col_offset);
    }

    close $out_fh;
}

# ==============================================================================
# Subroutine to format a duration in seconds into a human-readable string.
# ==============================================================================
sub format_duration {
    my ($seconds) = @_;
    if ($seconds >= 3600) {
        my $hours = int($seconds / 3600);
        my $minutes = int(($seconds % 3600) / 60);
        return sprintf("%dh %dm", $hours, $minutes);
    }
    return sprintf("%.2fs", $seconds) if $seconds < 60;
    my $minutes = int($seconds / 60);
    my $remaining_seconds = $seconds % 60;
    return sprintf("%dm %.2fs", $minutes, $remaining_seconds);
}

# Helper function to add months to a Time::Piece object
sub add_months {
    my ($time_obj, $months) = @_;

    my $year = $time_obj->year;
    my $month = $time_obj->mon + $months;
    my $day = $time_obj->mday;

    # Handle year rollover
    while ($month > 12) {
        $month -= 12;
        $year++;
    }
    while ($month < 1) {
        $month += 12;
        $year--;
    }

    # Handle day overflow (e.g., Jan 31 + 1 month should be Feb 28/29)
    my $days_in_month = days_in_month($year, $month);
    if ($day > $days_in_month) {
        $day = $days_in_month;
    }

    return Time::Piece->strptime(sprintf('%04d-%02d-%02d', $year, $month, $day), '%Y-%m-%d');
}

# Helper function to get days in a month
sub days_in_month {
    my ($year, $month) = @_;
    my @days = (31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31);

    # Check for leap year
    if ($month == 2 && (($year % 4 == 0 && $year % 100 != 0) || $year % 400 == 0)) {
        return 29;
    }

    return $days[$month - 1];
}

# ==============================================================================
# Calculates the start and end dates for a recurring monthly event based on a
# given reference date. It correctly handles month-end boundaries and invalid
# day-of-month configurations.
# ==============================================================================
sub _get_recurring_monthly_period {
    my ($base_date_obj, $day_of_period, $duration_days) = @_;

    my $end_date_obj;
    if ($day_of_period == -1) {
        # Last day of month: get first day of next month, then subtract one day
        my $next_month = add_months($base_date_obj, 1);
        my $first_of_next = Time::Piece->strptime(sprintf('%04d-%02d-01', $next_month->year, $next_month->mon), '%Y-%m-%d');
        $end_date_obj = $first_of_next - ONE_DAY;
    } else {
        # Specific day of month, clamped to be valid for that month
        my $year  = $base_date_obj->year;
        my $month = $base_date_obj->mon;
        my $day   = $day_of_period;

        my $days_in_target_month = days_in_month($year, $month);
        if ($day > $days_in_target_month) {
            # Clamp to the last day of the month if config is invalid (e.g., day 31 in Feb)
            $day = $days_in_target_month;
        }
        $end_date_obj = Time::Piece->strptime(sprintf('%04d-%02d-%02d', $year, $month, $day), '%Y-%m-%d');
    }

    # This calculation remains the same
    my $start_date_obj = $end_date_obj - (ONE_DAY * ($duration_days - 1));
    return ($start_date_obj, $end_date_obj);
}

# ==============================================================================
# Dedicated reporter for the multiplicative_seasonal model. It creates a
# single, rich final_forecast.csv file with all standard columns, the new
# SeasonalMultiplier column, and correctly offset Excel formulas.
# ==============================================================================
sub _write_multiplicative_seasonal_report {
    my ($system_id, $event_name, $timestamp, $forecast_data_href) = @_;

    my $s_id_safe = $system_id;
    my $e_name_safe = $event_name;
    $s_id_safe =~ s/[^a-zA-Z0-9_.-]//g;
    $e_name_safe =~ s/[^a-zA-Z0-9_.-]//g;

    my $filename = File::Spec->catfile($output_dir, "nfit-profile.$s_id_safe.$e_name_safe.final_forecast.$timestamp.csv");
    push @generated_files, $filename;
    print "  - Generating output file: $filename\n";

    open my $fh, '>', $filename or die "Warning: Could not open output file '$filename': $!";

    # Build the enhanced header with the new SeasonalMultiplier column
    my @header = (
        "VM", "TIER", "Hint", "Pattern", "Pressure", "PressureDetail", "SMT",
        "Serial", "SystemType", "Pool Name", "Pool ID", $PEAK_PROFILE_NAME
    );
    push @header, (map { $_->{name} } @profiles);
    push @header, ("SeasonalMultiplier", "Current - ENT", "NFIT - ENT", "NETT", "NETT%");
    print $fh join(",", map { quote_csv($_) } @header) . "\n";

    my $excel_row_num_counter = 1;
    foreach my $vm_name (sort keys %$forecast_data_href) {
        $excel_row_num_counter++;
        my $vm_data = $forecast_data_href->{$vm_name};
        my $report_data = $vm_data->{_report_data} || {};
        my $cfg_csv = $vm_config_data{$vm_name};

        # Gather standard fields for the row
        my $smt_out = $cfg_csv->{smt} // $default_smt_arg;
        my ($serial_out, $systype_out, $poolname_out, $poolid_out, $ent_out) = ('', '', '', '', '');
        if (defined $cfg_csv) {
            $serial_out = $cfg_csv->{serial} // ''; $systype_out = $cfg_csv->{systemtype} // '';
            $poolname_out = $cfg_csv->{pool_name} // ''; $poolid_out = $cfg_csv->{pool_id} // '';
            $ent_out = $cfg_csv->{entitlement} // '';
        }

        my @row = (
            $vm_name, "", $report_data->{hint} // "", $report_data->{pattern} // "",
            $report_data->{pressure} // "", $report_data->{pressure_detail} // "", $smt_out,
            $serial_out, $systype_out, $poolname_out, $poolid_out, ""
        );

        my $seasonal_multiplier_for_row = "N/A";
        foreach my $profile (@profiles) {
            my $p_name = $profile->{name};
            my $value = $vm_data->{$p_name} // '';
            push @row, (looks_like_number($value) ? sprintf("%.4f", $value) : $value);
            if ($seasonal_multiplier_for_row eq 'N/A' && exists $report_data->{seasonal_debug}{$p_name}) {
                $seasonal_multiplier_for_row = sprintf("%.2f", $report_data->{seasonal_debug}{$p_name}{multiplier});
            }
        }

        # Add the new SeasonalMultiplier and standard entitlement columns
        push @row, $seasonal_multiplier_for_row;
        push @row, (looks_like_number($ent_out) ? sprintf("%.2f", $ent_out) : $ent_out);

        # Generate the Excel formulas with the correct column offset
        my $nfit_ent_formula_str = generate_nfit_ent_formula($excel_row_num_counter, scalar(@profiles), 1); # <-- Pass offset of 1
        my $nett_user_formula_str = sprintf("=(%s-%s)", get_excel_col_name(13 + scalar(@profiles) + 2) . $excel_row_num_counter, get_excel_col_name(12 + scalar(@profiles) + 2) . $excel_row_num_counter);
        my $nett_perc_user_formula_str = sprintf("=IFERROR((%s-%s)/%s,\"\")", get_excel_col_name(13 + scalar(@profiles) + 2) . $excel_row_num_counter, get_excel_col_name(12 + scalar(@profiles) + 2) . $excel_row_num_counter, get_excel_col_name(12 + scalar(@profiles) + 2) . $excel_row_num_counter);
        push @row, $nfit_ent_formula_str, $nett_user_formula_str, $nett_perc_user_formula_str;

        print $fh join(",", map { quote_csv($_) } @row) . "\n";
    }
    close $fh;
}

# ==============================================================================
# Subroutine to log the rationale for a multiplicative seasonal forecast.
# It provides a clear, aligned, and explanatory summary of how the forecast
# was derived for each profile, matching the format of the standard log.
# ==============================================================================
sub log_multiplicative_seasonal_rationale
{
    my ($fh) = @_;

    # Ensure the script does not die if the log file handle is not valid.
    return unless $fh;

    # Iterate through each VM that has results, maintaining a consistent order.
    foreach my $vm_name (sort @vm_order)
    {
        # Check if there is seasonal debug information available for this VM.
        next unless exists $seasonal_debug_info{$vm_name};

        # Print a clear, top-level header for each VM in the log.
        print {$fh} "\n######################################################################\n";
        print {$fh} "# Rationale for VM: $vm_name\n";
        print {$fh} "######################################################################\n\n";
        print {$fh} "CPU Sizing Path: Multiplicative Seasonal Forecast (Event: $apply_seasonality_event)\n\n";

        # Iterate through each profile to log its specific forecast calculation.
        foreach my $profile (@profiles)
        {
            my $p_name = $profile->{name};
            next unless exists $seasonal_debug_info{$vm_name}{$p_name};

            my $s_data = $seasonal_debug_info{$vm_name}{$p_name};
            my $profile_desc = parse_profile_name_for_log($p_name);

            # Use a fixed width for labels to ensure consistent alignment of colons.
            my $label_width = 35;

            print {$fh} "======================================================================\n";
            printf {$fh} "%-${label_width}s : %s\n", "Profile Processed", $profile_desc;
            print {$fh} "----------------------------------------------------------------------\n";
            printf {$fh} "  %-${label_width}s : %.4f cores\n", "Current Baseline Value", $s_data->{baseline};
            printf {$fh} "  %-${label_width}s : %.4f\n", "Historical Multiplier", $s_data->{multiplier};

            # Add detailed breakdown of how the multiplier was derived
            if (exists $s_data->{historical_multipliers} && ref($s_data->{historical_multipliers}) eq 'ARRAY' && @{$s_data->{historical_multipliers}}) {
                printf {$fh} "  %-${label_width}s : %s\n", "  Multiplier Methodology", "Recency-weighted average of past events:";
                foreach my $hist_entry (@{$s_data->{historical_multipliers}}) {
                    my $hist_date = ref($hist_entry->{date}) ? $hist_entry->{date}->date : $hist_entry->{date};
                    printf {$fh} "  %-${label_width}s   %s: %.4f\n", "", $hist_date, $hist_entry->{value};
                }
            }

            printf {$fh} "  %-${label_width}s : %.4f\n", "Volatility Buffer", $s_data->{volatility};
            printf {$fh} "  %-${label_width}s : %.4f cores\n", "Forecasted Peak Residual", $s_data->{forecasted_residual};
            printf {$fh} "  %-${label_width}s : %.2f\n", "Peak Amplification Factor", $s_data->{amplification_factor};
            print {$fh} "  --------------------------------------------------------------------\n";
            print {$fh} "  Calculation : ((Baseline * Multiplier * Buffer) + Residual) * Amplification\n";
            print {$fh} "  --------------------------------------------------------------------\n";
            print {$fh} "  --------------------------------------------------------------------\n";
            printf {$fh} "  %-${label_width}s : %.4f cores\n", "Final Forecasted Value", $s_data->{forecast};
            # --- Print outlier warning if it exists ---
            if (defined $s_data->{OutlierWarning} && $s_data->{OutlierWarning} ne '') {
                print {$fh} "\n  --- Workload Volatility Alert ---\n";
                print {$fh} "  " . $s_data->{OutlierWarning} . "\n";
            }
            print {$fh} "======================================================================\n\n";
        }
    }
}

# ==============================================================================
# Subroutine to determine the correct seasonal analysis path.
# It checks if a multiplicative model has enough historical data to run. If not,
# it switches to the defined fallback event. This makes the tool resilient.
# It returns the name of the event that should ultimately be executed.
# ==============================================================================
sub determine_seasonal_analysis_path {
    my ($event_config, $system_cache_dir, $event_name) = @_;

    # This function is only relevant for models that have historical prerequisites.
    my $model_to_run = $event_config->{model} // '';
    return $event_name unless ($model_to_run eq 'multiplicative_seasonal' || $model_to_run eq 'predictive_peak');

    my $min_history_required = $event_config->{min_historical_years} || 1;

    # Read the unified history to count available snapshots for this event.
    my $unified_history = read_unified_history($system_cache_dir);
    my $history_count = 0;
    foreach my $month_data (values %$unified_history) {
        if (exists $month_data->{SeasonalEventSnapshots}{$event_name}) {
            $history_count++;
        }
    }

    if ($history_count >= $min_history_required) {
        print STDERR "INFO: Found $history_count historical snapshot(s) for event '$event_name'. Proceeding with forecast.\n";
        return $event_name;
    } else {
        my $fallback_event_name = $event_config->{fallback_event} // '';
        print STDERR "\nWARNING: Insufficient historical data for '$event_name' forecast.\n";
        print STDERR "  - Required historical snapshots: $min_history_required\n";
        print STDERR "  - Found: $history_count\n";

        if ($fallback_event_name ne '' && exists $seasonality_config->{$fallback_event_name}) {
            print STDERR "  - Executing fallback event: '$fallback_event_name'\n\n";
            return $fallback_event_name;
        } else {
            die "FATAL: Cannot run forecast for '$event_name' due to insufficient history, and no 'fallback_event' is configured.\n";
        }
    }
}

# ==============================================================================
# SUBROUTINE: _build_nfit_baseline_command
# PURPOSE:    Constructs the specific nfit command for calculating a baseline
#             for a seasonal model. It ensures a consistent command is built
#             and that any conflicting decay or growth flags from the profile
#             are removed. A baseline must be a pure measurement.
# ARGS:
#   1. $profile_flags_in (string): The raw flags from the profile config.
#   2. $baseline_start_str (string): The start date for the analysis.
#   3. $baseline_end_str (string): The end date for the analysis.
#   4. $system_cache_dir (string): Path to the target cache directory.
#   5. $enable_clipping_detection (boolean, optional): If true, adds the
#      --enable-clipping-detection flag to the command.
#   6. $is_generic_baseline    : if true, all decay and time filters are stripped.
#   7. $allow_growth_prediction: if true, growth predictions will be enabled.
#   8. $profile_name_for_label : profile name to record in the results cache (if specified)
# RETURNS:
#   - The fully constructed nfit command string.
# ==============================================================================
sub _build_nfit_baseline_command {
    my ($profile_flags_in, $baseline_start_str, $baseline_end_str, $system_cache_dir, $enable_clipping_detection, $is_generic_baseline, $allow_growth_prediction, $profile_name_for_label) = @_;

    my $profile_flags = $profile_flags_in; # Work on a copy.

    # --- NEW: Sanitise incoming flags to remove extraneous quotes from config files.
    # This is the primary fix for the "Unknown option" error.
    $profile_flags =~ s/^\s*"?|"?\s*$//g;

    # A baseline is a historical measurement, not a forecast. Growth prediction
    # should almost always be stripped, EXCEPT for the multiplicative model's
    # "CurrentBaseline", which needs to reflect the true current trend.
    unless ($allow_growth_prediction) {
        $profile_flags =~ s/--enable-growth-prediction\s*//g;
        $profile_flags =~ s/--growth-projection-days\s+\d+\s*//g;
        $profile_flags =~ s/--max-growth-inflation-percent\s+\d+\s*//g;
    }

    # The --enable-windowed-decay Model is a trending analysis:
    # Applying a trending model on top of a period that is supposed to be a static baseline measurement would be logically incorrect.
    $profile_flags =~ s/--enable-windowed-decay\s*//g;

    if ($is_generic_baseline) {
        $profile_flags =~ s/--(?:online|batch|no-weekends)\b\s*//g;
        $profile_flags =~ s/--decay\s+[\w-]+\s*//g;
        $profile_flags =~ s/--runq-decay\s+[\w-]+\s*//g;
        $profile_flags =~ s/--avg-method\s+\w+\s*//g;
    }
    $profile_flags =~ s/--decay-over-states\s*//g;

    # Construct the base command, ensuring --nmondir is always present.
    # the '-k' (Peak) flag is always included for statistical history.
    my $base_flags = "-q -k --nmondir \"$system_cache_dir\" $rounding_flags_for_nfit";

    # Only add date filters if they are actually defined.
    if (defined $baseline_start_str && defined $baseline_end_str) {
        $base_flags .= " --startdate $baseline_start_str --enddate $baseline_end_str";
    }

    my $vm_filter_arg = defined($target_vm_name) ? " -vm \"$target_vm_name\"" : "";
    my $smt_flag = "--smt $default_smt_arg";

    # Assemble the final, correct command.
    my $command = "$nfit_script_path $base_flags $vm_filter_arg $smt_flag $profile_flags";

    # Add clipping detection if requested
    if ($enable_clipping_detection) {
        $command .= " --enable-clipping-detection";
    }

    # Append the profile label if provided. This is for metadata and does not affect the L2 cache key.
    if (defined $profile_name_for_label && $profile_name_for_label ne '') {
        $command .= " --profile-label '$profile_name_for_label'";
    }

    return $command;
}

# ==============================================================================
# Main orchestrator for the 'predictive_peak' model.
# This version is enhanced to use the new two-part residual forecasting method.
# ==============================================================================
sub calculate_predictive_peak_forecast {
    my ($system_cache_dir, $system_identifier, $event_name, $event_config, $full_seasonality_config, $adaptive_runq_saturation_thresh) = @_;

    # --- Step 1: Get Historical Peak and Residual Data ---
    # Calls the enhanced helper to get a hash containing both data series.
    my $historical_data_href = _get_historical_peak_data($system_cache_dir, $event_name, $event_config);

    # --- Step 2: Calculate the Predicted Peak and Residual for each profile ---
    # Calls the enhanced prediction engine to get forecasts for both series.
    print STDERR " -> Step 2/3: Performing linear regression to predict next peak intensity and residual.\n";
    my ($predicted_components_href, $debug_info_for_vms) =
        _calculate_peak_prediction($historical_data_href, $event_config);

    # --- Step 3: Calculate the Non-Peak Baseline (excluding all historical peaks) ---
    # This logic remains unchanged.
    my $true_baseline_results_href = _get_true_baseline_results($system_cache_dir, $event_name, $event_config, $full_seasonality_config);

    # --- Step 4: Synthesize Final Results ---
    print STDERR " -> Step 3/3: Synthesizing final forecast from Predicted Peak, Predicted Residual, and Baseline.\n";
    my %final_results;
    foreach my $vm_name (keys %{$true_baseline_results_href}) {
        foreach my $profile (@profiles) {
            my $p_name = $profile->{name};
            my $baseline_val = $true_baseline_results_href->{$vm_name}{$p_name};

            # Get the two predicted components from the prediction engine.
            my $predicted_peak_val = $predicted_components_href->{$vm_name}{$p_name}{peak};
            my $predicted_residual_val = $predicted_components_href->{$vm_name}{$p_name}{residual};

            # Combine the signal and volatility forecasts. Undefined values are treated as zero.
            my $combined_prediction;
            if (defined $predicted_peak_val) {
                $combined_prediction = ($predicted_peak_val // 0) + ($predicted_residual_val // 0);
            }

            # The final recommendation is the higher of the baseline or the combined prediction.
            my ($final_value, $source) = (0, 'N/A');
            if (defined $baseline_val && defined $combined_prediction) {
                if ($baseline_val > $combined_prediction) {
                    $final_value = $baseline_val;
                    $source = 'TrueBaseline';
                } else {
                    $final_value = $combined_prediction;
                    $source = 'PeakPrediction';
                }
            } elsif (defined $combined_prediction) {
                $final_value = $combined_prediction;
                $source = 'PeakPrediction';
            } elsif (defined $baseline_val) {
                $final_value = $baseline_val;
                # This case indicates that prediction failed (e.g., insufficient history).
                $source = 'BaselineOnly (NoPrediction)';
            }

            # Apply the optional amplification factor
            my $amplification = $event_config->{peak_amplification_factor} // 1.0;
            my $final_recommendation = $final_value * $amplification;

            $final_results{$vm_name}{$p_name} = $final_recommendation;

            # Store all components in the debug hash for comprehensive logging.
            $seasonal_debug_info{$vm_name}{$p_name} = {
                TrueBaseline       => $baseline_val,
                PredictedPeak      => $predicted_peak_val,
                PredictedResidual  => $predicted_residual_val,
                CombinedPrediction => $combined_prediction,
                FinalSource        => $source,
                AmplificationFactor  => $amplification,
                FinalForecast        => $final_recommendation,
                PredictionDebug    => $debug_info_for_vms->{$vm_name}{$p_name},
                OutlierWarning     => _get_warning_for_vm_forecast($vm_name, \%outlier_warnings)
            };
        }
    }
    return \%final_results;
}

# ==============================================================================
# Helper to run nfit and get a "True/Non-Peak Baseline" by excluding all historical peak periods.
#
# ==============================================================================
sub _get_true_baseline_results {
    my ($system_cache_dir, $event_name, $event_config, $full_seasonality_config) = @_;

    my $data_cache_file = File::Spec->catfile($system_cache_dir, '.nfit.cache.data');
    return {} unless -f $data_cache_file;

    # --- Stage 1: Build a hash of dates to exclude ---
    my @all_peak_periods = find_all_historical_periods($full_seasonality_config, $data_cache_file);

    my %days_to_exclude_hash;
    my $total_days_to_process = 0;

    # Pre-calculate total days for an accurate progress bar and validation
    foreach my $period (@all_peak_periods) {
        next unless (defined $period && ref($period) eq 'ARRAY' && @$period == 2);
        my ($start, $end) = @{$period};
        next if ($start > $end);
        $total_days_to_process += (int(($end->epoch - $start->epoch) / 86400) + 1);
    }

    if ($total_days_to_process > 0) {
        print STDERR "  - Identifying $total_days_to_process unique peak day(s) for exclusion filter...\n";

        # Scope all counters locally to prevent bugs across function calls
        my $days_processed = 0;
        my $last_reported_perc = -1;
        my $period_num = 0;

        foreach my $period (@all_peak_periods) {
            $period_num++;
            next unless (defined $period && ref($period) eq 'ARRAY' && @$period == 2);
            my ($start, $end) = @{$period};
            next if ($start > $end);

            my $current = Time::Piece->new($start->epoch);
            while ($current <= $end) {

                $days_to_exclude_hash{$current->strftime('%Y-%m-%d')} = 1;
                $current += ONE_DAY;

                $days_processed++;

                my $current_perc = int(($days_processed / $total_days_to_process) * 100);
                if ($current_perc > $last_reported_perc) {
                    printf STDERR "\r    Expanding days: %d%% (%d/%d)", $current_perc, $days_processed, $total_days_to_process;
                    $last_reported_perc = $current_perc;
                }
            }
        }
        print STDERR "\n";
    }

    # --- Stage 2: Create the filtered data using the efficient hash lookup ---
    my ($filtered_fh, $filtered_filename) = tempfile(UNLINK => 1);

    my $unique_days_to_exclude = scalar(keys %days_to_exclude_hash);
    if ($unique_days_to_exclude > 0) {
        print STDERR "  - Filtering $unique_days_to_exclude day(s) from baseline data...";

        open(my $cache_fh, '<', $data_cache_file) or die "Cannot open $data_cache_file: $!";
        my $header = <$cache_fh>;
        print $filtered_fh $header;

        my $line_count = 0;
        while (my $line = <$cache_fh>) {
            $line_count++;
            print STDERR "." if $line_count % 500000 == 0;

            my $line_date = substr($line, 0, 10);
            next if exists $days_to_exclude_hash{$line_date};

            print $filtered_fh $line;
        }
        print STDERR " done\n";

        close $cache_fh;
    } else {
        print STDERR "  - No peak periods to filter; using full data cache for baseline.\n";
        system("cp '$data_cache_file' '$filtered_filename'");
    }

    close $filtered_fh;

    # --- Stage 3: Run nfit on the filtered file for each profile ---
    my %baseline_results;
    my $profile_count = scalar(@profiles);
    my $profile_num = 0;
    foreach my $profile (@profiles) {
        $profile_num++;
        print STDERR "    - Establishing Generic Non-Peak Baseline for profile $profile_num/$profile_count: $profile->{name}\n";

        # For a "True Baseline", we need a generic, unfiltered result. Enable generic baseline (no decay or time filters)
        my $nfit_cmd = _build_nfit_baseline_command($profile->{flags}, undef, undef, $system_cache_dir, 0, 1, undef, $profile->{name});

        my $nfit_output = '';
        # Use open3 to capture STDOUT, but let STDERR pass through to the terminal
        my $stderr_arg = ">&=" . fileno(STDERR);
        my $pid_nfit = open3(undef, my $stdout_nfit, $stderr_arg, "$nfit_cmd --show-progress");
        while(my $line = <$stdout_nfit>) {
            $nfit_output .= $line;
        }
        waitpid($pid_nfit, 0);

        next if $?;

        my $parsed = parse_nfit_json_output($nfit_output);
        my $p_key = "P" . clean_perc_label( ($profile->{flags} =~ /-p\s+([0-9.]+)/) ? $1 : $DEFAULT_PERCENTILE );

        foreach my $vm (keys %$parsed) {
            # For aggregated/decay runs, nfit returns a single JSON object per VM
            my @states_for_vm = @{$parsed->{$vm}};
            next unless @states_for_vm;

            my @valid_p_values;
            foreach my $state_hash (@states_for_vm) {
                my $metric_val = $state_hash->{metrics}{physc}{$p_key};
                if (defined $metric_val && looks_like_number($metric_val)) {
                    push @valid_p_values, $metric_val;
                }
            }

            if (@valid_p_values) {
                my $aggregated_value = sum0(@valid_p_values) / scalar(@valid_p_values);
                $baseline_results{$vm}{$profile->{name}} = $aggregated_value;
            }
        }
    }
    return \%baseline_results;
}

# ==============================================================================
# SUBROUTINE: _get_historical_peak_data
# PURPOSE:    Reads the snapshot cache for a given event and returns time series
#             data for historical peaks and residuals. This version is enhanced
#             to use the 'unclippedPeakEstimate' from the ClippingInfo block
#             if it exists, ensuring the returned data is corrected for
#             historical saturation.
# ARGS:
#   1. $system_cache_dir (string): Path to the target cache directory.
#   2. $event_name (string): The name of the event to retrieve data for.
#   3. $event_config (hash ref): The configuration for the event.
# RETURNS:
#   - A hash reference containing two keys, 'peaks' and 'residuals', each
#     pointing to a hash of historical data series, structured by profile and VM.
# ==============================================================================
sub _get_historical_peak_data {
    my ($system_cache_dir, $event_name, $event_config) = @_;

    print STDERR " -> Step 1/3: Loading historical peak and residual data for event '$event_name'.\n";

    my $unified_history = read_unified_history($system_cache_dir);
    my @event_history;
    # Extract all historical snapshots for the specified event.
    foreach my $month_key (sort keys %$unified_history) {
        my $month_data = $unified_history->{$month_key};
        if (exists $month_data->{SeasonalEventSnapshots}{$event_name}) {
            my $snapshot = $month_data->{SeasonalEventSnapshots}{$event_name};
            $snapshot->{_month_key} = $month_key; # Add date context
            push @event_history, $snapshot;
        }
    }

    my $max_peaks = $event_config->{max_historical_peaks} // 12;
    if (@event_history > $max_peaks) {
        @event_history = @event_history[-$max_peaks..-1]; # Keep only the most recent N peaks.
    }

    my %peaks_by_profile;
    my %residuals_by_profile; # New hash for residual data.

    foreach my $event (@event_history) {
        my $date = $event->{_month_key} . "-01"; # Use the first of the month as the date for the time series.
        my $hist_results = $event->{results};
        my $peak_results = $event->{results}{'PeakValue'} || {};
        my $residual_results = $event->{results}{'PeakResidual'} || {};

        # Process PeakValue data.
        foreach my $vm (keys %$peak_results) {
            foreach my $profile (keys %{$peak_results->{$vm}}) {
                my $value = $peak_results->{$vm}{$profile};
                if (exists $hist_results->{ClippingInfo}{$vm}{$profile}{unclippedPeakEstimate}) {
                    $value = $hist_results->{ClippingInfo}{$vm}{$profile}{unclippedPeakEstimate};
                }
                if (defined $value && looks_like_number($value)) {
                    push @{$peaks_by_profile{$profile}{$vm}}, { value => $value, date => $date };
                }
            }
        }

        # Process PeakResidual data.
        foreach my $vm (keys %$residual_results) {
            foreach my $profile (keys %{$residual_results->{$vm}}) {
                my $value = $residual_results->{$vm}{$profile};
                if (defined $value && looks_like_number($value)) {
                    push @{$residuals_by_profile{$profile}{$vm}}, { value => $value, date => $date };
                }
            }
        }
    }

    # Return a hash containing both data series.
    return {
        peaks => \%peaks_by_profile,
        residuals => \%residuals_by_profile
    };
}

# ==============================================================================
# Core statistical engine for the predictive_peak model.
# This version orchestrates a dual forecast for both the peak signal and the residual.
# ==============================================================================
sub _calculate_peak_prediction {
    my ($historical_data_href, $event_config) = @_;

    my $peak_series_per_profile = $historical_data_href->{peaks} || {};
    my $residual_series_per_profile = $historical_data_href->{residuals} || {};

    my %predictions;
    my %debug_info;

    my $min_peaks = $event_config->{min_historical_peaks} // 3;

    # This anonymous subroutine is a reusable, local prediction engine.
    # It takes a time series and returns a single predicted next value.
    my $_predict_next_value = sub {
        my ($series_aref) = @_;

        return (undef, "Insufficient history")
            unless (defined $series_aref && ref($series_aref) eq 'ARRAY' && scalar(@$series_aref) >= 2);

        # Perform simple outlier trimming by removing the min and max values
        # if the series is long enough to support it.
        my @values = map { $_->{value} } @$series_aref;
        @values = sort { $a <=> $b } @values;
        if (@values > 5) {
            shift @values; # Remove lowest
            pop @values;   # Remove highest
        }

        # Build points for the linear regression.
        my @points_for_regression;
        for my $i (0..$#values) {
            push @points_for_regression, [$i, $values[$i]];
        }

        my $regression = calculate_manual_linear_regression(\@points_for_regression);
        unless ($regression) {
            return (undef, "Regression failed");
        }

        my $slope = $regression->{slope};
        my $intercept = $regression->{intercept};

        # Project one step into the future.
        my $projected_value = ($slope * scalar(@values)) + $intercept;

        my $debug_str = sprintf("Success (Slope: %.4f, Points used: %d)", $slope, scalar(@values));

        return ($projected_value, $debug_str);
    };

    foreach my $profile_name (keys %{$peak_series_per_profile}) {
        foreach my $vm_name (keys %{$peak_series_per_profile->{$profile_name}}) {

            my $peak_series_aref = $peak_series_per_profile->{$profile_name}{$vm_name} || [];

            # Skip if there is not enough primary peak data to forecast.
            if (scalar(@$peak_series_aref) < $min_peaks) {
                $debug_info{$vm_name}{$profile_name} = {
                    peak_status => "Skipped: Insufficient history (" . scalar(@$peak_series_aref) . "/$min_peaks)",
                    residual_status => "Skipped: Dependent on peak forecast"
                };
                next;
            }

            # --- Perform the two separate forecasts ---
            my ($predicted_peak, $peak_debug) = $_predict_next_value->($peak_series_aref);

            my $residual_series_aref = $residual_series_per_profile->{$profile_name}{$vm_name} || [];
            my ($predicted_residual, $residual_debug) = $_predict_next_value->($residual_series_aref);

            # Store the individual forecast components.
            $predictions{$vm_name}{$profile_name} = {
                peak     => $predicted_peak,
                residual => $predicted_residual,
            };

            # Store the debug information for logging and rationale.
            $debug_info{$vm_name}{$profile_name} = {
                peak_status     => $peak_debug,
                residual_status => $residual_debug,
            };
        }
    }

    # Return the predictions and diagnostics to the calling function.
    return (\%predictions, \%debug_info);
}

# ==============================================================================
# Helper to calculate standard deviation.
# ==============================================================================
sub _calculate_std_dev {
    my ($data_aref) = @_;
    my $n = scalar(@$data_aref);
    return 0 if $n < 2;
    my $mean = sum0(@$data_aref) / $n;
    my $sum_sq_diff = sum0(map { ($_ - $mean)**2 } @$data_aref);
    return sqrt($sum_sq_diff / ($n - 1));
}

# ==============================================================================
# Finds all historical peak periods defined in the seasonality config that
# fall within the date range of the available cache data.
# This version is optimised for performance and robustness by pre-compiling
# regexes, reducing object creation, and preventing runaway loops.
# ==============================================================================
sub find_all_historical_periods {
    my ($full_seasonality_config, $data_cache_file) = @_;

    # Determine the actual time span of the data in the cache.
    my ($cache_start, $cache_end) = _get_cache_date_range($data_cache_file);
    return [] unless $cache_start && $cache_end;

    my @periods;
    my $now = gmtime(); # Cache current time to avoid repeated calls.

    # Pre-compile the regex once, outside the loops.
    my $date_range_regex = qr/(\d{4}-\d{2}-\d{2}):(\d{4}-\d{2}-\d{2})/;

    foreach my $e_name (keys %$full_seasonality_config) {
        my $e_config = $full_seasonality_config->{$e_name};

        # --- Path for events defined with fixed 'dates' ---
        if (defined $e_config->{dates}) {
            my @date_ranges = split /\s*,\s*/, $e_config->{dates};

            foreach my $range (@date_ranges) {
                next unless $range =~ $date_range_regex;

                # Only create Time::Piece objects after a successful regex match.
                my $start = Time::Piece->strptime($1, '%Y-%m-%d');
                my $end = Time::Piece->strptime($2, '%Y-%m-%d');
                push @periods, [$start, $end];
            }

        # --- Path for events defined with a recurring 'period' ---
        } elsif (defined $e_config->{period} && $e_config->{period} eq 'monthly') {
            # Pre-extract config values to avoid repeated hash lookups inside the loop.
            my $day_of_period = $e_config->{day_of_period} // -1;
            my $duration_days = $e_config->{duration_days} // 7;

            my $current_iterator = Time::Piece->new($cache_start->epoch)->truncate(to => 'month');

            # Add runaway protection to prevent infinite loops on very large date ranges.
            my $max_iterations = 1000; # Sensible limit for ~83 years of monthly data.
            my $iteration_count = 0;

            while ($current_iterator <= $cache_end && $iteration_count < $max_iterations) {
                my ($start, $end) = _get_recurring_monthly_period($current_iterator, $day_of_period, $duration_days);

                # Add the period if it's valid, historical, and within the cache's time span.
                if ($start && $end && $end < $now && $end <= $cache_end) {
                    push @periods, [$start, $end];
                }

                $current_iterator = $current_iterator->add_months(1);
                $iteration_count++;
            }

            # Warn the user if the protection limit was reached.
            warn "Monthly iteration limit reached for event '$e_name'" if $iteration_count >= $max_iterations;
        }
    }

    return @periods;
}

# ==============================================================================
# Subroutine to log the rationale for a predictive_peak seasonal forecast.
# ==============================================================================
sub log_predictive_peak_rationale {
    my ($fh) = @_;
    return unless $fh;

    foreach my $vm_name (sort @vm_order) {
        next unless exists $seasonal_debug_info{$vm_name};

        print {$fh} "\n######################################################################\n";
        print {$fh} "# Rationale for VM: $vm_name\n";
        print {$fh} "######################################################################\n\n";
        print {$fh} "CPU Sizing Path: Predictive Peak Forecast (Event: $apply_seasonality_event)\n\n";

        foreach my $profile (@profiles) {
            my $p_name = $profile->{name};
            next unless exists $seasonal_debug_info{$vm_name}{$p_name};

            my $s_data = $seasonal_debug_info{$vm_name}{$p_name};
            my $profile_desc = parse_profile_name_for_log($p_name);
            my $label_width = 38;

            print {$fh} "======================================================================\n";
            printf {$fh} "%-${label_width}s : %s\n", "Profile Processed", $profile_desc;
            print {$fh} "----------------------------------------------------------------------\n";

            my $predicted_peak_val = $s_data->{PredictedPeak};

            if (!defined $predicted_peak_val) {
                # This block handles cases where prediction was skipped (e.g., insufficient history).
                my $debug_info = $s_data->{PredictionDebug}{peak_status} // "Insufficient history";
                printf {$fh} "  %-${label_width}s : SKIPPED (%s)\n", "Peak Trend Prediction", $debug_info;
                printf {$fh} "  %-${label_width}s : %.4f cores\n", "1. Non-Peak Baseline", ($s_data->{TrueBaseline} // 0);
                print {$fh} "  --------------------------------------------------------------------\n";
                printf {$fh} "  %-${label_width}s : %.4f cores (Source: Baseline Only)\n", "Final Recommendation", ($s_data->{FinalForecast} // 0);

            } else {
                # This block logs a successful, multi-step forecast.
                printf {$fh} "  1. %-${label_width}s : %.4f cores\n", "Non-Peak Baseline (Floor)", ($s_data->{TrueBaseline} // 0);
                printf {$fh} "  2. %-${label_width}s : %.4f cores\n", "Predicted Peak (from trend)", $predicted_peak_val;
                printf {$fh} "  3. %-${label_width}s : %.4f cores\n", "Predicted Peak Residual", ($s_data->{PredictedResidual} // 0);
                printf {$fh} "  4. %-${label_width}s : %.2f\n", "Peak Amplification Factor", ($s_data->{AmplificationFactor} // 1.0);
                print {$fh} "  --------------------------------------------------------------------\n";
                printf {$fh} "  Calculation : MAX( Baseline, (Predicted Peak + Predicted Residual) * Amplification )\n";
                print {$fh} "  --------------------------------------------------------------------\n";
                printf {$fh} "  %-${label_width}s : %.4f cores (Source: %s)\n", "Final Forecasted Value",
                    ($s_data->{FinalForecast} // 0),
                    ($s_data->{FinalSource} // 'N/A');
            }

            # --- Print outlier warning if it exists ---
            if (defined $s_data->{OutlierWarning} && $s_data->{OutlierWarning} ne '') {
                print {$fh} "\n  --- Workload Volatility Alert ---\n";
                print {$fh} "  " . $s_data->{OutlierWarning} . "\n";
            }

            print {$fh} "======================================================================\n\n";
        }
    }
}

# ==============================================================================
# Calculate linear regression (slope, intercept, R-squared) manually
# ==============================================================================
sub calculate_manual_linear_regression
{
    my ($points_aref) = @_;
    my $n = scalar @{$points_aref};

    return undef if $n < 2;

    my ($sum_x, $sum_y, $sum_xy, $sum_x_squared, $sum_y_squared) = (0, 0, 0, 0, 0);

    foreach my $point (@{$points_aref})
    {
        my ($x_val, $y_val) = @{$point};
        $sum_x += $x_val;
        $sum_y += $y_val;
        $sum_xy += $x_val * $y_val;
        $sum_x_squared += $x_val**2;
        $sum_y_squared += $y_val**2;
    }

    my $denominator_slope = ($n * $sum_x_squared) - ($sum_x**2);

    if (abs($denominator_slope) > $FLOAT_EPSILON)
    {
        my $slope_calc = (($n * $sum_xy) - ($sum_x * $sum_y)) / $denominator_slope;
        my $intercept_calc = ($sum_y - ($slope_calc * $sum_x)) / $n;
        return {
            slope     => $slope_calc,
            intercept => $intercept_calc,
            n_points  => $n,
        };
    }

    # Cannot reliably determine a linear trend.
    return undef;
}

# ==============================================================================
# SUBROUTINE: read_unified_history
# PURPOSE:    Reads the new, unified monthly history cache file. This file
#             serves as the single source of truth for all historical analysis
#             for predictive models.
# ARGUMENTS:
#   1. $system_cache_dir (string): The path to a specific system's cache directory.
# RETURNS:
#   - A hash reference of the decoded JSON data.
#   - Returns an empty hash reference if the file does not exist, is empty,
#     or is corrupt, ensuring a safe default.
# ==============================================================================
sub read_unified_history {
    my ($system_cache_dir) = @_;

    my $history_file = File::Spec->catfile($system_cache_dir, $UNIFIED_HISTORY_FILE);
    my $history_data = {}; # Default to an empty hash.

    # A non-existent or empty file is a valid state for a new system.
    return $history_data unless (-f $history_file && -s $history_file);

    eval {
        my $json = JSON->new->allow_nonref;
        local $/; # Enable slurp mode to read the entire file.
        open my $fh, '<:encoding(utf8)', $history_file or die "Could not open '$history_file' for reading: $!";
        my $json_text = <$fh>;
        close $fh;
        $history_data = $json->decode($json_text);
    };
    if ($@) {
        warn "Warning: Could not decode JSON from unified history cache '$history_file'. It may be corrupt. Treating as empty. Error: $@";
        return {}; # Return an empty hash on error to prevent downstream failures.
    }

    return $history_data;
}

# ==============================================================================
# SUBROUTINE: write_unified_history
# PURPOSE:    Writes a new monthly entry to the unified history cache file.
#             This function handles file locking to prevent data corruption from
#             concurrent processes. It performs a safe read-modify-write of
#             the entire data structure.
# ARGUMENTS:
#   1. $system_cache_dir (string): The path to a system's cache directory.
#   2. $month_key (string): The key for the new entry (e.g., "2025-07").
#   3. $new_data_for_month_href (hash ref): The data for the new monthly entry.
# RETURNS:
#   - 1 on success, 0 on failure.
# ==============================================================================
sub write_unified_history {
    my ($system_cache_dir, $month_key, $new_data_for_month_href) = @_;

    my $history_file = File::Spec->catfile($system_cache_dir, $UNIFIED_HISTORY_FILE);
    my $lock_file    = File::Spec->catfile($system_cache_dir, '.nfit.history.lock');

    # Acquire an exclusive lock to ensure an atomic read-modify-write operation.
    open my $lock_fh, '>', $lock_file or do {
        warn "Warning: Could not create lock file '$lock_file' for writing history cache: $!. Skipping cache write.";
        return 0;
    };
    flock($lock_fh, LOCK_EX) or do {
        warn "Warning: Could not acquire exclusive lock on '$lock_file': $!. Skipping cache write.";
        close $lock_fh;
        return 0;
    };

    my $success = 0;
    eval {
        # Read the existing history file first.
        my $history_data = read_unified_history($system_cache_dir);

        # Add or overwrite the data for the specified month.
        $history_data->{$month_key} = $new_data_for_month_href;

        # Write the entire updated data structure back to the file.
        my $json = JSON->new->pretty->canonical;
        my $json_text = $json->encode($history_data);

        open my $fh, '>:encoding(utf8)', $history_file or die "Could not open '$history_file' for writing: $!";
        print $fh $json_text;
        close $fh;
        $success = 1;
    };
    if ($@) {
        warn "Warning: An error occurred while writing to the unified history cache '$history_file': $@";
        $success = 0;
    }

    # Release the lock.
    close $lock_fh;
    unlink $lock_file;

    return $success;
}

# ==============================================================================
# SUBROUTINE: _generate_seasonal_snapshot_for_period
# PURPOSE:    Helper function to perform the analysis for a single seasonal
#             event's peak and baseline periods.
# ==============================================================================
sub _generate_seasonal_snapshot_for_period {
    my ($system_cache_dir, $event_config, $event_name, $peak_start, $peak_end, $baseline_start, $baseline_end) = @_;

    my %peak_period_results;
    my %baseline_period_results;
    my %clipping_info_results;
    my %residual_results;
    my %sanitized_peak_results;

    my $model_type = $event_config->{model} // '';
    my ($peak_key, $baseline_key) = ('HistoricPeak', 'HistoricBaseline');
    if ($model_type eq 'predictive_peak' or $model_type eq 'adaptive_peak_forecasting') {
        ($peak_key, $baseline_key) = ('PeakValue', 'BaselineValue');
    }

    # --- 1. Analyse Baseline Period ---
    print STDERR "      - Analysing Baseline Period: " . $baseline_start->date . " to " . $baseline_end->date . "\n";
    my $profile_count = scalar(@profiles);
    my $profile_num = 0;
    foreach my $profile (@profiles) {
        $profile_num++;
        print STDERR "    - Baselining profile $profile_num/$profile_count: $profile->{name}\n";
        my $nfit_cmd = _build_nfit_baseline_command($profile->{flags}, $baseline_start->date, $baseline_end->date, $system_cache_dir, 0, 0, undef, $profile->{name});
        my $nfit_output = '';
        my $stderr_arg = ">&=" . fileno(STDERR);
        my $pid = open3(undef, my $stdout, $stderr_arg, $nfit_cmd);
        while(my $line = <$stdout>) { $nfit_output .= $line; }
        waitpid($pid, 0);
        next if $?;
        my $parsed = parse_nfit_json_output($nfit_output);
        my $p_metric_key = "P" . clean_perc_label(($profile->{flags} =~ /-p\s+([0-9.]+)/) ? $1 : $DEFAULT_PERCENTILE);
        foreach my $vm_name (keys %$parsed) {
            my $final_state = $parsed->{$vm_name}[-1];
            next unless $final_state;
            if (defined $final_state->{metrics}{physc}{$p_metric_key}) {
                $baseline_period_results{$vm_name}{$profile->{name}} = $final_state->{metrics}{physc}{$p_metric_key};
            }
        }
    }


    # --- 2. Get Sanitised Peak for Residual Calculation (if configured) ---
    if (defined $event_config->{residual_peak_profile} && $event_config->{residual_peak_profile} ne '') {
        print STDERR "      - Calculating Residual Peak for period: " . $peak_start->date . " to " . $peak_end->date . "\n";
        my $residual_flags = $event_config->{residual_peak_profile};
        my $nfit_cmd = _build_nfit_baseline_command($residual_flags, $peak_start->date, $peak_end->date, $system_cache_dir, 0, 0, undef, "ResidualPeakProfile");
        my $nfit_output = '';
        my $stderr_arg = ">&=" . fileno(STDERR);
        my $pid = open3(undef, my $stdout, $stderr_arg, $nfit_cmd);
        while(my $line = <$stdout>) { $nfit_output .= $line; }
        waitpid($pid, 0);

        if ($? == 0) {
            my $parsed = parse_nfit_json_output($nfit_output);
            my $p_metric_key = "P" . clean_perc_label(($residual_flags =~ /-p\s+([0-9.]+)/) ? $1 : $DEFAULT_PERCENTILE);
            foreach my $vm_name (keys %$parsed) {
                my $final_state = $parsed->{$vm_name}[-1];
                if ($final_state && defined $final_state->{metrics}{physc}{$p_metric_key}) {
                    $sanitized_peak_results{$vm_name} = $final_state->{metrics}{physc}{$p_metric_key};
                }
            }
        }
    }

    # --- 3. Analyse Standard Peak Period ---
    print STDERR "      - Analysing Standard Peak Period: " . $peak_start->date . " to " . $peak_end->date . "\n";
    $profile_num = 0;
    foreach my $profile (@profiles) {
        $profile_num++;
        print STDERR "    - Analysing standard peak $profile_num/$profile_count: $profile->{name}\n";
        my $nfit_cmd = _build_nfit_baseline_command($profile->{flags}, $peak_start->date, $peak_end->date, $system_cache_dir, 1, 0, undef, $profile->{name});
        my $nfit_output = '';
        my $stderr_arg = ">&=" . fileno(STDERR);
        my $pid = open3(undef, my $stdout, $stderr_arg, $nfit_cmd);
        while(my $line = <$stdout>) { $nfit_output .= $line; }
        waitpid($pid, 0);
        next if $?;

        my $parsed = parse_nfit_json_output($nfit_output);
        my $p_metric_key = "P" . clean_perc_label(($profile->{flags} =~ /-p\s+([0-9.]+)/) ? $1 : $DEFAULT_PERCENTILE);

        foreach my $vm_name (keys %$parsed) {
            my $final_state = $parsed->{$vm_name}[-1];
            next unless $final_state;

            my $standard_peak_value;
            if (defined $final_state->{metrics}{physc}{$p_metric_key}) {
                $standard_peak_value = $final_state->{metrics}{physc}{$p_metric_key};
                $peak_period_results{$vm_name}{$profile->{name}} = $standard_peak_value;
            }

            # --- FIX: Manually construct the complete, correctly structured ClippingInfo block ---
            my $clipping_metrics = $final_state->{metrics}{clipping};
            if ($clipping_metrics && ref($clipping_metrics) eq 'HASH' && exists $clipping_metrics->{isClipped}) {
                my $unmet_demand = $clipping_metrics->{unmetDemand} // 0;
                # nfit.pl returns a flat structure, but nfit-forecast expects the nested one. We reconstruct it here.
                $clipping_info_results{$vm_name}{$profile->{name}} = {
                    isClipped               => $clipping_metrics->{isClipped},
                    clippingConfidence      => $clipping_metrics->{confidence},
                    capacityLimit           => $final_state->{metadata}{configuration}{maxCpu},
                    unmetDemandEstimate     => $unmet_demand,
                    unclippedPeakEstimate   => ($standard_peak_value // 0) + $unmet_demand,
                    platformSpecificMarkers => {
                        aix_runq_saturation => $clipping_metrics->{runqSaturationScore}
                    }
                };
            }

            if (defined $standard_peak_value && exists $sanitized_peak_results{$vm_name}) {
                my $residual = $sanitized_peak_results{$vm_name} - $standard_peak_value;
                $residual_results{$vm_name}{$profile->{name}} = ($residual > 0) ? $residual : 0;
            }
        }
    }

    # --- 4. Final Validation and Return ---
    unless (scalar keys %peak_period_results && scalar keys %baseline_period_results) {
        warn "      - WARNING: Could not generate valid peak/baseline data for event '$event_name'. Skipping snapshot.\n";
        return undef;
    }

    return {
        eventName     => $event_name,
        periodEndDate => $peak_end->date,
        generatedOn   => localtime()->datetime(),
        results       => {
            $peak_key      => \%peak_period_results,
            $baseline_key  => \%baseline_period_results,
            'ClippingInfo' => \%clipping_info_results,
            'PeakResidual' => \%residual_results
        }
    };
}

# ==============================================================================
# SUBROUTINE: update_monthly_history
# PURPOSE:    Orchestrates a Two-Phase strategy to create and update the
#             unified monthly history cache (`.nfit.history.json`). This is the
#             primary data generation process for all predictive models.
# ==============================================================================
sub update_monthly_history {
    my ($system_cache_dir, $system_identifier, $seasonality_config, $min_days_for_history) = @_;

    print STDERR "\n--- Updating Unified Monthly History for system: $system_identifier ---\n";

    my $data_cache_file = File::Spec->catfile($system_cache_dir, '.nfit.cache.data');
    my ($cache_start_obj, $cache_end_obj) = _get_cache_date_range($data_cache_file);

    unless ($cache_start_obj && $cache_end_obj) {
        warn "Warning: Could not determine date range from cache file. Cannot update history.\n";
        return;
    }
    print STDERR "Data cache spans from " . $cache_start_obj->date . " to " . $cache_end_obj->date . "\n";

    my $existing_history = read_unified_history($system_cache_dir);
    my @months_to_process;

    # --- Planning Stage: Discover unprocessed or partially processed months ---
    print STDERR "Scanning cache for unprocessed or incomplete months...\n";
    my $iterator_month = Time::Piece->new($cache_start_obj->epoch)->truncate(to => 'month');
    while ($iterator_month <= $cache_end_obj) {
        my $month_key = $iterator_month->strftime('%Y-%m');

        my $month_start_obj = Time::Piece->new($iterator_month->epoch);
        my $month_end_obj   = Time::Piece->new($month_start_obj->epoch)->add_months(1) - ONE_DAY;

        my $effective_start_obj = ($cache_start_obj > $month_start_obj) ? $cache_start_obj : $month_start_obj;
        my $effective_end_obj   = ($cache_end_obj   < $month_end_obj)   ? $cache_end_obj   : $month_end_obj;

        my $days_in_period = ($effective_start_obj <= $effective_end_obj)
            ? (int(($effective_end_obj->epoch - $effective_start_obj->epoch) / 86400) + 1)
            : 0;

        if (exists $existing_history->{$month_key}) {
            my $stored_days = $existing_history->{$month_key}{Metadata}{ProcessedDays} // 0;
            if ($days_in_period > $stored_days) {
                 print STDERR "  - INFO: Found more complete data for $month_key ($days_in_period > $stored_days days). Queueing for reprocessing.\n";
            } else {
                $iterator_month = $iterator_month->add_months(1);
                next; # Skip, history is up-to-date for this month
            }
        }

        if ($days_in_period >= $min_days_for_history) {
            push @months_to_process, {
                key     => $month_key,
                start_obj => $effective_start_obj,
                end_obj   => $effective_end_obj,
                days    => $days_in_period
            };
        }
        $iterator_month = $iterator_month->add_months(1);
    }
    print STDERR "Scan complete.\n";

    if (@months_to_process) {
        print STDERR "Identified " . scalar(@months_to_process) . " month(s) to process: " . join(", ", map { $_->{key} } @months_to_process) . "\n";

        # --- Execution Stage: Two-Phase Priming ---
        my @all_historical_peaks = find_all_historical_periods($seasonality_config, $data_cache_file);

        # Phase 1: Generate the foundational monthly workload analysis.
        _generate_monthly_workload_history($system_cache_dir, \@months_to_process, $existing_history);

        # Phase 2: Generate the model-specific seasonal snapshots.
        _generate_seasonal_snapshot_history($system_cache_dir, \@months_to_process, $seasonality_config, \@all_historical_peaks);

    } else {
        print STDERR "No new months to process. History is up to date.\n";
    }

    # --- Phase 3: Final pass to enrich history with pre-calculated growth data from L2 cache ---
    _enrich_history_with_growth_rationale($system_cache_dir);
}

# ==============================================================================
# SUBROUTINE: _generate_monthly_workload_history (Phase 1)
# PURPOSE:    Processes a list of months to generate and save the generic,
#             model-agnostic `MonthlyWorkloadAnalysis` for each one.
# ==============================================================================
sub _generate_monthly_workload_history {
    my ($system_cache_dir, $months_to_process_aref, $existing_history_href) = @_;

    print "\n--- Phase 1: Generating Monthly Workload Analysis ---\n";
    foreach my $month_job (@$months_to_process_aref) {
        my $month_key = $month_job->{key};
        my $start_str = $month_job->{start_obj}->strftime('%Y-%m-%d');
        my $end_str   = $month_job->{end_obj}->strftime('%Y-%m-%d');

        print "  - Processing Month: $month_key (Period: $start_str to $end_str)\n";

        my $workload_analysis = _generate_monthly_workload_analysis($system_cache_dir, $start_str, $end_str);

        # Prepare the full entry for this month, preserving existing snapshots if reprocessing
        my %month_history_entry = %{$existing_history_href->{$month_key} || {}};
        $month_history_entry{MonthlyWorkloadAnalysis} = $workload_analysis;
        $month_history_entry{Metadata} = {
            ProcessedStartDate => $start_str,
            ProcessedEndDate   => $end_str,
            ProcessedDays      => $month_job->{days},
            LastUpdated        => localtime()->datetime(),
        };

        print "    - Writing analysis for $month_key to history cache...\n";
        write_unified_history($system_cache_dir, $month_key, \%month_history_entry);
    }
}

# ==============================================================================
# SUBROUTINE: _generate_seasonal_snapshot_history (Phase 2)
# PURPOSE:    Processes a list of months to generate model-specific snapshots.
#             Implements intelligent caching to avoid redundant `nfit` calls
#             for events that share peak/baseline periods. Includes critical logic
#             to prevent baseline contamination from preceding peaks.
# ==============================================================================
sub _generate_seasonal_snapshot_history {
    my ($system_cache_dir, $months_to_process_aref, $seasonality_config, $all_historical_peaks_aref) = @_;

    print "\n--- Phase 2: Generating Seasonal Event Snapshots ---\n";
    foreach my $month_job (@$months_to_process_aref) {
        my $month_key   = $month_job->{key};
        my $start_obj   = $month_job->{start_obj};
        my $end_obj     = $month_job->{end_obj};

        print "  - Discovering events for month: $month_key...\n";

        my %seasonal_snapshots_for_month;
        my %calculation_cache; # In-memory cache for this month's run

        foreach my $event_name (sort keys %$seasonality_config) {
            next if $event_name eq 'Global';
            my $event_config = $seasonality_config->{$event_name};
            next if (($event_config->{model} // '') eq 'recency_decay'); # Skip non-priming models

            my ($peak_start, $peak_end) = determine_event_period($event_config, $start_obj);

            if ($peak_start && $peak_end && $peak_start <= $end_obj && $peak_end >= $start_obj) {
                print "    - Found active event: '$event_name'\n";

                my $baseline_days = $event_config->{baseline_period_days} // 16;
                my $potential_baseline_start = $peak_start - ($baseline_days * ONE_DAY);

                my $latest_preceding_peak_end;
                foreach my $p (@{$all_historical_peaks_aref}) {
                    if ($p->[1] < $peak_start && (!defined $latest_preceding_peak_end || $p->[1] > $latest_preceding_peak_end)) {
                        $latest_preceding_peak_end = $p->[1];
                    }
                }

                my $baseline_start_obj = $potential_baseline_start;
                if (defined $latest_preceding_peak_end && $potential_baseline_start <= $latest_preceding_peak_end) {
                    $baseline_start_obj = $latest_preceding_peak_end + ONE_DAY;
                    my $available_days = ($peak_start->epoch - $baseline_start_obj->epoch) / ONE_DAY;
                    print "      - WARNING: Baseline for this event truncated to " . int($available_days) . " day(s) to avoid overlap with a preceding peak.\n";
                }

                my $baseline_end_obj = $peak_start - ONE_SECOND;

                # --- FIX: Make the cache key model-specific ---
                my $model_type_for_key = $event_config->{model} // 'unknown';
                my $cache_key = $peak_start->date . ":" . $peak_end->date . ":" . $baseline_start_obj->date . ":" . $model_type_for_key;

                my $snapshot_results;
                if (exists $calculation_cache{$cache_key}) {
                    print "      - INFO: Reusing cached calculations for identical time period and model type.\n";
                    $snapshot_results = $calculation_cache{$cache_key};
                } else {
                    $snapshot_results = _generate_seasonal_snapshot_for_period(
                        $system_cache_dir, $event_config, $event_name,
                        $peak_start, $peak_end,
                        $baseline_start_obj, $baseline_end_obj
                    );
                    $calculation_cache{$cache_key} = $snapshot_results; # Cache the result
                }

                if ($snapshot_results) {
                    $snapshot_results->{eventName} = $event_name;
                    $seasonal_snapshots_for_month{$event_name} = $snapshot_results;
                }
            }
        }

        if (scalar keys %seasonal_snapshots_for_month > 0) {
            print "    - Writing snapshots for $month_key to history cache...\n";
            my $history_data = read_unified_history($system_cache_dir);
            $history_data->{$month_key}{SeasonalEventSnapshots} = \%seasonal_snapshots_for_month;
            $history_data->{$month_key}{Metadata}{LastUpdated} = localtime()->datetime();
            _write_full_history($system_cache_dir, $history_data);
        }
    }
}

# ==============================================================================
# SUBROUTINE: _write_full_history
# PURPOSE:    A simple wrapper for writing the entire history data structure.
#             This is a simplified version of write_unified_history for clarity.
# ==============================================================================
sub _write_full_history {
    my ($system_cache_dir, $history_data_href) = @_;

    my $history_file = File::Spec->catfile($system_cache_dir, $UNIFIED_HISTORY_FILE);
    my $lock_file    = File::Spec->catfile($system_cache_dir, '.nfit.history.lock');

    open my $lock_fh, '>', $lock_file or die "Cannot create lock file '$lock_file': $!";
    flock($lock_fh, LOCK_EX) or die "Cannot acquire lock on '$lock_file': $!";

    eval {
        my $json = JSON->new->pretty->canonical;
        my $json_text = $json->encode($history_data_href);
        open my $fh, '>:encoding(utf8)', $history_file or die "Could not open '$history_file' for writing: $!";
        print $fh $json_text;
        close $fh;
    };
    if ($@) {
        warn "An error occurred while writing to unified history cache '$history_file': $@";
    }

    close $lock_fh;
    unlink $lock_file;
}

# ==============================================================================
# SUBROUTINE: _enrich_history_with_growth_rationale
# PURPOSE:    Performs a final enrichment pass on the unified history file.
#             It robustly scans the nfit L2 Results Cache for growth-enabled
#             model runs, reads the embedded 'profileLabel' from the data, and
#             injects the 'growthRationale' block into the correct location,
#             keyed by the profile name.
# ==============================================================================
sub _enrich_history_with_growth_rationale {
    my ($system_cache_dir) = @_;

    print "\n--- Phase 3: Enriching History with Growth Rationale from L2 Cache ---\n";

    my $history_file = File::Spec->catfile($system_cache_dir, $UNIFIED_HISTORY_FILE);
    my $l2_cache_file = File::Spec->catfile($system_cache_dir, '.nfit.cache.results');

    unless (-f $history_file && -f $l2_cache_file) {
        print "  - INFO: History file or L2 results cache not found. Skipping enrichment.\n";
        return;
    }

    # --- Step 1: Load both the history and the L2 cache ---
    my $history_data = read_unified_history($system_cache_dir);
    my $l2_cache_data = eval {
        local $/;
        open my $fh, '<:encoding(utf8)', $l2_cache_file or die "Could not open L2 cache '$l2_cache_file': $!";
        my $json_text = <$fh>;
        close $fh;
        JSON->new->allow_nonref->decode($json_text);
    };
    if ($@ || !ref($l2_cache_data) eq 'HASH') {
        warn "Warning: Could not decode L2 results cache '$l2_cache_file'. Skipping enrichment.\n";
        return;
    }

    # --- Step 2: Scan L2 cache and build a structured hash of all available growth data ---
    # This hash is the single source of truth for all growth rationale.
    # Structure: {vm_name}{profile_name}{model_type} = rationale_hash
    my %growth_data_found;
    foreach my $l2_key (keys %$l2_cache_data) {
        my $l2_results_aref = $l2_cache_data->{$l2_key};
        next unless (ref($l2_results_aref) eq 'ARRAY' && @$l2_results_aref);

        my $first_result = $l2_results_aref->[0];
        my $analysis_type = $first_result->{analysisType} // '';
        my $model_type;

        if ($analysis_type eq 'hybrid_decay_aggregated') {
            $model_type = 'decay_over_states';
        } elsif ($analysis_type eq 'windowed_decay_aggregated') {
            $model_type = 'windowed_decay';
        } else {
            next; # Not a growth-enabled model result, skip this L2 cache entry.
        }

        foreach my $result (@$l2_results_aref) {
            my $vm = $result->{vmName};
            my $profile_name = $result->{profileLabel};

            if ($vm && $profile_name && $result->{debug} && $result->{debug}{growthRationale}) {
                $growth_data_found{$vm}{$profile_name}{$model_type} = $result->{debug}{growthRationale};
            }
        }
    }

    # --- Step 3: Iterate through history and inject the found growth data ---
    my $was_modified = 0;
    foreach my $month_key (keys %$history_data) {
        next unless (ref($history_data->{$month_key}) eq 'HASH' && $history_data->{$month_key}{MonthlyWorkloadAnalysis});
        my $workload_analysis = $history_data->{$month_key}{MonthlyWorkloadAnalysis};

        foreach my $vm_name (keys %$workload_analysis) {
            # Skip VMs for which no growth data was ever found.
            next unless exists $growth_data_found{$vm_name};

            my $vm_analysis = $workload_analysis->{$vm_name};

            # Inject all found growth data for this VM.
            foreach my $profile_name (keys %{$growth_data_found{$vm_name}}) {
                foreach my $model_type (keys %{$growth_data_found{$vm_name}{$profile_name}}) {

                    my $new_data = $growth_data_found{$vm_name}{$profile_name}{$model_type};

                    # Ensure the nested structure exists.
                    $vm_analysis->{growthRationale} //= {};
                    $vm_analysis->{growthRationale}{$profile_name} //= {};

                    # Check if the data is new or different before marking as modified.
                    if (!exists $vm_analysis->{growthRationale}{$profile_name}{$model_type} ||
                        Dumper($vm_analysis->{growthRationale}{$profile_name}{$model_type}) ne Dumper($new_data))
                    {
                         $vm_analysis->{growthRationale}{$profile_name}{$model_type} = $new_data;
                         $was_modified = 1;
                    }
                }
            }
        }
    }

    # --- Step 4: Write back to the history file ONLY if it was modified ---
    if ($was_modified) {
        print "  - INFO: Found new/updated growth rationale data in L2 cache. Updating history file...\n";
        _write_full_history($system_cache_dir, $history_data);
    } else {
        print "  - INFO: No new growth rationale data found in L2 cache. History is up to date.\n";
    }
}

# ==============================================================================
# HELPER SUBROUTINE for the enrichment process.
# PURPOSE:    A robust replication of nfit.pl's generate_canonical_key function.
#             It builds the precise key used to store results in the L2 cache for
#             a specific growth analysis run.
# ==============================================================================
sub _generate_l2_cache_key_for_nfit {
    my ($profile_obj, $model_type, $system_cache_dir) = @_;

    my @key_parts;
    my $flags = $profile_obj->{flags};

    # This simulates the arguments nfit.pl would receive from a typical
    # nfit-profile run that enables growth prediction.

    # --- Flags passed from nfit-profile to nfit ---
    push @key_parts, "--nmondir $system_cache_dir";
    push @key_parts, "--smt $default_smt_arg";
    push @key_parts, "--runq-avg-method $nfit_runq_avg_method_str";
    push @key_parts, "--peak"; # nfit-profile always adds this

    # --- Flags from the profile definition ---
    # Parse them from the string to handle them individually
    if ($flags =~ /--percentile\s+([0-9.]+)/ || $flags =~ /-p\s+([0-9.]+)/) { push @key_parts, "--percentile $1"; }
    if ($flags =~ /--window\s+(\d+)/ || $flags =~ /-w\s+(\d+)/) { push @key_parts, "--window $1"; }
    if ($flags =~ /--avg-method\s+(\w+)/) { push @key_parts, "--avg-method $1"; }
    if ($flags =~ /--decay\s+([\w-]+)/) { push @key_parts, "--decay $1"; }
    if ($flags =~ /--runq-decay\s+([\w-]+)/) { push @key_parts, "--runq-decay $1"; }
    if ($flags =~ /--filter-above-perc\s+([0-9.]+)/) { push @key_parts, "--filter-above-perc $1"; }
    if ($flags =~ /--online/) { push @key_parts, "--online"; }
    if ($flags =~ /--batch/) { push @key_parts, "--batch"; }
    if ($flags =~ /--no-weekends/) { push @key_parts, "--no-weekends"; }

    # --- Flags specific to the growth model ---
    if ($model_type eq 'decay_over_states') {
        push @key_parts, "--decay-over-states";
    } elsif ($model_type eq 'windowed_decay') {
        push @key_parts, "--enable-windowed-decay";
        push @key_parts, "--process-window-unit $nfit_window_unit_str";
        push @key_parts, "--process-window-size $nfit_window_size_val";
    }

    # Growth flags are present in the profile, so they are included automatically
    if ($flags =~ /--enable-growth-prediction/) {
        push @key_parts, "--enable-growth-prediction";
        if ($flags =~ /--growth-projection-days\s+(\d+)/) { push @key_parts, "--growth-projection-days $1"; }
        if ($flags =~ /--max-growth-inflation-percent\s+(\d+)/) { push @key_parts, "--max-growth-inflation-percent $1"; }
    }

    # Note: This replication assumes a standard set of flags passed from nfit-profile.
    # It prioritizes the flags that define the analysis type and profile uniqueness.
    # This is far more robust than the previous simple string concatenation.

    return join(" ", sort @key_parts);
}

# ==============================================================================
# SUBROUTINE: _generate_monthly_workload_analysis
# PURPOSE:    A helper function that performs a full analysis for a given
#             time period (typically one month) and returns the comprehensive
#             'MonthlyWorkloadAnalysis' data structure.
# ==============================================================================
sub _generate_monthly_workload_analysis {
    my ($system_cache_dir, $start_date_str, $end_date_str) = @_;

    my %analysis_results;
    my %collated_results_table; # Temporary table for hint generation
    my %per_profile_runq_metrics_for_month;
    my %per_profile_raw_results_for_month;

    # --- Step 1: Run nfit for all defined profiles ---
    my $profile_count = scalar(@profiles);
    my $profile_num = 0;

    foreach my $profile (@profiles) {
        $profile_num++;
        my $profile_name = $profile->{name};

        # --- NEW: Added progress indicator ---
        print STDERR "    - Analysing profile $profile_num/$profile_count: $profile_name\n";

        # For a generic monthly analysis, we create a clean, unfiltered command.
        # We pass the date range and the profile's specific flags.
        my $nfit_cmd = _build_nfit_baseline_command($profile->{flags}, $start_date_str, $end_date_str, $system_cache_dir, 1, 1, undef, $profile->{name});

        my $nfit_output = '';
        my $stderr_arg = ">&=" . fileno(STDERR);
        my $pid_nfit = open3(undef, my $stdout_nfit, $stderr_arg, "$nfit_cmd --show-progress");
        while(my $line = <$stdout_nfit>) { $nfit_output .= $line; }
        waitpid($pid_nfit, 0);
        next if $?;

        my $parsed = parse_nfit_json_output($nfit_output);
        my $p_key  = "P" . clean_perc_label(($profile->{flags} =~ /-p\s+([0-9.]+)/) ? $1 : $DEFAULT_PERCENTILE);

        # Collate the results for each VM.
        foreach my $vm_name (keys %$parsed) {
            my @states = @{$parsed->{$vm_name}};
            $per_profile_raw_results_for_month{$vm_name}{$profile_name} = \@states;

            my @vals = map { $_->{metrics}{physc}{$p_key} } @states;
            my @valid_vals = grep { defined $_ && looks_like_number($_) } @vals;

            if (@valid_vals) {
                my $agg_val = sum0(@valid_vals) / scalar(@valid_vals);
                $collated_results_table{$vm_name}{$profile_name} = $agg_val;
            }

            if ($profile_name eq $MANDATORY_PEAK_PROFILE_FOR_HINT) {
                my $most_recent_state = $states[-1];
                 if ($most_recent_state) {
                    $per_profile_runq_metrics_for_month{$vm_name}{$profile_name} = $most_recent_state;
                    my @peak_vals = map { $_->{metrics}{physc}{'Peak'} } @states;
                    my @valid_peaks = grep { defined $_ && looks_like_number($_) } @peak_vals;
                    if (@valid_peaks) {
                        $collated_results_table{$vm_name}{$PEAK_PROFILE_NAME} = max(@valid_peaks);
                    }
                 }
            }
        }
    }

    # --- Step 2: Generate hints and extract key metrics for each VM ---
    foreach my $vm_name (keys %collated_results_table) {
        my $vm_config_ref = $vm_config_data{$vm_name};
        my $p99w1_results_aref = $per_profile_raw_results_for_month{$vm_name}{$MANDATORY_PEAK_PROFILE_FOR_HINT};
        my $last_state_data = (ref($p99w1_results_aref) eq 'ARRAY' && @$p99w1_results_aref) ? $p99w1_results_aref->[-1] : {};

        my $config_block = $last_state_data->{metadata}{configuration} || {};
        my $max_cpu_from_state = $config_block->{maxCpu} // 0;
        my $smt_from_state = $config_block->{smt} // $default_smt_arg;

        # Use the existing hint generator on the collated results.
        my ($hint, $pattern, $pressure, $pressure_detail) = generate_sizing_hint(
            log_fh_ref  => undef,
            results_ref => \%collated_results_table,
            vm          => $vm_name,
            config_ref  => $vm_config_ref,
            max_cpu_for_vm_numeric => $max_cpu_from_state,
            smt_used_for_vm_numeric => $smt_from_state,
            per_profile_runq_metrics_ref => \%per_profile_runq_metrics_for_month
        );

        my $runq_metrics_block = $per_profile_runq_metrics_for_month{$vm_name}{$MANDATORY_PEAK_PROFILE_FOR_HINT}{metrics}{runq} || {};

        # Extract the GrowthRationale block from the P-99W1 profile run
        my $growth_rationale_block = $last_state_data->{debug}{growthRationale} || {};

        # Assemble the final data structure for this VM.
        $analysis_results{$vm_name} = {
            Hint           => $hint,
            Pattern        => $pattern,
            Pressure       => $pressure,
            PressureDetail => $pressure_detail,
            RunQMetrics    => {
                AbsRunQ_P90  => $runq_metrics_block->{absolute}{'P90'},
                NormRunQ_P90 => $runq_metrics_block->{normalized}{'P90'}
            },
            ProfileValues  => $collated_results_table{$vm_name},
            growthRationale => $growth_rationale_block
        };
    }

    return \%analysis_results;
}

# ==============================================================================
# SUBROUTINE: detect_sampling_interval
# Robustly detects the NMON sampling interval from the .nfit.cache.data file.
# It reads the start of the cache, isolates timestamps for the first VM found,
# calculates the time difference between consecutive samples, and finds the mode
# of these deltas to determine the most likely interval.
#
# Note: This version introduces jitter bucketing: Before counting the frequency,
#  it checks if a delta is close to a standard interval (60, 300, 600, 900 seconds)
#  within a tolerance ($EPS = 3). If it is, it snaps the value to that standard
#  interval before incrementing the count.
# Returns:
#   - The detected interval in seconds (e.g., 60, 300), or undef on failure.
sub detect_sampling_interval {
    my ($data_cache_file) = @_;

    my $SAMPLES_TO_GATHER       = 50;
    my $MAX_LINES_TO_SCAN       = 5000;
    my $MIN_SAMPLES_FOR_DETECTION = 5;   # required count in a single interval bucket
    my @STD = (60, 300, 600, 900);
    my $EPS = 3;                          # jitter tolerance (seconds)

    return undef unless (-f $data_cache_file && -s $data_cache_file);

    open my $fh, '<:encoding(utf8)', $data_cache_file or do {
        warn "Warning: Could not open data cache '$data_cache_file' for interval detection: $!";
        return undef;
    };

    <$fh>; # Skip header

    my $target_vm_for_detection;
    my @timestamps;
    my $lines_scanned = 0;

    while (my $line = <$fh>) {
        $lines_scanned++;
        last if $lines_scanned > $MAX_LINES_TO_SCAN;

        my ($ts_str, $vm_name) = ($line =~ /^(\d{4}-\d{2}-\d{2}\s\d{2}:\d{2}:\d{2}),([^,]+)/);
        next unless ($ts_str && $vm_name);

        $target_vm_for_detection //= $vm_name;
        next unless $vm_name eq $target_vm_for_detection;

        eval {
            push @timestamps, Time::Piece->strptime($ts_str, "%Y-%m-%d %H:%M:%S")->epoch;
        };
        last if @timestamps >= $SAMPLES_TO_GATHER;
    }
    close $fh;

    # Need enough timestamps to produce at least MIN_SAMPLES_FOR_DETECTION deltas
    return undef if @timestamps < ($MIN_SAMPLES_FOR_DETECTION + 1);

    @timestamps = sort { $a <=> $b } @timestamps;

    my %delta_counts;
    for (my $i = 1; $i < @timestamps; $i++) {
        my $delta = $timestamps[$i] - $timestamps[$i-1];
        next unless ($delta > 10 && $delta < 1810);

        # Jitter bucketing: snap near-standards to the standard value
        my $bucket = $delta;
        my $best_d = $EPS + 1;
        for my $s (@STD) {
            my $d = abs($delta - $s);
            if ($d < $best_d) { $best_d = $d; $bucket = ($d <= $EPS) ? $s : $bucket; }
        }
        $delta_counts{$bucket}++;
    }

    return undef unless %delta_counts;

    # Find max frequency
    my $max_count = 0;
    foreach my $delta (keys %delta_counts) {
        $max_count = $delta_counts{$delta} if $delta_counts{$delta} > $max_count;
    }

    # Require minimum evidence in the winning bucket
    return undef if $max_count < $MIN_SAMPLES_FOR_DETECTION;

    # Collect all tied winners
    my @tied_deltas = grep { $delta_counts{$_} == $max_count } keys %delta_counts;

    # Deterministic tie-break: closest to a standard, then smallest delta
    if (@tied_deltas > 1) {
        my %distances;
        for my $delta (@tied_deltas) {
            my $min_dist = 1e9;
            foreach my $std (@STD) {
                my $dist = abs($delta - $std);
                $min_dist = $dist if $dist < $min_dist;
            }
            $distances{$delta} = $min_dist;
        }
        @tied_deltas = sort { $distances{$a} <=> $distances{$b} || $a <=> $b } @tied_deltas;
    }

    return $tied_deltas[0];
}

# ==============================================================================
# --- set_adaptive_thresholds ---
# Takes a detected sampling interval, snaps it to a standard value (e.g., 1, 5, 10 min),
# calculates the adaptive thresholds for saturation and efficiency, and logs the rationale.
# This function does NOT modify global state; it returns the calculated values.
#
# Returns:
#   - A list of three values:
#     1. The new saturation threshold (float).
#     2. The new target for efficiency calculation (float).
#     3. The new max downsizing percentage (as a factor, e.g., 0.05 for 5%).
sub set_adaptive_thresholds {
    my ($raw_interval_seconds, $log_fh) = @_;

    # --- Baseline (1-min) thresholds ---
    my $sat_thresh = $RUNQ_PRESSURE_P90_SATURATION_THRESHOLD;
    my $target_norm_runq = $DEFAULT_TARGET_NORM_RUNQ_FOR_EFFICIENCY_CALC;
    my $max_downsize_perc_factor = $MAX_EFFICIENCY_REDUCTION_PERCENTAGE;

    my $snapped_minutes = 1.0;
    my $detection_method_log = "cache-based detection using mode of deltas with epsilon bucketing";
    my $log_message;

    if (!defined $raw_interval_seconds) {
        $log_message = "No reliable interval found; defaulting to 1.0 min; thresholds at 1-min baseline.";
    } else {
        # Snap the detected raw seconds to the nearest standard interval with a tolerance
        my $epsilon = 5; # Allow +/- 5 seconds
        if    (abs($raw_interval_seconds - 60) <= $epsilon)  { $snapped_minutes = 1; }
        elsif (abs($raw_interval_seconds - 300) <= $epsilon) { $snapped_minutes = 5; }
        elsif (abs($raw_interval_seconds - 600) <= $epsilon) { $snapped_minutes = 10; }
        elsif (abs($raw_interval_seconds - 900) <= $epsilon) { $snapped_minutes = 15; }
        else {
             # If it doesn't snap cleanly, default to 1 min but log the anomaly.
            $log_message = sprintf("Unusual interval %ds detected; did not snap to a standard value. Defaulting to 1.0 min.", $raw_interval_seconds);
            $snapped_minutes = 1.0;
        }
    }

    if ($snapped_minutes > 1) {
        # It's a non-default interval, so calculate the adaptive thresholds.
        my $k    = $snapped_minutes;
        my $base = $RUNQ_PRESSURE_P90_SATURATION_THRESHOLD; # Follow the global 1-min baseline
        my $beta = 0.4; # Decay exponent, could be made configurable in the future
        # General formula: 1 + (base-1) * k^(-beta)
        # This removes the hard-coded '0.8' and derives it from the baseline (1.8 - 1.0)
        $sat_thresh = 1 + (($base - 1.0) * ($k**-$beta));
        # Downsizing thresholds: Use the lookup table for conservatism
        my %downsize_targets = ( 5 => 0.75, 10 => 0.72, 15 => 0.70 );
        my %downsize_caps    = ( 5 => 5,    10 => 5,    15 => 5 ); # Cap is 5% for all coarser intervals

        $target_norm_runq = $downsize_targets{$k} // $target_norm_runq;
        # NOTE: The cap is stored as a percentage and converted to a factor here.
        my $cap_perc = $downsize_caps{$k};
        if (defined $cap_perc) {
            $max_downsize_perc_factor = $cap_perc / 100.0;
        }

        $log_message = sprintf("Sampling interval detected %.1fs -> snapped to %.1f min.", $raw_interval_seconds, $snapped_minutes) if defined $raw_interval_seconds;
    } elsif (!defined $log_message) {
         $log_message = sprintf("Sampling interval detected %.1fs -> snapped to 1.0 min. Using baseline thresholds.", $raw_interval_seconds) if defined $raw_interval_seconds;
    }


    # --- Rationale Logging ---
    if ($log_fh) {
        print {$log_fh} "----------------------------------------------------------------------\n";
        print {$log_fh} "Adaptive Threshold Calculation\n";
        print {$log_fh} "  - Method                   : $detection_method_log\n";
        print {$log_fh} "  - Status                   : $log_message\n";
        print {$log_fh} "  - Saturation Threshold     : " . sprintf("%.2f", $sat_thresh) . " x LCPU\n";
        print {$log_fh} "  - Downsizing Target        : " . sprintf("%.2f", $target_norm_runq) . " / LCPU\n";
        print {$log_fh} "  - Downsizing Cap           : " . sprintf("%.1f%%", $max_downsize_perc_factor * 100) . "\n";
        print {$log_fh} "----------------------------------------------------------------------\n";
    }

    return ($sat_thresh, $target_norm_runq, $max_downsize_perc_factor);
}

# --- usage_wrapper ---
# Generates and returns the usage/help message for the script.
sub usage_wrapper
{
    my $script_name = $0;
    $script_name =~ s{.*/}{}; # Get only script name, remove path
    return <<END_USAGE;
Usage: $script_name --physc-data <pc_file> [options]

Runs 'nfit' for multiple profiles. Applies RunQ modifiers and generates hints.
The RunQ modifier behavior (standard vs. additive-only) can be controlled
per profile via the 'runq_modifier_behavior' attribute in nfit.profiles.cfg.
Optionally passes flags to nfit for its internal windowed decay processing.
A detailed rationale log is written to $LOG_FILE_PATH.

Input Method (Provide one):
  --nmondir <directory>    : Path to a base directory containing system-specific caches
                             (e.g., './stage/'), or to a single cache directory
                             (e.g., './stage/12345ABC/').
  --mgsys <serial>         : Specifies a managed system. If --nmondir is absent,
                             implicitly uses the default base cache at './stage/'.
                             Can be combined with --nmondir to select a system
                             from a non-default base path.
  --default-smt, --smt <N> : Optional. Default SMT level (Default: $DEFAULT_SMT_VALUE_PROFILE).

RunQ Metric Configuration (for nfit calls):
--runq-norm-percentiles <list> : Global default for Normalised RunQ (Default: "$DEFAULT_RUNQ_NORM_PERCS").
                                   This list is combined with profile-specific settings and ensures P50,P90.
  --runq-abs-percentiles <list>  : Global default for Absolute RunQ (Default: "$DEFAULT_RUNQ_ABS_PERCS").
                                   This list is combined with profile-specific settings and ensures P90.
  --runq-perc-behavior <mode>    : Controls which AbsRunQ percentile is used for additive logic.
                                   'fixed' (Default): Always use AbsRunQ_P90.
                                   'match': Use AbsRunQ percentile matching the profile's PhysC -p <X> value.

Configuration Files:
  -config <vm_cfg_csv>       : Optional. VM configuration CSV file.
  --profiles-config <path>   : Optional. Profiles definition file (INI format).
                               Can contain 'runq_modifier_behavior = additive_only' per profile,
                               and profile-specific 'nfit_flags' including --runq-norm-perc/--runq-abs-perc.

Standard Analysis Filtering Options:
  -s, --startdate <YYYY-MM-DD> : Global start date for analysis (passed to nfit). Optional.
  -vm, --lpar <name>           : Analyse only the specified VM/LPAR name (passed to nfit). Optional.

Seasonality and Business Cycle Analysis:
  This powerful feature uses 'etc/nfit.seasonality.cfg' to produce forecasts
  based on defined business events.

  --apply-seasonality <event>  : Generate a forecast using the specified seasonal model.
                                 For 'multiplicative_seasonal' models, this generates
                                 three CSV files (final, current_baseline, historic_snapshot).
                                 For 'recency_decay' models (e.g., 'month-end'), this
                                 anchors the analysis to the last peak to solve the
                                 "start-of-month" problem.

  --update-history             : Run in a special mode to populate the unified history cache.
                                 This command discovers and processes completed months in
                                 your data cache, generating both generic monthly analysis
                                 and specific seasonal event snapshots.
  --min-history-days <N>       : When updating history, process partial months that have
                                 at least N days of data. (Default: 28).

  --reset-seasonal-cache     : Deletes the seasonal snapshot cache for a clean start.

Control nfit's Internal Windowed Recency Decay (Optional):
  --enable-windowed-decay                : Enable nfit's internal windowed processing.
  --decay-over-states                    : Enable the Hybrid State-Time Decay Model. This advanced mode
                                           applies recency decay to state-based results.
                                           (Mutually exclusive with --enable-windowed-decay).
  --process-window-unit <days|weeks>     : Unit for nfit's window size (Default: $DEFAULT_PROCESS_WINDOW_UNIT_FOR_NFIT).
  --process-window-size <N>              : Size of nfit's window in units (Default: $DEFAULT_PROCESS_WINDOW_SIZE_FOR_NFIT).
  --decay-half-life-days <N>             : Half-life for nfit's recency weighting (Default: $DEFAULT_DECAY_HALF_LIFE_DAYS_FOR_NFIT).
  --analysis-reference-date <YYYY-MM-DD> : Reference date for nfit's recency calculation
                                            (Default: nfit uses date of last record in its filtered NMON data).
  --runq-avg-method <none|sma|ema>       : Averaging method for RunQ data within nfit before percentile.
                                            (Default: $DEFAULT_NFIT_RUNQ_AVG_METHOD). Uses main -w/--decay/--runq-decay from profile.

Rounding (Passed to nfit for its output):
  -r[=increment]             : Optional. nfit rounds results to NEAREST increment.
  -u[=increment]             : Optional. nfit rounds results UP to nearest increment.
                               (Default increment: $DEFAULT_ROUND_INCREMENT)
Other:
  --nfit-path <path>         : Optional. Path to the 'nfit' script.
  -h, --help                 : Display this help message.
  -v, --version              : Display script version and nfit version used.

CSV Output Options:
  --excel-formulas <true|false> : Add summary formulas to the bottom of the CSV for Excel.
                                  (Default: true).

Output CSV Columns (Illustrative):
  VM,TIER,Hint,Pattern,Pressure,PressureDetail,SMT,
  Serial,SystemType,Pool Name,Pool ID,Peak,
  [Profile Names (values are potentially recency-weighted by nfit & RunQ-modified by nfit-profile)...],
  Current_ENT,NFIT_ENT_UserFormula,NETT_UserFormula,NETT_Perc_UserFormula

END_USAGE
}
