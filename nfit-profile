#!/usr/bin/env perl

# NAME     : nfit-profile
# AUTHOR   : NiÃ«l Lambrechts (https://github.com/niel-lambrechts)
# PURPOSE  : Runs 'nfit' multiple times with user-defined profiles.
#            Applies RunQ modifiers, generates hints, logs rationale, and aggregates to CSV.
# REQUIRES : Perl, nfit, Time::Piece, List::Util, IPC::Open3, version

use strict;
use warnings;
use Getopt::Long qw(GetOptions);
use Cwd qw(abs_path);
use File::Basename qw(dirname basename);
use File::Spec qw(catfile);
use File::Path qw(make_path);
use File::Temp qw(tempfile);
use Time::Piece;
use Time::Seconds;
use List::Util qw(sum sum0 min max uniq); # sum0 is available from List::Util 1.33+
use Scalar::Util qw(looks_like_number);
use IPC::Open3;
use IO::Select;
use version;
use JSON;
use Fcntl qw(:DEFAULT :flock);
use constant ONE_SECOND => 1;
use Data::Dumper;
use Archive::Tar;
use IO::Zlib; # Standard in Perl core since 5.9.3

# --- Store original ARGV for logging ---
my @original_argv = @ARGV;

# --- Capture nfit-profile.pl start time ---
my $PROFILE_SCRIPT_START_TIME_EPOCH = time();
my $PROFILE_SCRIPT_START_TIME_STR = localtime($PROFILE_SCRIPT_START_TIME_EPOCH)->strftime("%Y-%m-%d %H:%M:%S %Z");

# --- Version ---
my $VERSION = '6.25.306.0';

# --- Configuration ---
my $DEFAULT_AVG_METHOD     = 'ema';
my $DEFAULT_DECAY_LEVEL    = 'medium';
my $DEFAULT_WINDOW_MINUTES = 15;
my $DEFAULT_PERCENTILE = 95;
my $DEFAULT_ROUND_INCREMENT = 0.05;
my $DEFAULT_SMT            = 8;
my $DEFAULT_VM_CONFIG_FILE = "config-all.csv";
my $DEFAULT_PROFILES_CONFIG_FILE = "nfit.profiles.cfg";
my $DEFAULT_SMT_VALUE_PROFILE = 8;
my $DEFAULT_RUNQ_NORM_PERCS = "50,90"; # Global default for nfit-profile if not in profile's flags
my $DEFAULT_RUNQ_ABS_PERCS  = "90";    # Global default for nfit-profile if not in profile's flags
my $DEFAULT_NFIT_RUNQ_AVG_METHOD = "ema";

# This profile's CPU value is used for MaxCPU pressure checks,
# and its RunQ metrics will now be used for global RunQ pressure hints.
my $MANDATORY_PEAK_PROFILE_FOR_HINT = "P-99W1";

# Windowed Decay Defaults (for when nfit-profile instructs nfit to use its internal decay)
my $DEFAULT_PROCESS_WINDOW_UNIT_FOR_NFIT = "weeks";
my $DEFAULT_PROCESS_WINDOW_SIZE_FOR_NFIT = 1;
my $DEFAULT_DECAY_HALF_LIFE_DAYS_FOR_NFIT = 30;

# Heuristic Thresholds (for sizing hints and RunQ modifiers)
my $PATTERN_RATIO_THRESHOLD = 2.0;     # For O vs B pattern determination
my $HIGH_PEAK_RATIO_THRESHOLD = 5.0;   # For "Very Peaky" shape
my $LOW_PEAK_RATIO_THRESHOLD  = 2.0;   # For "Moderately Peaky" shape
my $LIMIT_THRESHOLD_PERC = 0.98;       # P99W1 vs MaxCPU for pressure detection

# RunQ Modifier Thresholds (for calculate_runq_modified_physc)
my $WORKLOAD_PRESSURE_NORM_P90_TRIGGER_THRESHOLD = 2.0; # NormRunQ P90 above this may indicate workload pressure (NOT adaptive)
my $RUNQ_PRESSURE_P90_SATURATION_THRESHOLD = 1.5;       # AbsRunQ P90 / (MaxCPU * SMT) above this indicates RunQ pressure (ADAPTIVE)
my $RUNQ_ADDITIVE_TOLERANCE_FACTOR = 1.8;               # Tolerate AbsRunQ up to this Factor x Base_Profile_PhysC's LCPU capacity (NOT adaptive)
my $ENTITLEMENT_BURST_ALLOWANCE_FACTOR = 0.25;          # Allow uncapped VMs to burst 25% over their entitlement before pressure is assessed
my $BURST_ALLOWANCE_MIN_FACTOR = 0.0;
my $BURST_ALLOWANCE_MAX_FACTOR = 0.5;                   # Clamp at 50%

# Internal constants for growth heuristics (not user-configurable initially)
my $GROWTH_MIN_HISTORICAL_PERIODS       = 5;    # Min number of windowed periods to attempt trend
my $GROWTH_MAX_CV_THRESHOLD             = 0.50; # Max Coefficient of Variation (StdDev/Mean); if > this, data too volatile
my $GROWTH_MIN_POSITIVE_SLOPE_THRESHOLD = 0.01; # Min slope (units/period) to consider as actual growth for inflation
my $GROWTH_MAX_PROJECTION_HISTORY_RATIO = 2.0;  # Max ratio of projection duration to history duration used for trend
my $DEFAULT_GROWTH_PROJECTION_DAYS         = 90;
my $DEFAULT_MAX_GROWTH_INFLATION_PERCENT   = 25;

# RunQ Volatility Confidence Factor Thresholds & Values (adjusts additive CPU based on P90/P50 RunQ ratio)
my $VOLATILITY_SPIKY_THRESHOLD = 0.5;    my $VOLATILITY_SPIKY_FACTOR = 0.70;    # Very stable/spiky, less confidence in adding CPU
my $VOLATILITY_MODERATE_THRESHOLD = 0.8; my $VOLATILITY_MODERATE_FACTOR = 0.85; # Moderately stable
my $RUNQ_PRESSURE_SATURATION_CONFIDENCE_FACTOR = 1.0; # Full confidence if RunQ saturation is high

# --- Advanced Efficiency Adjustment Tunable Parameters ---
# Guard Rail for High Existing Constraint (CPU Downsizing Skip)
my $BASE_PHYSC_VS_MAXCPU_THRESH_FOR_CONSTRAINT_GUARD = 0.90; # If Base PhysC > 90% of MaxCPU
my $RUNQ_PRESSURE_FOR_CONSTRAINT_GUARD_FACTOR = 0.80;      # And RunQ Pressure > (this factor * saturation_threshold)

# Dynamic Blending Weights for Efficient Target (P_efficient_target_raw vs. BasePhysC)
my $NORM_P50_LOW_THRESH_FOR_BLEND1 = 0.25;       # If NormP50 < this, give more weight to raw target
my $BLEND_WEIGHT_BASE_FOR_LOW_P50_1 = 0.70;      #   70% Base / 30% Raw Target
my $NORM_P50_MODERATE_THRESH_FOR_BLEND2 = 0.40;  # If NormP50 < this (but >= BLEND1), moderate blend
my $BLEND_WEIGHT_BASE_FOR_LOW_P50_2 = 0.75;      #   75% Base / 25% Raw Target (original idea)
# If NormP50 >= MODERATE_THRESH_FOR_BLEND2 (but still low enough for efficiency consideration),
# lean more heavily on BasePhysC by default (e.g., 85% Base / 15% Target)
my $BLEND_WEIGHT_BASE_DEFAULT_LOW_P50 = 0.85;

# Volatility-Sensitive Cap for MAX_EFFICIENCY_REDUCTION_PERCENTAGE
# These thresholds are for Volatility Ratio (P90/P50)
my $VOLATILITY_MODERATE_LOW_CAP_THRESH = 1.2;  # Volatility above this starts to reduce the reduction cap
my $VOLATILITY_MODERATE_MEDIUM_CAP_THRESH = 1.5; # Intermediate threshold
my $VOLATILITY_MODERATE_HIGH_CAP_THRESH = 1.8; # Volatility above this reduces cap more significantly

# Factors to scale down MAX_EFFICIENCY_REDUCTION_PERCENTAGE
my $REDUCTION_CAP_SCALE_FOR_MODERATE_VOLATILITY = 0.66; # e.g., 15% * 0.66 = ~10% max cut
my $REDUCTION_CAP_SCALE_FOR_MODERATE_MEDIUM_VOLATILITY = 0.50; # e.g., 15% * 0.50 = 7.5% max cut
my $REDUCTION_CAP_SCALE_FOR_MODERATE_HIGH_VOLATILITY = 0.33; # e.g., 15% * 0.33 = ~5% max cut

# --- Single-Thread-Dominated (STD) Workload Heuristics ---
# These constants are used to detect bursting workloads that are likely inefficient
# and should still be considered for strategic (but not tactical) downsizing.
my $STD_NORM_P90_THRESH = 0.5; # NormRunQ P90 must be below this to be considered non-concurrent.
my $STD_IQRC_THRESH     = 0.3; # Volatility must be below this (very steady).
my $STD_PHYSC_STABILITY_THRESH = 0.15; # PhysC P90/P50 ratio must be less than 1.15.
my $STD_LCPU_TIER1_MAX = 8;     # For small VMs (less dampening)
my $STD_LCPU_TIER2_MAX = 32;    # For medium VMs (base dampening)
                                # VMs > TIER2_MAX are large (more dampening)

# --- Dispatch-Bound Workload (DBW) Anomaly Heuristics ---
# Detects workloads with very low CPU usage but extremely high dispatch contention.
my $DBW_LOW_UTIL_FACTOR = 0.5; # BasePhysC must be less than 50% of Entitlement.
my $DBW_DSR_THRESHOLD = 10.0;  # Dispatch Stress Ratio must be > 10.0.
my $DBW_MEDIAN_PRESSURE_THRESHOLD = 1.5; # NormRunQ P50 must be > 1.5.

# --- Enhanced Efficiency Factor Constants (for calculate_runq_modified_physc) ---
my $VOLATILITY_CAUTION_THRESHOLD = 2.5; # If NormRunQ P90/P50 ratio >= this, skip efficiency reduction
my $NORM_P50_THRESHOLD_FOR_EFFICIENCY_CONSIDERATION = 0.5; # NormRunQ P50 must be below this to consider efficiency
my $MAX_EFFICIENCY_REDUCTION_PERCENTAGE  = 0.25; # Max % a profile can be reduced by efficiency logic (ADAPTIVE)
my $MIN_P50_DENOMINATOR_FOR_VOLATILITY = 0.1;    # Min P50 value to avoid division by zero in volatility calc
my $DEFAULT_TARGET_NORM_RUNQ_FOR_EFFICIENCY_CALC = 0.8; # Base, SMT-dependent adjustments in calc sub (ADAPTIVE)

# --- Hot Thread Workload (HTW) Additive Dampening Heuristics ---
# These constants are used to detect and dampen additive CPU for workloads
# that appear constrained (e.g., single-threaded) despite high normalized RunQ.
my $HOT_THREAD_WL_ENT_FACTOR = 0.80;    # BasePhysC < Entitlement * this_factor
my $HOT_THREAD_WL_MAXCPU_FACTOR = 0.25;   # OR BasePhysC < MaxCPU * this_factor
my $HOT_THREAD_WL_HIGH_NORM_P50_THRESHOLD = 3.0; # NormRunQ P50 > this_threshold
# RUNQ_PRESSURE_P90_SATURATION_THRESHOLD (1.8) is an existing constant, also used here.
my $HOT_THREAD_WL_IQRC_THRESHOLD = 1.0;    # NormRunQ_IQRC > this_threshold (tune based on data)
my $HOT_THREAD_WL_DETECTION_MIN_CONDITIONS_MET = 4; # Minimum number of detection conditions to be met

# Dynamic Dampening Multipliers for HTW
my $HOT_THREAD_WL_BASE_DAMPENING_FACTOR = 0.25; # Base factor for the dynamic dampening calculation
my $HOT_THREAD_WL_MIN_DYNAMIC_DAMPENING = 0.05; # Floor for the final dynamic dampening factor
my $HOT_THREAD_WL_MAX_DYNAMIC_DAMPENING = 0.75; # Ceiling for the final dynamic dampening factor

# Enhanced Safety Caps for Additive CPU (applied AFTER all other factors)
my $ADDITIVE_CPU_SAFETY_CAP_FACTOR_OF_BASE = 2.0; # Max additive CPU as a multiple of BasePhysC
my $ADDITIVE_CPU_SAFETY_CAP_ABSOLUTE = 0.5;       # Absolute maximum additive CPU in cores

my $POOL_CONSTRAINT_CONFIDENCE_FACTOR = 0.80; # Reduction factor for additive CPU if VM is in a non-default pool

# --- Built-in default Target NormRunQ values for the adaptive heuristic ---
my %DEFAULT_RUNQ_TARGETS = (
    'P'  => 0.25,
    'O1' => 0.30, 'O2' => 0.40, 'O3' => 0.50, 'O4' => 0.65,
    'G1' => 0.80, 'G2' => 0.90, 'G3' => 1.00, 'G4' => 1.10,
    'B1' => 1.20, 'B2' => 1.30, 'B3' => 1.40, 'B4' => 1.50,
);
my %custom_runq_targets;
my %tier_override_for_csv;

my $LOG_FILE_PATH = "/tmp/nfit-profile.log";        # Default log file path
my $log_file_path_for_run = $LOG_FILE_PATH;

my $CACHE_STATES_FILE = ".nfit.cache.states";
my $DATA_CACHE_FILE   = ".nfit.cache.data";
my $UNIFIED_HISTORY_FILE = ".nfit.history.json";    # Unified history file

# --- Profile Definitions (Loaded from file) ---
my @profiles;
my @csv_visible_profiles;
my $PEAK_PROFILE_NAME = "Peak"; # Standardized name for the peak metric column in output
my @output_header_cols_csv;

# --- Argument Parsing ---
my $physc_data_file;
my $runq_data_file_arg;
my $vm_config_file_arg;
my $profiles_config_file_arg;
my $start_date_str;
my $end_date_str;
my $target_vm_name;
my $round_arg;                     # For nfit's -r (round to nearest)
my $roundup_arg;                   # For nfit's -u (round up)
my $default_smt_arg = $DEFAULT_SMT_VALUE_PROFILE;
my $runq_norm_perc_list_str = $DEFAULT_RUNQ_NORM_PERCS; # Global default for nfit-profile itself
my $runq_abs_perc_list_str  = $DEFAULT_RUNQ_ABS_PERCS;  # Global default for nfit-profile itself
my $runq_perc_behavior_mode = 'fixed';                  # Default behavior: use fixed P90 for RunQ. Alternative: 'match'.
my $help = 0;
my $show_version = 0;
my $script_dir = dirname(abs_path($0));
my $nfit_script_path = "$script_dir/nfit"; # Default path to nfit script
my $nfit_enable_windowed_decay = 0;        # Flag to instruct nfit to use its internal decay
my $nfit_window_unit_str = $DEFAULT_PROCESS_WINDOW_UNIT_FOR_NFIT;
my $nfit_window_size_val = $DEFAULT_PROCESS_WINDOW_SIZE_FOR_NFIT;
my $nfit_decay_half_life_days_val = $DEFAULT_DECAY_HALF_LIFE_DAYS_FOR_NFIT;
my $nfit_analysis_reference_date_str;
my $nfit_runq_avg_method_str = $DEFAULT_NFIT_RUNQ_AVG_METHOD; # 'none', 'sma', 'ema' for nfit's RunQ processing
my $nfit_decay_over_states = 0;
my $nmon_dir;
my $excel_formulas_flag = "false";
my $mgsys_filter;
my $verbose = 0;
my $force_update = 0;

# Seasonality related variables
my $apply_seasonality_event;
my $update_history_flag = 0; # New unified history update flag
my $min_history_days_arg;    # New flag for partial month processing
my $reset_seasonal_cache = 0;
my $DEFAULT_SEASONALITY_CONFIG_FILE = "nfit.seasonality.cfg";

# The Global Lock ensures atomic access to the history directory.
my $HISTORY_LOCK_FILENAME = ".nfit.history.lock";

GetOptions(
    'nmondir=s'                  => \$nmon_dir,
    'config=s'                   => \$vm_config_file_arg,
    'profiles-config=s'          => \$profiles_config_file_arg,
    'startdate|s=s'              => \$start_date_str,
    'enddate|e=s'                => \$end_date_str,
    'vm|lpar=s'                  => \$target_vm_name,
    'round|r:f'                  => \$round_arg,
    'roundup|u:f'                => \$roundup_arg,
    'default-smt|smt=i'          => \$default_smt_arg,
    'runq-norm-percentiles=s'    => \$runq_norm_perc_list_str,      # Global default list for NormRunQ
    'runq-abs-percentiles=s'     => \$runq_abs_perc_list_str,       # Global default list for AbsRunQ
    'runq-perc-behavior=s'       => \$runq_perc_behavior_mode,      # default: use 'AbsRunQ_P90' for all profiles ('match': match the RunQ percentile to the PhysC profile percentile)
    'help|h'                     => \$help,
    'nfit-path=s'                => \$nfit_script_path,
    'version|v'                  => \$show_version,
    'enable-windowed-decay'      => \$nfit_enable_windowed_decay,
    'process-window-unit=s'      => \$nfit_window_unit_str,
    'process-window-size=i'      => \$nfit_window_size_val,
    'decay-half-life-days=i'     => \$nfit_decay_half_life_days_val,
    'analysis-reference-date=s'  => \$nfit_analysis_reference_date_str,
    'runq-avg-method=s'          => \$nfit_runq_avg_method_str,
    'decay-over-states'          => \$nfit_decay_over_states,
    'excel-formulas=s'           => \$excel_formulas_flag,
    'mgsys|system|serial|host=s' => \$mgsys_filter,
    'apply-seasonality=s'        => \$apply_seasonality_event,
    'update-history'             => \$update_history_flag,
    'min-history-days=i'         => \$min_history_days_arg,
    'reset-seasonal-cache'       => \$reset_seasonal_cache,
    'verbose'                    => \$verbose,
    'force'                      => \$force_update,
) or die usage_wrapper();

# --- Validation ---
my $nfit_ver = "N/A"; # Store nfit version
# This block now runs every time to get the nfit version for logging
if (-x $nfit_script_path) {
	my $nfit_ver_output = `$nfit_script_path --version 2>&1`;
	my ($parsed_nfit_ver) = ($nfit_ver_output =~ /nfit version\s*([0-9.a-zA-Z-]+)/i);
	if (defined $parsed_nfit_ver) {
		$nfit_ver = $parsed_nfit_ver;
		# Check nfit version compatibility for certain features
		my $required_nfit_ver_for_windowing = "2.27.0";
		my $required_nfit_ver_for_runq_avg_and_decay = "2.28.0.4";

		if ($nfit_enable_windowed_decay && version->parse($nfit_ver) < version->parse($required_nfit_ver_for_windowing)) {
			print STDERR "Warning: --enable-windowed-decay requires nfit version $required_nfit_ver_for_windowing or higher. Your nfit version ($nfit_ver) may not support this.\n";
		}
		if (defined $nfit_runq_avg_method_str && $nfit_runq_avg_method_str ne 'none' && version->parse($nfit_ver) < version->parse($required_nfit_ver_for_runq_avg_and_decay)) {
			print STDERR "Warning: --runq-avg-method (sma/ema) may require nfit features from version $required_nfit_ver_for_runq_avg_and_decay or higher. Your nfit version ($nfit_ver) behavior might differ for RunQ processing, especially if --runq-decay is intended.\n";
		}
	} else {
		print STDERR "Could not determine nfit version from output: $nfit_ver_output\n";
		if ($nfit_enable_windowed_decay || (defined $nfit_runq_avg_method_str && $nfit_runq_avg_method_str ne 'none')) {
			print STDERR "Warning: Advanced nfit features are enabled but nfit version cannot be verified.\n";
		}
	}
} else {
    print STDERR "nfit script at '$nfit_script_path' not found or not executable.\n";
}

if ($show_version)
{
    print STDERR "nfit-profile version $VERSION\n";
    print STDERR "  nfit version $nfit_ver\n";
}

# --- Validation for runq-perc-behavior ---
$runq_perc_behavior_mode = lc($runq_perc_behavior_mode);
unless ($runq_perc_behavior_mode eq 'fixed' || $runq_perc_behavior_mode eq 'match')
{
    die "Error: Invalid value for --runq-perc-behavior. Must be 'fixed' or 'match' instead of '$runq_perc_behavior_mode'.\n";
}

# --- Data Source Validation ---
# The script now operates exclusively on pre-built cache directories.
my $DEFAULT_BASE_STAGE_DIR = File::Spec->catfile($script_dir, 'stage');

if ($help)
{
    print STDERR usage_wrapper();
    exit 0;
}

# A cache source must be specified via --nmondir or --mgsys.
if (!defined $nmon_dir && !defined $mgsys_filter)
{
    print STDERR usage_wrapper();
    die "Error: No data source cache specified. Please use --nmondir or --mgsys.\n";
}

# If --mgsys is provided without a base --nmondir, set --nmondir to the default.
# This allows the Smart Dispatcher to find the system-specific cache.
if (defined $mgsys_filter && !defined $nmon_dir)
{
    $nmon_dir = $DEFAULT_BASE_STAGE_DIR;
    # Only print the info message if it's a regular run, not a staging/snapshotting run.
    if (!$update_history_flag) {
        print STDERR "Cache Path (default): '$nmon_dir'\n";
    }
}

# Final check to ensure the base directory exists.
if (defined $nmon_dir && !-d $nmon_dir)
{
    die "Error: The specified cache directory (--nmondir) was not found: '$nmon_dir'\n";
}


if ($default_smt_arg <= 0)
{
    die "Error: --default-smt value must be a positive integer (e.g., 4, 8).\n";
}
if (! -x $nfit_script_path)
{
    die "Error: Cannot find or execute 'nfit' script at '$nfit_script_path'. Use --nfit-path.\n";
}
if (defined($round_arg) && defined($roundup_arg))
{
    die "Error: -round (-r) and -roundup (-u) options are mutually exclusive.\n";
}
if (defined $start_date_str && $start_date_str !~ /^\d{4}-\d{2}-\d{2}$/)
{
    die "Error: Invalid startdate (-s) format '$start_date_str'. Use YYYY-MM-DD.\n";
}
if (defined $end_date_str && $end_date_str !~ /^\d{4}-\d{2}-\d{2}$/)
{
    die "Error: Invalid enddate (-e) format '$end_date_str'. Use YYYY-MM-DD.\n";
}
if (defined $start_date_str && defined $end_date_str)
{
    my ($s_tp, $e_tp);
    eval { $s_tp = Time::Piece->strptime($start_date_str, "%Y-%m-%d"); };
    eval { $e_tp = Time::Piece->strptime($end_date_str, "%Y-%m-%d"); };

    if ($s_tp && $e_tp && $e_tp < $s_tp)
    {
        die "Error: --enddate ($end_date_str) cannot be before --startdate ($start_date_str).\n";
    }
}

# Validate the relationship between enddate and analysis-reference-date
if (defined $end_date_str && defined $nfit_analysis_reference_date_str)
{
    my ($e_tp, $ref_tp);
    eval { $e_tp = Time::Piece->strptime($end_date_str, "%Y-%m-%d"); };
    eval { $ref_tp = Time::Piece->strptime($nfit_analysis_reference_date_str, "%Y-%m-%d"); };

    if ($e_tp && $ref_tp && $ref_tp > $e_tp)
    {
        die "Error: --analysis-reference-date ($nfit_analysis_reference_date_str) cannot be after --enddate ($end_date_str).\n" .
            "       The reference date is used to calculate recency weights and should not be beyond the data cutoff.\n" .
            "       If you want to analyse data up to $end_date_str, either:\n" .
            "         1. Omit --analysis-reference-date (it will default to the last data point), or\n" .
            "         2. Set --analysis-reference-date to $end_date_str or earlier.\n";
    }
}

# Validations for nfit's windowed decay options, if enabled by nfit-profile
if ($nfit_enable_windowed_decay && $nfit_decay_over_states)
{
    die "Error: --enable-windowed-decay and --decay-over-states are mutually exclusive analysis modes.\n";
}
if ($nfit_enable_windowed_decay)
{
    if ($nfit_window_unit_str ne "days" && $nfit_window_unit_str ne "weeks")
    {
        die "Error: --process-window-unit must be 'days' or 'weeks'.\n";
    }
    if ($nfit_window_size_val < 1)
    {
        die "Error: --process-window-size must be at least 1.\n";
    }
    if ($nfit_decay_half_life_days_val < 1)
    {
        die "Error: --decay-half-life-days must be at least 1.\n";
    }
    if (defined $nfit_analysis_reference_date_str && $nfit_analysis_reference_date_str !~ /^\d{4}-\d{2}-\d{2}$/)
    {
        die "Error: Invalid --analysis-reference-date format. Use YYYY-MM-DD.\n";
    }
    print STDERR "nfit's internal windowed decay processing is enabled via nfit-profile.\n";
}

# Validate nfit's RunQ averaging method, if specified
if (defined $nfit_runq_avg_method_str)
{
    $nfit_runq_avg_method_str = lc($nfit_runq_avg_method_str);
    unless ($nfit_runq_avg_method_str eq 'none' || $nfit_runq_avg_method_str eq 'sma' || $nfit_runq_avg_method_str eq 'ema')
    {
        die "Error: --runq-avg-method must be 'none', 'sma', or 'ema'. Got '$nfit_runq_avg_method_str'.\n";
    }
    print STDERR "RunQ Averaging Method: $nfit_runq_avg_method_str.\n";
}

# --- Excel Formula Validation ---
my $add_excel_formulas = 0; # Default to off
if (defined $excel_formulas_flag)
{
    # If flag is just --excel-formulas, $excel_formulas_flag will be empty string.
    # Default to 'true' if flag is present but no value is given.
    my $val = lc($excel_formulas_flag // 'true');
    if ($val eq 'true' || $val eq '1')
    {
        $add_excel_formulas = 1;
    }
    elsif ($val eq 'false' || $val eq '0')
    {
        $add_excel_formulas = 0;
    }
    else
    {
        die "Error: Invalid value for --excel-formulas. Must be 'true', 'false', or omitted. Got '$excel_formulas_flag'.\n";
    }
}

my $output_dir = File::Spec->catfile($script_dir, 'output');
make_path($output_dir);
if (! -d $output_dir) {
    die "Unable to create output directory '$output_dir': $!";
}
my @generated_files; # Global array to store names of all generated files

# --- Smart Dispatcher Logic ---
# This block determines which cache directories to process. It can handle
# being pointed at a single cache directory or a parent directory containing
# multiple system-specific caches.

my @target_systems_to_process;
my $base_cache_dir = $nmon_dir; # Assume the provided dir is the base by default.

# First, check if the provided --nmondir is ITSELF a valid cache directory.
if (-f File::Spec->catfile($nmon_dir, '.nfit_stage_id'))
{
    # This is a singular run targeting a specific cache directory.
    print STDERR "Single cache directory detected. Enabling singular analysis mode.\n";
    push @target_systems_to_process, undef; # 'undef' signals a single run.
    $base_cache_dir = dirname($nmon_dir); # The base is the parent of the cache dir.
}
else
{
    # If not, check if it's a PARENT directory containing multiple caches.
    opendir(my $dh, $nmon_dir) or die "Cannot open directory $nmon_dir: $!";
    my @subdirs = grep { -d File::Spec->catfile($nmon_dir, $_) && !/^\./ } readdir($dh);
    closedir($dh);

    my @found_serials;
    foreach my $subdir (@subdirs)
    {
        if (-f File::Spec->catfile($nmon_dir, $subdir, '.nfit_stage_id'))
        {
            push @found_serials, $subdir;
        }
    }

    if (@found_serials)
    {
        # This is a multi-system run.
        print STDERR "Managed system cache hierarchy detected. Enabling multi-system mode.\n";
        $base_cache_dir = $nmon_dir; # The provided dir is the base.

        # Apply --mgsys filter if provided, otherwise target all found systems.
        if (defined $mgsys_filter)
        {
            my %serials_from_args = map { $_ => 1 } split /,/, $mgsys_filter;
            @target_systems_to_process = grep { exists $serials_from_args{$_} } @found_serials;
        }
        else
        {
            @target_systems_to_process = @found_serials;
        }
    }
    else
    {
        # The directory is neither a cache itself, nor does it contain any caches.
        die "Error: The directory '$nmon_dir' is not a valid nFit cache and does not contain any cache subdirectories.\n";
    }
}

# If a --vm filter was provided, we must refine our list of target systems.
# This only makes sense in a multi-system context where we can check each one.
if (scalar(@target_systems_to_process) > 1 && defined $target_vm_name)
{
    my %vms_to_find = map { $_ => 1 } split /,/, $target_vm_name;
    my %systems_with_target_vms;

    foreach my $serial (@target_systems_to_process)
    {
        my $states_file = File::Spec->catfile($base_cache_dir, $serial, $CACHE_STATES_FILE);
        next unless -f $states_file;

        my $json_text = do { open my $fh, '<:encoding(utf8)', $states_file or next; local $/; my $content = <$fh>; close $fh; $content; };
        next unless defined $json_text;

        my $states = eval { decode_json($json_text) };
        if ($@) { warn "Warning: Could not decode JSON from '$states_file': $@. Skipping for VM discovery."; next; }

        foreach my $vm_in_state (keys %$states)
        {
            if (exists $vms_to_find{$vm_in_state})
            {
                $systems_with_target_vms{$serial} = 1;
                last; # Found a match for this system.
            }
        }
    }
    # The new target list is only the systems that contain the specified VMs.
    @target_systems_to_process = sort keys %systems_with_target_vms;
}

if (!@target_systems_to_process)
{
    die "Error: No target managed systems could be identified for processing based on the provided filters.\n";
}

print STDERR "Dispatcher will process " . scalar(@target_systems_to_process) . " managed system(s).\n";

# --- Load Profile Definitions ---
# The log file will now be created inside the main processing loop for each system.
print STDERR "Profile Definitions:\n";
my $profiles_config_path_to_load;
if (defined $profiles_config_file_arg)
{
    if (-f $profiles_config_file_arg)
    {
        $profiles_config_path_to_load = $profiles_config_file_arg;
    }
    else
    {
        die "Error: Specified profiles config (--profiles-config) not found: $profiles_config_file_arg\n";
    }
}
else # Attempt to find default profiles config
{
    $profiles_config_path_to_load = "$script_dir/etc/$DEFAULT_PROFILES_CONFIG_FILE";
    unless (-f $profiles_config_path_to_load)
    {
        $profiles_config_path_to_load = "$script_dir/$DEFAULT_PROFILES_CONFIG_FILE"; # Try in script's root
    }
    unless (-f $profiles_config_path_to_load)
    {
        die "Error: Default profiles config '$DEFAULT_PROFILES_CONFIG_FILE' not found in '$script_dir/etc/' or '$script_dir/'. Use --profiles-config.\n";
    }
}
print STDERR "  - Profile configurations: $profiles_config_path_to_load\n";
@profiles = load_profile_definitions($profiles_config_path_to_load);
if (scalar @profiles == 0)
{
    die "Error: No profiles loaded from '$profiles_config_path_to_load'.\n";
}

# Filter to only profiles with csv_output enabled
@csv_visible_profiles = grep { $_->{csv_output} } @profiles;

# load custom VM tiers
my %vm_tier_overrides = %{parse_vm_tier_overrides("$script_dir/etc/nfit.vms.cfg")};

# --- Generate CSV header columns ---

# Define the order of columns for the CSV output
@output_header_cols_csv = (
    "VM", "TIER", "Hint", "Pattern", "Pressure", "PressureDetail", "SMT",
    "Serial", "SystemType", "Pool Name", "Pool ID",
    "RunQ_Tactical", "RunQ_Strategic", "RunQ_Potential", "RunQ_Source", $PEAK_PROFILE_NAME
);

# Add profile names as column headers (these will contain the RunQ-modified PhysC values)
push @output_header_cols_csv, map { $_->{name} } @csv_visible_profiles;

# Add "Current - ENT" which is always shown.
push @output_header_cols_csv, ("Current - ENT");
if ($add_excel_formulas) {
    push @output_header_cols_csv, ("NFIT - ENT", "NETT", "NETT%");
}

# --- Ensure the Mandatory P-99W1 Profile is Defined ---
# This check occurs after profiles are loaded.
my $mandatory_profile_is_present = 0;
foreach my $profile_entry (@profiles)
{
    if (defined $profile_entry->{name} && $profile_entry->{name} eq $MANDATORY_PEAK_PROFILE_FOR_HINT)
    {
        $mandatory_profile_is_present = 1;
        last;
    }
}

unless ($mandatory_profile_is_present)
{
    die "ERROR: Mandatory profile \"$MANDATORY_PEAK_PROFILE_FOR_HINT\" is not defined in the profiles configuration file: '$profiles_config_path_to_load'.\n" .
    "       This profile is essential for core pressure detection logic in nfit-profile.\n" .
    "       Please add a profile named \"$MANDATORY_PEAK_PROFILE_FOR_HINT\" to your profiles configuration.\n";
}

print STDERR "  - Loaded " . scalar(@profiles) . " profiles.\n";

# --- Load Seasonality Definitions ---
my $seasonality_config_path;
my $seasonality_config_file_arg; # Assume this could be a future flag
my $seasonality_config;

if (defined $seasonality_config_file_arg && -f $seasonality_config_file_arg) {
    $seasonality_config_path = $seasonality_config_file_arg;
} else {
    my $default_path_etc = "$script_dir/etc/$DEFAULT_SEASONALITY_CONFIG_FILE";
    my $default_path_root = "$script_dir/$DEFAULT_SEASONALITY_CONFIG_FILE";
    if (-f $default_path_etc) {
        $seasonality_config_path = $default_path_etc;
    } elsif (-f $default_path_root) {
        $seasonality_config_path = $default_path_root;
    }
}

if ($seasonality_config_path) {
    print STDERR "Seasonality Definitions: $seasonality_config_path\n";
    # Replace Config::Tiny->read() with our custom, dependency-free parser.
    $seasonality_config = parse_seasonality_config($seasonality_config_path);
    unless (defined $seasonality_config) {
        # The custom sub will die on error, but this is a safeguard.
        die "Error: Could not parse seasonality config file '$seasonality_config_path'.\n";
    }
    my $event_count = scalar(keys %$seasonality_config);
    print STDERR "  - Loaded $event_count seasonal event definitions.\n";
} elsif ($apply_seasonality_event || $update_history_flag) {
    # It's an error to request seasonality if the config file doesn't exist.
    die "Error: Seasonality feature requested, but the configuration file '$DEFAULT_SEASONALITY_CONFIG_FILE' was not found in '$script_dir/etc/' or '$script_dir/'.\n";
}

# --- Determine Minimum Days for History Processing ---
my $min_days_for_history;
my $MIN_HISTORY_DAYS_DEFAULT = 28; # Hard-coded default

if (defined $min_history_days_arg) {
    $min_days_for_history = $min_history_days_arg; # Command line takes precedence
} elsif (defined $seasonality_config && exists $seasonality_config->{Global}{min_history_days}) {
    $min_days_for_history = $seasonality_config->{Global}{min_history_days}; # Config file is second
} else {
    $min_days_for_history = $MIN_HISTORY_DAYS_DEFAULT; # Fallback to default
}

# --- Validation for Seasonality and Decay Model Interaction ---
if (defined $apply_seasonality_event) {
    # Abort if the specified event doesn't exist in the loaded configuration.
    unless (exists $seasonality_config->{$apply_seasonality_event}) {
        die "Error: Seasonal event '$apply_seasonality_event' could not be found as a valid section in $seasonality_config_path\n";
    }

    my $event_config = $seasonality_config->{$apply_seasonality_event} // {};
    my $model_type = $event_config->{model} // '';

    # The multiplicative_seasonal model is a complete workflow and cannot be
    # combined with the top-level decay flags.
    if ($model_type eq 'multiplicative_seasonal' && ($nfit_enable_windowed_decay || $nfit_decay_over_states)) {
        die "Error: Incompatible arguments. The '--apply-seasonality' flag for a 'multiplicative_seasonal' event cannot be used with '--enable-windowed-decay' or '--decay-over-states'.\n";
    }
    # Automatically enable the correct decay model if needed
    elsif ($model_type eq 'recency_decay') {
        print STDERR "INFO: Enabling '--enable-windowed-decay' for 'recency_decay' model.\n";
        $nfit_enable_windowed_decay = 1; # Use the time-based windowed decay engine.
        $nfit_decay_over_states = 0;     # Ensure the state-based engine is disabled.
    }
}

# --- Locate and Load VM Configuration Data ---
# This data provides SMT, MaxCPU, Entitlement, etc., per VM.
my $vm_config_file_path = undef;
my $vm_config_found = 0;
my %vm_config_data;         # Stores parsed VM config: $vm_config_data{hostname}{key} = value
my %vm_config_col_idx;      # Maps column names (lowercase) to their index in the CSV
my $vm_config_header_count = 0; # Number of columns in VM config header

if (defined $vm_config_file_arg) # User specified a VM config file
{
    if (-f $vm_config_file_arg)
    {
        $vm_config_file_path = $vm_config_file_arg;
        $vm_config_found = 1;
        print STDERR "VM Configurations: $vm_config_file_path\n";
    }
    else
    {
        die "Error: Specified VM configuration file (-config) not found: $vm_config_file_arg\n";
    }
}
elsif (!defined $apply_seasonality_event)
{
    # No VM config file specified, try default locations
    my $dp_etc = "$script_dir/etc/$DEFAULT_VM_CONFIG_FILE";  # Default path: script_dir/etc/
    my $dp_root = "$script_dir/$DEFAULT_VM_CONFIG_FILE"; # Alternative path: script_dir/
    if (-f $dp_etc)
    {
        $vm_config_file_path = $dp_etc;
        $vm_config_found = 1;
        print STDERR "Found default VM configuration file: $vm_config_file_path\n";
    }
    elsif (-f $dp_root)
    {
        $vm_config_file_path = $dp_root;
        $vm_config_found = 1;
        print STDERR "Found default VM configuration file: $vm_config_file_path\n";
    }
    else
    {
        if ($nmon_dir) {
            # Message for the modern, self-sufficient --nmondir mode
            print STDERR "Info: Default VM configuration file '$DEFAULT_VM_CONFIG_FILE' not found. VM metadata columns 'SystemType' and 'Pool Name' will be blank. Dynamically sourcing SMT and MaxCPU from NMON.\n";
        } else {
            # Original message for the standard file source (--pc / --rq) modes
            print STDERR "Info: Default VM configuration file '$DEFAULT_VM_CONFIG_FILE' not found. VM config/SMT/MaxCPU columns will be blank/default. RunQPressure_P90 logic will be impacted.\n";
        }
    }
}

if ($vm_config_found)
{
#    print STDERR "Loading VM configuration data (manual parse)...\n";
    open my $cfg_fh, '<:encoding(utf8)', $vm_config_file_path or die "Error: Cannot open VM config file '$vm_config_file_path': $!\n";
    my $hdr = <$cfg_fh>; # Read header line
    unless (defined $hdr)
    {
        die "Error: Could not read header from VM config '$vm_config_file_path'\n";
    }
    chomp $hdr;
    $hdr =~ s/\r$//;        # Remove CR if present (Windows line endings)
    $hdr =~ s/^\x{FEFF}//;  # Remove BOM if present (UTF-8 Byte Order Mark)
    my @rhdrs = split /,/, $hdr;
    $vm_config_header_count = scalar @rhdrs;
    my %hmap; # Map lowercase header name to index
    for my $i (0 .. $#rhdrs)
    {
        my $cn = $rhdrs[$i];
        $cn =~ s/^\s*"?|"?\s*$//g; # Trim spaces and quotes from column name
        if ($cn ne '')
        {
            $hmap{lc($cn)} = $i; # Store lowercase column name
        }
    }

    # Check for required columns
    my @req_cols = qw(hostname serial systemtype procpool_name procpool_id entitledcpu maxcpu);
    my $has_smt_col = exists $hmap{'smt'}; # Check if SMT column exists
    unless (exists $hmap{'maxcpu'})
    {
        warn "  - 'maxcpu' column not found in VM configuration file. This can affect MaxCPU capping logic.\n";
    }
    if ($has_smt_col)
    {
        print STDERR "  - Found 'SMT' column in VM configuration file.\n";
    }
    else
    {
        print STDERR "  - Info: 'SMT' column not found in VM configuration file. Using default SMT: $default_smt_arg for RunQ calculations.\n";
    }

    foreach my $rc (@req_cols)
    {
        unless (exists $hmap{$rc})
        {
            die "Error: Required column '$rc' not found in VM config file header '$vm_config_file_path'\n";
        }
        $vm_config_col_idx{$rc} = $hmap{$rc};
    }
    if ($has_smt_col) # If SMT column exists, store its index
    {
        $vm_config_col_idx{'smt'} = $hmap{'smt'};
    }

    # Read data lines from VM config
    while (my $ln = <$cfg_fh>)
    {
        chomp $ln;
        $ln =~ s/\r$//;
        next if $ln =~ /^\s*$/; # Skip empty lines

        # Attempt to parse CSV with quoted fields (handles commas within quotes)
        my @rvals = ($ln =~ /"([^"]*)"/g);
        if (scalar @rvals != $vm_config_header_count)
        { # Fallback to simple comma split if quote parsing fails or count mismatches
            @rvals = split /,/, $ln;
            if (scalar @rvals != $vm_config_header_count)
            {
                warn "Warning: Mismatched field count on VM config line $. Skipping: $ln\n";
                next;
            }
            # Trim whitespace for values from simple split
            $_ =~ s/^\s+|\s+$//g for @rvals;
        }
        # Else, if quote parsing worked, values in @rvals are already unquoted and trimmed by regex.

        my $hn = $rvals[ $vm_config_col_idx{'hostname'} ]; # Get hostname
        if (defined $hn && $hn ne '')
        {
            my $smt_v = $default_smt_arg; # Default SMT value
            if ($has_smt_col && defined $rvals[$vm_config_col_idx{'smt'}] && $rvals[$vm_config_col_idx{'smt'}] ne '')
            {
                my $sf = $rvals[$vm_config_col_idx{'smt'}];
                if ($sf =~ /(\d+)$/) # Extract trailing digits for SMT value (e.g., "SMT4" -> 4)
                {
                    $smt_v = $1;
                    if ($smt_v <= 0) # Validate SMT
                    {
                        warn "Warning: Invalid SMT '$sf' for '$hn'. Using default $default_smt_arg.\n";
                        $smt_v = $default_smt_arg;
                    }
                }
                else
                {
                    warn "Warning: Could not parse SMT '$sf' for '$hn'. Using default $default_smt_arg.\n";
                }
            }

            my $max_cpu_val = $rvals[$vm_config_col_idx{'maxcpu'}];
            # Validate MaxCPU value
            unless (defined $max_cpu_val && $max_cpu_val =~ /^[0-9.]+$/ && ($max_cpu_val+0) > 0)
            {
                $max_cpu_val = 0; # Default to 0 if invalid, meaning no effective MaxCPU cap from config
            }

            # Store parsed VM config data
            $vm_config_data{$hn} = {
                serial      => $rvals[$vm_config_col_idx{'serial'}],
                systemtype  => $rvals[$vm_config_col_idx{'systemtype'}],
                pool_name   => $rvals[$vm_config_col_idx{'procpool_name'}],
                pool_id     => $rvals[$vm_config_col_idx{'procpool_id'}],
                entitlement => $rvals[$vm_config_col_idx{'entitledcpu'}],
                maxcpu      => ($max_cpu_val + 0), # Store as number
                smt         => $smt_v,
            };
        }
        else # Hostname missing or empty
        {
            warn "Warning: Missing hostname on VM config line $. Skipping.\n";
        }
    }
    close $cfg_fh;
    print STDERR "  - Loaded configurations for " . scalar(keys %vm_config_data) . " VMs.\n\n";
}
else # VM config file not found or not specified
{
    print STDERR "Warning: VM configuration file not loaded. MaxCPU capping logic will be affected, and SMT will use default.\n";
}

# --- Construct Common Flags for nfit ---
# These flags are common to ALL nfit runs initiated by nfit-profile.
# Note: RunQ percentile flags (--runq-norm-perc, --runq-abs-perc) are now handled PER PROFILE run.
my $common_nfit_flags_base = "-q";
if ($nmon_dir)
{
    $common_nfit_flags_base .= " -k --nmondir \"$nmon_dir\"";
    # When using --nmondir, runq data comes from within the NMON files, so --runq-data is not used.
    # However, we still need to pass the RunQ averaging method to nfit if specified.
    if (defined $nfit_runq_avg_method_str)
    {
        $common_nfit_flags_base .= " --runq-avg-method \"$nfit_runq_avg_method_str\"";
    }
}
else # The original path using --physc-data
{
    $common_nfit_flags_base .= " -k --physc-data \"$physc_data_file\"";
    if (defined $runq_data_file_arg)
    {
        $common_nfit_flags_base .= " --runq-data \"$runq_data_file_arg\"";
        if (defined $nfit_runq_avg_method_str)
        {
            $common_nfit_flags_base .= " --runq-avg-method \"$nfit_runq_avg_method_str\"";
        }
    }
}
if (defined $start_date_str) # Global start date for all nfit runs
{
    $common_nfit_flags_base .= " -s $start_date_str";
}

# Common rounding flags (passed to nfit for its output formatting)
my $rounding_flags_for_nfit = ""; # These are applied by nfit itself
if (defined $round_arg)
{
    $rounding_flags_for_nfit .= " -r";
    if (length $round_arg && $round_arg !~ /^\s*$/) # Check if round_arg has a value (e.g. -r=0.1)
    {
        $rounding_flags_for_nfit .= "=$round_arg";
    }
}
elsif (defined $roundup_arg)
{
    $rounding_flags_for_nfit .= " -u";
    if (length $roundup_arg && $roundup_arg !~ /^\s*$/) # Check if roundup_arg has a value (e.g. -u=0.1)
    {
        $rounding_flags_for_nfit .= "=$roundup_arg";
    }
}
$common_nfit_flags_base .= $rounding_flags_for_nfit; # Add rounding to common flags if specified

# --- Main Logic: Run nfit Profiles ---
my %results_table; # Stores PhysC values from nfit for each profile: $results_table{vm_name}{profile_name}
my %runq_modifier_values;
my %nfit_growth_adjustments;
my %runq_uncapped_values;
my %runq_potential_values;
my %hint_tier_for_csv;
my %hint_pattern_for_csv;
my %hint_pressure_for_csv;
my @vm_order;      # To maintain CSV output order consistent with first nfit run that reports VMs
my %vm_seen;       # Tracks VMs seen to populate @vm_order correctly
my %primary_runq_metrics_captured_for_vm; # Tracks if global P50/P90 RunQ metrics captured for hints
my %source_profile_for_global_runq; # Which profile's output sourced the global RunQ P50/P90 for hints
my %per_profile_runq_metrics; # Stores ALL RunQ metrics (e.g. AbsRunQ_P80, AbsRunQ_P98) from EACH profile's nfit run
# Structure: $per_profile_runq_metrics{vm_name}{profile_name}{runq_metric_key}
my %per_profile_nfit_raw_results;
my %pressure_details_for_csv;
my %seasonal_debug_info;
my %outlier_warnings;

my %parsed_growth_adj_values;      # Stores GrowthAdj from nfit output
my %parsed_growth_adj_abs_values;  # Stores GrowthAdjAbs from nfit output
my $FLOAT_EPSILON = 1e-9;

print STDERR "nfit-profile version $VERSION\n";

my $LOG_FH;
my %open_log_files;

my $is_seasonal_run = $apply_seasonality_event || $update_history_flag;

# --- Main Processing Loop (iterates through systems for InfluxDB cache, runs once for standard file source modes) ---
foreach my $system_serial (@target_systems_to_process)
{

    # To store the unique YYYYMMDD[-N] suffix
    my $unique_date_suffix;

    # Flag for the multiplicative seasonal model
    my $is_multiplicative_forecast_run = 0;
    my $is_predictive_peak_model_run = 0;
    my $historic_data_for_csv_href;

    # First, determine the full path to the cache we are processing in this iteration.
    my $current_cache_path = defined($system_serial) ? File::Spec->catfile($base_cache_dir, $system_serial) : $nmon_dir;

    # Now, robustly determine the system identifier for logging.
    # Default to the directory name, but prefer the canonical name from the ID file.
    my $system_identifier = basename($current_cache_path); # Tier 2 Fallback identifier
    my $id_file_path = File::Spec->catfile($current_cache_path, '.nfit_stage_id');

    if (-f $id_file_path) {
        eval {
            open my $id_fh, '<:encoding(utf8)', $id_file_path;
            my $id_content = <$id_fh>;
            close $id_fh;

            # Tier 1 Attempt: Extract the system name from the file.
            if ($id_content && $id_content =~ /for system\s+(.+)/) {
                my $candidate_name = $1;
                $candidate_name =~ s/^\s+|\s+$//g; # Trim leading/trailing whitespace

                # --> ADDED: Validate the extracted name.
                # It must not be empty and must not contain illegal filename characters.
                if ($candidate_name ne '' && $candidate_name !~ /[\\\/:\*\?"<>\|]/) {
                    # Validation passed. Use the canonical name.
                    $system_identifier = $candidate_name;
                } else {
                    # Validation failed. The fallback (directory name) will be used.
                    warn "Warning: Unusable system identifier ('$candidate_name') found in '$id_file_path'. Reverting to directory name.";
                }
            }
        };
        if ($@) {
            # This catches errors during file open/read. The fallback will be used.
            warn "Warning: Could not read or parse '$id_file_path'. Using directory name for log. Error: $@";
        }
    }

    my $report_type_for_log = 'state-based'; # Default for the "no flags" forensic model
    if (defined $apply_seasonality_event && $apply_seasonality_event ne '') {
        $report_type_for_log = $apply_seasonality_event;
    } elsif ($nfit_decay_over_states) {
        $report_type_for_log = 'hybrid-state-decay';
    } elsif ($nfit_enable_windowed_decay) {
        $report_type_for_log = 'windowed-decay';
    }

    if ($system_identifier && $system_identifier ne '.')
    {
        print STDERR "\n--- Processing Managed System: $system_identifier ---\n";

        # Efficiently report the data cache's time-span for user context.
        my $data_cache_file = File::Spec->catfile($current_cache_path, '.nfit.cache.data');
        my ($start_date, $end_date) = _get_cache_date_range($data_cache_file);
        if ($start_date && $end_date) {
            print STDERR "Data Cache Timespan: " . $start_date->strftime('%Y-%m-%d') . " to " . $end_date->strftime('%Y-%m-%d') . "\n";
        }

        # Determine and print the analysis type for clarity
        my $analysis_type_desc = "Standard Profile Analysis";
        if ($apply_seasonality_event) {
            my $event_config = $seasonality_config->{$apply_seasonality_event} // {};
            my $model_type = $event_config->{model} // '';
            if ($model_type eq 'multiplicative_seasonal') {
                $analysis_type_desc = "Seasonal Forecast (Multiplicative Model for event '$apply_seasonality_event')";
            } elsif ($model_type eq 'recency_decay') {
                 $analysis_type_desc = "Seasonal Analysis (Recency-Decay Model for event '$apply_seasonality_event')";
            }
        } elsif ($nfit_decay_over_states) {
            $analysis_type_desc = "Hybrid State-Time Decay Analysis (--decay-over-states)";
        } elsif ($nfit_enable_windowed_decay) {
            $analysis_type_desc = "Time-Based Windowed Decay Analysis (--enable-windowed-decay)";
        }
        print STDERR "Analysis Type: $analysis_type_desc\n";

        # --- Generate unique, collision-resistant, paired filenames ---
        my $date_str = localtime->strftime('%Y%m%d');

        # Sanitise system identifiers for use in the filename.
        my $log_suffix_system = $system_identifier;
        $log_suffix_system =~ s/[^a-zA-Z0-9_.-]//g;
        my $log_suffix_report = $report_type_for_log;
        $log_suffix_report =~ s/[^a-zA-Z0-9_.-]//g;

        my $base_name_prefix = "nfit-profile.$log_suffix_system.$log_suffix_report";
        my $date_suffix = $date_str;
        my $counter = 0;
        # Check for collision on either file extension
        while (
            -f File::Spec->catfile($output_dir, "$base_name_prefix.$date_suffix.log") ||
            -f File::Spec->catfile($output_dir, "$base_name_prefix.$date_suffix.csv")
        ) {
            $counter++;
            $date_suffix = "$date_str-$counter";
        }
        $unique_date_suffix = $date_suffix; # Store for CSV naming
        $log_file_path_for_run = File::Spec->catfile($output_dir, "$base_name_prefix.$unique_date_suffix.log");
    }
    else
    {

        # Fallback to a default log path inside the output directory for single/non-serial runs.
        my $date_str = localtime->strftime('%Y%m%d');
        my $base_name_prefix = "nfit-profile.default_system.$report_type_for_log";
        my $date_suffix = $date_str;
        my $counter = 0;
        while (
            -f File::Spec->catfile($output_dir, "$base_name_prefix.$date_suffix.log") ||
            -f File::Spec->catfile($output_dir, "$base_name_prefix.$date_suffix.csv")
        ) {
            $counter++;
            $date_suffix = "$date_str-$counter";
        }
        $unique_date_suffix = $date_suffix; # Store for CSV naming
        $log_file_path_for_run = File::Spec->catfile($output_dir, "$base_name_prefix.$unique_date_suffix.log");
    }

    # --- Open Log File for this specific system ---
    my $log_fh_for_system = IO::File->new($log_file_path_for_run, '>')
        or warn "Error: Cannot open rationale log for '$system_serial' at '$log_file_path_for_run': $!. Rationale logging will be skipped for this system.\n";

    if ($log_fh_for_system)
    {
        $open_log_files{$system_identifier} = $log_fh_for_system;
        my $LOG_FH = $log_fh_for_system; # Use a lexical variable for printing the header

        $LOG_FH->autoflush(1); # Ensure immediate writing to log
        print {$LOG_FH} "======================================================================\n";
        print {$LOG_FH} "nFit Profile Rationale Log\n";
        print {$LOG_FH} "======================================================================\n";
        print {$LOG_FH} "nfit-profile.pl Run Started: $PROFILE_SCRIPT_START_TIME_STR\n";
        print {$LOG_FH} "nfit-profile.pl Version  : $VERSION\n";
        print {$LOG_FH} "nfit Version Used        : $nfit_ver\n";
        print {$LOG_FH} "----------------------------------------------------------------------\n";
        my @quoted_original_argv_log = map { $_ =~ /\s/ ? qq/"$_"/ : $_ } @original_argv;
        print {$LOG_FH} "Invocation: $0 " . join(" ", @quoted_original_argv_log) . "\n";
        print {$LOG_FH} "----------------------------------------------------------------------\n";
        print {$LOG_FH} "Key Global Settings for System: " . ($system_identifier // 'N/A') . "\n";
        print {$LOG_FH} "  - Profiles Config File       : $profiles_config_path_to_load\n";
        print {$LOG_FH} "  - VM Config File             : " . ($vm_config_file_path // "Not Provided/Default Attempted") . "\n";
        print {$LOG_FH} "  - Common Flags               : -q, -k, rounding, smt, runq-avg-method\n";
        print {$LOG_FH} "  - Dynamic Flags              : Date filters, RunQ percentiles, and Decay models (dynamic).\n";
        print {$LOG_FH} "  - RunQ Avg Method            : $nfit_runq_avg_method_str\n";
        print {$LOG_FH} "  - Default SMT for Profile    : $default_smt_arg\n";
        print {$LOG_FH} "======================================================================\n\n";
    }

    # --- Adaptive Threshold Initialisation for this system ---
    # This block is self-contained and does not mutate global variables.
    # It detects the interval and gets the appropriate thresholds for this system's run.
    my $data_cache_for_interval_detection = File::Spec->catfile($current_cache_path, $DATA_CACHE_FILE);
    my $detected_interval_secs = detect_sampling_interval($data_cache_for_interval_detection);

    # The set_adaptive_thresholds function returns the new values, which are stored
    # in local variables. This avoids modifying the global defaults.
    my ($adaptive_runq_saturation_thresh, $adaptive_target_norm_runq, $adaptive_max_efficiency_reduction) = set_adaptive_thresholds($detected_interval_secs, $log_fh_for_system);

    # --- Seasonality Engine: Main Controller ---
    # This block determines if a seasonal model should be applied and orchestrates the analysis.
    # This block handles all three modes:
    # 1. Updating a snapshot.
    # 2. Applying a recency_decay forecast.
    # 3. Applying a multiplicative_seasonal forecast.

    if ($update_history_flag) {
        # --- Mode 1: Update the Unified Monthly History Cache ---
        # This is a special run mode. Its only purpose is to analyse completed
        # months in the data cache and save the results to the new history file.
        # It does not produce a CSV output.
        # Call the new subroutine to handle the history generation.
        update_monthly_history($current_cache_path, $system_identifier, $seasonality_config, $min_days_for_history, $adaptive_runq_saturation_thresh, $force_update);

        # After updating the history, we skip the rest of the processing for this system.
        print STDERR "--- History update complete for system: $system_identifier ---\n";
        next; # Proceed to the next system in the loop.
    } elsif ($apply_seasonality_event) {
        # --- Apply a Seasonal Forecast ---
        my $is_seasonal_run = 1;

        print STDERR "\n--- Applying Seasonality Model for Event: '$apply_seasonality_event' ---\n";
        # Get the configuration for the event the user requested.
        my $requested_event_config = $seasonality_config->{$apply_seasonality_event} // {};

        # check for unsupported model types
        # Prevent this script from incorrectly running models handled by other tools.
        my $requested_model_type = $requested_event_config->{model} // '';
        if ($requested_model_type eq 'adaptive_peak_forecasting') {
            die "ERROR: The 'adaptive_peak_forecasting' model must be run using the 'nfit-forecast.py' script.\n" .
                "       Please use the command: ./nfit-forecast.py --nmondir $current_cache_path --apply-seasonality $apply_seasonality_event\n";
        }

        # Determine the correct analysis path. This function checks for sufficient
        # history for multiplicative models and returns the effective event name to run.
        my $effective_event_name = determine_seasonal_analysis_path(
            $requested_event_config,
            $current_cache_path,
            $apply_seasonality_event
        );

        # Get the configuration for the event we are actually going to run.
        my $effective_config = $seasonality_config->{$effective_event_name} // {};
        my $effective_model_type = $effective_config->{model} // '';

        # Now, set up the analysis based on the EFFECTIVE model type.
        if ($effective_model_type eq 'recency_decay') {
            # This model uses --enable-windowed-decay, making it mutually exclusive
            # with the state-based decay model.
            if ($nfit_decay_over_states) {
                die "FATAL: The 'recency_decay' model (for event '$effective_event_name') cannot be used with the --decay-over-states flag.\n";
            }

            my ($event_start_obj, $event_end_obj) = determine_event_period($effective_config);

            if ($event_start_obj && $event_end_obj) {
                # DO NOT set a start date. This ensures nfit analyses the entire cache.
                # The model's behavior is controlled by the analysis reference date only.
                # $start_date_str = $event_start_obj->strftime('%Y-%m-%d');
                $nfit_analysis_reference_date_str = $event_end_obj->strftime('%Y-%m-%d');
                print STDERR "Recency-Decay Model: Anchoring analysis to reference date: $nfit_analysis_reference_date_str\n";
                # Programmatically enable the correct decay model for the main profile loop.
                $nfit_decay_over_states = 1;
            } else {
                 warn "Warning: Could not determine a valid period for the '$effective_event_name' model. Analysis will proceed without date anchoring.\n";
            }

        } elsif ($effective_model_type eq 'multiplicative_seasonal') {
            # The check passed. Set the flag to run the multiplicative forecast AFTER
            # the main profile loop has gathered the Peak value.
            $is_multiplicative_forecast_run = 1;
        } elsif ($effective_model_type eq 'predictive_peak') {
            $is_predictive_peak_model_run = 1;
            # This is the new model. It's a self-contained forecast that runs
            # instead of the standard profile loop.
            my $forecast_results = calculate_predictive_peak_forecast(
                $current_cache_path,
                $system_identifier,
                $effective_event_name,
                $effective_config,
                $seasonality_config,
                $adaptive_runq_saturation_thresh
            );
            # Overwrite the main results table with the forecast
            %results_table = %$forecast_results;

            # Populate the global vm_order array so the reporter knows which VMs to process.
            @vm_order = sort keys %results_table;

            # Generate and store the necessary hint/pressure data for each VM for the CSV.
            foreach my $vm_name (@vm_order) {
                my $cfg_for_hint = $vm_config_data{$vm_name};
                my $smt_for_hint = (defined $cfg_for_hint && defined $cfg_for_hint->{smt}) ? $cfg_for_hint->{smt} : $default_smt_arg;
                my $maxcpu_for_hint = (defined $cfg_for_hint && defined $cfg_for_hint->{maxcpu}) ? $cfg_for_hint->{maxcpu} : 0;

                # Build the VM map structure that generate_sizing_hint expects
                my %vm_map_for_hint = (
                    Configuration => {
                        vm_name     => $vm_name,
                        max_cpu     => $maxcpu_for_hint,
                        smt         => $smt_for_hint,
                        entitlement => $cfg_for_hint->{entitlement} // 0,
                        %{$cfg_for_hint || {}}  # Include any other config fields
                    },
                    CoreResults => {
                        ProfileValues => $results_table{$vm_name}
                    },
                    RunQMetrics => {}  # No RunQ metrics available in this path
                );

                # Call generate_sizing_hint with correct signature (3 positional args)
                my ($hint_type_tier, $hint_pattern_shape, $hint_pressure_bool, $pressure_detail_str, $rationale, $has_abs, $has_norm) =
                    generate_sizing_hint(
                        \%vm_map_for_hint,
                        undef,  # No log file handle needed here
                        $adaptive_runq_saturation_thresh
                    );

                $hint_tier_for_csv{$vm_name}      = $hint_type_tier;
                $hint_pattern_for_csv{$vm_name}   = $hint_pattern_shape;
                $hint_pressure_for_csv{$vm_name}  = $hint_pressure_bool;
                $pressure_details_for_csv{$vm_name} = $pressure_detail_str // 'N/A';

                # The final peak is the higher of the baseline and prediction for the P-99W1 profile.
                my $baseline_peak = $seasonal_debug_info{$vm_name}{'P-99W1'}{'TrueBaseline'} // 0;
                my $predicted_peak = $seasonal_debug_info{$vm_name}{'P-99W1'}{'PredictedPeak'} // 0;
                $results_table{$vm_name}{$PEAK_PROFILE_NAME} = ($baseline_peak > $predicted_peak) ? $baseline_peak : $predicted_peak;

            }
        }
    }

    # --- Capture the start time for this specific system's analysis ---
    my $system_analysis_start_time = time();

    # A single, clean call to the new single-pass engine.
    my $parsed_nfit_results = run_single_pass_analysis(
        $current_cache_path,
        \@profiles,
        {
            # ... (all necessary global arguments are passed in this hash) ...
            nfit_path               => $nfit_script_path,
            rounding_flags          => $rounding_flags_for_nfit,
            start_date              => $start_date_str,
            end_date                => $end_date_str,
            vm_name                 => $target_vm_name,
            default_smt             => $default_smt_arg,
            runq_avg_method         => $nfit_runq_avg_method_str,
            enable_windowed_decay   => $nfit_enable_windowed_decay,
            decay_over_states       => $nfit_decay_over_states,
            analysis_reference_date => $nfit_analysis_reference_date_str,
            enable_growth_prediction => $nfit_enable_windowed_decay || $nfit_decay_over_states,
        }
    );

    _phase("Assimilating nFit Engine Results");

    # This single function call replaces the previous complex assimilation logic.
    # It consumes the raw JSON from nfit and produces a clean, predictable map.
    # The global @profiles array is passed to assist with per-state averaging logic.
    my $assimilation_map_ref = build_assimilation_map($parsed_nfit_results, \@profiles, $adaptive_runq_saturation_thresh);

    # The @vm_order array, which drives the main processing loop,
    # must be populated from the keys of our newly created assimilation map.
    @vm_order = sort keys %{$assimilation_map_ref};

    # --- DEBUG CHECKPOINT ---
    # As requested, this block will print the contents of the newly created
    # assimilation map and then halt execution. This allows for a clean
    # evaluation of the data structure before proceeding with further refactoring.
    #print STDERR "\n\n==================== nFit Profile Debug Checkpoint ====================\n\n";
    #print STDERR "--- STAGE 1: Assimilation Map Contents ---\n";
    #print STDERR "This map is the new single source of truth for all downstream logic.\n\n";
    #print STDERR Dumper($assimilation_map_ref);
    #print STDERR "\n================== End Debug. Halting Execution. ==================\n\n";
    #die "Exiting after assimilation map generation for review.";
    # --- END DEBUG CHECKPOINT ---

    # --- Rationale Logging and Hint Generation ---
    # This block now uses a conditional to call the correct logging subroutine
    # based on the analysis model that was run. It also now captures all hint
    # components for later use in the CSV report, improving efficiency
    if ($is_multiplicative_forecast_run) {
        # Path for the multiplicative seasonal model, which needs a unique log format.
        # First, generate and store the hint data needed for the CSV report.
        foreach my $vm_name (@vm_order) {
            my $cfg_for_hint = $vm_config_data{$vm_name};
            my $smt_for_hint = $default_smt_arg;
            my $maxcpu_for_hint = 0;
            my $ent_for_hint = 0;
            my $p99w1_results_aref = $per_profile_nfit_raw_results{$vm_name}{$MANDATORY_PEAK_PROFILE_FOR_HINT};

            if (ref($p99w1_results_aref) eq 'ARRAY' && @$p99w1_results_aref) {
                my $last_state_data = $p99w1_results_aref->[-1];
                my $config = $last_state_data->{metadata}{configuration} || {};

				if (defined $config->{maxCpu} && looks_like_number($config->{maxCpu}) && $config->{maxCpu} > 0) {
                    $maxcpu_for_hint = $config->{maxCpu};
                }
				if (defined $config->{smt} && looks_like_number($config->{smt}) && $config->{smt} > 0) {
                    $smt_for_hint = $config->{smt};
                }
				if (defined $config->{entitlement} && looks_like_number($config->{entitlement})) {
                    $ent_for_hint = $config->{entitlement};
                }
        }

            # Build the VM map structure that generate_sizing_hint expects
            my %vm_map_for_hint = (
                Configuration => {
                    vm_name     => $vm_name,
                    max_cpu     => $maxcpu_for_hint,
                    smt         => $smt_for_hint,
                    entitlement => $ent_for_hint,
                    %{$cfg_for_hint || {}}  # Include any other config fields
                },
                CoreResults => {
                    ProfileValues => $results_table{$vm_name}
                },
                RunQMetrics => $per_profile_runq_metrics{$vm_name} || {}
            );

            # Call generate_sizing_hint with correct signature (3 positional args)
            my ($hint_type_tier, $hint_pattern_shape, $hint_pressure_bool, $pressure_detail_str, $rationale, $has_abs, $has_norm) =
            generate_sizing_hint(
                \%vm_map_for_hint,
                undef,  # We don't need the rationale text here, just the CSV values
                $adaptive_runq_saturation_thresh
            );

            $hint_tier_for_csv{$vm_name} = $hint_type_tier;
            $hint_pattern_for_csv{$vm_name} = $hint_pattern_shape;
            $hint_pressure_for_csv{$vm_name} = $hint_pressure_bool;
            $pressure_details_for_csv{$vm_name} = $pressure_detail_str // 'N/A';
        }
        log_multiplicative_seasonal_rationale($open_log_files{$system_identifier});
    } elsif ($is_predictive_peak_model_run) {
        # Path for the new predictive peak model
        log_predictive_peak_rationale($open_log_files{$system_identifier});
    } else {
        _phase("Applying Modifiers and Generating Rationale");
        # --- FINAL POST-PROCESSING, HINT GENERATION, and RATIONALE LOGGING ---

        # --- Progress Bar Initialization ---
        my $total_vms = scalar @vm_order;
        my $total_prof = scalar @profiles;
        my $total_units_for_progress = $total_vms * $total_prof;
        my $done_units_for_progress = 0;
        my $vm_count_for_progress = 0;
        my $show_profile_progress_flag = ($verbose || -t STDERR);

        foreach my $vm_name (@vm_order) {
            $vm_count_for_progress++;

            # Get this VM's complete map entry.
            my $vm_map_ref = $assimilation_map_ref->{$vm_name};
            next unless (ref($vm_map_ref) eq 'HASH');
            $vm_map_ref->{Configuration}{vm_name} = $vm_name; # Self-reference for convenience

            # --- Phase 1: Generate Hints and Pressure Flags ---
            my ($hint_type_tier, $hint_pattern_shape, $pressure_bool, $pressure_detail_str, $pressure_rationale_text, $p99w1_has_abs_pressure, $p99w1_has_norm_pressure) =
                generate_sizing_hint(
                    $vm_map_ref,
                    $log_fh_for_system,
                    $adaptive_runq_saturation_thresh
                );

            # Store hint results in the map for use by other functions and the CSV writer.
            $vm_map_ref->{Hinting}{AutoTier} = $hint_type_tier;
            $vm_map_ref->{Hinting}{Pattern} = $hint_pattern_shape;
            $vm_map_ref->{Hinting}{Pressure} = $pressure_bool;
            $vm_map_ref->{Hinting}{PressureDetail} = $pressure_detail_str // 'N/A';
            $vm_map_ref->{Hinting}{P99W1_AbsRunQPressure} = $p99w1_has_abs_pressure;
            $vm_map_ref->{Hinting}{P99W1_NormRunQPressure} = $p99w1_has_norm_pressure;
            $vm_map_ref->{Hinting}{FinalTierForVM} = $vm_tier_overrides{$vm_name} // $hint_type_tier;
            $tier_override_for_csv{$vm_name} = $vm_tier_overrides{$vm_name} // "";

            # Log the detailed pressure rationale text once per VM.
            if ($log_fh_for_system) {
                print {$log_fh_for_system} "\n######################################################################\n";
                print {$log_fh_for_system} "# Rationale for VM: $vm_name\n";
                print {$log_fh_for_system} "######################################################################\n\n";
                print {$log_fh_for_system} $pressure_rationale_text . "\n" if ($pressure_rationale_text);
            }

            # --- Phase 2: Calculate Modifiers (ONCE) and Final Profile Values ---

            # First, determine the single source profile for RunQ modifiers for this VM
            # Priority: User TIER > AutoTier > Fallback to 'G'.
            my $user_tier_override_runq = $vm_map_ref->{Hinting}{FinalTierForVM} // "";
            my $auto_tier_runq = $vm_map_ref->{Hinting}{AutoTier} // "G";

            my $pattern_source_runq = ($user_tier_override_runq ne "") ? $user_tier_override_runq : $auto_tier_runq;
            my ($hint_pattern) = ($pattern_source_runq =~ /^([A-Z])/);
            $hint_pattern //= 'G'; # Default to 'G' if regex fails

            my %pattern_to_profile_map = ('O'=>'O3-95W15', 'B'=>'B3-95W15', 'G'=>'G3-95W15', 'P'=>'G3-95W15');
            my $runq_source_profile_name = $pattern_to_profile_map{$hint_pattern} // 'G3-95W15';

            # Find the profile object for the source profile
            my ($runq_source_profile_obj) = grep { $_->{name} eq $runq_source_profile_name } @profiles;

            if ($runq_source_profile_obj) {
                my %pressure_flags = ( abs_pressure => $p99w1_has_abs_pressure, norm_pressure => $p99w1_has_norm_pressure );
                my %adaptive_thresholds = ( saturation => $adaptive_runq_saturation_thresh, target => $adaptive_target_norm_runq, max_reduction => $adaptive_max_efficiency_reduction );

                # Calculate modifiers ONLY for the source profile
                my (undef, $debug_info_ref) = calculate_runq_modified_physc(
                    $vm_name, $vm_map_ref, $runq_source_profile_obj, \%pressure_flags, \%adaptive_thresholds
                );

                # Store the authoritative modifiers for the CSV report
                $vm_map_ref->{CSVModifiers}{RunQ_Tactical}  = $debug_info_ref->{'FinalAdditive'};
                $vm_map_ref->{CSVModifiers}{RunQ_Strategic} = $debug_info_ref->{'RunQ_Strategic'};
                $vm_map_ref->{CSVModifiers}{RunQ_Potential} = $debug_info_ref->{'RunQ_Potential'};
                $vm_map_ref->{CSVModifiers}{RunQ_Source}    = $runq_source_profile_name;
            }

            # Now, iterate through all profiles to size them and log rationale
            foreach my $profile (@profiles) {
                my $profile_name = $profile->{name};
                $done_units_for_progress++;

                # Progress bar
                if ($show_profile_progress_flag) {
                    my $perc_done = ($total_units_for_progress > 0) ? (100.0 * $done_units_for_progress / $total_units_for_progress) : 0;
                    my $profile_name_for_progress = $profile->{name};
                    printf STDERR "\r    o  Processing VM %d/%d (%s), Profile %d/%d (%s) [%.1f%%]...",
                        $vm_count_for_progress,
                        $total_vms,
                        $vm_name,
                        ($done_units_for_progress % $total_prof) || $total_prof,
                        $total_prof,
                        $profile_name_for_progress,
                        $perc_done;
                }

                my $base_physc_for_log = $vm_map_ref->{CoreResults}{ProfileValues}{$profile_name};
                next unless (defined $base_physc_for_log && looks_like_number($base_physc_for_log));

                if ($profile_name eq $MANDATORY_PEAK_PROFILE_FOR_HINT) {
                    log_peak_profile_rationale($log_fh_for_system, $vm_map_ref, $profile, $base_physc_for_log);
                } else {
                    # Recalculate for logging, but do NOT store these values for the CSV
                    my %pressure_flags = ( abs_pressure => $p99w1_has_abs_pressure, norm_pressure => $p99w1_has_norm_pressure );
                    my %adaptive_thresholds = ( saturation => $adaptive_runq_saturation_thresh, target => $adaptive_target_norm_runq, max_reduction => $adaptive_max_efficiency_reduction );

                    my ($adjusted_physc, $debug_info_ref) = calculate_runq_modified_physc(
                        $vm_name, $vm_map_ref, $profile, \%pressure_flags, \%adaptive_thresholds
                    );

                    my $final_csv_value = looks_like_number($adjusted_physc) ? sprintf("%.3f", $adjusted_physc) : "N/A";
                    $vm_map_ref->{CoreResults}{ProfileValues}{$profile_name} = $final_csv_value;

                    my $raw_states_aref_for_log = $vm_map_ref->{RawNfitStates} || [];

                    log_profile_rationale(
                        $log_fh_for_system, $vm_map_ref, $profile,
                        $base_physc_for_log, $final_csv_value, $debug_info_ref,
                        $raw_states_aref_for_log,
                        $adaptive_runq_saturation_thresh
                    );
                }
            }

       }

        # --- Progress Bar Finalization ---
        if ($show_profile_progress_flag) {
            printf STDERR "\r    o  Processing complete. (100.0%%)%s\n", ' ' x 70;
        }
    }

    # -- Reporting and Reset Block --
    # This block executes after all profiles have been run for the current system.

    _phase("Writing CSV report");
    my $is_recency_decay_run = 0;
    my $report_type_for_filename = 'state-based';
    if ($nfit_decay_over_states) {
        $report_type_for_filename = 'hybrid-state-decay';
    } elsif ($nfit_enable_windowed_decay) {
        $report_type_for_filename = 'windowed-decay';
    }

    if ($is_seasonal_run && !$is_multiplicative_forecast_run) {
        my $event_config = $seasonality_config->{$apply_seasonality_event} // {};
        $is_recency_decay_run = (($event_config->{model} // '') eq 'recency_decay');
    }

   if ($is_multiplicative_forecast_run) {
        # This is the new path for the multiplicative model, which now uses the standard reporter.
        if (@vm_order) {
            _write_standard_csv_report($assimilation_map_ref, $report_type_for_filename, $system_identifier, $unique_date_suffix, 0, 0, 0, $adaptive_runq_saturation_thresh);

            # If verbose mode is on, generate the extra audit trail files.
            if ($verbose) {
                print "  - Verbose mode: Generating additional audit trail files...\n";
                # The baseline results are the final values *before* the forecast was applied.
                # The %results_table was overwritten, so we can't re-use it.
                # However, the baseline is available inside the seasonal_debug_info hash.
                my %baseline_data_for_verbose;
                foreach my $vm (keys %seasonal_debug_info) {
                    foreach my $prof (keys %{$seasonal_debug_info{$vm}}) {
                        $baseline_data_for_verbose{$vm}{$prof} = $seasonal_debug_info{$vm}{$prof}{baseline};
                    }
                }
                # Use $apply_seasonality_event as it's in scope here.
                write_seasonal_csv_output("current_baseline", $system_identifier, $apply_seasonality_event, $unique_date_suffix, \%baseline_data_for_verbose);
                write_seasonal_csv_output("historic_snapshot", $system_identifier, $apply_seasonality_event, $unique_date_suffix, $historic_data_for_csv_href);
            }
        }
    }
    elsif ($is_seasonal_run) {
        # This path is for all non-multiplicative seasonal models (recency_decay, predictive_peak).
        my $event_config = $seasonality_config->{$apply_seasonality_event} // {};
        my $model_type = $event_config->{model} // '';

        if ($model_type eq 'recency_decay' || $model_type eq 'predictive_peak') {
            if (@vm_order) {
                # For recency_decay, pass a true flag to add its specific columns.
                # For predictive_peak, pass a false flag to generate a standard report.
                my $is_recency_flag = ($model_type eq 'recency_decay') ? 1 : 0;
                my $is_predictive_flag = ($model_type eq 'predictive_peak') ? 1 : 0;
                _write_standard_csv_report($assimilation_map_ref, $apply_seasonality_event, $system_identifier, $unique_date_suffix, 0, $is_recency_flag, $is_predictive_flag, $adaptive_runq_saturation_thresh);
            } else {
                print STDERR "  - INFO: No VM output data was generated for seasonal event '$apply_seasonality_event'.\n";
            }
        }
    } else {
        # This is the standard, non-seasonal run path. It now generates one report
        # per system before proceeding to the next.
        if (@vm_order) {
            _write_standard_csv_report($assimilation_map_ref, $report_type_for_filename, $system_identifier, $unique_date_suffix, 0, 0, 0, $adaptive_runq_saturation_thresh);
        }
    }

    # CRITICAL: Reset global data structures before processing the next system.
    @vm_order = ();
    %vm_seen = ();
    %results_table = ();
    %per_profile_runq_metrics = ();
    %primary_runq_metrics_captured_for_vm = ();
    %per_profile_nfit_raw_results = ();
    %seasonal_debug_info = ();
    # Reset date filters to avoid them leaking into a subsequent standard run.
    $start_date_str = undef;
    $nfit_analysis_reference_date_str = undef;

    # Report the duration for the completed system's analysis
    my $system_analysis_duration = time() - $system_analysis_start_time;
    print STDERR "--- System processing complete for: $system_identifier [DONE: " . format_duration($system_analysis_duration) . "] ---\n";


} # End foreach system

print STDERR "\nnFit profiling completed.\n";

# --- Collect unique serials that were part of the output ---
my %serials_in_output_map;
if (%vm_config_data && @vm_order) { # Ensure vm_config_data was loaded and there are VMs to process
    foreach my $vm_name_in_order (@vm_order) {
        if (exists $vm_config_data{$vm_name_in_order} &&
            defined $vm_config_data{$vm_name_in_order}{serial} &&
            $vm_config_data{$vm_name_in_order}{serial} ne '') {
            $serials_in_output_map{$vm_config_data{$vm_name_in_order}{serial}} = 1;
        }
    }
}
my @sorted_unique_serials_list = sort keys %serials_in_output_map;
my $excel_row_num_counter = 1; # Excel rows are 1-based; header is row 1, so first data row is 2.

# --- Final Report Generation ---
# This block now only handles the default, non-seasonal run.
# All seasonal models now handle their own output from within the main system loop.

# --- Script Footer ---
my $final_message = "\n";
if (@generated_files) {
    # Use List::Util::uniqstr if available, otherwise a simple hash works.
    my %seen;
    my @unique_files = grep { !$seen{$_}++ } @generated_files;

    $final_message .= "\nOutput:\n";
    foreach my $file (@unique_files) {
        $final_message .= "  - $file\n";
    }
} elsif ($update_history_flag) {
    $final_message .= " Unified monthly history update completed.";
} else {
    $final_message .= " No output files were generated.";
}
$final_message .= "Rationale Log:\n  - $log_file_path_for_run\n";
print STDERR "$final_message\n";

# --- Report final script duration summary to STDERR ---
my $PROFILE_SCRIPT_END_TIME_EPOCH = time();
my $PROFILE_SCRIPT_DURATION = $PROFILE_SCRIPT_END_TIME_EPOCH - $PROFILE_SCRIPT_START_TIME_EPOCH;
print STDERR "Total execution time: " . format_duration($PROFILE_SCRIPT_DURATION) . "\n";

# --- Finalise and close all rationale log files ---
my $final_end_time_str = localtime($PROFILE_SCRIPT_END_TIME_EPOCH)->strftime("%Y-%m-%d %H:%M:%S %Z");
foreach my $system_id (keys %open_log_files) {
    my $log_fh = $open_log_files{$system_id};
    if ($log_fh) {
        print {$log_fh} "\n----------------------------------------------------------------------\n";
        print {$log_fh} "Analysis for System '$system_id' completed at: " . $final_end_time_str . "\n";
        # Note: A per-system duration would require storing start times in a hash as well.
        # This is sufficient to close the log with a final timestamp.
        print {$log_fh} "======================================================================\n";
        close $log_fh;
    }
}

exit 0;

# ==============================================================================
# Subroutines
# ==============================================================================

sub _phase {
    my ($msg) = @_;
    return unless ($verbose || -t STDERR);
    printf STDERR "  => %s...\n", $msg;
}

# --- get_excel_col_name ---
# Converts a 1-based column index to an Excel column name (e.g., 1 -> A, 27 -> AA).
sub get_excel_col_name {
    my ($idx) = @_;
    my $name = '';
    die "Column index must be positive" if (!defined $idx || $idx <= 0);
    while ($idx > 0) {
        my $mod = ($idx - 1) % 26;
        $name = chr(65 + $mod) . $name;
        $idx = int(($idx - $mod - 1) / 26); # Corrected logic for 1-based index progression
    }
    return $name;
}

# --- generate_nfit_ent_formula ---
# Generates the dynamic Excel formula for the "NFIT_ENT_UserFormula" column.
sub generate_nfit_ent_formula {
    my ($excel_row_num, $num_profiles, $column_offset) = @_;

	$column_offset //= 0;

    # First profile column is M (13th column).
    # Peak (L) is the 12th column. Profiles start after Peak.
    my $first_profile_excel_col_letter = get_excel_col_name(12 + 1);
    my $last_profile_excel_col_letter = get_excel_col_name(12 + $num_profiles);

    # The fixed array string for the MATCH function, as provided by the user.
    my $tier_match_array_str_for_formula = '{"P","G1","G2","G3","G4","O1","O2","O3","O4","B1","B2","B3","B4"}';

    # Dynamic column index for 'NFIT - Ent'.
    # 13 fixed leading columns (A-M) + num_profiles columns + 1 (for "NFIT - Ent" itself).
    my $entitlement_column_index = 13 + $num_profiles + 1 + $column_offset;

    # Using "A:AZ" as the VLOOKUP range as requested for stability.
    my $vlookup_range_for_peer_ent = "A:AZ";

    my $formula_body = sprintf(
        'IF(ISNUMBER(SEARCH("PowerHA Standby", I%d)),VLOOKUP(VLOOKUP(A%d, PowerHA!A:B, 2, FALSE),%s, %d, FALSE) * $L$258, CEILING(INDEX(%s%d:%s%d, MATCH(B%d, %s, 0)), 0.05))',
        $excel_row_num,                            # For I%d (SystemType)
        $excel_row_num,                            # For A%d (VM Name for inner VLOOKUP)
        $vlookup_range_for_peer_ent,               # Range for outer VLOOKUP (e.g., A:AZ)
        $entitlement_column_index,                 # Dynamic column index for Current_ENT of the peer
        $first_profile_excel_col_letter, $excel_row_num, # For M%d (start of profile data range)
        $last_profile_excel_col_letter,  $excel_row_num, # For e.g. Y%d (end of profile data range)
        $excel_row_num,                            # For C%d (Hint column, containing the tier string like "G3")
        $tier_match_array_str_for_formula          # For {"P","G1",...} array
    );
    return "=" . $formula_body; # Excel formulas start with "="
}

# --- print_csv_footer ---
# Prints the summary footer section with labels and Excel formulas.
# Make sure Time::Piece is used if not already at the top of your script for strftime
# use Time::Piece; # Already in the full script you provided.
# use List::Util qw(sum min max); # Already in the full script.
# Ensure get_excel_col_name and quote_csv are defined elsewhere or within this sub's scope.
# It accepts an offset to correctly calculate column letters when extra
# columns (like SeasonalMultiplier) are present in the report.
sub print_csv_footer {
    my ($fh, $last_data_row, $nmon_physc_file, $num_profiles, $sorted_unique_serials_list_ref, $col_offset) = @_;
    $col_offset //= 0; # Default to 0 if not provided

    my @sorted_unique_serials = @$sorted_unique_serials_list_ref;
    my $count_of_unique_serials = scalar(@sorted_unique_serials);
    my $loop_count_for_serials = ($count_of_unique_serials == 0) ? 1 : $count_of_unique_serials;

    # --- Calculate dynamic column letters based on script's output structure ---
    my $col_serial_letter = get_excel_col_name(8);
    my $col_system_type_letter = get_excel_col_name(9);

    # Apply the offset to all subsequent column calculations
    my $idx_current_ent = 12 + $num_profiles + 1 + $col_offset;
    my $col_current_ent_letter = get_excel_col_name($idx_current_ent);
    my $idx_nfit_ent_user_formula = 12 + $num_profiles + 2 + $col_offset;
    my $col_nfit_ent_user_formula_letter = get_excel_col_name($idx_nfit_ent_user_formula);

    # --- Get NMON physc data file modification timestamp ---
    my $file_timestamp_str = "N/A";
    if (defined $nmon_physc_file && -f $nmon_physc_file) {
        my $mtime_epoch = (stat($nmon_physc_file))[9];
        if (defined $mtime_epoch) {
            $file_timestamp_str = localtime($mtime_epoch)->strftime("%Y-%m-%d %H:%M:%S");
        } else {
            $file_timestamp_str = "Timestamp N/A (stat fetch failed for $nmon_physc_file)";
        }
    } else {
        $file_timestamp_str = "Timestamp N/A (File not found or not provided)";
    }

    # --- Define starting row for footer elements ---
    print $fh "\n"; # Blank line after main data
    my $footer_start_row = $last_data_row + 2;

    my $row_data_age = $footer_start_row;
    my $row_timestamp = $footer_start_row + 1;
    my $row_as_is_nfit_labels = $footer_start_row + 3;
    my $row_ent_col_headers = $footer_start_row + 4;
    my $row_unique_serials_start = $footer_start_row + 5;

    my $empty = "";
    my @csv_row;

    # --- Row 1 of Footer: Data Age ---
    my $col_letter_data_age_sum_current_incl = get_excel_col_name(23 + $col_offset);
    my $col_letter_data_age_sum_nfit_incl = get_excel_col_name(24 + $col_offset);
    my $col_letter_data_age_delta_incl = get_excel_col_name(25 + $col_offset);
    my $col_letter_data_age_perc_incl = get_excel_col_name(26 + $col_offset);

    my $formula_sum_current_ent_incl_sby = sprintf("=SUM(%s2:%s%d)", $col_current_ent_letter, $col_current_ent_letter, $last_data_row);
    my $formula_sum_nfit_ent_incl_sby = sprintf("=SUM(%s2:%s%d)", $col_nfit_ent_user_formula_letter, $col_nfit_ent_user_formula_letter, $last_data_row);
    my $formula_delta_incl_sby = sprintf("=%s%d-%s%d", $col_letter_data_age_sum_nfit_incl, $row_data_age, $col_letter_data_age_sum_current_incl, $row_data_age);
    my $formula_perc_incl_sby = sprintf("=IFERROR(%s%d/%s%d,\"\")", $col_letter_data_age_delta_incl, $row_data_age, $col_letter_data_age_sum_current_incl, $row_data_age);

    @csv_row = ($empty) x (22 + $col_offset);
    $csv_row[0] = "Data Age";
    $csv_row[21 + $col_offset] = "Incl. SBY";
    push @csv_row, $formula_sum_current_ent_incl_sby, $formula_sum_nfit_ent_incl_sby, $formula_delta_incl_sby, $formula_perc_incl_sby;
    print $fh join(",", map { quote_csv($_) } @csv_row) . "\n";

    # --- Row 2 of Footer: Timestamp ---
    my $col_letter_ts_sum_current_excl = $col_letter_data_age_sum_current_incl;
    my $col_letter_ts_sum_nfit_excl    = $col_letter_data_age_sum_nfit_incl;
    my $col_letter_ts_delta_excl       = $col_letter_data_age_delta_incl;
    my $col_letter_ts_perc_excl        = $col_letter_data_age_perc_incl;

    my $formula_sum_current_ent_excl_sby = sprintf("=SUMIFS(%s\$2:%s\$%d, %s\$2:%s\$%d, \"<>*PowerHA Standby*\")", $col_current_ent_letter, $col_current_ent_letter, $last_data_row, $col_system_type_letter, $col_system_type_letter, $last_data_row);
    my $formula_sum_nfit_ent_excl_sby = sprintf("=SUMIFS(%s\$2:%s\$%d, %s\$2:%s\$%d, \"<>*PowerHA Standby*\")", $col_nfit_ent_user_formula_letter, $col_nfit_ent_user_formula_letter, $last_data_row, $col_system_type_letter, $col_system_type_letter, $last_data_row);
    my $formula_delta_excl_sby = sprintf("=%s%d-%s%d", $col_letter_ts_sum_nfit_excl, $row_timestamp, $col_letter_ts_sum_current_excl, $row_timestamp);
    my $formula_perc_excl_sby = sprintf("=IFERROR(%s%d/%s%d,\"\")", $col_letter_ts_delta_excl, $row_timestamp, $col_letter_ts_sum_current_excl, $row_timestamp);

    @csv_row = ($empty) x (22 + $col_offset);
    $csv_row[0] = $file_timestamp_str;
    $csv_row[21 + $col_offset] = "Excl. SBY";
    push @csv_row, $formula_sum_current_ent_excl_sby, $formula_sum_nfit_ent_excl_sby, $formula_delta_excl_sby, $formula_perc_excl_sby;
    print $fh join(",", map { quote_csv($_) } @csv_row) . "\n";

    print $fh "\n";

    # --- Row: AS-IS NFIT Labels ---
    @csv_row = ($empty) x (23 + $col_offset);
    $csv_row[17 + $col_offset] = "AS-IS";
    $csv_row[18 + $col_offset] = "NFIT";
    $csv_row[20 + $col_offset] = "AS-IS";
    $csv_row[21 + $col_offset] = "NFIT";
    print $fh join(",", map { quote_csv($_) } @csv_row) . "\n";

    # --- Row: ENT Column Headers and other labels ---
    @csv_row = ($empty) x (22 + $col_offset);
    $csv_row[0] = "ENT"; $csv_row[1] = "ENT-NOVIO"; $csv_row[2] = "ENT-HA"; $csv_row[3] = "ENT-NFIT";
    $csv_row[4] = "NFIT-ENT-NO-VIO"; $csv_row[5] = "NFIT-ENT-NO-POWERHA-STANDBY";
    $csv_row[6] = "NFIT-ENT-NO-POWERHA-SBY-NO-VIO"; $csv_row[7] = "NFIT-ENT-POWERHA-SBY-AS-IS";
    $csv_row[8] = "NFIT-ENT-POWERHA-SBY-AS-IS-NOVIO";
    $csv_row[12] = "PowerHA SBY% TGT"; $csv_row[13] = "0.25";

    my $largest_frame_formula_as_is = sprintf("=LET(sys,%s\$2:%s\$%d,type,%s\$2:%s\$%d,ent,%s\$2:%s\$%d,rows,FILTER(HSTACK(sys,ent),NOT(type=\"VIO Server\")),uniqSys,UNIQUE(INDEX(rows,,1)),sums,BYROW(uniqSys,LAMBDA(s,SUM(FILTER(INDEX(rows,,2),INDEX(rows,,1)=s)))),XLOOKUP(MAX(sums),sums,uniqSys))", $col_serial_letter, $col_serial_letter, $last_data_row, $col_system_type_letter, $col_system_type_letter, $last_data_row, $col_current_ent_letter, $col_current_ent_letter, $last_data_row);
    my $largest_frame_formula_nfit = sprintf("=LET(sys,%s\$2:%s\$%d,type,%s\$2:%s\$%d,ent,%s\$2:%s\$%d,rows,FILTER(HSTACK(sys,ent),NOT(type=\"VIO Server\")),uniqSys,UNIQUE(INDEX(rows,,1)),sums,BYROW(uniqSys,LAMBDA(s,SUM(FILTER(INDEX(rows,,2),INDEX(rows,,1)=s)))),XLOOKUP(MAX(sums),sums,uniqSys))", $col_serial_letter, $col_serial_letter, $last_data_row, $col_system_type_letter, $col_system_type_letter, $last_data_row, $col_nfit_ent_user_formula_letter, $col_nfit_ent_user_formula_letter, $last_data_row);
    my $largest_powerha_formula_as_is = sprintf("=LET(sys,%s\$2:%s\$%d,type,%s\$2:%s\$%d,ent,%s\$2:%s\$%d,pharows,FILTER(HSTACK(sys,ent),ISNUMBER(SEARCH(\"PowerHA Primary\",type))),uniqSysPHA,UNIQUE(INDEX(pharows,,1)),sumsPHA,BYROW(uniqSysPHA,LAMBDA(s,SUM(FILTER(INDEX(pharows,,2),INDEX(pharows,,1)=s)))),XLOOKUP(MAX(sumsPHA),sumsPHA,uniqSysPHA,\"\"))", $col_serial_letter, $col_serial_letter, $last_data_row, $col_system_type_letter, $col_system_type_letter, $last_data_row, $col_current_ent_letter, $col_current_ent_letter, $last_data_row);
    my $largest_powerha_formula_nfit = sprintf("=LET(sys,%s\$2:%s\$%d,type,%s\$2:%s\$%d,ent,%s\$2:%s\$%d,pharows,FILTER(HSTACK(sys,ent),ISNUMBER(SEARCH(\"PowerHA Primary\",type))),uniqSysPHA,UNIQUE(INDEX(pharows,,1)),sumsPHA,BYROW(uniqSysPHA,LAMBDA(s,SUM(FILTER(INDEX(pharows,,2),INDEX(pharows,,1)=s)))),XLOOKUP(MAX(sumsPHA),sumsPHA,uniqSysPHA,\"\"))", $col_serial_letter, $col_serial_letter, $last_data_row, $col_system_type_letter, $col_system_type_letter, $last_data_row, $col_nfit_ent_user_formula_letter, $col_nfit_ent_user_formula_letter, $last_data_row);

    $csv_row[16 + $col_offset] = "Largest Frame";
    $csv_row[17 + $col_offset] = $largest_frame_formula_as_is;
    $csv_row[18 + $col_offset] = $largest_frame_formula_nfit;
    $csv_row[19 + $col_offset] = "Largest PowerHA";
    $csv_row[20 + $col_offset] = $largest_powerha_formula_as_is;
    $csv_row[21 + $col_offset] = $largest_powerha_formula_nfit;
    print $fh join(",", map { quote_csv($_) } @csv_row) . "\n";

    # --- Per-Serial Summary Rows ---
    for (my $i = 0; $i < $loop_count_for_serials; $i++) {
        my $current_formula_row = $row_unique_serials_start + $i;
        my @csv_row_serial_summary;

        if ($i == 0) {
            my $formula_unique_serials = sprintf("=UNIQUE(%s\$2:%s\$%d)", $col_serial_letter, $col_serial_letter, $last_data_row);
            push @csv_row_serial_summary, $formula_unique_serials;
        } else {
            push @csv_row_serial_summary, $empty;
        }

        # Formulas for columns B-J
        push @csv_row_serial_summary, sprintf("=IFERROR(SUMIFS(\$%s\$2:\$%s\$%d, \$%s\$2:\$%s\$%d, A%d),\"\")", $col_current_ent_letter, $col_current_ent_letter, $last_data_row, $col_serial_letter, $col_serial_letter, $last_data_row, $current_formula_row);
        push @csv_row_serial_summary, sprintf("=IFERROR(SUMIFS(\$%s\$2:\$%s\$%d, \$%s\$2:\$%s\$%d, A%d, \$%s\$2:\$%s\$%d, \"<>VIO Server\"),\"\")", $col_current_ent_letter, $col_current_ent_letter, $last_data_row, $col_serial_letter, $col_serial_letter, $last_data_row, $current_formula_row, $col_system_type_letter, $col_system_type_letter, $last_data_row);
        push @csv_row_serial_summary, sprintf("=IFERROR(SUMIFS(\$%s\$2:\$%s\$%d, \$%s\$2:\$%s\$%d, A%d, \$%s\$2:\$%s\$%d, \"*PowerHA Primary*\"),\"\")", $col_current_ent_letter, $col_current_ent_letter, $last_data_row, $col_serial_letter, $col_serial_letter, $last_data_row, $current_formula_row, $col_system_type_letter, $col_system_type_letter, $last_data_row);
        push @csv_row_serial_summary, sprintf("=IFERROR(SUMIFS(\$%s\$2:\$%s\$%d, \$%s\$2:\$%s\$%d, A%d),\"\")", $col_nfit_ent_user_formula_letter, $col_nfit_ent_user_formula_letter, $last_data_row, $col_serial_letter, $col_serial_letter, $last_data_row, $current_formula_row);
        push @csv_row_serial_summary, sprintf("=IFERROR(SUMIFS(\$%s\$2:\$%s\$%d, \$%s\$2:\$%s\$%d, A%d, \$%s\$2:\$%s\$%d, \"<>VIO Server\"),\"\")", $col_nfit_ent_user_formula_letter, $col_nfit_ent_user_formula_letter, $last_data_row, $col_serial_letter, $col_serial_letter, $last_data_row, $current_formula_row, $col_system_type_letter, $col_system_type_letter, $last_data_row);
        push @csv_row_serial_summary, sprintf("=IFERROR(SUMIFS(\$%s\$2:\$%s\$%d, \$%s\$2:\$%s\$%d, A%d, \$%s\$2:\$%s\$%d, \"<>*PowerHA Standby*\"),\"\")", $col_nfit_ent_user_formula_letter, $col_nfit_ent_user_formula_letter, $last_data_row, $col_serial_letter, $col_serial_letter, $last_data_row, $current_formula_row, $col_system_type_letter, $col_system_type_letter, $last_data_row);
        push @csv_row_serial_summary, sprintf("=IFERROR(SUMIFS(\$%s\$2:\$%s\$%d, \$%s\$2:\$%s\$%d, A%d, \$%s\$2:\$%s\$%d, \"<>*PowerHA Standby*\", \$%s\$2:\$%s\$%d, \"<>VIO Server\"),\"\")", $col_nfit_ent_user_formula_letter, $col_nfit_ent_user_formula_letter, $last_data_row, $col_serial_letter, $col_serial_letter, $last_data_row, $current_formula_row, $col_system_type_letter, $col_system_type_letter, $last_data_row, $col_system_type_letter, $col_system_type_letter, $last_data_row);
        push @csv_row_serial_summary, sprintf("=IFERROR(SUMPRODUCT((\$%s\$2:\$%s\$%d=A%d)*IF(ISNUMBER(SEARCH(\"PowerHA Standby\",\$%s\$2:\$%s\$%d)),\$%s\$2:\$%s\$%d,\$%s\$2:\$%s\$%d)),\"\")", $col_serial_letter, $col_serial_letter, $last_data_row, $current_formula_row, $col_system_type_letter, $col_system_type_letter, $last_data_row, $col_current_ent_letter, $col_current_ent_letter, $last_data_row, $col_nfit_ent_user_formula_letter, $col_nfit_ent_user_formula_letter, $last_data_row);
        push @csv_row_serial_summary, sprintf("=IFERROR(SUMPRODUCT((\$%s\$2:\$%s\$%d=A%d)*(\$%s\$2:\$%s\$%d<>\"VIO Server\")*IF(ISNUMBER(SEARCH(\"PowerHA Standby\",\$%s\$2:\$%s\$%d)),\$%s\$2:\$%s\$%d,\$%s\$2:\$%s\$%d)),\"\")", $col_serial_letter, $col_serial_letter, $last_data_row, $current_formula_row, $col_system_type_letter, $col_system_type_letter, $last_data_row, $col_system_type_letter, $col_system_type_letter, $last_data_row, $col_current_ent_letter, $col_current_ent_letter, $last_data_row, $col_nfit_ent_user_formula_letter, $col_nfit_ent_user_formula_letter, $last_data_row);

        if ($i == 0) {
            my $num_main_formulas = scalar(@csv_row_serial_summary);
            my $padding_needed = (16 + $col_offset) - $num_main_formulas;
            push @csv_row_serial_summary, ($empty) x $padding_needed if $padding_needed > 0;

            my $col_R_header_cell = get_excel_col_name(18 + $col_offset) . $row_ent_col_headers;
            my $col_S_header_cell = get_excel_col_name(19 + $col_offset) . $row_ent_col_headers;
            my $col_U_header_cell = get_excel_col_name(21 + $col_offset) . $row_ent_col_headers;

            push @csv_row_serial_summary, "Largest Frame ENT (Excl. VIO)";
            push @csv_row_serial_summary, sprintf("=IFERROR(SUMIFS(\$%s\$2:\$%s\$%d, \$%s\$2:\$%s\$%d, %s, \$%s\$2:\$%s\$%d, \"<>VIO Server\"),\"\")", $col_current_ent_letter, $col_current_ent_letter, $last_data_row, $col_serial_letter, $col_serial_letter, $last_data_row, $col_R_header_cell, $col_system_type_letter, $col_system_type_letter, $last_data_row);
            push @csv_row_serial_summary, sprintf("=IFERROR(SUMIFS(\$%s\$2:\$%s\$%d, \$%s\$2:\$%s\$%d, %s, \$%s\$2:\$%s\$%d, \"<>VIO Server\"),\"\")", $col_nfit_ent_user_formula_letter, $col_nfit_ent_user_formula_letter, $last_data_row, $col_serial_letter, $col_serial_letter, $last_data_row, $col_S_header_cell, $col_system_type_letter, $col_system_type_letter, $last_data_row);
            push @csv_row_serial_summary, "Largest PowerHA ENT";
            push @csv_row_serial_summary, sprintf("=IFERROR(SUMIFS(\$%s\$2:\$%s\$%d, \$%s\$2:\$%s\$%d, %s, \$%s\$2:\$%s\$%d, \"*PowerHA Primary*\"),\"\")", $col_current_ent_letter, $col_current_ent_letter, $last_data_row, $col_serial_letter, $col_serial_letter, $last_data_row, $col_U_header_cell, $col_system_type_letter, $col_system_type_letter, $last_data_row);
            push @csv_row_serial_summary, sprintf("=IFERROR(SUMIFS(\$%s\$2:\$%s\$%d, \$%s\$2:\$%s\$%d, %s, \$%s\$2:\$%s\$%d, \"*PowerHA Primary*\"),\"\")", $col_nfit_ent_user_formula_letter, $col_nfit_ent_user_formula_letter, $last_data_row, $col_serial_letter, $col_serial_letter, $last_data_row, $col_U_header_cell, $col_system_type_letter, $col_system_type_letter, $last_data_row);
        }
        print $fh join(",", map { quote_csv($_) } @csv_row_serial_summary) . "\n";
    }

    # --- Rows after per-serial summary (Frame Evac, etc.) ---
    my $actual_row_unique_serials_end = $row_unique_serials_start + $loop_count_for_serials - 1;
    my $row_after_serials_block = $row_unique_serials_start + $loop_count_for_serials;

    my $cell_largest_frame_asis_val = get_excel_col_name(18 + $col_offset) . $row_ent_col_headers;
    my $cell_largest_frame_nfit_val = get_excel_col_name(19 + $col_offset) . $row_ent_col_headers;
    my $cell_largest_pha_asis_val   = get_excel_col_name(21 + $col_offset) . $row_ent_col_headers;
    my $cell_largest_pha_nfit_val   = get_excel_col_name(22 + $col_offset) . $row_ent_col_headers;

    # Row: Frame Evac - Max Required
    @csv_row = ($empty) x (22 + $col_offset);
    $csv_row[16 + $col_offset] = "Frame Evac - Max Required";
    $csv_row[17 + $col_offset] = sprintf("=MAX(%s,%s)", $cell_largest_frame_asis_val, $cell_largest_pha_asis_val);
    $csv_row[21 + $col_offset] = sprintf("=MAX(%s,%s)", $cell_largest_frame_nfit_val, $cell_largest_pha_nfit_val);
    print $fh join(",", map { quote_csv($_) } @csv_row) . "\n";

    # Row: Frame Evac - Required per frame
    my $current_print_row_for_evac_max = $row_after_serials_block;
    my $cell_max_req_as_is_val = get_excel_col_name(18 + $col_offset) . $current_print_row_for_evac_max;
    my $cell_max_req_nfit_val  = get_excel_col_name(22 + $col_offset) . $current_print_row_for_evac_max;

    @csv_row = ($empty) x (22 + $col_offset);
    $csv_row[16 + $col_offset] = "Frame Evac - Required per frame";
    $csv_row[17 + $col_offset] = sprintf("=IFERROR(%s/COUNTA(UNIQUE(\$%s\$2:\$%s\$%d)),\"N/A\")", $cell_max_req_as_is_val, $col_serial_letter, $col_serial_letter, $last_data_row);
    $csv_row[21 + $col_offset] = sprintf("=IFERROR(%s/COUNTA(UNIQUE(\$%s\$2:\$%s\$%d)),\"N/A\")", $cell_max_req_nfit_val, $col_serial_letter, $col_serial_letter, $last_data_row);
    print $fh join(",", map { quote_csv($_) } @csv_row) . "\n";

    # --- Total Row for Per-Serial Summary ---
    @csv_row = ();
    push @csv_row, "Total";
    for my $col_idx (2..10) {
        my $col_letter = get_excel_col_name($col_idx);
        if ($count_of_unique_serials > 0) {
            push @csv_row, sprintf("=SUM(%s%d:%s%d)", $col_letter, $row_unique_serials_start, $col_letter, $actual_row_unique_serials_end);
        } else {
            push @csv_row, "0";
        }
    }
    my $current_cols = scalar(@csv_row);
    push @csv_row, ($empty) x ((22 + $col_offset) - $current_cols) if (22 + $col_offset) > $current_cols;
    print $fh join(",", map { quote_csv($_) } @csv_row) . "\n";
}


# ==============================================================================
# Subroutine to format nfit flags for display
# ==============================================================================
sub format_nfit_flags_for_display {
    my ($profile_name, $profile_specific_flags, $runq_perc_flags, $runq_behavior) = @_;
    my @output_lines;

    my $temp_profile_flags = $profile_specific_flags; # Work on a copy

    my @core_fit_parts;
    my @decay_parts;
    my @growth_parts;
    my @other_parts; # For flags not specifically categorized

    # Helper sub-subroutine to extract and remove a flag pattern
    # Arguments:
    #   1. Regex for the flag and its potential value (e.g., qr/-p\s+[^\s]+/)
    #   2. Array reference to store the extracted flag string
    #   3. Scalar reference to the string of flags to be processed (will be modified)
    sub _extract_flag {
        my ($flag_regex, $parts_array_ref, $flags_string_ref) = @_;
        if ($$flags_string_ref =~ s/($flag_regex)//) {
            my $extracted_part = $1;
            $extracted_part =~ s/^\s+|\s+$//g; # Trim whitespace
            push @$parts_array_ref, $extracted_part if $extracted_part;
        }
    }

    # --- Core Fit Parameters ---
    _extract_flag(qr/--percentile\s+(?:\"[^\"]*\"|\'[^\']*\'|[^\s\"]+)|-p\s+(?:\"[^\"]*\"|\'[^\']*\'|[^\s\"]+)/, \@core_fit_parts, \$temp_profile_flags);
    _extract_flag(qr/--process-window-size\s+(?:\"[^\"]*\"|\'[^\']*\'|[^\s\"]+)|-w\s+(?:\"[^\"]*\"|\'[^\']*\'|[^\s\"]+)/, \@core_fit_parts, \$temp_profile_flags);
    _extract_flag(qr/--filter-above-perc\s+(?:\"[^\"]*\"|\'[^\']*\'|[^\s\"]+)/, \@core_fit_parts, \$temp_profile_flags);
    # Add more related flags here if needed, e.g.:
    # _extract_flag(qr/--filter-metric\s+[^\s]+/, \@core_fit_parts, \$temp_profile_flags);
    # _extract_flag(qr/--filter-limit\s+[^\s]+/, \@core_fit_parts, \$temp_profile_flags);

    # --- Decay Options ---
    _extract_flag(qr/--decay\s+(?:\"[^\"]*\"|\'[^\']*\'|[^\s\"]+)/, \@decay_parts, \$temp_profile_flags);
    _extract_flag(qr/--runq-decay\s+(?:\"[^\"]*\"|\'[^\']*\'|[^\s\"]+)/, \@decay_parts, \$temp_profile_flags);
    # Add more related flags here, e.g.:
    # _extract_flag(qr/--ema-period\s+[^\s]+/, \@decay_parts, \$temp_profile_flags);
    # _extract_flag(qr/--sma-period\s+[^\s]+/, \@decay_parts, \$temp_profile_flags);

    # --- Growth Prediction ---
    _extract_flag(qr/--enable-growth-prediction\b/, \@growth_parts, \$temp_profile_flags); # \b for word boundary
    _extract_flag(qr/--max-growth-inflation-percent\s+(?:\"[^\"]*\"|\'[^\']*\'|[^\s\"]+)/, \@growth_parts, \$temp_profile_flags);
    _extract_flag(qr/--growth-period-days\s+(?:\"[^\"]*\"|\'[^\']*\'|[^\s\"]+)/, \@growth_parts, \$temp_profile_flags);

    if (@core_fit_parts) {
        push @output_lines, "  Core         : " . join(" ", @core_fit_parts);
    }
    if (@decay_parts) {
        push @output_lines, "  Decay        : " . join(" ", @decay_parts);
    }
    if (@growth_parts) {
        push @output_lines, "  Growth       : " . join(" ", @growth_parts);
    }

    # --- RunQ Percentiles (these are from the already processed $runq_perc_flags) ---
    my $trimmed_runq_perc_flags = $runq_perc_flags;
    $trimmed_runq_perc_flags =~ s/^\s+|\s+$//g; # Trim
    if ($trimmed_runq_perc_flags) {
        push @output_lines, "  RunQ Percs   : " . $trimmed_runq_perc_flags;
    }

    # --- Other/Remaining Profile Flags ---
    # Any flags left in $temp_profile_flags are considered "Other"
    $temp_profile_flags =~ s/^\s+|\s+$//g; # Trim remaining
    my @remaining_flags = split(/\s+/, $temp_profile_flags); # Split remaining by space
    @remaining_flags = grep { $_ ne "" } @remaining_flags; # Filter out empty strings
    if (@remaining_flags) {
        # Reconstruct to handle flags that might have been split from their values if not perfectly matched above
        # This simplistic split might not be perfect if un-extracted flags had quoted spaces.
        # For robust handling of complex "Other" flags, more sophisticated parsing of $temp_profile_flags would be needed.
        # However, ideally, most common flags are explicitly extracted above.
        push @output_lines, "  Other Args   : " . join(" ", @remaining_flags);
    }

    # --- RunQ Behavior ---
    # This comes directly from the profile config, not from the flag strings
    if (defined $runq_behavior && $runq_behavior ne 'default') {
        push @output_lines, "  RunQBehavior : $runq_behavior";
    }

    return join("\n", @output_lines);
}

# --- parse_profile_name_for_log ---
# Parses common nfit-profile profile name patterns for a more descriptive log output.
# Adheres to Allman style and includes comments.
sub parse_profile_name_for_log
{
    my ($profile_name_str) = @_;

    my $description = $profile_name_str; # Default to original name if no pattern matches
    my @parts;

    # Regex to capture common patterns like O3-95W15, P-99W1, G2-BatchSpecial etc.
    # This regex looks for: TypeChar [TierNum] - Percentile W WindowNum [SuffixLetters]
    if ($profile_name_str =~ /^([OBGP])(?:-?(\d+))?-?(\d{2,3})(?:W(\d+))?([A-Z]*)?$/i)
    {
        my $type_char   = uc($1);
        my $tier_num    = $2; # Optional
        my $perc_val    = $3;
        my $win_val     = $4; # Optional
        my $suffix_char = $5; # Optional

        my $type_desc = "Unknown Type"; # Default for safety
        if ($type_char eq 'O')
        {
            $type_desc = "Online";
        }
        elsif ($type_char eq 'B')
        {
            $type_desc = "Batch";
        }
        elsif ($type_char eq 'G')
        {
            $type_desc = "General";
        }
        elsif ($type_char eq 'P')
        {
            $type_desc = "Peak";
        }

        if (defined $tier_num && $tier_num ne "")
        {
            $type_desc .= " (Tier $tier_num)";
        }
        push @parts, $type_desc;

        if (defined $perc_val)
        {
            push @parts, "$perc_val" . "th Percentile";
        }
        if (defined $win_val)
        {
            push @parts, "$win_val-minute Window";
        }
        if (defined $suffix_char && $suffix_char ne "")
        {
            push @parts, "Variant '$suffix_char'";
        }

        $description = join(", ", @parts);
    }
    elsif (lc($profile_name_str) eq "peak") # Handle specific "Peak" profile name
    {
        $description = "Absolute Peak Value";
    }
    # Add more 'elsif' blocks here for other distinct profile naming conventions if needed.

    return "$profile_name_str ($description)";
}

# ==============================================================================
# SUBROUTINE: log_peak_profile_rationale
# PURPOSE:    Logs a simplified, explicit rationale for the P-99W1 profile,
#             clarifying that it is an unmodified, pure measurement.
# ==============================================================================
sub log_peak_profile_rationale {
    my ($fh, $vm_map_ref, $profile_ref, $base_physc) = @_;
    return unless $fh;

    my $vm_name = $vm_map_ref->{Configuration}{vm_name};
    my $profile_desc = parse_profile_name_for_log($profile_ref->{name});

    print {$fh} "\n======================================================================\n";
    printf {$fh} "VM Name                                      : %s\n", $vm_name;
    printf {$fh} "Profile Processed                            : %s\n", $profile_desc;
    print {$fh} "----------------------------------------------------------------------\n";
    print {$fh} "CPU Sizing Path: Pure Peak Measurement\n\n";
    print {$fh} "  - The P-99W1 profile is a special case used for peak analysis.\n";
    print {$fh} "  - It represents the smoothed (1-min SMA) 99.75th percentile of the\n";
    print {$fh} "    unfiltered historical data.\n";
    print {$fh} "  - NO growth, RunQ, or forecasting modifiers are applied to this value.\n\n";
    printf {$fh} "Final Unmodified Value                       : %s cores\n", ($base_physc // 'N/A');
    print {$fh} "======================================================================\n\n";
}

# --- log_profile_rationale ---
# Logs the detailed rationale for how a profile's PhysC value was adjusted.
# Incorporates a summary-first approach and clearer narrative for planners.
# Adheres to Allman style and includes comments.
sub log_profile_rationale
{
    # This function is now refactored to read from the assimilation map.
    my ($fh, $vm_map_ref, $profile_ref, $base_physc_for_log, $final_csv_value_for_profile, $debug_info_ref, $raw_nfit_states_aref, $adaptive_runq_saturation_thresh) = @_;


    # Declare and initialise variables for STD rationale block
    my $eff_p_base_numeric = (defined $base_physc_for_log && looks_like_number($base_physc_for_log)) ? ($base_physc_for_log + 0) : undef;

    # Ensure script doesn't die if log handle isn't valid
    return unless $fh;

    # --- Unpack all required values from the map and arguments ---
    my $vm_name = $vm_map_ref->{Configuration}{vm_name};
    my $profile_obj = $profile_ref;
    my $base_physc_for_profile = $base_physc_for_log;
    my $calc_debug_info_ref = $debug_info_ref;

    # Source from map's Configuration block
    my $cfg = $vm_map_ref->{Configuration};
    my $smt_val = $cfg->{smt};
    my $entitlement_val = $cfg->{entitlement};
    my $lpar_max_cpu_cfg_val_from_config = $cfg->{max_cpu};
    my $curr_ent_numeric = (defined $entitlement_val && looks_like_number($entitlement_val)) ? ($entitlement_val + 0) : undef;

    # Source from map's RunQMetrics block
    my $runq = $vm_map_ref->{RunQMetrics};
    my $runq_metrics_source_profile_name_for_this_calc = $runq->{SourceProfile};
    my $normP50_for_this_calc = $runq->{'NormRunQ_P50'};
    my $normP90_for_this_calc = $runq->{'NormRunQ_P90'};
    my $abs_runq_value_used_for_calc = $calc_debug_info_ref->{AbsRunQValueUsedForCalc};

    # Source other values
    my $profile_rq_behavior = $profile_ref->{runq_behavior};


    # --- Determine if a seasonal run occurred to select the correct logging path ---
    my $event_config = defined($apply_seasonality_event) ? ($seasonality_config->{$apply_seasonality_event} // {}) : {};

    # --- Get profile name and metric key from the profile object ---
	my $model_type = $event_config->{model} // '';
	my $profile_being_adjusted = $profile_obj->{name}; # Get name from the object
	my $profile_desc = parse_profile_name_for_log($profile_being_adjusted);
	my $profile_physc_perc_val_num;
	if (defined $profile_obj->{flags} && $profile_obj->{flags} =~ /(?:-p|--percentile)\s+([0-9.]+)/) {
		$profile_physc_perc_val_num = $1 + 0;
	}
	my $p_metric_key = "P" . clean_perc_label($profile_physc_perc_val_num // $DEFAULT_PERCENTILE);

    # --- PATH A: Multiplicative Seasonal Model has its own log format ---
    if ($model_type eq 'multiplicative_seasonal' && exists $seasonal_debug_info{$vm_name}{$profile_being_adjusted}) {
        my $s_data = $seasonal_debug_info{$vm_name}{$profile_being_adjusted};
        print {$fh} "\n======================================================================\n";
        printf {$fh} "VM Name                                : %s\n", $vm_name;
        printf {$fh} "Profile Processed                      : %s\n", $profile_desc;
        print {$fh} "----------------------------------------------------------------------\n";
        print {$fh} "CPU Sizing Path: Multiplicative Seasonal Forecast\n\n";
        printf {$fh} "  - Current Baseline Value      : %.4f cores\n", $s_data->{baseline};
        printf {$fh} "  - Historical Multiplier       : %.4f\n", $s_data->{multiplier};
        printf {$fh} "  - Volatility Buffer           : %.4f\n", $s_data->{volatility};
        print {$fh} "  - Calculation                 : Baseline * Multiplier * Volatility\n";
        printf {$fh} "  - Final Forecasted Value      : %.4f cores\n", $s_data->{forecast};
        print {$fh} "======================================================================\n\n";
        return;
    }

    # --- PATH B: Standard Rationale (now also used by recency_decay model) ---
    my $na = 'N/A'; # Consistent N/A string for display
    my $abs_runq_key_reported_in_log = $calc_debug_info_ref->{AbsRunQKeyUsed} // 'AbsRunQ_P90 (default)';

    # --- Top Summary Block ---
    my $profile_description_log = parse_profile_name_for_log($profile_being_adjusted);

    # Use the unrounded final value from debug_info for precise change calculation
    my $final_recommendation_unrounded_str = $calc_debug_info_ref->{'FinalAdjustedPhysC'} // $na;
    my $final_recommendation_unrounded_num = ($final_recommendation_unrounded_str ne $na && $final_recommendation_unrounded_str =~ /^-?[0-9.]+$/)
    ? ($final_recommendation_unrounded_str + 0) : undef;

    my $base_physc_val_num = looks_like_number($base_physc_for_profile) ? $base_physc_for_profile + 0 : undef;

    my $net_change_str = $na;
    if (defined $base_physc_val_num && defined $final_recommendation_unrounded_num) {
        my $delta = $final_recommendation_unrounded_num - $base_physc_val_num;
        my $perc_change_str = (abs($base_physc_val_num) > $FLOAT_EPSILON) ? sprintf(" (Change: %s%.1f%%)", ($delta >=0 ? "+" : ""), ($delta / $base_physc_val_num) * 100) : "";
        $net_change_str = sprintf("%s%.4f cores%s", ($delta >=0 ? "+" : ""), abs($delta), $perc_change_str);
    }

    print {$fh} "\n======================================================================\n";
    printf {$fh} "VM Name                                      : %s\n", $vm_name;
    printf {$fh} "Profile Processed                            : %s\n", $profile_description_log;
    print {$fh} "----------------------------------------------------------------------\n";

	if ($model_type eq 'recency_decay') {
		# --- PATH B: Log the Recency Decay Rationale ---
		my $profile_desc = parse_profile_name_for_log($profile_being_adjusted);

		# Extract the growth adjustment value from the nfit results
		my $growth_adj = 0;
		if (ref($raw_nfit_states_aref) eq 'ARRAY' && @$raw_nfit_states_aref) {
			# For a decay run, nfit returns a single aggregated result line
			my $result_line = $raw_nfit_states_aref->[0];
			if (defined $result_line->{GrowthAdj} && looks_like_number($result_line->{GrowthAdj})) {
				$growth_adj = $result_line->{GrowthAdj};
			}
		}

		my $base_val_unrounded = (looks_like_number($base_physc_for_profile))
		? $base_physc_for_profile - $growth_adj
		: "N/A";

		print {$fh} "======================================================================\n";
		printf {$fh} "VM Name                             : %s\n", $vm_name;
		printf {$fh} "Profile Processed                   : %s\n", $profile_desc;
		print {$fh} "----------------------------------------------------------------------\n";
		print {$fh} "CPU Sizing Path: Recency-Anchored Decay (Seasonal: '$apply_seasonality_event')\n\n";
		print {$fh} "  - This model solves the 'Start-of-Month' problem and includes nfit's\n";
		print {$fh} "    standard growth prediction.\n\n";
		printf {$fh} "  - Analysis Reference Date         : %s\n", ($nfit_analysis_reference_date_str // "N/A");
		printf {$fh} "  - Base Value (Recency-Anchored)   : %.4f cores\n", $base_val_unrounded if (looks_like_number($base_val_unrounded));
		printf {$fh} "  - Growth Adjustment               : +%.4f cores\n", $growth_adj;
		print {$fh} "  --------------------------------------------------------------------\n";
		printf {$fh} "  - Final nfit Value (Unrounded)    : %s cores\n", ($base_physc_for_profile // "N/A");
		print {$fh} "======================================================================\n\n";
		return;
	}

    # --- Section A: nfit Raw State Analysis ---
    # CRITICAL: Track pre-growth and post-growth values separately for audit transparency
    # base_physc_for_log now contains the growth-inclusive value (FinalValue from nfit).
    # Retrieve the true pre-growth BaseValue from the Growth.base_values storage.
    my $pre_growth_base_physc = $vm_map_ref->{Growth}{base_values}{$profile_being_adjusted};
    # Fallback: If base_values not available, compute from current value minus growth adj
    if (!defined $pre_growth_base_physc || !looks_like_number($pre_growth_base_physc)) {
        my $nfit_growth_adj = $vm_map_ref->{Growth}{adjustments}{$profile_being_adjusted} // 0;
        $pre_growth_base_physc = (looks_like_number($base_physc_for_log) ? $base_physc_for_log : 0) - $nfit_growth_adj;
    }
    my $nfit_growth_adj = $vm_map_ref->{Growth}{adjustments}{$profile_being_adjusted} // 0;
    my $post_growth_base_physc = (looks_like_number($pre_growth_base_physc) ? $pre_growth_base_physc : 0) + $nfit_growth_adj;

    if (ref($raw_nfit_states_aref) eq 'ARRAY' && @$raw_nfit_states_aref) {
        print {$fh} "Section A: nfit Raw State Analysis & Base Value Calculation\n";
        printf {$fh} "  - nfit reported the following configuration states for this profile:\n";

        my $first_result = $raw_nfit_states_aref->[0];
        my $is_aggregated = ($first_result->{analysisType} || '') =~ /aggregated/;

        if ($is_aggregated) {
            # Use the already-parsed configuration and base value from the assimilation map.
            my $config = $vm_map_ref->{Configuration} || {};
            my $metric_val = $pre_growth_base_physc; # This is the correct pre-growth base value.

            printf {$fh} "    - %-39s: Ent=%.2f, MaxCPU=%.2f, SMT=%d, BaseValue=%.4f\n",
                "Aggregated Result",
                $config->{entitlement} // 0,
                $config->{max_cpu} // 0,
                $config->{smt} // 0,
                (defined($metric_val) && looks_like_number($metric_val)) ? $metric_val : 0;
			my $state_count = $first_result->{state}{stateCount} // 1;
            printf {$fh} "    - Aggregation Method                     : Time-weighted decay model applied across %d configuration states.\n", $state_count;

       } else {
            # Standard logging for non-aggregated results
            foreach my $state_res (@$raw_nfit_states_aref) {
                my $state_id_str = $state_res->{state}{id} // 'N/A';
                my $config = $state_res->{metadata}{configuration} || {};
                my $metric_val = $state_res->{metrics}{physc}{$p_metric_key};

                printf {$fh} "    - %-26s: Ent=%.2f, MaxCPU=%.2f, SMT=%d, %s=%s\n",
                    $state_id_str,
                    $config->{entitlement} // 0,
                    $config->{maxCpu} // 0,
                    $config->{smt} // 0,
                    $p_metric_key,
                    defined($metric_val) ? sprintf("%.4f", $metric_val) : $na;
            }
            if (@$raw_nfit_states_aref > 1) {
                printf {$fh} "    - Aggregation Method                      : Simple Average of %d states was used.\n", scalar(@$raw_nfit_states_aref);
            } else {
                printf {$fh} "    - Aggregation Method                      : Direct value from a single state was used.\n";
            }
        }
    }

    printf {$fh} "Initial Base PhysC for Profile               : %s cores (Aggregated value from nfit)\n",
        (defined($pre_growth_base_physc) && looks_like_number($pre_growth_base_physc))
            ? sprintf("%.4f", $pre_growth_base_physc) : $na;
    printf {$fh} "Final nfit-profile Recommendation            : %s cores (Unrounded: %s)\n",
        ($final_csv_value_for_profile // $na), $final_recommendation_unrounded_str;
    printf {$fh} "Net Adjustment by nfit-profile               : %s\n", $net_change_str;
    print {$fh} "======================================================================\n";

    # --- Section A.1: Comprehensive Growth Adjustment Rationale ---
    # Retrieve growth rationale from the assimilation map (harvested during map building)
    my $gd = $vm_map_ref->{GrowthRationaleByProfile}{$profile_being_adjusted} || {};

    # Display growth section if adjustment is non-zero OR if rationale exists
    if (abs($nfit_growth_adj) > $FLOAT_EPSILON || (ref($gd) eq 'HASH' && scalar keys %$gd > 0)) {
        print {$fh} "Section A.1: nfit GrowthAdj (Theil-Sen Robust Trend)\n";
        printf {$fh} "  - nfit GrowthAdj Applied                   : %s%.4f cores\n",
            ($nfit_growth_adj >= 0 ? "+" : ""), $nfit_growth_adj;

        # Helper for consistent boolean/skipped formatting
        my $format_check_result = sub {
            my $val = shift;
            return "Skipped" if (!defined $val || $val eq 'Skipped');
            return $val ? "Passed" : "Failed";
        };

        # --- Print Adaptive Projection Metadata (if available) ---
        if (defined $gd->{projection_days}) {
            print {$fh} "  - Rationale for GrowthAdj (from nfit):\n";
            printf {$fh} "    - Projection Horizon    : %d days (%s)\n",
                $gd->{projection_days}, $gd->{projection_days_source} // 'unknown';
            printf {$fh} "    - Analysis Window       : %d days\n",
                $gd->{analysis_days} if defined $gd->{analysis_days};
            printf {$fh} "    - Sampling Interval     : %d minutes (average)\n",
                $gd->{avg_sampling_interval_mins} if defined $gd->{avg_sampling_interval_mins};

            # Calculate and display extrapolation ratio for adaptive projections
            if (($gd->{projection_days_source} // '') eq 'adaptive' &&
                defined $gd->{analysis_days} && $gd->{analysis_days} > 0) {
                my $ratio = sprintf("%.2f", $gd->{projection_days} / $gd->{analysis_days});
                printf {$fh} "    - Extrapolation Ratio   : %s:1 (projection:analysis)\n", $ratio;
            }
        }

        # --- Detailed Rationale for Robust Trend Analysis (if growth was calculated) ---
        print {$fh} "    - Rationale for GrowthAdj (Theil-Sen):\n";

        # Check if we have Theil-Sen specific fields (indicates v6.25+ cache)
        my $has_theil_sen_data = (defined $gd->{sen_slope} || defined $gd->{mk_variant});

        if ($has_theil_sen_data) {
            # NEW FORMAT: Comprehensive Theil-Sen rationale
            printf {$fh} "    1. Method         : %s (trend analysis on %d %s samples)\n",
                $gd->{method_used} // 'Theil-Sen',
                $gd->{sample_points} // ($gd->{num_hist_periods} // 0),
                $gd->{aggregation_basis} // 'daily';

            printf {$fh} "    2. Volatility Check : CV = %.4f (Limit: < %.2f) [Result: %s]\n",
                $gd->{volatility_cv} // ($gd->{stats_cv} // 0),
                $GROWTH_MAX_CV_THRESHOLD,
                $format_check_result->($gd->{volatility_check_passed} // $gd->{cv_check_passed});

            printf {$fh} "    3. Trend Analysis   : Theil-Sen Estimator (Sen's Slope)\n";
            printf {$fh} "       - Slope          : %+.6f cores/day\n", $gd->{sen_slope} // 0;
            printf {$fh} "       - Trend          : %s\n", $gd->{sen_trend} // 'none';

            printf {$fh} "    4. Significance     : Mann-Kendall Test (Variant: %s)\n",
                $gd->{mk_variant} // 'standard';
            printf {$fh} "       - p-value        : %.4f (Significant if < 0.05)\n",
                $gd->{sen_p_value} // 1.0;
            printf {$fh} "       - Kendall's Tau  : %.4f\n", $gd->{sen_tau} // 0;

            # Print result or skip reason
            if ($gd->{skip_reason}) {
                printf {$fh} "    5. Result         : Growth skipped. Reason: %s\n",
                    $gd->{skip_reason};
            } elsif (defined $gd->{growth_adj}) {
                my $capping_msg = (defined $gd->{was_capped} && $gd->{was_capped} eq '1')
                    ? sprintf(" (CAPPED from %.4f by %.0f%% limit)",
                              $gd->{original_growth_adj} // $gd->{growth_adj},
                              $gd->{cap_percent_applied} // $DEFAULT_MAX_GROWTH_INFLATION_PERCENT)
                    : "";
                printf {$fh} "    5. Result         : Trend is significant. GrowthAdj (%.4f) applied.%s\n",
                    $gd->{growth_adj}, $capping_msg;
            }

            # Print OLS comparison
            printf {$fh} "    6. OLS Comparison : Slope = %+.6f, R-squared = %.4f\n",
                $gd->{ols_slope} // 0, $gd->{ols_r2} // 0;

            # --- Hamed-Rao Correction Diagnostics (if present) ---
            printf {$fh} "    7. Mann-Kendall Test Details\n";
            if (defined $gd->{mk_variant} && $gd->{mk_variant} eq 'hamed-rao') {
                print {$fh} "    - Hamed-Rao Correction Diagnostics:\n";
                printf {$fh} "      - Adjustment Factor : %.4f\n",
                    $gd->{mk_adjustment_factor} // 1;
                printf {$fh} "      - Effective N       : %.0f (Reduced from %d due to autocorrelation)\n",
                    $gd->{mk_effective_n} // ($gd->{sample_points} // 0),
                    $gd->{sample_points} // 0;
                printf {$fh} "      - Lag-1 Autocorr    : %.4f\n",
                    $gd->{lag1_autocorr} // 0;
            }

            # Display MK variant and routing information
                if (my $mk_variant = $gd->{mk_variant}) {
                    if ($mk_variant eq 'standard_mk_trend_dominant') {
                        print {$fh} "    - Test Variant       : Standard MK (strong trend detected, |Ï| â¥ 0.4)\n";
                        if (my $reason = $gd->{hamed_rao_skip_reason}) {
                            print {$fh} "    - Note               : $reason\n";
                        }
                    } elsif ($mk_variant eq 'standard_mk_via_hamed_rao_fallback') {
                        print {$fh} "    - Test Variant       : Standard MK (Hamed-Rao fallback)\n";
                        if (my $reason = $gd->{hamed_rao_fallback_reason}) {
                            print {$fh} "    - Fallback Reason    : $reason\n";
                        }
                        if (my $af = $gd->{hamed_rao_adjustment_factor}) {
                            print {$fh} "    - Attempted Adj Factor: $af\n";
                        }
                    } elsif ($mk_variant eq 'hamed-rao') {
                        # Already displayed above in existing section
                    } elsif ($mk_variant eq 'standard') {
                        print {$fh} "    - Test Variant       : Standard Mann-Kendall\n";
                    }
                }

                # Display autocorrelation max lag when available
                if (my $K = $gd->{hamed_rao_autocorr_max_lag}) {
                    print {$fh} "    - Max Lag (K)        : $K\n";
                }

        } elsif (defined $gd->{slope_check_passed} && $gd->{slope_check_passed} eq '1') {
            # LEGACY FORMAT: Old OLS-based rationale (pre-v6.25 cache)
            my $proj_val_str = looks_like_number($gd->{projected_val})
                ? sprintf("%.4f", $gd->{projected_val}) : "N/A";
            printf {$fh} "        4. Projection                    : Trend projected to %s cores over %d days.\n",
                $proj_val_str, $DEFAULT_GROWTH_PROJECTION_DAYS;

            my $inflation_str = looks_like_number($gd->{inflation_perc})
                ? sprintf("%.2f", $gd->{inflation_perc}) : "N/A";
            my $capping_msg = (defined $gd->{was_capped} && $gd->{was_capped} eq '1')
                ? "CAPPED" : "not capped";
            printf {$fh} "        5. Inflation                         : Calculated inflation is %s%%. This was %s to the max of %d%%.\n",
                $inflation_str, $capping_msg, $DEFAULT_MAX_GROWTH_INFLATION_PERCENT;
        } else {
            # No growth calculated or insufficient data
            print {$fh} "    - Rationale details not available (pre-v6.25 cache or no growth calculated).\n";
        }

        # Subtotal after growth adjustment
        printf {$fh} "  - Subtotal (Growth-Adjusted Base)            : %.4f cores\n",
            $post_growth_base_physc;
        print {$fh} "======================================================================\n";
    }
    print {$fh} "\n";

    # --- Section B: Key Inputs & Configuration ---

    print {$fh} "Section B: Key Inputs & Configuration for Modifier Logic\n";
    printf {$fh} "  1. Key RunQ Metrics (source: %s, state: %s)\n", $runq_metrics_source_profile_name_for_this_calc, "Most Recent";
    printf {$fh} "     - AbsRunQ for Upsizing (%s)    : %s threads\n", $abs_runq_key_reported_in_log, ($abs_runq_value_used_for_calc // $na);
    printf {$fh} "     - NormRunQ P25                          : %s\n", (looks_like_number($calc_debug_info_ref->{'NormRunQ_P25_Val'}) ? sprintf("%.4f", $calc_debug_info_ref->{'NormRunQ_P25_Val'}) : $na);
    printf {$fh} "     - NormRunQ P50                          : %.4f\n", ($normP50_for_this_calc // ($calc_debug_info_ref->{'NormRunQ_P50_Val'} // $na) );
    printf {$fh} "     - NormRunQ P75                          : %.4f\n", ($calc_debug_info_ref->{'NormRunQ_P75_Val'} // $na);
    printf {$fh} "     - NormRunQ P90                          : %.4f\n", ($normP90_for_this_calc // $na);

    my $iqrc_val_for_log_A_sec = looks_like_number($calc_debug_info_ref->{'NormRunQ_IQRC_Val'}) ? sprintf("%.2f", $calc_debug_info_ref->{'NormRunQ_IQRC_Val'}) : $na;
    printf {$fh} "     - NormRunQ IQRC (Volatility)            : %s", $iqrc_val_for_log_A_sec;
    my $iqrc_interpretation_log_A_sec = $na;
    if ($iqrc_val_for_log_A_sec ne $na && $iqrc_val_for_log_A_sec =~ /^-?[0-9.]+$/)
    {
        my $iqrc_num_A_sec = $iqrc_val_for_log_A_sec + 0;
        if    ($iqrc_num_A_sec < 0.3)  { $iqrc_interpretation_log_A_sec = "Very steady"; }
        elsif ($iqrc_num_A_sec <= 0.6) { $iqrc_interpretation_log_A_sec = "Moderate variability"; }
        elsif ($iqrc_num_A_sec <= 1.0) { $iqrc_interpretation_log_A_sec = "High variability"; }
        else                           { $iqrc_interpretation_log_A_sec = "Very bursty/erratic"; }
        printf {$fh} " (%s)\n", $iqrc_interpretation_log_A_sec;
    } else {
        print {$fh} "\n";
    }

    my $is_runq_pressure_C_log_sec = ($calc_debug_info_ref->{'IsRunQPressure'} // "False") eq "True";
    my $is_workload_pressure_C_log_sec = ($calc_debug_info_ref->{'IsWorkloadPressure'} // "False") eq "True";
    printf {$fh} "  2. VM Configuration & Profile Behavior\n";
    printf {$fh} "     - SMT                                   : %s\n", ($smt_val // $na);
	my $entitlement_display_A_log_sec = (defined $entitlement_val && looks_like_number($entitlement_val)) ? $entitlement_val : $na;
    printf {$fh} "     - Current Entitlement                   : %s cores\n", $entitlement_display_A_log_sec;
    my $lpar_max_cpu_display_A_log_sec = ($lpar_max_cpu_cfg_val_from_config > 0) ? sprintf("%.2f", $lpar_max_cpu_cfg_val_from_config) : $na;
    printf {$fh} "     - LPAR MaxCPU                           : %s cores\n", $lpar_max_cpu_display_A_log_sec;
    printf {$fh} "     - Profile RunQ Behaviour                : %s\n", ($profile_rq_behavior // $na);
	printf {$fh} "  3. Pressure Assessment Summary\n";
    # Enhanced line for Overall LPAR RunQ Pressure
    my $abs_runq_source_str = "$runq_metrics_source_profile_name_for_this_calc " . ($calc_debug_info_ref->{AbsRunQKeyUsed} // '');

    # Correctly retrieve the rationale string from the debug info hash
    my $pressure_basis_str = $calc_debug_info_ref->{'PressureBasisRationale'} // "MaxCPU";
    my $lpar_pressure_reason;

    if (($calc_debug_info_ref->{'IsRunQPressure'} // "False") eq "True") {
        $lpar_pressure_reason = sprintf("Ratio %.2f > %.2f (Pressure detected)", ($calc_debug_info_ref->{'RunQPressure_P90_Val'} // 0), $adaptive_runq_saturation_thresh);
    } else {
        $lpar_pressure_reason = sprintf("Ratio %.2f <= %.2f (No pressure detected)", ($calc_debug_info_ref->{'RunQPressure_P90_Val'} // 0), $adaptive_runq_saturation_thresh);
    }
    printf {$fh} "     - Overall LPAR RunQ Pressure            : %s (Source: %s)\n", (($calc_debug_info_ref->{'IsRunQPressure'} // "False") eq "True" ? "True" : "False"), $abs_runq_source_str;
    printf {$fh} "         Basis for Pressure Calc             : %s\n", $pressure_basis_str;
    printf {$fh} "         Reason for Pressure Flag            : %s\n", $lpar_pressure_reason;

    # Enhanced line for Normalised Workload Pressure
    my $norm_runq_source_str = "$runq_metrics_source_profile_name_for_this_calc";
    printf {$fh} "     - Normalised Workload Pressure          : %s (Source: %s; Reason: %s)\n\n",
        (($calc_debug_info_ref->{'IsWorkloadPressure'} // "False") eq "True" ? "True" : "False"),
        $norm_runq_source_str,
        ($calc_debug_info_ref->{'WorkloadPressureReason'} // "N/A");

    if (defined $calc_debug_info_ref->{'ReasonForNoModification'} && $calc_debug_info_ref->{'ReasonForNoModification'} ne '')
    {
        printf {$fh} "CPU Modification Path Skipped: %s\n", $calc_debug_info_ref->{'ReasonForNoModification'};
    }
    else
    {
        # --- Section C: CPU Downsizing (Efficiency Assessment) ---
        print {$fh} "Section C: CPU Downsizing (Efficiency Assessment)\n";
        my $downsizing_reason_B_log = $calc_debug_info_ref->{'DownsizingReason'} // "Not calculated or N/A."; # Use renamed key
        my $downsizing_factor_B_log = $calc_debug_info_ref->{'DownsizingFactor'} // "1.00"; # Use renamed key
		my $physc_after_downsizing_B_log = $calc_debug_info_ref->{'DownsizedPhysC'} // $na;

        if ($downsizing_reason_B_log =~ /Single-Threaded Dominant \(STD\) Workload Pattern Detected/) {
            # --- Path 1: STD Pattern was detected ---
            printf {$fh} "  - Overall Status                           : Tactical Downsizing Skipped (Entitlement Floor Guard)\n";
            printf {$fh} "  - Guardrail Rationale                      : Base PhysC (%.4f) > Entitlement (%.2f)\n", ($eff_p_base_numeric // 0), ($curr_ent_numeric // 0);
            printf {$fh} "  - Heuristic Override                       : High-Confidence STD workload pattern was detected.\n";
            printf {$fh} "      - Confidence Checks                    : %s\n", ($calc_debug_info_ref->{'STDConfidenceChecks'} // 'N/A');
            print  {$fh} "  - Strategic Signal Calculation (RunQ_Strategic):\n";
            my $runq_uncapped_val = $calc_debug_info_ref->{'RunQ_Strategic'} // 'N/A';
            my $potential_downsizing = (looks_like_number($calc_debug_info_ref->{'EffActualReductionCores'}) ? $calc_debug_info_ref->{'EffActualReductionCores'} : 0);
            printf {$fh} "      - Potential Downsizing                 : %.4f cores\n", $potential_downsizing;
            printf {$fh} "      - Dampening Tier                       : %s\n", ($calc_debug_info_ref->{'STDDampeningTier'} // 'N/A');
            my $final_dampening = $calc_debug_info_ref->{'STDFinalDampeningFactor'} // 0;
            printf {$fh} "      - Final Dampening Factor Applied       : %.0f%%\n", ($final_dampening * 100);
            printf {$fh} "      - Final RunQ_Strategic Value           : %.4f cores\n", (looks_like_number($runq_uncapped_val) ? $runq_uncapped_val : 0);

        } elsif ($downsizing_reason_B_log =~ /workload does not match STD pattern/) {
            # --- Path 2: Bursting, but STD Pattern was NOT detected ---
            printf {$fh} "  - Overall Status                           : Tactical Downsizing Skipped (Entitlement Floor Guard)\n";
            printf {$fh} "  - Guardrail Rationale                      : Base PhysC (%.4f) > Entitlement (%.2f)\n", ($eff_p_base_numeric // 0), ($curr_ent_numeric // 0);
            printf {$fh} "  - Heuristic Assessment                     : Workload does NOT match STD pattern. No override applied.\n";
            printf {$fh} "      - Confidence Checks                    : %s\n", ($calc_debug_info_ref->{'STDConfidenceChecks'} // 'N/A');

        } else {
            # --- Path 3: Standard logging for all other downsizing scenarios ---
            printf {$fh} "  - Overall Status                           : %s\n", $downsizing_reason_B_log;
        }

        printf {$fh} "  - Final Downsizing Factor                  : %s\n", $downsizing_factor_B_log;

        ## --- END: Enhanced Section C Rationale Logging ---

        # Conditionally print detailed analytical breakdown for downsizing
        if ($downsizing_reason_B_log =~ /^Analytical/ &&
            defined $calc_debug_info_ref->{'EffPEfficientTarget'} && # Internal keys can remain Eff...
            defined $calc_debug_info_ref->{'EffCondNormP50Met'} &&
            defined $calc_debug_info_ref->{'EffCondVolatilityMet'})
        {
            printf {$fh} "  - Detailed Analytical Path for Downsizing:\n";
            printf {$fh} "     a. Initial Condition Checks for Downsizing Path:\n";
            my $eff_cond_norm_p50_met_str_B = $calc_debug_info_ref->{'EffCondNormP50Met'} ? "YES (Low P50)" : "NO (P50 not low enough)";
            printf {$fh} "        - NormRunQ P50                       : %-5s (Condition: < %.2f for consideration? %s)\n",
            ($normP50_for_this_calc // $na),
            $NORM_P50_THRESHOLD_FOR_EFFICIENCY_CONSIDERATION,
            $eff_cond_norm_p50_met_str_B;

            my $eff_cond_volatility_met_str_B = $calc_debug_info_ref->{'EffCondVolatilityMet'} ? "YES (Not excessively volatile)" : "NO (Too volatile)";
            printf {$fh} "        - Workload Volatility                : %-5s (NormP90 %.2f / NormP50 %.2f. Condition: < %.2f to proceed? %s)\n",
            ($calc_debug_info_ref->{'EffVolatilityRatio'} // $na),
            ($normP90_for_this_calc ne $na ? ($normP90_for_this_calc+0):0), # Ensure numeric for sprintf
            ($normP50_for_this_calc ne $na ? ($normP50_for_this_calc+0):0),
            $VOLATILITY_CAUTION_THRESHOLD,
            $eff_cond_volatility_met_str_B;
            print {$fh} "\n";

            printf {$fh} "     b. Calculating Raw Efficient PhysC Target (Theoretical Minimum if RunQ was at Target Norm):\n";
            printf {$fh} "        - Base PhysC for Profile             : %s cores\n", ($calc_debug_info_ref->{'EffPBase'} // $na);
            printf {$fh} "        - AbsRunQ Metric Used                : %s (value: %.2f threads)\n", $abs_runq_key_reported_in_log, ($abs_runq_value_used_for_calc // $na);
            printf {$fh} "        - SMT Value                          : %s\n", ($calc_debug_info_ref->{'EffSMTValue'} // $na);
            my $smt_txt = defined $calc_debug_info_ref->{'EffSMTValue'}
                          ? $calc_debug_info_ref->{'EffSMTValue'} : $na;
            my $tgt_val = $calc_debug_info_ref->{'EffTargetNormRunQ'};
            my $tgt_txt = (defined $tgt_val && looks_like_number($tgt_val))
                          ? sprintf('%.2f', $tgt_val) : $na;
            printf {$fh} "        - Target NormRunQ for SMT%-11s : %s (internal heuristic for optimal queue/LCPU)\n", $smt_txt, $tgt_txt;
            printf {$fh} "        - Raw Efficient PhysC Target         : %s / (SMT * Target NormRunQ)\n", $abs_runq_key_reported_in_log;
            printf {$fh} "                                               %s / (%s * %.2f) = %s cores\n",
            ($abs_runq_value_used_for_calc // $na),
            ($calc_debug_info_ref->{'EffSMTValue'} // $na),
            ($calc_debug_info_ref->{'EffTargetNormRunQ'} // $na),
            ($calc_debug_info_ref->{'EffPEfficientTargetRaw'} // $na);
            print {$fh} "\n";

            printf {$fh} "     c. Blending Raw Target with Observed Base PhysC (Applying Confidence):\n";
            printf {$fh} "        - Blending Weights                   : %.0f%% Base PhysC / %.0f%% Raw Target\n",
            defined $calc_debug_info_ref->{'EffBlendWeightBase'} ? (($calc_debug_info_ref->{'EffBlendWeightBase'} // 0) * 100) : 0,
            defined $calc_debug_info_ref->{'EffBlendWeightTarget'} ? (($calc_debug_info_ref->{'EffBlendWeightTarget'} // 0) * 100) : 0;
            printf {$fh} "        - Blending Rationale                 : %s\n", ($calc_debug_info_ref->{'EffBlendReason'} // $na);
            printf {$fh} "        - Blended Efficient Target           : (Base PhysC * Weight) + (Raw Target * Weight)\n";
            printf {$fh} "                                               (%s * %.2f) + (%s * %.2f) = %s cores\n",
            ($calc_debug_info_ref->{'EffPBase'} // $na),
            ($calc_debug_info_ref->{'EffBlendWeightBase'} // 0.0),
            ($calc_debug_info_ref->{'EffPEfficientTargetRaw'} // $na),
            ($calc_debug_info_ref->{'EffBlendWeightTarget'} // 0.0),
            ($calc_debug_info_ref->{'EffPEfficientTarget'} // $na);
            print {$fh} "\n";

            printf {$fh} "     d. Determining Potential CPU Downsizing (Based on Blended Target):\n";
            my $eff_comp_base_vs_target_met_str_B = defined($calc_debug_info_ref->{'EffComparisonBaseVsTargetMet'})
            ? ($calc_debug_info_ref->{'EffComparisonBaseVsTargetMet'} ? "YES" : "NO") : $na;
            printf {$fh} "        - Comparison                         : Base PhysC (%s) > Blended Efficient Target (%s)? %s\n",
            ($calc_debug_info_ref->{'EffPBase'} // $na),
            ($calc_debug_info_ref->{'EffPEfficientTarget'} // $na),
            $eff_comp_base_vs_target_met_str_B;

            if (defined $calc_debug_info_ref->{'EffComparisonBaseVsTargetMet'} && $calc_debug_info_ref->{'EffComparisonBaseVsTargetMet'})
            {
                printf {$fh} "        - Potential CPU Downsize             : %s - %s = %s cores\n",
                ($calc_debug_info_ref->{'EffPBase'} // $na),
                ($calc_debug_info_ref->{'EffPEfficientTarget'} // $na),
                ($calc_debug_info_ref->{'EffPotentialReduction'} // $na);
                printf {$fh} "        - Max Downsize Cap %%                 : %.1f%% (Reason: %s)\n",
                ($calc_debug_info_ref->{'EffMaxAllowableReductionPerc'} eq $na ? ($MAX_EFFICIENCY_REDUCTION_PERCENTAGE*100) : ($calc_debug_info_ref->{'EffMaxAllowableReductionPerc'} +0) ),
                ($calc_debug_info_ref->{'EffReductionCapReason'} // $na);
                printf {$fh} "        - Max Allowable Downsize             : %s cores (Base PhysC * Max Downsize Cap %%)\n",
                ($calc_debug_info_ref->{'EffMaxAllowableReductionCores'} // $na);
                printf {$fh} "        - Actual CPU Downsized By            : %s cores (min of Potential and Max Allowable)\n",
                ($calc_debug_info_ref->{'EffActualReductionCores'} // $na);
            }
            else
            {
                printf {$fh} "        - No potential for downsizing based on Blended Target, or reduction was zero.\n";
            }
            print {$fh} "\n";

            printf {$fh} "     e. Final Downsizing Factor Calculation:\n";
            printf {$fh} "        - Calculated Factor                  : (Base PhysC - Actual Reduction) / Base PhysC\n";
            my $eff_p_base_val_for_div_B = ($calc_debug_info_ref->{'EffPBase'} ne $na && ($calc_debug_info_ref->{'EffPBase'} + 0) != 0)
            ? ($calc_debug_info_ref->{'EffPBase'} + 0) : 1.0;
            my $eff_p_base_display_for_div_B = ($eff_p_base_val_for_div_B == 1.0 && ($calc_debug_info_ref->{'EffPBase'} eq $na || ($calc_debug_info_ref->{'EffPBase'} + 0) == 0))
            ? "$eff_p_base_val_for_div_B (adj for display)" : ($calc_debug_info_ref->{'EffPBase'} // $na);
            printf {$fh} "                                               (%s - %s) / %s = %s\n",
            ($calc_debug_info_ref->{'EffPBase'} // $na),
            ($calc_debug_info_ref->{'EffActualReductionCores'} // "0.0000"),
            $eff_p_base_display_for_div_B,
            ($calc_debug_info_ref->{'EffCalculatedFactor'} // $na);
        }
        printf {$fh} "  => PhysC after Downsizing                  : %s cores\n\n", $physc_after_downsizing_B_log;

        # --- Section D: CPU Upsizing (Additive CPU) ---
        print {$fh} "Section D: CPU Upsizing (Additive CPU)\n";
        my $apply_additive_C_log_sec = $is_runq_pressure_C_log_sec || $is_workload_pressure_C_log_sec;

		printf {$fh} "  - Additive Logic Triggered                 : %s\n", ($apply_additive_C_log_sec ? "Yes" : "No");

        # --- Tier-Aware Scaling (if applied) ---
        if (exists $calc_debug_info_ref->{'TierScalingFactor'} &&
            $calc_debug_info_ref->{'TierScalingFactor'} ne '1.00') {

            printf {$fh} "  - Tier Scaling Applied                     : Tier %s (Factor: %sx)\n",
                   ($calc_debug_info_ref->{'TierNumber'} // 'N/A'),
                   ($calc_debug_info_ref->{'TierScalingFactor'} // '1.00');
            printf {$fh} "      Before Tier Scaling                    : %.4f cores\n",
                   (looks_like_number($calc_debug_info_ref->{'AdditiveCPU_PreTierScaling'})
                    ? $calc_debug_info_ref->{'AdditiveCPU_PreTierScaling'} : 0);
            printf {$fh} "      After Tier Scaling                     : %.4f cores\n",
                   (looks_like_number($calc_debug_info_ref->{'AdditiveCPU_PostTierScaling'})
                    ? $calc_debug_info_ref->{'AdditiveCPU_PostTierScaling'} : 0);
            printf {$fh} "      Rationale                              : %s\n",
                   ($calc_debug_info_ref->{'TierScalingRationale'} // 'N/A');
        }

        if ($apply_additive_C_log_sec)
        {
            printf {$fh} "    Details of Upsizing Calculation:\n";
            printf {$fh} "    - Base for Upsizing                      : %s cores (PhysC after Downsizing)\n", $physc_after_downsizing_B_log;
            printf {$fh} "    - Effective LCPUs at Base                : %s threads\n", ($calc_debug_info_ref->{'EffectiveLCPUsAtBase'} // $na);
            printf {$fh} "    - Excess Threads Calculated              : %s threads\n", ($calc_debug_info_ref->{'ExcessThreads'} // $na);
            printf {$fh} "    - Raw Additive CPU                       : %s cores (excess threads / SMT)\n", ($calc_debug_info_ref->{'RawAdditive'} // $na);
            printf {$fh} "    - Entitlement-Based Cap                  : %s cores (Max Additive Cap: %s)\n",
            (defined $calc_debug_info_ref->{'HotThreadWLDampenedAdditiveFrom'} && $calc_debug_info_ref->{'HotThreadWLDampeningApplied'} eq "True"
                ? $calc_debug_info_ref->{'HotThreadWLDampenedAdditiveFrom'}
                : ($calc_debug_info_ref->{'CappedRawAdditive'} // $na) # Show original value before HTW if HTW was applied
            ), ($calc_debug_info_ref->{'MaxAdditiveCap'} // $na);

            # Log enhanced burst and small entitlement handling details
            printf {$fh} "  Pressure Basis Rationale                   : %s\n", ($calc_debug_info_ref->{'PressureBasisRationale'} // "N/A");
            printf {$fh} "  Burst Allowance Used                       : %s\n", ($calc_debug_info_ref->{'BurstAllowanceUsed'} // "N/A");
            printf {$fh} "  Small Entitlement Handler Active           : %s\n", ($calc_debug_info_ref->{'SmallEntitlementHandler'} // "No");
            printf {$fh} "  Effective LCPUs for Pressure Calc          : %s\n", ($calc_debug_info_ref->{'EffectiveLCPUsForPressure'} // "N/A");

            if (defined $calc_debug_info_ref->{'HotThreadWLDampeningApplied'} && $calc_debug_info_ref->{'HotThreadWLDampeningApplied'} eq "True")
            {
                printf {$fh} "    - Hot Thread Dampening                : Applied\n";
                printf {$fh} "        Conditions Summary                : %s\n", ($calc_debug_info_ref->{'HotThreadWLConditionsString'} // $na);
                printf {$fh} "        Dynamic Dampen Factor             : %s\n", ($calc_debug_info_ref->{'HotThreadWLDynamicFactor'} // $na);
                printf {$fh} "        Additive (Before HTW)             : %s cores -> (After HTW): %s cores\n",
                ($calc_debug_info_ref->{'HotThreadWLDampenedAdditiveFrom'} // $na),
                ($calc_debug_info_ref->{'HotThreadWLDampenedAdditiveTo'} // $na);
            }
            elsif (defined $calc_debug_info_ref->{'HotThreadWLDampeningApplied'}) # Checked but not applied
            {
                printf {$fh} "    - Hot Thread Dampening                   : Not Applied (Details: %s)\n", ($calc_debug_info_ref->{'HotThreadWLConditionsString'} // "Conditions not met");
            }

            printf {$fh} "    - Volatility Factor Applied              : %s (Reason: %s)\n", ($calc_debug_info_ref->{'VoltFactor'} // $na), ($calc_debug_info_ref->{'VoltFactorReason'} // $na);
            printf {$fh} "    - Pool Factor Applied                    : %s\n", ($calc_debug_info_ref->{'PoolFactor'} // $na);
            if (defined $calc_debug_info_ref->{'DBWDampeningApplied'} && $calc_debug_info_ref->{'DBWDampeningApplied'} ne 'False') {
            printf {$fh} "    - Dispatch-Bound (DBW) Anomaly Dampening : %s\n", $calc_debug_info_ref->{'DBWDampeningApplied'};
            }

            if (defined $calc_debug_info_ref->{'AdditiveSafetyCapApplied'} && $calc_debug_info_ref->{'AdditiveSafetyCapApplied'} =~ /^True/)
            {
                printf {$fh} "    - Final Additive Safety Cap              : %s\n", $calc_debug_info_ref->{'AdditiveSafetyCapApplied'};
            }
        }
		printf {$fh} "  => Final Additive CPU                      : %s cores\n", ($calc_debug_info_ref->{'FinalAdditive'} // "0.0000");
		printf {$fh} "  => PhysC after Upsizing                    : %s cores\n\n", ($calc_debug_info_ref->{'PreMaxCpuCapRec'} // $na);


        # --- Section E: Maximum CPU Sizing Sanity Checks ---
        print {$fh} "Section E: Maximum CPU Sizing Sanity Checks\n";
        printf {$fh} "  - Recommendation before Max Sanity Check   : %s cores\n", ($calc_debug_info_ref->{'PreMaxCpuCapRec'} // $na);
        printf {$fh} "  - LPAR MaxCPU (from VM config)             : %s cores\n", $lpar_max_cpu_display_A_log_sec;
        printf {$fh} "  - Entitlement (for forecast multiplier)    : %s cores\n", $entitlement_display_A_log_sec;
        printf {$fh} "  - Forecast Multiplier Used                 : %s\n", ($calc_debug_info_ref->{'ForecastMultiplier'} // $na);
        printf {$fh} "  - Effective MaxCPU Sanity Limit            : %s cores\n", ($calc_debug_info_ref->{'EffectiveMaxCPUCap'} // $na);
        printf {$fh} "  - Limited by MaxCPU Sanity Check?          : %s\n", ($calc_debug_info_ref->{'CappedByMaxCPU'} // $na);
        printf {$fh} "  => Recommendation after Max Sanity Check   : %s cores\n\n", ($final_recommendation_unrounded_str // $na);
    } # End else for ReasonForNoModification (main calculation block)

    # --- Footer Block (Bottom Summary) ---
    print {$fh} "----------------------------------------------------------------------\n";
    printf {$fh} "Overall Summary for Profile: %s\n", $profile_being_adjusted;
    printf {$fh} "  - Initial Base PhysC (pre-growth)        : %.4f cores\n", $pre_growth_base_physc;
    printf {$fh} "  - nfit GrowthAdj (Theil-Sen)             : %s%.4f cores\n",
        ($nfit_growth_adj >= 0 ? "+" : ""), $nfit_growth_adj;
    printf {$fh} "  - nfit-profile Downsizing                : %s%.4f cores (Factor: %s)\n",
        (($calc_debug_info_ref->{'DownsizedPhysC'} // 0) - $post_growth_base_physc >= 0 ? "+" : ""),
        abs(($calc_debug_info_ref->{'DownsizedPhysC'} // 0) - $post_growth_base_physc),
        ($calc_debug_info_ref->{'DownsizingFactor'} // $na);

    my $downsizing_summary_reason_footer = $calc_debug_info_ref->{'DownsizingReason'} // "N/A";
    printf {$fh} "    - CPU Downsizing                         : Factor %s -> PhysC became %s cores. (Summary: %s)\n",
        ($calc_debug_info_ref->{'DownsizingFactor'} // $na),
        ($calc_debug_info_ref->{'DownsizedPhysC'} // $na),
        $downsizing_summary_reason_footer;

    my $additive_final_val_footer = $calc_debug_info_ref->{'FinalAdditive'} // "0.0000";
    my $additive_reason_summary_footer;

    if ($calc_debug_info_ref->{'IsRunQPressure'} eq "True" || $calc_debug_info_ref->{'IsWorkloadPressure'} eq "True") {
        $additive_reason_summary_footer = "Pressure detected";
        my @pressure_types_footer;
        if ($calc_debug_info_ref->{'IsRunQPressure'} eq "True") { push @pressure_types_footer, "Overall LPAR"; }
        if ($calc_debug_info_ref->{'IsWorkloadPressure'} eq "True") { push @pressure_types_footer, "Normalized Workload"; }
        if (@pressure_types_footer) { $additive_reason_summary_footer .= " (" . join(", ", @pressure_types_footer) . ")";}

        if (defined $calc_debug_info_ref->{'HotThreadWLDampeningApplied'} && $calc_debug_info_ref->{'HotThreadWLDampeningApplied'} eq "True")
        {
            $additive_reason_summary_footer .= "; HTW Dampened";
        }
        if (defined $calc_debug_info_ref->{'AdditiveSafetyCapApplied'} && $calc_debug_info_ref->{'AdditiveSafetyCapApplied'} =~ /^True/) {
            $additive_reason_summary_footer .= "; Safety Capped";
        }
        # Add note if additive ended up zero despite pressure
        if (abs(($additive_final_val_footer // 0)+0) < $FLOAT_EPSILON && ($calc_debug_info_ref->{'IsRunQPressure'} eq "True" || $calc_debug_info_ref->{'IsWorkloadPressure'} eq "True")) {
            $additive_reason_summary_footer .= "; Final Additive Zero (due to caps/dampening)";
        }
    }
    else {
        # This path is taken only when no pressure flags were set.
        if (defined $calc_debug_info_ref->{'ExcessThreads'} && $calc_debug_info_ref->{'ExcessThreads'} =~ /No excess threads/i) {
             $additive_reason_summary_footer = "No excess threads calculated for additive.";
         }
        else {
             $additive_reason_summary_footer = "No significant pressure detected.";
        }
    }
    printf {$fh} "    - CPU Upsizing (Additive)                : %s cores. (Reason: %s)\n",
        $additive_final_val_footer, $additive_reason_summary_footer;

    printf {$fh} "  - nfit-profile Upsizing (Additive)         : %s%.4f cores\n",
        ($additive_final_val_footer >= 0 ? "+" : ""), $additive_final_val_footer;

    my $lpar_cap_applied_summary_footer = "Not applied or N/A";
    if (defined $calc_debug_info_ref->{'CappedByMaxCPU'})
    {
        $lpar_cap_applied_summary_footer = $calc_debug_info_ref->{'CappedByMaxCPU'} eq "True"
        ? "Applied (Effective Limit: " . ($calc_debug_info_ref->{'EffectiveMaxCPUCap'} // $na) . ")"
        : "Not Applied (Effective Limit: " . ($calc_debug_info_ref->{'EffectiveMaxCPUCap'} // $na) . ")";
    }
    printf {$fh} "  - Max CPU Sanity Check                   : %s\n", $lpar_cap_applied_summary_footer;
    print {$fh} "  --------------------------------------------------------------------\n";
    printf {$fh} "  Final Recommended Value : %s cores (Unrounded: %s)\n",
        ($final_csv_value_for_profile // $na), # This is rounded for CSV
        $final_recommendation_unrounded_str;    # This is unrounded from calc
    print {$fh} "======================================================================\n\n";

}
# End of log_profile_rationale

# --- Helper subroutine to parse and collate percentile lists for nfit calls ---
# Takes an existing list of percentiles (as a comma-separated string) and an array
# of numeric percentiles that must be ensured to be present in the final list.
# Returns a sorted, unique array of percentile strings, formatted for nfit.
sub parse_and_collate_percentiles
{
    my ($existing_perc_list_str, @ensure_these_percs_numeric) = @_;
    my %final_percs_map; # Use a hash to store unique percentiles to avoid duplicates

    # Add percentiles from the existing string (e.g., from profile flags or global nfit-profile default)
    if (defined $existing_perc_list_str && $existing_perc_list_str ne '')
    {
        my @raw_list = split /,\s*/, $existing_perc_list_str;
        foreach my $p_str (@raw_list)
        {
            next if $p_str eq ''; # Skip empty strings that might result from split (e.g. "80,,90")

            # Validate if it looks like a percentile number, then format it consistently for nfit
            if ($p_str =~ /^[0-9.]+$/ && $p_str >= 0 && $p_str <= 100)
            {
                my $p_label = sprintf("%.2f", $p_str + 0); # Normalize format (e.g., "90.00")
                $p_label =~ s/\.?0+$//;                    # Clean trailing ".00" (e.g., "90")
                $p_label = "0" if $p_label eq "" && abs(($p_str+0) - 0) < 0.001; # Handle case of "0.00" -> "0"
                $final_percs_map{$p_label} = 1; # Add to hash (value 1 is arbitrary, key is what matters)
            }
            else
            {
                # If it's not a simple number, it might be an invalid value.
                # nfit will ultimately validate it. For now, include as is.
                # Alternatively, one could issue a warning here:
                # warn "Warning: Non-standard percentile string '$p_str' found in list '$existing_perc_list_str'. Passing to nfit as is.\n";
                $final_percs_map{$p_str} = 1;
            }
        }
    }

    # Add percentiles that must be ensured (e.g., P90 for AbsRunQ, P50/P90 for NormRunQ)
    foreach my $p_num (@ensure_these_percs_numeric)
    {
        my $p_label = sprintf("%.2f", $p_num); # Format, e.g., 90 -> "90.00", 98.5 -> "98.50"
        $p_label =~ s/\.?0+$//;                # Clean to "90", "98.5"
        $p_label = "0" if $p_label eq "" && abs($p_num - 0) < 0.001; # "0.00" -> "0"
        $final_percs_map{$p_label} = 1; # Add/overwrite in hash to ensure it's present
    }

    # Return a numerically sorted list of unique percentile strings
    my @sorted_keys = sort {
        # Robust sort: treat as numbers if possible, otherwise string compare
        my $is_a_num = ($a =~ /^[0-9.]+$/); # Check if $a looks like a number
        my $is_b_num = ($b =~ /^[0-9.]+$/); # Check if $b looks like a number
        if ($is_a_num && $is_b_num) { return ($a+0) <=> ($b+0); } # Both are numbers, numeric sort
        elsif ($is_a_num) { return -1; } # Numbers come before non-numbers
        elsif ($is_b_num) { return 1;  } # Non-numbers come after numbers
        else { return $a cmp $b; }       # Both are non-numbers (e.g. invalid values), string compare
    } keys %final_percs_map; # Get unique keys from hash and sort them

    return @sorted_keys;
}

# --- quote_csv ---
# Ensures a string is properly quoted for CSV output, escaping internal double quotes.
sub quote_csv {
    my ($field) = @_;
    if (!defined $field) # Handle undefined fields as empty strings
    {
        $field = '';
    }
    $field =~ s/"/""/g; # Escape any double quotes within the field by doubling them
    return qq/"$field"/; # Enclose the entire field in double quotes
}

# --- load_profile_definitions ---
# Loads profile configurations from the specified INI-like file.
# Each section [Profile Name] defines a profile.
# Keys: nfit_flags (mandatory), runq_modifier_behavior (optional, default: 'default').
# This version correctly handles multi-line values for the 'nfit_flags' key.
sub load_profile_definitions {
    my ($filepath) = @_;
    my @loaded_profiles_list;
    my $current_profile = undef;
    my $last_key = '';

    open my $fh, '<:encoding(utf8)', $filepath or die "Error: Cannot open profiles config file '$filepath': $!\n";

    while (my $line = <$fh>) {
        chomp $line;
        $line =~ s/\s*[#;].*//;    # Remove comments
        $line =~ s/^\s+|\s+$//g; # Trim whitespace
        next if $line eq '';

        if ($line =~ /^\s*\[\s*([^\]]+?)\s*\]\s*$/) { # New section
            my $section_name = $1;
            $current_profile = { name => $section_name, flags => '', runq_behavior => 'default', csv_output => 1 };
            push @loaded_profiles_list, $current_profile;
            $last_key = '';
        } elsif ($current_profile && $line =~ /^\s*([^=]+?)\s*=\s*(.*)$/) { # New key-value pair
            my $key = lc($1);
            my $value = $2;
            $key =~ s/^\s+|\s+$//g;
            $value =~ s/^\s+|\s+$//g;
            $last_key = $key;

            if ($key eq 'nfit_flags') {
                $current_profile->{flags} = $value;
            } elsif ($key eq 'runq_modifier_behavior') {
                $current_profile->{runq_behavior} = $value;
            } elsif ($key eq 'csv_output') {
                $current_profile->{csv_output} = ($value =~ /^(true|1|yes)$/i) ? 1 : 0;
            }
        } elsif ($current_profile && $last_key eq 'nfit_flags') { # Continuation of nfit_flags
            $current_profile->{flags} .= " $line";
        }
    }
    close $fh;

    # Final validation and cleanup
    my @valid_profiles;
    foreach my $p_ref (@loaded_profiles_list) {
        # Clean up any leading/trailing whitespace from concatenated flags
        $p_ref->{flags} =~ s/^\s+|\s+$//g if defined $p_ref->{flags};

        if (defined $p_ref->{flags} && $p_ref->{flags} ne '') {
            # (existing validation logic for mandatory P-99W1, etc.)
            push @valid_profiles, $p_ref;
        } else {
            warn "Warning: Profile '[$p_ref->{name}]' in '$filepath' is missing 'nfit_flags'. Skipping.\n";
        }
    }
    return @valid_profiles;
}

# --- calculate_graduated_burst ---
# Returns graduated burst allowance based on entitlement size
# Small entitlements get higher burst allowance to reflect real-world burst capability
sub calculate_graduated_burst {
    my ($entitlement) = @_;

    if ($entitlement < 0.25) {
        return 0.50;  # 50% burst for micro-partitions
    } elsif ($entitlement < 0.50) {
        return 0.40;  # 40% burst for very small
    } elsif ($entitlement < 1.00) {
        return 0.30;  # 30% burst for small
    } else {
        return 0.25;  # 25% standard burst
    }
}

# --- calculate_structural_availability_factor ---
# For small entitlements, estimates the effective availability of vCPU threads
# based on entitlement size. Scales from 0.5 at Ent=0.1 to 0.95 at Ent=0.9
sub calculate_structural_availability_factor {
    my ($entitlement) = @_;

    # Ensure entitlement is in valid range
    return 0.5 if $entitlement <= 0.1;
    return 0.95 if $entitlement >= 0.9;

    # Linear scaling: 0.5 + (0.45 * entitlement)
    return 0.5 + (0.45 * $entitlement);
}

# --- apply_minimum_lcpu_floor ---
# Applies a minimum LCPU floor based on SMT level
# Prevents unrealistic near-zero LCPU calculations for micro-partitions
sub apply_minimum_lcpu_floor {
    my ($effective_lcpus, $smt_used) = @_;

    # Ensure at least 1 full hardware thread for micro-partitions
    # Scale slightly with SMT to be platform-aware
    my $min_floor = max(1, int($smt_used / 4));  # At SMT=8, floor=2; at SMT=4, floor=1

    return max($min_floor, $effective_lcpus);
}

# ==============================================================================
# SUBROUTINE: apply_tier_aware_upsizing_scaling
# PURPOSE:    Applies tier-number-aware scaling to upsizing (additive CPU)
#             adjustments. Higher tiers (1-2) receive more aggressive responses
#             to RunQ pressure due to lower latency tolerance. Tier 3 is the
#             baseline. Tier 4 receives slightly moderated adjustments.
#
# ARGUMENTS:
#   1. $additive_cpu (float): The calculated additive CPU before tier scaling
#   2. $final_tier (string): The tier identifier (e.g., "G3", "O1", "P")
#
# RETURNS:
#   - Hash containing:
#       scaled_value: The tier-adjusted additive CPU value
#       scaling_factor: The multiplier applied
#       tier_number: The extracted tier number
#       rationale: Human-readable explanation for logging
# ==============================================================================
sub apply_tier_aware_upsizing_scaling {
    my ($additive_cpu, $final_tier) = @_;

    # Default result (no scaling)
    my %result = (
        scaled_value   => $additive_cpu,
        scaling_factor => 1.00,
        tier_number    => 'N/A',
        rationale      => 'No tier-based scaling applied',
    );

    # Only scale positive values (upsizing adjustments)
    return \%result if (!defined $additive_cpu || $additive_cpu <= 0);

    # Extract tier number from tier identifier
    # Handles formats: G3, O1, B4, P (P is treated as tier 1)
    my $tier_num;
    if (defined $final_tier && $final_tier =~ /(\d)$/) {
        $tier_num = $1;
    } elsif (defined $final_tier && $final_tier eq 'P') {
        $tier_num = 1;  # P-tier is equivalent to tier 1
    } else {
        return \%result;  # Cannot determine tier, no scaling
    }

    # Tier-based scaling factors
    my %tier_upsizing_factors = (
        1 => 1.30,  # Tier 1: +30% aggressive response (low latency tolerance)
        2 => 1.15,  # Tier 2: +15% enhanced response
        3 => 1.00,  # Tier 3: Baseline STANDARD correction
        4 => 0.95,  # Tier 4: Slightly moderated (still substantial)
    );

    # Apply scaling if tier is valid
    if (exists $tier_upsizing_factors{$tier_num}) {
        my $factor = $tier_upsizing_factors{$tier_num};
        my $scaled = $additive_cpu * $factor;

        $result{scaled_value}   = $scaled;
        $result{scaling_factor} = $factor;
        $result{tier_number}    = $tier_num;

        # Generate detailed rationale
        if ($factor > 1.0) {
            my $increase_pct = ($factor - 1.0) * 100;
            $result{rationale} = sprintf(
                "Tier %d: Enhanced upsizing (+%.0f%%) for lower latency tolerance. Base: %.3f Ã¢â â Scaled: %.3f cores",
                $tier_num, $increase_pct, $additive_cpu, $scaled
            );
        } elsif ($factor < 1.0) {
            my $decrease_pct = (1.0 - $factor) * 100;
            $result{rationale} = sprintf(
                "Tier %d: Moderated upsizing (-%.0f%%) for standard efficiency. Base: %.3f Ã¢â â Scaled: %.3f cores",
                $tier_num, $decrease_pct, $additive_cpu, $scaled
            );
        } else {
            $result{rationale} = sprintf(
                "Tier %d: Standard baseline correction (no scaling). Value: %.3f cores",
                $tier_num, $scaled
            );
        }
    }

    return \%result;
}

# --- calculate_runq_modified_physc (with enhanced efficiency logic and detailed debug output) ---
# Calculates the final PhysC value for a profile after applying efficiency factors
# and RunQ-driven additive CPU adjustments.
# Takes the raw PhysC from nfit, RunQ metrics, SMT, entitlement, MaxCPU, etc.
# Returns the adjusted PhysC value and a hash of debug information for logging.
# --- calculate_runq_modified_physc (refactor: decoupled C & D, no early-returns, adds RunQ_Strategic) ---
#
sub calculate_runq_modified_physc
{
    my ($vm_name, $vm_map_ref, $profile_ref, $pressure_flags_href, $adaptive_thresholds_href) = @_;

    # --- Unpack all required values from the assimilation map and arguments ---
    my $profile_name = $profile_ref->{name};
    my $profile_runq_behavior_setting = $profile_ref->{runq_behavior} // 'default';

    # Core values from the map
    my $selected_tier_physc_value_str = $vm_map_ref->{CoreResults}{ProfileValues}{$profile_name};
    my $pure_p99w1_physc              = $vm_map_ref->{CoreResults}{ProfileValues}{$MANDATORY_PEAK_PROFILE_FOR_HINT};

    # Configuration from the map
    my $cfg_ref                 = $vm_map_ref->{Configuration};
    my $smt_used                = $cfg_ref->{smt};
    my $current_entitlement_str = $cfg_ref->{entitlement};
    my $max_cpu_config_str      = $cfg_ref->{max_cpu};
    my $vcpu_for_lpar_numeric   = $cfg_ref->{virtual_cpus};
    my $pool_cpu_for_lpar_numeric = $cfg_ref->{pool_cpu};
    my $is_in_non_default_pool  = (defined $cfg_ref->{pool_id} && $cfg_ref->{pool_id} != 0);

    # RunQ Metrics from the map
    my $runq_metrics_ref = $vm_map_ref->{RunQMetrics};
    my $norm_runq_p25_str = $runq_metrics_ref->{'NormRunQ_P25'};
    my $norm_runq_p50_str = $runq_metrics_ref->{'NormRunQ_P50'};
    my $norm_runq_p75_str = $runq_metrics_ref->{'NormRunQ_P75'};
    my $norm_runq_p90_str = $runq_metrics_ref->{'NormRunQ_P90'};

    # Determine which AbsRunQ percentile value to use based on profile behaviour.
    my $abs_runq_key_to_use = "AbsRunQ_P90"; # Default to fixed
    if ($runq_perc_behavior_mode eq 'match' && $profile_ref->{flags} =~ /-p\s+([0-9.]+)/) {
        $abs_runq_key_to_use = "AbsRunQ_P" . clean_perc_label($1);
    }
    my $abs_runq_p_value_str = $runq_metrics_ref->{$abs_runq_key_to_use};
    my $abs_runq_key_for_debug = $abs_runq_key_to_use;

    # Pressure flags and adaptive thresholds passed in from the main loop
    my $p99w1_overall_vm_has_abs_runq_pressure  = $pressure_flags_href->{abs_pressure};
    my $p99w1_overall_vm_has_norm_runq_pressure = $pressure_flags_href->{norm_pressure};
    my $adaptive_runq_saturation_thresh   = $adaptive_thresholds_href->{saturation};
    my $adaptive_target_norm_runq         = $adaptive_thresholds_href->{target};
    my $adaptive_max_efficiency_reduction = $adaptive_thresholds_href->{max_reduction};

    # The tier for all modifier calculations must come from the profile being processed,
    # not the global VM hint, to ensure logical consistency.
    my $final_tier = $profile_ref->{name};
    $final_tier =~ s/-.*//; # Extract the tier part (e.g., "G2-98W10" -> "G2")

    my %debug_info;
    my $na_str = "N/A";

    # --- Initialize all debug fields to sensible defaults or N/A ---
    $debug_info{'AbsRunQKeyUsed'} = $abs_runq_key_for_debug // 'N/A (key not provided)';
    $debug_info{'AbsRunQValueUsedForCalc'} = $abs_runq_p_value_str;
    $debug_info{'BasePhysC'} = $selected_tier_physc_value_str // $na_str;
    $profile_runq_behavior_setting //= 'default';

    my $base_physc = ($selected_tier_physc_value_str ne $na_str && $selected_tier_physc_value_str =~ /^-?[0-9.]+$/)
        ? ($selected_tier_physc_value_str + 0)
        : undef;

    # Efficiency related fields - meticulously initialized
    $debug_info{'DownsizingReason'} = "Efficiency calculation not initiated or skipped by initial guards.";
    $debug_info{'DownsizingFactor'} = "1.00"; # final sprintf form later
    $debug_info{'EffCondNormP50Met'} = undef;
    $debug_info{'EffCondVolatilityMet'} = undef;
    $debug_info{'EffVolatilityRatio'} = $na_str;
    $debug_info{'EffPBase'} = (defined $base_physc) ? sprintf("%.4f", $base_physc) : $na_str;
    $debug_info{'EffSMTValue'} = $smt_used // $na_str;
    $debug_info{'EffTargetNormRunQ'} = $na_str;
    $debug_info{'EffPEfficientTargetRaw'} = $na_str;
    $debug_info{'EffBlendReason'} = "Blending not applied or not applicable.";
    $debug_info{'EffBlendWeightBase'} = $na_str;
    $debug_info{'EffBlendWeightTarget'} = $na_str;
    $debug_info{'EffPEfficientTarget'} = $na_str;
    $debug_info{'EffComparisonBaseVsTargetMet'} = undef;
    $debug_info{'EffPotentialReduction'} = $na_str;
    $debug_info{'EffMaxAllowableReductionPerc'} = $adaptive_max_efficiency_reduction * 100;
    $debug_info{'EffReductionCapReason'} = "Default reduction cap applied.";
    $debug_info{'EffMaxAllowableReductionCores'} = $na_str;
    $debug_info{'EffActualReductionCores'} = $na_str;
    $debug_info{'EffCalculatedFactor'} = "1.0000";
    $debug_info{'EffFinalFactorApplied'} = "1.00";

    # General fields
    $debug_info{'DownsizedPhysC'} = defined($base_physc) ? sprintf("%.4f", $base_physc) : $na_str;
    $debug_info{'RunQ_Strategic'} = "0.0000";
    $debug_info{'RunQ_Potential'} = "0.0000";
    $debug_info{'RunQPressure_P90_Val'} = 0; $debug_info{'IsRunQPressure'} = "False";
    $debug_info{'IsWorkloadPressure'} = "False"; $debug_info{'WorkloadPressureReason'} = "Conditions not met or N/A inputs";
    $debug_info{'EffectiveLCPUsAtBase'} = $na_str; $debug_info{'ExcessThreads'} = $na_str;
    $debug_info{'RawAdditive'} = "0.0000"; $debug_info{'MaxAdditiveCap'} = "0.0000"; $debug_info{'CappedRawAdditive'} = "0.0000";
    $debug_info{'VoltFactorReason'} = "Default (no overriding condition met or additive not applied)";
    $debug_info{'VoltFactor'} = "1.00";
    $debug_info{'PoolFactor'} = "1.00";
    $debug_info{'FinalAdditive'} = "0.0000";
    $debug_info{'PreMaxCpuCapRec'} = $debug_info{'DownsizedPhysC'};
    $debug_info{'LPARMaxCPUConfig'} = ($max_cpu_config_str ne "" && $max_cpu_config_str =~ /^[0-9.]+$/ && ($max_cpu_config_str+0) > 0)
        ? ($max_cpu_config_str+0) : $na_str;
    $debug_info{'EntitlementForForecast'} = (defined $current_entitlement_str && $current_entitlement_str ne "" && $current_entitlement_str =~ /^-?[0-9.]+$/)
        ? ($current_entitlement_str + 0) : 0;
    $debug_info{'ForecastMultiplier'} = $na_str; $debug_info{'EffectiveMaxCPUCap'} = $na_str;
    $debug_info{'CappedByMaxCPU'} = $na_str;
    $debug_info{'FinalAdjustedPhysC'} = $debug_info{'BasePhysC'};
    $debug_info{'ReasonForNoModification'} = "";

    my $curr_ent_numeric = $debug_info{'EntitlementForForecast'};
    my $eff_p_base_numeric = defined($base_physc) ? $base_physc : undef;

    # NOTE: Growth adjustment is now pre-applied in ProfileValues (stored as FinalValue).
    # The base_physc value retrieved from ProfileValues already includes GrowthAdj.
    # This ensures all subsequent calculations (downsizing, upsizing, capping) work with
    # the growth-adjusted baseline, maintaining proper audit trail transparency.
    # The manual addition that was previously here has been removed to prevent double-counting.
    $debug_info{'EffPBase'} = defined($eff_p_base_numeric) ? sprintf("%.4f", $eff_p_base_numeric) : $na_str;

    unless (defined $base_physc)
    {
        $debug_info{'ReasonForNoModification'} = "BasePhysC for profile not numeric or N/A";
        $debug_info{'FinalAdjustedPhysC'} = $selected_tier_physc_value_str // $na_str;
        return ($selected_tier_physc_value_str // $na_str, \%debug_info);
    }

    my $norm_p50_numeric = ($norm_runq_p50_str ne $na_str && $norm_runq_p50_str =~ /^-?[0-9.]+$/) ? ($norm_runq_p50_str + 0) : undef;
    my $norm_p90_numeric = ($norm_runq_p90_str ne $na_str && $norm_runq_p90_str =~ /^-?[0-9.]+$/) ? ($norm_runq_p90_str + 0) : undef;
    my $abs_runq_p_numeric  = ($abs_runq_p_value_str ne $na_str && $abs_runq_p_value_str  =~ /^-?[0-9.]+$/) ? ($abs_runq_p_value_str + 0)  : undef;
    $debug_info{'NormRunQ_P90_Val'} = (defined $norm_p90_numeric) ? sprintf("%.2f", $norm_p90_numeric) : $na_str;

    # Calculate Effective LCPUs for Pressure
    my ($effective_lcpus_for_pressure_calc, $pressure_basis_rationale);
    my $max_cpu_for_lpar_numeric = (defined $max_cpu_config_str && looks_like_number($max_cpu_config_str)) ? ($max_cpu_config_str + 0) : 0;
    my $base_burst_factor = 0.50;
    my $evr_tier_rationale = "Conservative/Standard (EVR < 5x)";
    if (defined $curr_ent_numeric && $curr_ent_numeric > 0 && defined $vcpu_for_lpar_numeric && $vcpu_for_lpar_numeric > 0) {
        my $evr = $vcpu_for_lpar_numeric / $curr_ent_numeric;
        if ($evr > 10) {
            $base_burst_factor = 3.00;
            $evr_tier_rationale = sprintf("Aggressive (EVR %.1fx > 10x)", $evr);
        } elsif ($evr >= 5) {
            $base_burst_factor = 1.50;
            $evr_tier_rationale = sprintf("Moderate (EVR %.1fx >= 5x)", $evr);
        }
    }
    my $pool_constraint_multiplier = 1.0;
    my $pool_constraint_rationale = "Default Pool (ID 0), no constraint applied";
    if ($is_in_non_default_pool && $pool_cpu_for_lpar_numeric > 0 && $curr_ent_numeric > 0) {
        my $vm_share_of_pool = $curr_ent_numeric / $pool_cpu_for_lpar_numeric;
        $pool_constraint_multiplier = 1.0 - (0.3 * min(1.0, $vm_share_of_pool * 2));
        $pool_constraint_multiplier = max(0.7, $pool_constraint_multiplier);
        $pool_constraint_rationale = sprintf("User Pool (Share %.1f%%), multiplier of %.2f applied", $vm_share_of_pool*100, $pool_constraint_multiplier);
    }
    my $final_burst_allowance = $base_burst_factor * $pool_constraint_multiplier;
    my $theoretical_lcpus = ($curr_ent_numeric * (1 + $final_burst_allowance)) * $smt_used;
    my $p99w1_physc_val = $pure_p99w1_physc;
    my $capacity_headroom = ($max_cpu_for_lpar_numeric > 0) ? (1 - ($p99w1_physc_val / $max_cpu_for_lpar_numeric)) : 0;
    my $headroom_confidence_modifier = 1.0;
    my $headroom_rationale = "High Headroom (>30%), full confidence in observed peak";
    if ($capacity_headroom < 0.10) {
        $headroom_confidence_modifier = 0.75;
        $headroom_rationale = sprintf("Low Headroom (%.1f%%), low confidence in further bursting", $capacity_headroom * 100);
    } elsif ($capacity_headroom < 0.30) {
        $headroom_confidence_modifier = 0.90;
        $headroom_rationale = sprintf("Medium Headroom (%.1f%%), medium confidence", $capacity_headroom * 100);
    }
    my $confidence_adjusted_peak_lcpus = ($p99w1_physc_val * $smt_used) * $headroom_confidence_modifier;
    my $absolute_ceiling_lcpus = $max_cpu_for_lpar_numeric > 0 ? ($max_cpu_for_lpar_numeric * $smt_used) : ($vcpu_for_lpar_numeric * $smt_used);
    $effective_lcpus_for_pressure_calc = min($absolute_ceiling_lcpus, max($theoretical_lcpus, $confidence_adjusted_peak_lcpus));
    $pressure_basis_rationale = sprintf(
        "\n   1. Theoretical Capacity (Config)\n".
        "      - Base Burst Factor                    : %.0f%% (Reason: %s)\n".
        "      - Pool Constraint                      : %.2fx (Reason: %s)\n".
        "      => Theoretical LCPUs                   : %.2f Ent * (1 + %.2f Final Burst) * %d SMT = %.1f\n".
        "   2. Observed Capacity (Evidence-based)\n".
        "      - Observed Peak (P-99W1)               : %.2f cores (Headroom: %.1f%%)\n".
        "      - Confidence Modifier                  : %.2fx (Reason: %s)\n".
        "      => Conf-Adjusted LCPUs                 : (%.2f Cores * %d SMT) * %.2f = %.1f\n".
        "   3. Final Decision\n".
        "      => Effective LCPUs                     : min(%.1f Max LCPUs, max(%.1f Theoretical, %.1f Observed)) = %.1f",
        $base_burst_factor*100, $evr_tier_rationale,
        $pool_constraint_multiplier, $pool_constraint_rationale,
        $curr_ent_numeric, $final_burst_allowance, $smt_used, $theoretical_lcpus,
        $p99w1_physc_val, $capacity_headroom*100,
        $headroom_confidence_modifier, $headroom_rationale,
        $p99w1_physc_val, $smt_used, $headroom_confidence_modifier, $confidence_adjusted_peak_lcpus,
        $absolute_ceiling_lcpus, $theoretical_lcpus, $confidence_adjusted_peak_lcpus, $effective_lcpus_for_pressure_calc
    );

    my $norm_p25_numeric = ($norm_runq_p25_str ne $na_str && $norm_runq_p25_str =~ /^-?[0-9.]+$/) ? ($norm_runq_p25_str + 0) : undef;
    my $norm_p75_numeric = ($norm_runq_p75_str ne $na_str && $norm_runq_p75_str =~ /^-?[0-9.]+$/) ? ($norm_runq_p75_str + 0) : undef;
    my $normrunq_iqrc_val = undef;
    if (defined $norm_p25_numeric && defined $norm_p50_numeric && defined $norm_p75_numeric)
    {
        my $p50_denominator_for_iqrc = (abs($norm_p50_numeric) > $MIN_P50_DENOMINATOR_FOR_VOLATILITY)
            ? $norm_p50_numeric
            : (($norm_p50_numeric >= 0) ? $MIN_P50_DENOMINATOR_FOR_VOLATILITY : -$MIN_P50_DENOMINATOR_FOR_VOLATILITY);
        if (abs($p50_denominator_for_iqrc) > $FLOAT_EPSILON)
        {
            $normrunq_iqrc_val = ($norm_p75_numeric - $norm_p25_numeric) / $p50_denominator_for_iqrc;
        }
        elsif (abs($norm_p75_numeric - $norm_p25_numeric) < $FLOAT_EPSILON) { $normrunq_iqrc_val = 0.0; }
        else { $normrunq_iqrc_val = 999.0; }
    }
    $debug_info{'NormRunQ_P25_Val'} = (defined $norm_p25_numeric) ? sprintf("%.2f", $norm_p25_numeric) : $na_str;
    $debug_info{'NormRunQ_P50_Val'} = (defined $norm_p50_numeric) ? sprintf("%.2f", $norm_p50_numeric) : $na_str;
    $debug_info{'NormRunQ_P75_Val'} = (defined $norm_p75_numeric) ? sprintf("%.2f", $norm_p75_numeric) : $na_str;
    $debug_info{'EffectiveLCPUsForPressure'} = sprintf("%.1f", $effective_lcpus_for_pressure_calc);
    $debug_info{'EffectiveLCPUsAtBase'} = sprintf("%.4f", $effective_lcpus_for_pressure_calc);
    my $runq_pressure_p_val = ($effective_lcpus_for_pressure_calc > 0) ? (($abs_runq_p_numeric // 0) / $effective_lcpus_for_pressure_calc) : 0;
    $debug_info{'NormRunQ_IQRC_Val'} = (defined $normrunq_iqrc_val) ? sprintf("%.3f", $normrunq_iqrc_val) : $na_str;
    $debug_info{'RunQPressure_P90_Val'} = sprintf("%.4f", $runq_pressure_p_val);
    $debug_info{'PressureBasisRationale'} = $pressure_basis_rationale;
    $debug_info{'BurstAllowanceUsed'} = sprintf("%.2f%% (EVR-based, Pool-adjusted)", $final_burst_allowance * 100);
    $debug_info{'SmallEntitlementHandlerActive'} = ($curr_ent_numeric < 1.0 && $max_cpu_for_lpar_numeric > $curr_ent_numeric) ? "Yes" : "No";

    # =========================
    # Section C: Strategic Efficiency / Downsizing (ALWAYS RUNS)
    # =========================
    my $efficiency_factor_numeric = 1.00;
    $debug_info{'DownsizingFactor'} = sprintf("%.2f", $efficiency_factor_numeric);
    my $base_adjusted_physc = (defined $eff_p_base_numeric ? $eff_p_base_numeric : 0) * $efficiency_factor_numeric;
    $debug_info{'DownsizedPhysC'} = sprintf("%.4f", $base_adjusted_physc);
    $debug_info{'DownsizingReason'} = "Default (No CPU downsizing applied or eligible).";
    if (defined $norm_p50_numeric && defined $norm_p90_numeric && defined $abs_runq_p_numeric && $smt_used > 0 && defined $eff_p_base_numeric)
    {
        $debug_info{'EffCondNormP50Met'} = ($norm_p50_numeric < $NORM_P50_THRESHOLD_FOR_EFFICIENCY_CONSIDERATION);
        my $effective_p50_for_volatility = ($norm_p50_numeric > 0.0001) ? max($norm_p50_numeric, $MIN_P50_DENOMINATOR_FOR_VOLATILITY) : $MIN_P50_DENOMINATOR_FOR_VOLATILITY;
        my $volatility_ratio = (defined $norm_p90_numeric && $effective_p50_for_volatility > 0.0001) ? ($norm_p90_numeric / $effective_p50_for_volatility) : 1.0;
        $debug_info{'EffVolatilityRatio'} = sprintf("%.2f", $volatility_ratio);
        $debug_info{'EffCondVolatilityMet'} = ($volatility_ratio < $VOLATILITY_CAUTION_THRESHOLD);
        if (!$debug_info{'EffCondVolatilityMet'}) {
            $debug_info{'DownsizingReason'} = sprintf("Skipped CPU Downsizing: Workload volatile (NormP90/P50 ratio %.2f >= %.2f). No analytical reduction.", $volatility_ratio, $VOLATILITY_CAUTION_THRESHOLD);
        }
        elsif (!$debug_info{'EffCondNormP50Met'}) {
            $debug_info{'DownsizingReason'} = sprintf("Skipped CPU Downsizing: NormRunQ P50 (%.2f) not below threshold (%.2f) for efficiency consideration. No analytical reduction.", $norm_p50_numeric, $NORM_P50_THRESHOLD_FOR_EFFICIENCY_CONSIDERATION);
        }
        else {
            my $target_norm_runq_eff_calc;
            my $adaptive_target_reason;
            my $tier_to_check = $final_tier;
            if (exists $custom_runq_targets{$tier_to_check}) {
                $target_norm_runq_eff_calc = $custom_runq_targets{$tier_to_check};
                $adaptive_target_reason = "User-defined target for tier '$tier_to_check' from nfit.profiles.cfg";
            } elsif (exists $DEFAULT_RUNQ_TARGETS{$tier_to_check}) {
                $target_norm_runq_eff_calc = $DEFAULT_RUNQ_TARGETS{$tier_to_check};
                $adaptive_target_reason = "Built-in default target for tier '$tier_to_check'";
            } else {
                my ($pattern) = ($final_tier =~ /^([OGBP])/);
                my $base_tier = $pattern // 'G';
                $target_norm_runq_eff_calc = $DEFAULT_RUNQ_TARGETS{$base_tier} // $DEFAULT_TARGET_NORM_RUNQ_FOR_EFFICIENCY_CALC;
                $adaptive_target_reason = "Fallback target for base pattern '$base_tier'";
            }
            $debug_info{'AdaptiveTargetNormRunQReason'} = "$adaptive_target_reason: ${target_norm_runq_eff_calc}";
            my $p_efficient_target_raw = ($smt_used * $target_norm_runq_eff_calc > 0.0001) ? ($abs_runq_p_numeric / ($smt_used * $target_norm_runq_eff_calc)) : $eff_p_base_numeric + 1;
            $debug_info{'EffPEfficientTargetRaw'} = sprintf("%.4f", $p_efficient_target_raw);
            $debug_info{'EffTargetNormRunQ'} = sprintf("%.2f", $target_norm_runq_eff_calc);
            my $base_physc_weight = $BLEND_WEIGHT_BASE_DEFAULT_LOW_P50;
            my $efficient_target_weight = 1.0 - $base_physc_weight;
            my $blending_details_str = sprintf("Default low P50 blend (%.0f%% Base / %.0f%% Target).", $base_physc_weight*100, $efficient_target_weight*100);
            if ($norm_p50_numeric < $NORM_P50_LOW_THRESH_FOR_BLEND1) {
                $base_physc_weight = $BLEND_WEIGHT_BASE_FOR_LOW_P50_1;
                $efficient_target_weight = 1.0 - $base_physc_weight;
                $blending_details_str = sprintf("NormP50 (%.2f) < %.2f, using more aggressive blend (%.0f%% Base / %.0f%% Target).", $norm_p50_numeric, $NORM_P50_LOW_THRESH_FOR_BLEND1, $base_physc_weight*100, $efficient_target_weight*100);
            } elsif ($norm_p50_numeric < $NORM_P50_MODERATE_THRESH_FOR_BLEND2) {
                $base_physc_weight = $BLEND_WEIGHT_BASE_FOR_LOW_P50_2;
                $efficient_target_weight = 1.0 - $base_physc_weight;
                $blending_details_str = sprintf("NormP50 (%.2f) < %.2f, using moderate blend (%.0f%% Base / %.0f%% Target).", $norm_p50_numeric, $NORM_P50_MODERATE_THRESH_FOR_BLEND2, $base_physc_weight*100, $efficient_target_weight*100);
            }
            $debug_info{'EffBlendWeightBase'}   = sprintf("%.2f", $base_physc_weight);
            $debug_info{'EffBlendWeightTarget'} = sprintf("%.2f", $efficient_target_weight);
            $debug_info{'EffBlendReason'}       = $blending_details_str;
            my $blended_efficient_target = ($eff_p_base_numeric * $base_physc_weight) + ($p_efficient_target_raw * $efficient_target_weight);
            $debug_info{'EffPEfficientTarget'}  = sprintf("%.4f", $blended_efficient_target);
            $debug_info{'EffComparisonBaseVsTargetMet'} = ($eff_p_base_numeric > $blended_efficient_target);
            if ($debug_info{'EffComparisonBaseVsTargetMet'})
            {
                my $potential_reduction_cores = $eff_p_base_numeric - $blended_efficient_target;
                $debug_info{'EffPotentialReduction'} = sprintf("%.4f", $potential_reduction_cores);
                $debug_info{'RunQ_Potential'} = sprintf("%.4f", -1 * $potential_reduction_cores);
                my $base_max_reduction_factor;
                my $size_aware_cap_reason;
                if ($eff_p_base_numeric < 1.0) {
                    $base_max_reduction_factor = 0.30;
                    $size_aware_cap_reason = sprintf("BasePhysC (%.2f) < 1.0, using %.0f%% cap", $eff_p_base_numeric, $base_max_reduction_factor * 100);
                } elsif ($eff_p_base_numeric < 4.0) {
                    $base_max_reduction_factor = 0.25;
                    $size_aware_cap_reason = sprintf("BasePhysC (%.2f) < 4.0, using %.0f%% cap", $eff_p_base_numeric, $base_max_reduction_factor * 100);
                } else {
                    $base_max_reduction_factor = 0.20;
                    $size_aware_cap_reason = sprintf("BasePhysC (%.2f) >= 4.0, using %.0f%% cap", $eff_p_base_numeric, $base_max_reduction_factor * 100);
                }
                my $current_max_reduction_perc_val = $base_max_reduction_factor;
                my $volatility_reason = "";
                if ($volatility_ratio > $VOLATILITY_MODERATE_HIGH_CAP_THRESH) {
                    $current_max_reduction_perc_val *= $REDUCTION_CAP_SCALE_FOR_MODERATE_HIGH_VOLATILITY;
                    $volatility_reason = sprintf(" (scaled by %.2fx for high volatility)", $REDUCTION_CAP_SCALE_FOR_MODERATE_HIGH_VOLATILITY);
                } elsif ($volatility_ratio > $VOLATILITY_MODERATE_MEDIUM_CAP_THRESH) {
                    $current_max_reduction_perc_val *= $REDUCTION_CAP_SCALE_FOR_MODERATE_MEDIUM_VOLATILITY;
                    $volatility_reason = sprintf(" (scaled by %.2fx for medium volatility)", $REDUCTION_CAP_SCALE_FOR_MODERATE_MEDIUM_VOLATILITY);
                } elsif ($volatility_ratio > $VOLATILITY_MODERATE_LOW_CAP_THRESH) {
                    $current_max_reduction_perc_val *= $REDUCTION_CAP_SCALE_FOR_MODERATE_VOLATILITY;
                    $volatility_reason = sprintf(" (scaled by %.2fx for moderate volatility)", $REDUCTION_CAP_SCALE_FOR_MODERATE_VOLATILITY);
                }
                $debug_info{'EffReductionCapReason'} = $size_aware_cap_reason . $volatility_reason;
                $debug_info{'EffMaxAllowableReductionPerc'} = $current_max_reduction_perc_val * 100;
                my $max_allowable_reduction_cores = $eff_p_base_numeric * $current_max_reduction_perc_val;
                $debug_info{'EffMaxAllowableReductionCores'} = sprintf("%.4f", $max_allowable_reduction_cores);
                my $actual_reduction_cores = min($potential_reduction_cores, $max_allowable_reduction_cores);
                $actual_reduction_cores = max(0, $actual_reduction_cores);
                $debug_info{'EffActualReductionCores'} = sprintf("%.4f", $actual_reduction_cores);
                $debug_info{'RunQ_Strategic'} = sprintf("%.4f", -1 * ($actual_reduction_cores+0));
                if ($actual_reduction_cores > 0.0001) {
                    my $new_physc_after_reduction = $eff_p_base_numeric - $actual_reduction_cores;
                    $efficiency_factor_numeric = ($eff_p_base_numeric > 0.0001) ? ($new_physc_after_reduction / $eff_p_base_numeric) : 1.00;
                    my $min_expected_eff_factor = 1 - $current_max_reduction_perc_val;
                    if ($efficiency_factor_numeric < ($min_expected_eff_factor - 0.001) ) { $efficiency_factor_numeric = $min_expected_eff_factor; }
                    $efficiency_factor_numeric = 1.00 if $efficiency_factor_numeric > 1.00;
                    $efficiency_factor_numeric = max(0, $efficiency_factor_numeric);
                    $debug_info{'EffCalculatedFactor'}   = sprintf("%.4f", $efficiency_factor_numeric);
                    $debug_info{'EffFinalFactorApplied'} = sprintf("%.2f",  $efficiency_factor_numeric);
                    $debug_info{'DownsizingReason'}      = sprintf("Analytical CPU Downsizing (using blended target & dynamic cap): Reduction of %.4f cores applied.", $actual_reduction_cores);
                } else {
                    $debug_info{'EffFinalFactorApplied'} = "1.00";
                    $debug_info{'EffCalculatedFactor'}   = "1.0000";
                    $debug_info{'DownsizingReason'} = sprintf("Analytical CPU Downsizing (using blended target & dynamic cap): Base_PhysC %.4f, Blended_Target %.4f. Calculated reduction (%.4f) negligible or zero. No adjustment from this path.", $eff_p_base_numeric, $blended_efficient_target, $actual_reduction_cores // 0.0);
                }
            } else {
                $debug_info{'EffFinalFactorApplied'} = "1.00";
                $debug_info{'EffCalculatedFactor'}   = "1.0000";
                $debug_info{'DownsizingReason'} = sprintf("Analytical CPU Downsizing: Base_PhysC %.4f not greater than Blended_Efficient_Target_PhysC %.4f. No reduction. Blending Reason: %s", $eff_p_base_numeric, $blended_efficient_target, $blending_details_str);
                $debug_info{'RunQ_Potential'} = "0.0000";
            }
        }
    } else {
        $debug_info{'DownsizingReason'} = "Key metrics (NormP50/P90, AbsRunQ) N/A for full analytical CPU downsizing check. No efficiency adjustment applied.";
    }

    my $skip_downsizing_reason;
    if (($p99w1_overall_vm_has_abs_runq_pressure && $p99w1_overall_vm_has_abs_runq_pressure) || ($p99w1_overall_vm_has_norm_runq_pressure && $p99w1_overall_vm_has_norm_runq_pressure)) {
        $skip_downsizing_reason = sprintf("VM's %s profile shows RunQ pressure.", $MANDATORY_PEAK_PROFILE_FOR_HINT);
    } elsif ($profile_runq_behavior_setting eq 'additive_only') {
       $skip_downsizing_reason = "Profile runq_behavior=additive_only.";
    } elsif (defined $curr_ent_numeric && $curr_ent_numeric > 0 && defined $eff_p_base_numeric && ($eff_p_base_numeric > $curr_ent_numeric)) {

        # --- Enhanced STD Pattern Detection with Multiple Confidence Boosters ---

        # 1. Dynamically determine the source profile for PhysC stability metrics.
        #    Priority: User TIER > AutoTier > Fallback to 'G'.
        my $user_tier_override_std = $vm_map_ref->{Hinting}{FinalTierForVM} // "";
        my $auto_tier_std = $vm_map_ref->{Hinting}{AutoTier} // "G";

        my $pattern_source_std = ($user_tier_override_std ne "") ? $user_tier_override_std : $auto_tier_std;
        my ($pattern) = ($pattern_source_std =~ /^([A-Z])/);
        $pattern //= 'G'; # Default to 'G' if regex fails

        my %pattern_to_profile_map = ('O' => 'O3-95W15', 'B' => 'B3-95W15', 'G' => 'G3-95W15', 'P' => 'G3-95W15');
        my $source_profile_base = $pattern_to_profile_map{$pattern} // 'G3-95W15';

        my $p50_profile_name = $source_profile_base; $p50_profile_name =~ s/-95W/-50W/;

        # 2. Look up PhysC P50 and P95 from the assimilation map's CoreResults.
        my $physc_p50 = $vm_map_ref->{CoreResults}{ProfileValues}{$p50_profile_name};
        my $physc_p95 = $vm_map_ref->{CoreResults}{ProfileValues}{$source_profile_base};

        # 3. Calculate PhysC stability using the P95/P50 ratio.
        my $physc_stability_ratio = (defined $physc_p50 && looks_like_number($physc_p50) && $physc_p50 > 0.01) ? ((looks_like_number($physc_p95) ? $physc_p95 : 0) / $physc_p50) : 999;
        my $is_physc_stable = ($physc_stability_ratio < (1.0 + $STD_PHYSC_STABILITY_THRESH));

        # 4. Core STD pattern detection (all 3 conditions must be met).
        my $is_std_pattern = (defined $norm_p90_numeric && $norm_p90_numeric < $STD_NORM_P90_THRESH) && (defined $normrunq_iqrc_val && $normrunq_iqrc_val < $STD_IQRC_THRESH) && $is_physc_stable;
        $debug_info{'STDConfidenceChecks'} = sprintf("NormP90<%.1f(%.2f), IQRC<%.1f(%.3f), PhysC_Stable(P95/P50)<%.2f(%.2f)", $STD_NORM_P90_THRESH, ($norm_p90_numeric // 0), $STD_IQRC_THRESH, ($normrunq_iqrc_val // 0), (1.0 + $STD_PHYSC_STABILITY_THRESH), $physc_stability_ratio);

        # If pattern matched: This is a high-confidence inefficient workload.
        if ($is_std_pattern) {
            my $actual_reduction_num = (defined $debug_info{'EffActualReductionCores'} && looks_like_number($debug_info{'EffActualReductionCores'})) ? ($debug_info{'EffActualReductionCores'} + 0) : 0;

            # 5. Impact-based dampening.
            my $spinning_thread_impact_ratio = 1.0 / max(1, $effective_lcpus_for_pressure_calc);
            my $graduated_dampening_factor;
            my $tier_rationale;

            if ($spinning_thread_impact_ratio >= 0.20) {
                # High impact
                $graduated_dampening_factor = 0.85;
                $tier_rationale = sprintf("High Impact (%.1f%% of VM capacity)", $spinning_thread_impact_ratio * 100);
            } elsif ($spinning_thread_impact_ratio >= 0.10) {
                # Medium impact
                $graduated_dampening_factor = 0.70;
                $tier_rationale = sprintf("Medium Impact (%.1f%% of VM capacity)", $spinning_thread_impact_ratio * 100);
            } else {
                $graduated_dampening_factor = 0.30;
                $tier_rationale = sprintf("Low Impact (%.1f%% of VM capacity)", $spinning_thread_impact_ratio * 100);
            }
            my $dampened_reduction = $actual_reduction_num * $graduated_dampening_factor;
            $debug_info{'RunQ_Strategic'} = sprintf("%.4f", -$dampened_reduction);
            $skip_downsizing_reason = "Single-Threaded Dominant (STD) Workload Pattern Detected. Tactical downsizing skipped; updating strategic RunQ_Strategic recommendation.";
            $debug_info{'STDDampeningTier'} = sprintf("%s, L_eff=%.1f", $tier_rationale, $effective_lcpus_for_pressure_calc);
            $debug_info{'STDFinalDampeningFactor'} = $graduated_dampening_factor;
        } else {
            # PATTERN NOT MATCHED: This is a genuine burst. Block all downsizing.
            $skip_downsizing_reason = sprintf("Base PhysC (%.4f) > Entitlement (%.2f) and workload does not match STD pattern.", $eff_p_base_numeric, $curr_ent_numeric);
            $debug_info{'RunQ_Strategic'} = "0.0000";
        }
    }
    if (defined $skip_downsizing_reason) {
        $base_adjusted_physc = $eff_p_base_numeric;
        $efficiency_factor_numeric = 1.0;
        $debug_info{'DownsizingReason'} = "Skipped Applying Downsizing: " . $skip_downsizing_reason;
    } else {
        $base_adjusted_physc = $eff_p_base_numeric * $efficiency_factor_numeric;
        if ($efficiency_factor_numeric < 1 - 1e-4) {
            $debug_info{'DownsizingReason'} = "Analytical CPU Downsizing applied.";
        }
    }

    # Now, definitively set the final factor that was actually used.
    $debug_info{'DownsizingFactor'} = sprintf("%.2f", $efficiency_factor_numeric);
    $debug_info{'DownsizedPhysC'} = sprintf("%.4f", $base_adjusted_physc);

    # =========================
    # Section D: Tactical Additive CPU
    # =========================
    my $tier_scaling_factor = 1.00;
    # Default to Tier 3 (baseline, 1.00x scaling) if the tier cannot be determined.
    # This is a safe fallback that prevents uninitialised value warnings.
    my $tier_num_for_scaling = 3;
    my $is_runq_pressure = ($runq_pressure_p_val > $adaptive_runq_saturation_thresh);
    $debug_info{'IsRunQPressure'} = $is_runq_pressure ? "True" : "False";
    my $is_workload_pressure_calc = 0;
    my $workload_pressure_reason_str_calc = "Workload pressure conditions not met or inputs N/A.";
    my $min_absrunq_for_workload_pressure_check = $smt_used > 0 ? $smt_used : 1.0;
    if (defined $norm_p90_numeric) {
        if ($norm_p90_numeric > $WORKLOAD_PRESSURE_NORM_P90_TRIGGER_THRESHOLD) {
            if (defined $abs_runq_p_numeric && $abs_runq_p_numeric >= $min_absrunq_for_workload_pressure_check) {
                $is_workload_pressure_calc = 1;
                $workload_pressure_reason_str_calc = sprintf("NormRunQ P90 (%.2f) > threshold (%.2f) AND AbsRunQ (%s=%.2f) >= SMT-based min threshold (%.2f)", $norm_p90_numeric, $WORKLOAD_PRESSURE_NORM_P90_TRIGGER_THRESHOLD, $debug_info{AbsRunQKeyUsed}, $abs_runq_p_numeric, $min_absrunq_for_workload_pressure_check);
            } else {
                $workload_pressure_reason_str_calc = sprintf("NormRunQ P90 (%.2f) > threshold (%.2f), BUT AbsRunQ (%s=%.2f) < SMT-based min threshold (%.2f). Workload Pressure NOT flagged.", $norm_p90_numeric, $WORKLOAD_PRESSURE_NORM_P90_TRIGGER_THRESHOLD, $debug_info{AbsRunQKeyUsed}, $abs_runq_p_numeric // $na_str, $min_absrunq_for_workload_pressure_check);
            }
        } else {
            $workload_pressure_reason_str_calc = sprintf("NormRunQ P90 (%.2f) <= threshold (%.2f)", $norm_p90_numeric, $WORKLOAD_PRESSURE_NORM_P90_TRIGGER_THRESHOLD);
        }
    } else {
        $workload_pressure_reason_str_calc = "NormRunQ P90 N/A";
    }
    $debug_info{'IsWorkloadPressure'} = $is_workload_pressure_calc ? "True" : "False";
    $debug_info{'WorkloadPressureReason'} = $workload_pressure_reason_str_calc;


    # --- RunQ Design Philosophy ---
    # These three signals provide complementary views of CPU pressure at different
    # planning horizons, cleanly separating statistical measurement from business
    # policy. All values are ADDITIVE (delta cores), not absolute totals.
    #
    # RunQ_Potential: Pure Statistical Demand (Measurement)
    #   - Formula: excess_threads / SMT
    #   - No caps, no policy, no constraints applied
    #   - Answers: "What is the raw, unconstrained CPU demand that is not being met?"
    #   - Purpose: Baseline workload signal for trend analysis and anomaly detection
    #   - Time Horizon: Instantaneous snapshot of current pressure
    #
    # RunQ_Strategic: Architecturally Feasible Demand (Measurement)
    #   - Formula: min(RunQ_Potential, MaxCPU - BasePhysC)
    #   - Capped ONLY by available architectural headroom before hitting MaxCPU
    #   - No entitlement constraints, no tier policy, no volatility dampening
    #   - Answers: "What is the maximum demand that could be met within the VM's
    #              current architectural limits?"
    #   - Purpose: Identify VMs that are architecturally constrained (hitting MaxCPU)
    #   - Time Horizon: Quarterly planning cycle (3-6 months)
    #   - Diagnostic: If Tactical > Strategic, MaxCPU must be raised to implement
    #                 the recommendation
    #
    # RunQ_Tactical: Actionable Monthly Increment (Recommendation)
    #   - Formula: Raw demand passed through all safety gates and policy adjustments
    #   - Applies: EntCap (tier-aware), volatility factor, pool constraints,
    #              tier scaling, final adaptive safety cap
    #   - Answers: "What is the single, safe, incremental change I should make
    #              this month to begin addressing the pressure?"
    #   - Purpose: Safe, implementable monthly entitlement adjustment
    #   - Time Horizon: Monthly operational cycle (30 days)
    #
    # CRITICAL DESIGN PRINCIPLES:
    #   1. Potential and Strategic are MEASUREMENTS (no business policy)
    #   2. Tactical is a RECOMMENDATION (all business policy applied)
    #   3. Never contaminate Potential/Strategic with tier classifications or
    #      operational constraints (entitlement) - these are statistical signals
    #   4. All three signals are additive deltas for direct comparability
    #   5. The relationship Tactical Ã¢â°Â¤ Strategic Ã¢â°Â¤ Potential should hold under
    #      normal conditions (violations indicate architectural constraints)
    # =============================================================================

    my $apply_additive_logic = ($is_runq_pressure || $is_workload_pressure_calc);
    my $additive_cpu = 0.0;
    my $raw_additive_cpu = 0.0;
    my $max_additive_cap_sliding = 0.0;
    my $capped_raw_additive_val = 0.0;
    my $volatility_confidence_factor = 1.0;
    my $pool_confidence_factor = 1.0;
    my $excess_threads = 0;

    # The additive logic path is only entered if significant pressure has been detected.
    if ($apply_additive_logic && defined $smt_used && $smt_used > 0)
    {
        # Calculate how many threads are waiting above a tolerated ceiling.
        # The tolerance factor allows for a healthy, non-problematic queue before recommending upsizing.

        # --- Tier-Aware Tolerance Factor ---
        # Select the tolerance for calculating "excess threads" based on the VM's tier.
        # Performance-critical tiers have a much lower tolerance for queuing.
        my $tolerance_factor = $RUNQ_ADDITIVE_TOLERANCE_FACTOR; # Default to 1.80

        if ($final_tier eq 'P') {
            # Extreme Performance (e.g., VIO Servers) - virtually no tolerance for queuing.
            $tolerance_factor = 1.10;
        } elsif ($final_tier =~ /^O1$/) {
            # High Performance (e.g., Core OLTP DBs) - very low tolerance.
            $tolerance_factor = 1.25;
        } elsif ($final_tier =~ /^O2$|^G1$|^G2$/) {
            # Balanced Performance (e.g., Web Apps, General DBs) - moderate tolerance.
            $tolerance_factor = 1.50;
        }
        # Efficiency Tiers (G3, G4, B1-B4) will use the default 1.80.

        my $capacity_threshold_for_excess = $tolerance_factor * $effective_lcpus_for_pressure_calc;

        $excess_threads = (defined $abs_runq_p_numeric ? $abs_runq_p_numeric : 0) - $capacity_threshold_for_excess;
        if ($excess_threads > 0)
        {
            # Convert the excess threads into a raw CPU core requirement. This is the unconstrained demand.
            $debug_info{'ExcessThreads'} = sprintf( "\n         1. Observed Run Queue (%s) : %.2f threads\n". "         2. Tolerated Capacity               : %.1f LCPUs * %.2f (Tolerance Factor) = %.2f threads\n". "         => Excess Threads to Service        : %.2f - %.2f = %.2f", $debug_info{AbsRunQKeyUsed}, ($abs_runq_p_numeric // 0.0), $effective_lcpus_for_pressure_calc, $tolerance_factor, $capacity_threshold_for_excess, ($abs_runq_p_numeric // 0.0), $capacity_threshold_for_excess, $excess_threads);
            $raw_additive_cpu = $excess_threads / $smt_used if $smt_used > 0;

            # Determine the business policy scaling factor based on the VM's final tier.
            if (defined $final_tier && $final_tier =~ /(\d)$/) {
                $tier_num_for_scaling = $1;
            } elsif (defined $final_tier && $final_tier eq 'P') {
                $tier_num_for_scaling = 1;
            }

            if (defined $tier_num_for_scaling) {
                my %tier_upsizing_factors = (
                    1 => 1.30, 2 => 1.15, 3 => 1.00, 4 => 0.95
                );
                $tier_scaling_factor = $tier_upsizing_factors{$tier_num_for_scaling} // 1.00;
            }

            # --- Apply the Tier-Aware Entitlement-Based Cap (EntCap) ---
            # This is the first major safety gate. It limits the upsizing recommendation to a proportion
            # of the VM's current size, enforcing incremental change. Performance tiers get more aggressive caps.
            # IMPORTANT: This cap is applied ONLY to the TACTICAL path.
            my ($max_add_abs1, $max_add_perc2, $max_add_perc3, $max_add_perc_else);

            # Determine which tier group the VM falls into
            if ($final_tier =~ /^[PO]1$|^[PO]2$|^G1$|^G2$/) {
                # Performance Tiers (Aggressive Response)
                $max_add_abs1      = 2.0;  # Allow up to +2.0 cores for small critical VMs
                $max_add_perc2     = 1.50; # 150% of Ent
                $max_add_perc3     = 1.00; # 100% of Ent
                $max_add_perc_else = 0.75; # 75% of Ent
            } else {
                # Efficiency Tiers (Conservative, Original Logic)
                $max_add_abs1      = 1.0;  # 1.0 core
                $max_add_perc2     = 1.00; # 100% of Ent
                $max_add_perc3     = 0.75; # 75% of Ent
                $max_add_perc_else = 0.50; # 50% of Ent
            }

            if ($curr_ent_numeric < 1.0)      { $max_additive_cap_sliding = $max_add_abs1; }
            elsif ($curr_ent_numeric < 2.0)   { $max_additive_cap_sliding = $curr_ent_numeric * $max_add_perc2; }
            elsif ($curr_ent_numeric < 4.0)   { $max_additive_cap_sliding = $curr_ent_numeric * $max_add_perc3; }
            else                               { $max_additive_cap_sliding = $curr_ent_numeric * $max_add_perc_else; }
            if ($curr_ent_numeric == 0 && $max_additive_cap_sliding == 0) { $max_additive_cap_sliding = $max_add_abs1; }

            # The EntCap is applied to the raw demand to begin the TACTICAL calculation path.
            $capped_raw_additive_val = min($raw_additive_cpu, $max_additive_cap_sliding);

            # --- Heuristic Checks for Pathological Workloads (HTW, DBW, etc.) ---
            # These checks identify workloads where simply adding CPU is inefficient or incorrect.
            $debug_info{'HotThreadWLDampeningApplied'} = "False"; $debug_info{'HotThreadWLConditionsString'} = "N/A";
            $debug_info{'HotThreadWLDynamicFactor'} = "N/A"; $debug_info{'HotThreadWLDampenedAdditiveFrom'} = "N/A";
            $debug_info{'HotThreadWLDampenedAdditiveTo'} = "N/A";
            if ($capped_raw_additive_val > $FLOAT_EPSILON)
            {
                my @htw_conditions_met_details; my $htw_condition_count = 0;
                if ($is_workload_pressure_calc) { push @htw_conditions_met_details, "HighNormP90"; $htw_condition_count++; }
                my $cond2_underutilized = 0;
                if (defined $base_physc && $base_physc > $FLOAT_EPSILON) {
                    my $underutilized_vs_ent = (defined $curr_ent_numeric && $curr_ent_numeric > $FLOAT_EPSILON && $base_physc < ($curr_ent_numeric * $HOT_THREAD_WL_ENT_FACTOR));
                    my $underutilized_vs_maxcpu = (defined $max_cpu_for_lpar_numeric && $max_cpu_for_lpar_numeric > $FLOAT_EPSILON && $base_physc < ($max_cpu_for_lpar_numeric * $HOT_THREAD_WL_MAXCPU_FACTOR));
                    if ($underutilized_vs_ent || $underutilized_vs_maxcpu) {
                        $cond2_underutilized = 1;
                        my $detail_ent_str = $underutilized_vs_ent ? sprintf("BaseP(%.2f)<Ent(%.2f)*%.1f", $base_physc, $curr_ent_numeric // 0, $HOT_THREAD_WL_ENT_FACTOR) : "";
                        my $detail_max_str = $underutilized_vs_maxcpu ? sprintf("BaseP(%.2f)<MaxP(%.2f)*%.1f", $base_physc, $max_cpu_for_lpar_numeric // 0, $HOT_THREAD_WL_MAXCPU_FACTOR) : "";
                        push @htw_conditions_met_details, "UnderutilizedCap(" . join(" or ", grep { $_ ne "" } $detail_ent_str, $detail_max_str) . ")";
                        $htw_condition_count++;
                    }
                }
                if (defined $norm_p50_numeric && $norm_p50_numeric > $HOT_THREAD_WL_HIGH_NORM_P50_THRESHOLD) { push @htw_conditions_met_details, sprintf("HighNormP50(%.2f>%.1f)", $norm_p50_numeric, $HOT_THREAD_WL_HIGH_NORM_P50_THRESHOLD); $htw_condition_count++; }
                if (!$is_runq_pressure && defined $debug_info{'RunQPressure_P90_Val'} && $debug_info{'RunQPressure_P90_Val'} ne $na_str) { push @htw_conditions_met_details, sprintf("NoLPARRunQSat(AbsPVal:%.2f<%.1f)", ($debug_info{'RunQPressure_P90_Val'} + 0), $RUNQ_PRESSURE_P90_SATURATION_THRESHOLD); $htw_condition_count++; }
                if (defined $normrunq_iqrc_val && abs($normrunq_iqrc_val) > $HOT_THREAD_WL_IQRC_THRESHOLD) { push @htw_conditions_met_details, sprintf("HighIQRC(%.2f>%.1f)", $normrunq_iqrc_val, $HOT_THREAD_WL_IQRC_THRESHOLD); $htw_condition_count++; }
                my $hot_thread_wl_conditions_met_str = @htw_conditions_met_details ? join("; ", @htw_conditions_met_details) : "No specific conditions met";
                if ($htw_condition_count >= $HOT_THREAD_WL_DETECTION_MIN_CONDITIONS_MET)
                {
                    my $util_ratio = (defined $max_cpu_for_lpar_numeric && $max_cpu_for_lpar_numeric > $FLOAT_EPSILON && defined $base_physc) ? ($base_physc / $max_cpu_for_lpar_numeric) : 1.0;
                    my $util_damp_multiplier = max(0.1, min(1.0, $util_ratio));
                    my $iqrc_damp_multiplier = (defined $normrunq_iqrc_val) ? min(1.0, 1.0 / (1.0 + abs($normrunq_iqrc_val))) : 1.0;
                    my $base_physc_severity_multiplier = (defined $base_physc && $base_physc > $FLOAT_EPSILON) ? min(1.0, $base_physc / 1.0) : 0.1;
                    my $calculated_dynamic_damp_factor = $HOT_THREAD_WL_BASE_DAMPENING_FACTOR * $util_damp_multiplier * $iqrc_damp_multiplier * $base_physc_severity_multiplier;
                    my $final_dynamic_dampening_factor_calc = max($HOT_THREAD_WL_MIN_DYNAMIC_DAMPENING, min($HOT_THREAD_WL_MAX_DYNAMIC_DAMPENING, $calculated_dynamic_damp_factor));
                    my $original_additive_val = $capped_raw_additive_val;
                    $capped_raw_additive_val *= $final_dynamic_dampening_factor_calc;
                    $debug_info{'HotThreadWLDampeningApplied'} = "True";
                    $debug_info{'HotThreadWLConditionsString'} = $hot_thread_wl_conditions_met_str;
                    $debug_info{'HotThreadWLDynamicFactor'} = sprintf("%.4f (Base:%.2f UtilM:%.2f IqrcM:%.2f SevM:%.2f -> RawCalc:%.4f)", $final_dynamic_dampening_factor_calc, $HOT_THREAD_WL_BASE_DAMPENING_FACTOR, $util_damp_multiplier, $iqrc_damp_multiplier, $base_physc_severity_multiplier, $calculated_dynamic_damp_factor);
                    $debug_info{'HotThreadWLDampenedAdditiveFrom'} = sprintf("%.4f", $original_additive_val);
                    $debug_info{'HotThreadWLDampenedAdditiveTo'}   = sprintf("%.4f", $capped_raw_additive_val);
                    $debug_info{'CappedRawAdditive'} = sprintf("%.4f", $capped_raw_additive_val);
                } else {
                    $debug_info{'HotThreadWLDampeningApplied'} = "False";
                    $debug_info{'HotThreadWLConditionsString'} = sprintf("Conditions not met for HTW dampening (%d/5 met: %s).", $htw_condition_count, $hot_thread_wl_conditions_met_str);
                }
            }
            if ($is_runq_pressure) {
                $volatility_confidence_factor = $RUNQ_PRESSURE_SATURATION_CONFIDENCE_FACTOR;
                $debug_info{'VoltFactorReason'} = sprintf("RunQPressure Saturation (Factor set to %.2f)", $RUNQ_PRESSURE_SATURATION_CONFIDENCE_FACTOR);
            } elsif ($is_workload_pressure_calc && defined $norm_p50_numeric && defined $norm_p90_numeric && $norm_p90_numeric > 0.01) {
                my $volatility_ratio_for_factor = ($norm_p50_numeric > 0.01) ? ($norm_p90_numeric / $norm_p50_numeric) : 999;
                if ($volatility_ratio_for_factor < $VOLATILITY_SPIKY_THRESHOLD) { $volatility_confidence_factor = $VOLATILITY_SPIKY_FACTOR; }
                elsif ($volatility_ratio_for_factor < $VOLATILITY_MODERATE_THRESHOLD){ $volatility_confidence_factor = $VOLATILITY_MODERATE_FACTOR; }
                else { $volatility_confidence_factor = 1.0; }
                $debug_info{'VoltFactorReason'} = sprintf("Calculated (NormRQ P90/P50 ratio %.2f for WorkloadPressure -> Factor %.2f)", $volatility_ratio_for_factor, $volatility_confidence_factor);
            } else {
                $debug_info{'VoltFactorReason'} = "Additive logic applied, but conditions for specific Volatility Factor adjustment not met (e.g., WorkloadPressure False or P50/P90 N/A for ratio). Using default factor.";
            }
            $additive_cpu = $capped_raw_additive_val * $volatility_confidence_factor;
            if ($is_in_non_default_pool && $additive_cpu > 0) {
                $pool_confidence_factor = $POOL_CONSTRAINT_CONFIDENCE_FACTOR;
                $additive_cpu *= $pool_confidence_factor;
            }

            # --- FINAL SIGNAL ASSIGNMENT ---
            my $sanity_bound = (defined $max_cpu_for_lpar_numeric && $max_cpu_for_lpar_numeric > 0) ? ($max_cpu_for_lpar_numeric * 2.0) : 10.0;

            # 1. Potential is the pure, unscaled, raw demand.
            $debug_info{'RunQ_Potential'} = $raw_additive_cpu;

            # 2. RunQ_Strategic: Architecturally feasible demand, capped only by MaxCPU.
            # formula: min(RunQ_Potential, MaxCPU - BasePhysC)
            my $strategic_val = $raw_additive_cpu;
            if ($max_cpu_for_lpar_numeric > 0) {
                my $available_headroom = $max_cpu_for_lpar_numeric - $base_physc;
                $strategic_val = min($raw_additive_cpu, $available_headroom);
            }
            $debug_info{'RunQ_Strategic'} = $strategic_val;

            # 3. Check for pathological state (Signal Inversion)
            if ($raw_additive_cpu > $sanity_bound) {
                # Handle pathological state (Signal Inversion)
                $debug_info{'RunQ_Potential'} = "ANOMALY (XRQ)";
                $debug_info{'RunQ_Strategic'} = $max_cpu_for_lpar_numeric;
                $debug_info{'ReasonForSignalInversion'} = sprintf(
                    "Raw Additive CPU (%.2f) exceeded sanity bound (%.2f), indicating a probable system backlog or malfunction.",
                    $raw_additive_cpu, $sanity_bound
                );
            }

      } else {
            $debug_info{'ExcessThreads'} = sprintf("0.0000 (No excess above tolerated capacity of %.2f from %s; AbsRunQ %s was %.2f)", $capacity_threshold_for_excess, ($pressure_basis_rationale =~ /Burst-Tolerant/ ? "Entitlement+Burst" : "MaxCPU"), $debug_info{AbsRunQKeyUsed}, ($abs_runq_p_numeric // $na_str));
            $raw_additive_cpu = 0.0;
            $capped_raw_additive_val = 0.0;
            $additive_cpu = 0.0;
            $debug_info{'VoltFactorReason'} = "No excess threads, so no additive CPU calculated.";
            $debug_info{'RunQ_Strategic'} = "0.0000";
            $debug_info{'RunQ_Potential'} = "0.0000";
        }
    } else {
        $debug_info{'ExcessThreads'} = "N/A (Additive logic not applied as no significant pressure detected or missing inputs)";
        $debug_info{'VoltFactorReason'} = "Additive logic not applied.";
    }
    $debug_info{'RawAdditive'}       = sprintf("%.4f", $raw_additive_cpu);
    $debug_info{'MaxAdditiveCap'}    = sprintf("%.4f", $max_additive_cap_sliding);
    $debug_info{'CappedRawAdditive'} = sprintf("%.4f", $capped_raw_additive_val);
    $debug_info{'VoltFactor'}        = sprintf("%.2f", $volatility_confidence_factor);
    $debug_info{'PoolFactor'}        = sprintf("%.2f", $pool_confidence_factor);
    my $dbw_dampening_applied = "False";
    my $cond1_low_util = (defined $base_physc && $base_physc < ($curr_ent_numeric * $DBW_LOW_UTIL_FACTOR));
    my $dsr = 0;
    if (defined $base_physc && $base_physc > 0.01 && defined $abs_runq_p_numeric) { $dsr = $abs_runq_p_numeric / ($base_physc * $smt_used); }
    my $cond2_high_dsr = ($dsr > $DBW_DSR_THRESHOLD);
    my $cond3_high_median_pressure = (defined $norm_p50_numeric && $norm_p50_numeric > $DBW_MEDIAN_PRESSURE_THRESHOLD);
    if ($cond1_low_util && $cond2_high_dsr && $cond3_high_median_pressure) {
        my $original_additive = $additive_cpu;
        $debug_info{'RunQ_Potential'} = "ANOMALY (DBW)";
        $additive_cpu *= 0.25;
        $dbw_dampening_applied = sprintf("True (75%% dampening applied).\n                                               Additive CPU reduced from %.4f to %.4f.\n                                               Conditions met: Low Util (BaseP %.2f < %.2f Ent * %.2f), High DSR (%.1f > %.1f), High Median Pressure (NormP50 %.2f > %.1f)", $original_additive, $additive_cpu, $base_physc, $curr_ent_numeric, $DBW_LOW_UTIL_FACTOR, $dsr, $DBW_DSR_THRESHOLD, $norm_p50_numeric, $DBW_MEDIAN_PRESSURE_THRESHOLD);
    }
    $debug_info{'DBWDampeningApplied'} = $dbw_dampening_applied;

    # This is the final and most important safety cap. It prevents recommendations from violating
    # the VM's hard limits (MaxCPU) while allowing for "Intelligent Headroom" based on pressure severity.
    # --- Adaptive Safety Hard-Cap with Intelligent Headroom ---
    my $original_additive_before_safety_cap = $additive_cpu;
    my $additive_safety_cap_applied_reason = "Not applied";

    # Step 1: Calculate Pressure Severity to determine Virtual Headroom
    my $pressure_ratio = ($effective_lcpus_for_pressure_calc > 0) ? (($excess_threads // 0) / $effective_lcpus_for_pressure_calc) : 0;
    my $virtual_headroom_factor = 0;
    if ($apply_additive_logic) { # Only apply headroom if there's pressure
        if ($pressure_ratio >= 1.0) {
            $virtual_headroom_factor = 0.50; # 50% for Severe Pressure
        } else {
            $virtual_headroom_factor = 0.25; # 25% for Significant Pressure
        }
    }
    my $virtual_max_cpu = $max_cpu_for_lpar_numeric * (1 + $virtual_headroom_factor);

    # Step 2: Calculate the Primary Dynamic Cap using Virtual Headroom
    my $cap_from_ent = $curr_ent_numeric * 0.40;
    my $headroom = ($virtual_max_cpu > $curr_ent_numeric) ? ($virtual_max_cpu - $curr_ent_numeric) : 0;
    my $cap_from_headroom = $headroom * 0.50;
    my $primary_cap = ($headroom > 0) ? min($cap_from_ent, $cap_from_headroom) : $cap_from_ent;

    # The Pressure Gate moderates the cap for borderline cases.
    # If pressure is only moderate, we apply a small dampening to the cap.
    # For significant or severe pressure, we apply the full cap without dampening.
    my $pressure_gate_modifier = 1.0;
    if ($pressure_ratio < 0.5) {
        $pressure_gate_modifier = 0.75;
    }

    my $gated_cap = $primary_cap * $pressure_gate_modifier;
    my $relative_floor = 0.02 * max($curr_ent_numeric, 1.0);
    my $final_safety_cap_value = max($relative_floor, $gated_cap);
    $final_safety_cap_value = min($final_safety_cap_value, 2.0);
    my $cap_rationale = sprintf("min(EntCap=%.2f, HeadCap=%.2f) * PresGate=%.2f -> floor/ceil -> %.4f", $cap_from_ent, $cap_from_headroom, $pressure_gate_modifier, $final_safety_cap_value);
    if ($additive_cpu > $final_safety_cap_value) {
        $additive_cpu = $final_safety_cap_value;
        $additive_safety_cap_applied_reason = sprintf("True: Additive CPU hard-capped to %.4f (%s)", $additive_cpu, $cap_rationale);
    } else {
        $additive_safety_cap_applied_reason = sprintf("False (Additive %.4f within adaptive hard cap of %.4f from %s)", $original_additive_before_safety_cap, $final_safety_cap_value, $cap_rationale);
    }
    $debug_info{'AdditiveSafetyCapApplied'} = $additive_safety_cap_applied_reason;

    # --- Final Tier-Aware Scaling for the Tactical Recommendation ---
    # This is the last step, applying the business policy (tier importance) to the final, safety-vetted additive value.
    if ($tier_scaling_factor != 1.00 && $additive_cpu > 0) {
        my $pre_scaled_additive = $additive_cpu;
        $additive_cpu *= $tier_scaling_factor;

        # Reconstruct the rationale for logging
        my $increase_pct = ($tier_scaling_factor - 1.0) * 100;
        my $rationale_text = sprintf(
            "Tier %d: Enhanced upsizing (+%.0f%%) for lower latency tolerance. Base: %.3f Ã¢â â Scaled: %.3f cores",
            $tier_num_for_scaling, $increase_pct, $pre_scaled_additive, $additive_cpu
        );

        $debug_info{'TierScalingFactor'}        = sprintf("%.2f", $tier_scaling_factor);
        $debug_info{'TierNumber'}               = $tier_num_for_scaling;
        $debug_info{'TierScalingRationale'}     = $rationale_text;
        $debug_info{'AdditiveCPU_PreTierScaling'}  = sprintf("%.4f", $pre_scaled_additive);
        $debug_info{'AdditiveCPU_PostTierScaling'} = sprintf("%.4f", $additive_cpu);
    }

    my $final_tactical_modifier = 0;
    if ($additive_cpu > $FLOAT_EPSILON) {
        $final_tactical_modifier = $additive_cpu;
    } elsif ($efficiency_factor_numeric < (1.0 - $FLOAT_EPSILON)) {
        my $actual_reduction = (defined $debug_info{'EffActualReductionCores'} && looks_like_number($debug_info{'EffActualReductionCores'})) ? ($debug_info{'EffActualReductionCores'} + 0) : 0;
        $final_tactical_modifier = -1 * $actual_reduction;
    }
    $debug_info{'RunQ_Tactical'} = $final_tactical_modifier;
    $debug_info{'FinalAdditive'} = sprintf("%.4f", $final_tactical_modifier);

    # =========================
    # Final Synthesis + MaxCPU
    # =========================
    my $calculated_demand = $base_adjusted_physc + $additive_cpu;
    my $runq_modified_rec = $calculated_demand;
    $debug_info{'PreMaxCpuCapRec'} = sprintf("%.4f", $runq_modified_rec);
    my $forecast_multiplier_val = 1.25;
    if ($curr_ent_numeric < 0.5)    { $forecast_multiplier_val = 2.5; }
    elsif ($curr_ent_numeric < 1.0) { $forecast_multiplier_val = 2.0; }
    elsif ($curr_ent_numeric < 2.0) { $forecast_multiplier_val = 1.75; }
    elsif ($curr_ent_numeric < 4.0) { $forecast_multiplier_val = 1.5; }
    $debug_info{'ForecastMultiplier'} = $forecast_multiplier_val;
    my $effective_max_cpu_cap_val = ($max_cpu_for_lpar_numeric > 0) ? ($max_cpu_for_lpar_numeric * $forecast_multiplier_val) : undef;
    $debug_info{'EffectiveMaxCPUCap'} = defined($effective_max_cpu_cap_val) ? sprintf("%.4f", $effective_max_cpu_cap_val) : $na_str;
    if (defined $effective_max_cpu_cap_val && $calculated_demand > $effective_max_cpu_cap_val) {
        $runq_modified_rec = $effective_max_cpu_cap_val;
        $debug_info{'CappedByMaxCPU'} = "True";
    } else {
        $debug_info{'CappedByMaxCPU'} = (defined $effective_max_cpu_cap_val) ? "False" : "N/A (No LPAR MaxCPU for cap check or MaxCPU not exceeded)";
    }
    if ($runq_modified_rec < 0) { $runq_modified_rec = 0; }
    $debug_info{'FinalAdjustedPhysC'} = sprintf("%.4f", $runq_modified_rec);

    return ($runq_modified_rec, \%debug_info);
}

# --- generate_sizing_hint (Unified Global Pressure Detection with Logging Rationale) ---
# Generates a sizing tier hint, pattern, and overall pressure indication for a VM.
# Also returns a detailed rationale string for its pressure assessment, AND
# specific boolean flags for P-99W1's RunQ pressure conditions.
sub generate_sizing_hint
{
    # This function is now refactored to read all its inputs from the assimilation map.
    my ($vm_map_ref, $log_fh, $adaptive_saturation_thresh) = @_;

    # --- Unpack all required values from the assimilation map ---
    my $vm_name                  = $vm_map_ref->{Configuration}{vm_name}; # We will add vm_name to the map
    my $config_ref               = $vm_map_ref->{Configuration};
    my $results_ref              = $vm_map_ref->{CoreResults}{ProfileValues};
    my $runq_metrics_ref         = $vm_map_ref->{RunQMetrics};
    my $max_cpu_for_vm_numeric   = $config_ref->{max_cpu} // 0;
    my $smt_used_for_vm_numeric  = $config_ref->{smt} // $default_smt_arg;
    my $LOG_FH                   = $log_fh;

    my $na_str_hint = "N/A";
    my @global_pressure_rationale_lines;

    # ... (initial part of rationale logging: Inputs like MaxCPU, SMT etc. - as in previous version) ...
    push @global_pressure_rationale_lines, sprintf("  Input LPAR MaxCPU            : %.2f cores", $max_cpu_for_vm_numeric);
    push @global_pressure_rationale_lines, sprintf("  Input SMT for VM             : %d", $smt_used_for_vm_numeric);
    # Determine if small entitlement handler would be active
    my $vm_entitlement = $config_ref->{'entitlement'} // 0;
    my $is_small_ent_handler = ($vm_entitlement > 0 && $vm_entitlement < 1.0 &&
                                 $max_cpu_for_vm_numeric > $vm_entitlement) ? "Yes" : "No";
    my $burst_used = ($vm_entitlement < 1.0 && $is_small_ent_handler eq "Yes") ?
                     calculate_graduated_burst($vm_entitlement) : 0.25;

    push @global_pressure_rationale_lines, sprintf("  Small Entitlement Handler    : %s", $is_small_ent_handler);
    push @global_pressure_rationale_lines, sprintf("  Burst Allowance Factor       : %.0f%% (Ent=%.2f)",
                                                   $burst_used * 100, $vm_entitlement);

    # --- VIO Server Check ---
    my $is_vio_server = 0;
    if (defined $config_ref->{systemtype} && $config_ref->{systemtype} =~ /VIO Server/i) {
       $is_vio_server = 1;
    }

    # --- Profile Value Parsing (Pattern/Peakiness & P-99W1 PhysC) ---
    my $o3_val_str = $results_ref->{'O3-95W15'} // "0";
    my $o3_val_num = ($o3_val_str ne $na_str_hint && $o3_val_str =~ /^-?[0-9.]+\z/) ? ($o3_val_str + 0) : 0;
    my $b3_val_str = $results_ref->{'B3-95W15'} // "0";
    my $b3_val_num = ($b3_val_str ne $na_str_hint && $b3_val_str =~ /^-?[0-9.]+\z/) ? ($b3_val_str + 0) : 0;
    my $g3_val_str = $results_ref->{'G3-95W15'} // "0";
    my $g3_val_num = ($g3_val_str ne $na_str_hint && $g3_val_str =~ /^-?[0-9.]+\z/) ? ($g3_val_str + 0) : 0;
    my $p99w1_physc_val_str = looks_like_number($results_ref->{$MANDATORY_PEAK_PROFILE_FOR_HINT}) ? sprintf("%.3f", $results_ref->{$MANDATORY_PEAK_PROFILE_FOR_HINT}) : $na_str_hint;
    my $p99w1_physc_val_num = ($p99w1_physc_val_str ne $na_str_hint && $p99w1_physc_val_str =~ /^-?[0-9.]+\z/)
    ? ($p99w1_physc_val_str + 0)
    : 0;
    push @global_pressure_rationale_lines, sprintf("  Input %s PhysC Value     : %s (from nfit output for %s)", $MANDATORY_PEAK_PROFILE_FOR_HINT, $p99w1_physc_val_str, $MANDATORY_PEAK_PROFILE_FOR_HINT);

    # (Pattern and Peakiness logic - unchanged)
    my $suggested_pattern = "G";
    if ($b3_val_num > 0.01 && $o3_val_num > ($b3_val_num * $PATTERN_RATIO_THRESHOLD)) { $suggested_pattern = "O"; }
    elsif ($o3_val_num > 0.01 && $b3_val_num > ($o3_val_num * $PATTERN_RATIO_THRESHOLD)) { $suggested_pattern = "B"; }
    my $peakiness_ratio = ($g3_val_num > 0.001) ? ($p99w1_physc_val_num / $g3_val_num) : 0;
    my $shape_descriptor = "Steady";
    if ($peakiness_ratio >= $HIGH_PEAK_RATIO_THRESHOLD) { $shape_descriptor = "Very Peaky"; }
    elsif ($peakiness_ratio >= $LOW_PEAK_RATIO_THRESHOLD) { $shape_descriptor = "Moderately Peaky"; }


    # --- Unified Pressure Detection ---
    my $pressure_detected_maxcpu_limit = 0;
    # Specific P-99W1 RunQ pressure flags to be returned
    my $p99w1_has_absolute_runq_pressure = 0;
    my $p99w1_has_normalized_runq_pressure = 0;
    my @pressure_points;

    # Fetch P-99W1's specific RunQ metrics
    my $p99w1_abs_runq_p90_val_str = looks_like_number($runq_metrics_ref->{'AbsRunQ_P90'}) ? sprintf("%.3f", $runq_metrics_ref->{'AbsRunQ_P90'}) : $na_str_hint;
    my $p99w1_norm_runq_p90_val_str = looks_like_number($runq_metrics_ref->{'NormRunQ_P90'}) ? sprintf("%.3f", $runq_metrics_ref->{'NormRunQ_P90'}) : $na_str_hint;

    push @global_pressure_rationale_lines, sprintf("  Input %s AbsRunQ P90     : %s threads (from nfit output for %s)", $MANDATORY_PEAK_PROFILE_FOR_HINT, $p99w1_abs_runq_p90_val_str, $MANDATORY_PEAK_PROFILE_FOR_HINT);
    push @global_pressure_rationale_lines, sprintf("  Input %s NormRunQ P90    : %s (from nfit output for %s)", $MANDATORY_PEAK_PROFILE_FOR_HINT, $p99w1_norm_runq_p90_val_str, $MANDATORY_PEAK_PROFILE_FOR_HINT);
    push @global_pressure_rationale_lines, "";


    # 1. MaxCPU Limit Pressure
    # ... (logic as before, sets $pressure_detected_maxcpu_limit, logs to @global_pressure_rationale_lines) ...
    # ... (ensure push @pressure_points, "MaxCPU"; happens if $pressure_detected_maxcpu_limit = 1;) ...
    push @global_pressure_rationale_lines, sprintf("  1. MaxCPU Limit Pressure Check (%s PhysC vs LPAR MaxCPU):", $MANDATORY_PEAK_PROFILE_FOR_HINT);
    # ... (detailed logging lines for MaxCPU check)
    my $maxcpu_limit_calc_threshold = ($max_cpu_for_vm_numeric > 0) ? ($max_cpu_for_vm_numeric * $LIMIT_THRESHOLD_PERC) : 0;
    my $maxcpu_condition_met_str = "FALSE";
    if ($max_cpu_for_vm_numeric > 0 && $p99w1_physc_val_num >= $maxcpu_limit_calc_threshold)
    {
        $pressure_detected_maxcpu_limit = 1;
        $maxcpu_condition_met_str = "TRUE";
        push @pressure_points, "MaxCPU";
    }
    push @global_pressure_rationale_lines, sprintf("     - %s PhysC Value      : %.2f", $MANDATORY_PEAK_PROFILE_FOR_HINT, $p99w1_physc_val_num);
    push @global_pressure_rationale_lines, sprintf("     - LPAR MaxCPU             : %.2f", $max_cpu_for_vm_numeric);
    push @global_pressure_rationale_lines, sprintf("     - Threshold (>= %.0f%%)      : %.2f cores", $LIMIT_THRESHOLD_PERC * 100, $maxcpu_limit_calc_threshold);
    push @global_pressure_rationale_lines, sprintf("     - Condition Met           : (%.2f >= %.2f) -> %s", $p99w1_physc_val_num, $maxcpu_limit_calc_threshold, $maxcpu_condition_met_str);
    push @global_pressure_rationale_lines, sprintf("     - MaxCPU Pressure Flag    : %s", $maxcpu_condition_met_str);
    push @global_pressure_rationale_lines, "";


    # 2. Absolute RunQ Pressure (using P-99W1's AbsRunQ_P90)
    # ... (logic as before, sets $p99w1_has_absolute_runq_pressure, logs to @global_pressure_rationale_lines) ...
    # ... (ensure push @pressure_points, ... happens if $p99w1_has_absolute_runq_pressure = 1;) ...
    my $p99w1_abs_runq_p90_num = ($p99w1_abs_runq_p90_val_str ne $na_str_hint && $p99w1_abs_runq_p90_val_str =~ /^-?[0-9.]+$/) ? ($p99w1_abs_runq_p90_val_str + 0) : undef;
    my $calculated_abs_runq_pressure_ratio = 0;
    my $lpar_max_lcpu_capacity = ($max_cpu_for_vm_numeric > 0 && $smt_used_for_vm_numeric > 0) ? ($max_cpu_for_vm_numeric * $smt_used_for_vm_numeric) : 0;
    push @global_pressure_rationale_lines, sprintf("  2. Absolute RunQ Pressure Check (%s AbsRunQ P90 vs LPAR Capacity):", $MANDATORY_PEAK_PROFILE_FOR_HINT);
    # ... (detailed logging lines for AbsRunQ check)
    if (defined $p99w1_abs_runq_p90_num && $lpar_max_lcpu_capacity > 0)
    {
        $calculated_abs_runq_pressure_ratio = $p99w1_abs_runq_p90_num / $lpar_max_lcpu_capacity;
    }
    my $absrunq_cond_met_str = "FALSE";
    if ($calculated_abs_runq_pressure_ratio > $adaptive_saturation_thresh)
    {
        $p99w1_has_absolute_runq_pressure = 1; # Set the specific flag
        $absrunq_cond_met_str = "TRUE";
        push @pressure_points, sprintf("RunQAbs_%s(P90=%.2f)", $MANDATORY_PEAK_PROFILE_FOR_HINT, $calculated_abs_runq_pressure_ratio);
    }
    push @global_pressure_rationale_lines, sprintf("     - %s AbsRunQ P90      : %s threads", $MANDATORY_PEAK_PROFILE_FOR_HINT, $p99w1_abs_runq_p90_val_str);
    push @global_pressure_rationale_lines, sprintf("     - LPAR Max LCPU Capacity  : (%.2f MaxCPU * %d SMT) = %.2f threads", $max_cpu_for_vm_numeric, $smt_used_for_vm_numeric, $lpar_max_lcpu_capacity);
    push @global_pressure_rationale_lines, sprintf("     - Calculated Ratio        : %.4f", $calculated_abs_runq_pressure_ratio);
    push @global_pressure_rationale_lines, sprintf("     - Threshold               : > %.2f", $adaptive_saturation_thresh);
    push @global_pressure_rationale_lines, sprintf("     - Condition Met           : (%.4f > %.2f) -> %s", $calculated_abs_runq_pressure_ratio, $adaptive_saturation_thresh, $absrunq_cond_met_str);
    push @global_pressure_rationale_lines, sprintf("     - %s Specific Absolute RunQ Pressure Flag: %s", $MANDATORY_PEAK_PROFILE_FOR_HINT, $absrunq_cond_met_str);
    push @global_pressure_rationale_lines, "";


    # 3. Normalised Workload Pressure (using P-99W1's NormRunQ_P90)
    # ... (logic as before, sets $p99w1_has_normalized_runq_pressure, logs to @global_pressure_rationale_lines) ...
    # ... (ensure push @pressure_points, ... happens if $p99w1_has_normalized_runq_pressure = 1;) ...
    my $p99w1_norm_runq_p90_num = ($p99w1_norm_runq_p90_val_str ne $na_str_hint && $p99w1_norm_runq_p90_val_str =~ /^-?[0-9.]+$/) ? ($p99w1_norm_runq_p90_val_str + 0) : undef;
    my $min_abs_runq_for_norm_check = $smt_used_for_vm_numeric > 0 ? $smt_used_for_vm_numeric : 1.0;
    push @global_pressure_rationale_lines, sprintf("  3. Normalised Workload Pressure Check (%s NormRunQ P90):", $MANDATORY_PEAK_PROFILE_FOR_HINT);
    # ... (detailed logging lines for NormRunQ check)
    my $normrunq_cond1_met_str = (defined $p99w1_norm_runq_p90_num && $p99w1_norm_runq_p90_num > $WORKLOAD_PRESSURE_NORM_P90_TRIGGER_THRESHOLD) ? "TRUE" : "FALSE";
    my $normrunq_cond2_met_str = (defined $p99w1_abs_runq_p90_num && $p99w1_abs_runq_p90_num >= $min_abs_runq_for_norm_check) ? "TRUE" : "FALSE";
    my $normrunq_overall_cond_met_str = "FALSE";
    if ($normrunq_cond1_met_str eq "TRUE" && $normrunq_cond2_met_str eq "TRUE")
    {
        $p99w1_has_normalized_runq_pressure = 1; # Set the specific flag
        $normrunq_overall_cond_met_str = "TRUE";
        push @pressure_points, sprintf("RunQNorm_%s(P90=%.2f)", $MANDATORY_PEAK_PROFILE_FOR_HINT, defined $p99w1_norm_runq_p90_num ? $p99w1_norm_runq_p90_num : 0);
    }
    push @global_pressure_rationale_lines, sprintf("     - %s NormRunQ P90     : %s", $MANDATORY_PEAK_PROFILE_FOR_HINT, $p99w1_norm_runq_p90_val_str);
    push @global_pressure_rationale_lines, sprintf("     - Threshold               : > %.2f", $WORKLOAD_PRESSURE_NORM_P90_TRIGGER_THRESHOLD);
    push @global_pressure_rationale_lines, sprintf("     - Condition Met (Norm)    : (%s > %.2f) -> %s", $p99w1_norm_runq_p90_val_str, $WORKLOAD_PRESSURE_NORM_P90_TRIGGER_THRESHOLD, $normrunq_cond1_met_str);
    push @global_pressure_rationale_lines, sprintf("     - %s AbsRunQ P90      : %s (for magnitude check)", $MANDATORY_PEAK_PROFILE_FOR_HINT, $p99w1_abs_runq_p90_val_str);
    push @global_pressure_rationale_lines, sprintf("     - Min AbsRunQ Threshold   : %.2f (typically SMT)", $min_abs_runq_for_norm_check);
    push @global_pressure_rationale_lines, sprintf("     - Condition Met (Mag)     : (%s >= %.2f) -> %s", $p99w1_abs_runq_p90_val_str, $min_abs_runq_for_norm_check, $normrunq_cond2_met_str);
    push @global_pressure_rationale_lines, sprintf("     - %s Specific Normalised Workload Pressure Flag: %s", $MANDATORY_PEAK_PROFILE_FOR_HINT, $normrunq_overall_cond_met_str);
    push @global_pressure_rationale_lines, "";


    # --- Combine pressure flags & Pool Context ---
    my $overall_pressure_detected_for_csv = $pressure_detected_maxcpu_limit ||
    $p99w1_has_absolute_runq_pressure || # Use the specific P-99W1 flags here for overall CSV flag
    $p99w1_has_normalized_runq_pressure;
    # Pool context logic, using @pressure_points
    push @global_pressure_rationale_lines, "  4. Pool Context:";

    # Get Pool ID from nfit's dynamic data, and Pool Name from the optional config file
    my $pool_id_from_nmon = $config_ref->{pool_id} // 0;
    my $pool_name_from_config = $vm_config_data{$vm_name}{pool_name} // undef; # Still need the original config file for this

    # A non-default pool is one where the ID is not 0.
    my $is_non_default_pool = (looks_like_number($pool_id_from_nmon) && $pool_id_from_nmon != 0);

    # Pool ID is now directly from the JSON output of nfit
    push @global_pressure_rationale_lines, sprintf("     - Pool ID                 : %s", $pool_id_from_nmon);
    push @global_pressure_rationale_lines, sprintf("     - Pool Name (from config) : %s", $pool_name_from_config // "N/A");
    push @global_pressure_rationale_lines, sprintf("     - Is Non-Default Pool     : %s", $is_non_default_pool ? "TRUE" : "FALSE");

    if ($is_non_default_pool && $overall_pressure_detected_for_csv) {
        # If we have a pool name, use it. Otherwise, fall back to the ID.
        my $pool_identifier = defined($pool_name_from_config) && $pool_name_from_config ne '' ? $pool_name_from_config : "ID:$pool_id_from_nmon";
        push @pressure_points, "Pool($pool_identifier)";
    }
    push @global_pressure_rationale_lines, "";

    my $overall_pressure_detected_for_csv_str = $overall_pressure_detected_for_csv ? "TRUE" : "FALSE";
    my $pressure_detail_str = @pressure_points ? join(", ", @pressure_points) : "None";
    push @global_pressure_rationale_lines, sprintf("  5. Overall Global Hint Pressure Flag (for CSV): %s", $overall_pressure_detected_for_csv_str);
    push @global_pressure_rationale_lines, sprintf("  6. Final PressureDetail string for CSV     : \"%s\"", $pressure_detail_str);

    my $global_pressure_rationale_text = "Section G: Global Sizing Hint Pressure Assessment (source: generate_sizing_hint)\n" .
    join("\n", @global_pressure_rationale_lines);


    # --- Tiering logic (remains unchanged) ---
    my $initial_tier_range_str = "3/4";
    # --- Tiering logic (with added safety checks for numeric values) ---
    my $p99w1_val_for_tier = $results_ref->{'P-99W1'};
    my $g3_val_for_tier    = $results_ref->{'G3-95W15'};

    if (looks_like_number($p99w1_val_for_tier) && looks_like_number($g3_val_for_tier) && $g3_val_for_tier > 0) {
        if ($p99w1_val_for_tier > $g3_val_for_tier * $HIGH_PEAK_RATIO_THRESHOLD) {
            $shape_descriptor = "Very Peaky";
        }
        elsif ($p99w1_val_for_tier > $g3_val_for_tier * $LOW_PEAK_RATIO_THRESHOLD) {
            $shape_descriptor = "Moderately Peaky";
        }
    }

    if ($shape_descriptor eq "Very Peaky") { $initial_tier_range_str = "1/2"; }
    elsif ($shape_descriptor eq "Moderately Peaky") { $initial_tier_range_str = "2/3"; }
    my $adjusted_tier_str = $initial_tier_range_str;
    if ($overall_pressure_detected_for_csv)
    {
        if ($initial_tier_range_str eq "3/4") { $adjusted_tier_str = "3"; }
        elsif ($initial_tier_range_str eq "2/3") { $adjusted_tier_str = "2"; }
        elsif ($initial_tier_range_str eq "1/2") { $adjusted_tier_str = "1"; }
    }
    my $pattern_tier_string = $suggested_pattern . $adjusted_tier_str;

    if ($is_vio_server)
    {
        # For VIOs, we return the *actual* calculated pressure flags and details,
        # but override the final tier/pattern hint for safety and clarity.
        return (
            "P",    # Override Hint to "P" for Peak/Manual
            $shape_descriptor, # Override Pattern
            $overall_pressure_detected_for_csv, # Return ACTUAL calculated pressure
            $pressure_detail_str,               # Return ACTUAL pressure details
            $global_pressure_rationale_text,
            $p99w1_has_absolute_runq_pressure,
            $p99w1_has_normalized_runq_pressure
        );
    }
    else
    {
        # For non-VIOs, return the standard, calculated hint and pattern.
        return (
            $pattern_tier_string,
            $shape_descriptor,
            $overall_pressure_detected_for_csv,
            $pressure_detail_str,
            $global_pressure_rationale_text,
            $p99w1_has_absolute_runq_pressure,
            $p99w1_has_normalized_runq_pressure
        );
    }
}
# end of generate_sizing_hint

# --- parse_percentile_list_for_header ---
# This sub is used by the OLD global RunQ metric collection logic (which is now superseded
# by per-profile RunQ metrics). It might still be called if that logic path is hit,
# or could be refactored/removed if that path is fully deprecated.
# For now, keeping it as it might be used by initial population of $results_table{$vm_name}{$rq_metric_name}.
# It prepares percentile numbers for use as metric name suffixes.
sub parse_percentile_list_for_header
{
    my ($perc_str, $clean_zeros) = @_;
    $clean_zeros = 1 if !defined $clean_zeros; # Default to cleaning "X.00" to "X"
    my @percentiles_cleaned;
    if (defined $perc_str && $perc_str ne '')
    {
        my @raw_percentiles = split /,\s*/, $perc_str;
        foreach my $p (@raw_percentiles)
        {
            if ($p =~ /^[0-9]+(?:\.[0-9]+)?$/ && $p >= 0 && $p <= 100) # Validate numeric and range
            {
                my $p_label = $p;
                if ($clean_zeros)
                {
                    $p_label = sprintf("%.2f", $p); # Format to two decimal places
                    $p_label =~ s/\.?0+$//;         # Remove trailing ".00" or ".0"
                    $p_label = "0" if $p_label eq "" && ($p eq "0" || $p eq "0.00"); # Handle "0.00" -> "0"
                }
                push @percentiles_cleaned, $p_label;
            }
            else # Invalid percentile value
            {
                die "Error: Invalid percentile value '$p' found in list '$perc_str'. Must be numeric between 0 and 100.\n";
            }
        }
    }
    return \@percentiles_cleaned; # Return reference to array of cleaned percentile labels
}

# --- ensure_percentiles_requested ---
# Checks if a list of required percentiles are present in a given percentile string.
# Potentially used for validating if nfit was asked to calculate necessary percentiles
# for the old global RunQ metric collection. May be less relevant with per-profile logic.
sub ensure_percentiles_requested
{
    my ($perc_list_str, @required_percs) = @_; # perc_list_str is comma-separated, required_percs are numbers
    return 1 unless defined $perc_list_str && $perc_list_str ne ''; # If no list provided, assume not applicable or handled elsewhere

    # Parse the provided list string into a map for easy lookup
    my $parsed_percs_ref = parse_percentile_list_for_header($perc_list_str, 0); # Get raw numbers, no zero cleaning for comparison
    my %present_map = map { $_ => 1 } @{$parsed_percs_ref};

    foreach my $req_p_num (@required_percs) # Iterate through numerically required percentiles
    {
        # Check if the numeric value (or its string representation) exists in the parsed list
        my $req_p_str = "$req_p_num"; # Simple string conversion
        my $req_p_str_formatted = sprintf("%.2f", $req_p_num); # e.g. 90.00
        my $req_p_str_cleaned = $req_p_str_formatted;
        $req_p_str_cleaned =~ s/\.?0+$//;
        $req_p_str_cleaned = "0" if $req_p_str_cleaned eq "" && abs($req_p_num -0) < 0.001;


        unless (exists $present_map{$req_p_str} ||
            exists $present_map{$req_p_str_formatted} ||
            exists $present_map{$req_p_str_cleaned} )
        {
            # Check common string representations due to potential formatting differences
            my $found = 0;
            foreach my $key (keys %present_map) {
                if (abs($key - $req_p_num) < 0.001) { # Floating point comparison
                    $found = 1;
                    last;
                }
            }
            return 0 unless $found; # Required percentile not found
        }
    }
    return 1; # All required percentiles found
}

# --- get_nfit_output_dp_from_flags ---
# Determines the number of decimal places nfit is expected to use for a profile's output,
# based on the rounding flags (-r or -u) passed to nfit for that profile.
# This helps nfit-profile format its *own* adjusted values consistently.
sub get_nfit_output_dp_from_flags
{
    my ($nfit_flags_str_for_this_run) = @_; # Combined global and profile-specific flags for nfit

    # Regex to find -r[=increment] or -u[=increment]
    # It captures the increment value if provided.
    if ($nfit_flags_str_for_this_run =~ /-r(?:=(\d*\.\d+))?|-u(?:=(\d*\.\d+))?/)
    {
        my $increment_val_str = $1 // $2; # $1 for -r=val, $2 for -u=val

        # If -r or -u is present but no increment value, nfit uses its default increment.
        if (!(defined $increment_val_str && $increment_val_str ne ""))
        {
            # nfit's default increment is $DEFAULT_ROUND_INCREMENT (from nfit, assumed here to be same as nfit-profile's)
            # For robustness, it's better if nfit-profile knows nfit's default or this is coordinated.
            # Using nfit-profile's default as a proxy.
            return get_decimal_places($DEFAULT_ROUND_INCREMENT);
        }
        else # Increment value was specified
        {
            return get_decimal_places($increment_val_str);
        }
    }
    # If no -r or -u flag, nfit typically outputs with more precision (e.g., 4 decimal places by default internally).
    # nfit version 2.28.0.4 defaults to 4 DP if no rounding.
    return 4;
}

# --- get_decimal_places ---
# Calculates the number of decimal places in a given number string.
sub get_decimal_places
{
    my ($number_str) = @_;
    # Handle scientific notation by converting to fixed point string first
    $number_str = sprintf("%.15f", $number_str) if ($number_str =~ /e/i);

    if ($number_str =~ /\.(\d+)$/) # If there's a decimal part
    {
        return length($1); # Length of the digits after decimal point
    }
    else # No decimal part
    {
        return 0;
    }
}

# Helper to format a percentile number into a clean string for metric keys.
sub clean_perc_label
{
    my ($p) = @_;
    my $label = sprintf("%.2f", $p);
    $label =~ s/\.?0+$//;
    $label = "0" if $label eq "" && abs($p-0)<0.001;
    return $label;
}

# --- parse_nfit_json_output ---
# Parses the multi-line JSON output from nfit. Each line is a distinct JSON object.
# Returns a hash where keys are VM names and values are arrays of the parsed JSON objects (as Perl hashes).
sub parse_nfit_json_output
{
    my ($raw_output) = @_;
    my %parsed_data;
    my $json_decoder = JSON->new->utf8;
    my @lines = split /\n/, $raw_output;

    foreach my $line (@lines)
    {
        next if $line =~ /^\s*$/; # Skip empty lines

        my $decoded_hash = eval { $json_decoder->decode($line) };
        if ($@ || !ref($decoded_hash) eq 'HASH') {
            warn "Warning: Could not decode JSON line from nfit: $line. Error: $@";
            next;
        }

        my $vm_name = $decoded_hash->{vmName};
        if ($vm_name) {
            push @{$parsed_data{$vm_name}}, $decoded_hash;
        }

    }

    return \%parsed_data;
}

# ==============================================================================
# Subroutine to determine the start and end date of a seasonal event period.
# It reads the event's configuration and calculates the absolute date range
# for the analysis.
# It accepts a base_date_obj to correctly calculate periods for
# historical months during an --update-history run.
# ==============================================================================
sub determine_event_period {
    my ($event_config, $base_date_obj_for_recurring) = @_;

    my $model_type = $event_config->{model} // '';
    # Use the provided base date for historical calculations, or default to today for forecasts.
    my $base_date = (defined $base_date_obj_for_recurring) ? $base_date_obj_for_recurring : gmtime()->truncate(to => 'day');

    # --- Path for events defined by fixed 'dates' ---
    if (defined $event_config->{dates}) {
        my @date_ranges = split /\s*,\s*/, $event_config->{dates};
        my ($next_event_start, $next_event_end);

        foreach my $range (@date_ranges) {
            if ($range =~ /(\d{4}-\d{2}-\d{2}):(\d{4}-\d{2}-\d{2})/) {
                my $start_obj = Time::Piece->strptime($1, '%Y-%m-%d')->truncate(to => 'day');
                my $end_obj   = Time::Piece->strptime($2, '%Y-%m-%d')->truncate(to => 'day');

                if (!defined $base_date_obj_for_recurring) {
                    # FORECAST CONTEXT: Find the *next* event period relative to the current date.
                    if ($end_obj >= $base_date) {
                        if (!defined $next_event_start || $start_obj < $next_event_start) {
                            $next_event_start = $start_obj;
                            $next_event_end   = $end_obj;
                        }
                    }
                } else {
                    # HISTORY CONTEXT: Check if this specific fixed-date event falls
                    # within the historical month being processed. The base_date is the
                    # first day of that historical month.
                    my $month_end = Time::Piece->new($base_date->epoch)->add_months(1) - ONE_DAY;
                    if ($start_obj >= $base_date && $end_obj <= $month_end) {
                        return ($start_obj, $end_obj);
                    }
                }
            }
        }
        # In a forecast context, we return the closest future event found.
        return ($next_event_start, $next_event_end) if (defined $next_event_start && !defined $base_date_obj_for_recurring);
    }
    # --- Path for events defined by a recurring 'period' ---
    elsif (defined $event_config->{period} && lc($event_config->{period}) eq 'monthly') {
        my $day_of_period = $event_config->{day_of_period} // -1;
        my $duration_days = $event_config->{duration_days} // 7;

        # FORECAST CONTEXT for 'recency_decay' model: Find the *last completed* peak.
        if ($model_type eq 'recency_decay' && !defined $base_date_obj_for_recurring) {
            my ($current_month_start, $current_month_end) = _get_recurring_monthly_period($base_date, $day_of_period, $duration_days);
            if ($current_month_end > $base_date) {
                my $last_month_base = add_months($base_date, -1);
                return _get_recurring_monthly_period($last_month_base, $day_of_period, $duration_days);
            } else {
                return ($current_month_start, $current_month_end);
            }
        }
        # FORECAST CONTEXT for other models: Find the *next upcoming* peak.
        elsif (!defined $base_date_obj_for_recurring) {
            my ($current_month_start, $current_month_end) = _get_recurring_monthly_period($base_date, $day_of_period, $duration_days);
            if ($current_month_end >= $base_date) {
                return ($current_month_start, $current_month_end);
            } else {
                my $next_month_base = add_months($base_date, 1);
                return _get_recurring_monthly_period($next_month_base, $day_of_period, $duration_days);
            }
        }
        # HISTORY CONTEXT for any recurring model: Calculate the period for the *specific historical month* provided.
        else {
            return _get_recurring_monthly_period($base_date, $day_of_period, $duration_days);
        }
    }

    # Fallback: return nothing if no valid period is found.
    return;
}

# ==============================================================================
# Subroutine to parse a simple INI-style configuration file.
# This replaces the need for the external Config::Tiny module.
#
# Takes:
# 1. The path to the configuration file.
#
# Returns:
# - A hash reference representing the parsed INI data.
# - Dies on file open error.
# ==============================================================================
sub parse_seasonality_config {
    my ($filepath) = @_;
    my %config_data;
    my $current_section = '';

    open my $fh, '<:encoding(utf8)', $filepath
        or die "Error: Cannot open seasonality config file '$filepath': $!";

    while (my $line = <$fh>) {
        chomp $line;
        $line =~ s/^\s+|\s+$//g; # Trim whitespace
        $line =~ s/\s*[#;].*//;   # Remove comments

        next if $line eq ''; # Skip empty or comment-only lines

        if ($line =~ /^\s*\[\s*([^\]]+?)\s*\]\s*$/) {
            # This is a section header
            $current_section = $1;
        } elsif ($current_section ne '' && $line =~ /^\s*([^=]+?)\s*=\s*(.+)$/) {
            # This is a key-value pair within a section
            my $key = $1;
            my $value = $2;
            $key =~ s/^\s+|\s+$//g;
            $value =~ s/^\s+|\s+$//g;
            $config_data{$current_section}{$key} = $value;
        }
    }
    close $fh;

    return \%config_data;
}

# ==============================================================================
# SUBROUTINE: calculate_multiplicative_forecast (NEW ROBUST DESIGN)
# PURPOSE:    Calculates a forecast using the multiplicative seasonal model.
#             This version implements a robust, multi-stage forecast. It
#             intelligently determines the most appropriate recent baseline and
#             applies a recency-weighted seasonal multiplier derived from
#             historical, saturation-corrected peak data. It also includes
#             logic for volatility, residual peak forecasting, and compounding.
# RETURNS:
#   - A hash reference containing the final forecasted results.
#   - A hash reference containing historical data for verbose reporting.
# ==============================================================================
sub calculate_multiplicative_forecast {
    my ($system_cache_dir, $system_identifier, $event_name, $event_config, $full_seasonality_config, $initial_results_table_href) = @_;

    # This subroutine implements a robust, multi-stage forecast for multiplicative seasonal events.
    # It intelligently determines the most appropriate recent baseline and applies a recency-weighted
    # seasonal multiplier derived from historical, saturation-corrected peak data.

    print STDERR "\n--- Applying Multiplicative Seasonal Forecast ---\n";
    print STDERR "Executing robust forecast for event '$event_name' on system '$system_identifier'.\n";

    # --- Configuration Constants ---
    # The number of days of stable, non-peak activity to use for the baseline.
    my $baseline_days_config = $event_config->{baseline_period_days} // 16;
    # A baseline must have at least this many clean days to be considered statistically valid.
    my $MIN_BASELINE_DAYS = 7;
    # Placeholder for future enhancement: A trend adjustment factor could be applied here.
    my $trend_adjustment_factor = 1.0;

    # --- Step 1: Get Key Dates and Read Unified History ---
    my $data_cache_file = File::Spec->catfile($system_cache_dir, '.nfit.cache.data');
    my (undef, $cache_end_date_obj) = _get_cache_date_range($data_cache_file);

    unless ($cache_end_date_obj) {
        warn "Warning: Could not determine data cache end date. Cannot proceed with forecast.\n";
        return ({}, {});
    }

    my $unified_history = read_unified_history($system_cache_dir);
    my @event_history_initial;
    my ($most_recent_peak_snapshot, $most_recent_peak_end_obj);

    # Find all historical snapshots for this specific event from the unified history.
    foreach my $month_key (sort { $b cmp $a } keys %$unified_history) {
        my $month_data = $unified_history->{$month_key};
        if (exists $month_data->{SeasonalEventSnapshots}{$event_name}) {
            my $snapshot = $month_data->{SeasonalEventSnapshots}{$event_name};
            # Enrich the snapshot with the month key for date context.
            $snapshot->{_month_key} = $month_key;
            push @event_history_initial, $snapshot;
        }
    }

    # --- Step 2: Two-Tiered Baseline Strategy ---
    my %current_baseline_results;
    my $baseline_source_log = "N/A";
    my @event_history_final = @event_history_initial; # Start with all available history.

    # --- Path A (Ideal): Attempt to calculate a "Post-Peak" baseline ---
    # The ideal baseline is the N days of data ending on the last day of the cache.
    my $post_peak_baseline_start_obj = $cache_end_date_obj->truncate(to => 'day') - (($baseline_days_config - 1) * 86400);

    # Check if this ideal baseline period is contaminated by the most recent historical peak.
    my $is_contaminated = 0;
    if ($most_recent_peak_end_obj) {
        $is_contaminated = ($post_peak_baseline_start_obj <= $most_recent_peak_end_obj);
    }
    # Also check if the clean period is long enough to be statistically valid.
    my $clean_days_available = $is_contaminated ?
    ($cache_end_date_obj->epoch - $most_recent_peak_end_obj->epoch) / 86400 - 1 :
    $baseline_days_config;

    if (!$is_contaminated && $clean_days_available >= $MIN_BASELINE_DAYS) {
        $baseline_source_log = "Post-Peak (calculated from " . $post_peak_baseline_start_obj->date . " to " . $cache_end_date_obj->date . ")";
        print STDERR "  - Baseline Strategy: Using ideal Post-Peak baseline.\n";

        my $start_str = $post_peak_baseline_start_obj->strftime('%Y-%m-%d');
        my $end_str   = $cache_end_date_obj->strftime('%Y-%m-%d');

        my $profile_count = scalar(@profiles);
        my $profile_num = 0;
        foreach my $profile (@profiles) {
            $profile_num++;
            my $profile_name = $profile->{name};
            print STDERR "    - Analysing post-peak baseline $profile_num/$profile_count: $profile_name\n";
            # This is a contextual baseline, so is_generic is 0.
            my $nfit_cmd = _build_nfit_baseline_command($profile->{flags}, $start_str, $end_str, $system_cache_dir, 0, 0, undef, $profile->{name});
            my $nfit_output = '';
            my $stderr_arg = ">&=" . fileno(STDERR);
            my $pid_nfit = open3(undef, my $stdout_nfit, $stderr_arg, $nfit_cmd);
            while (my $line = <$stdout_nfit>) { $nfit_output .= $line; }
            waitpid($pid_nfit, 0);
            next if $?; # Skip if nfit command failed

            my $parsed = parse_nfit_json_output($nfit_output);
            my $p_key  = "P" . clean_perc_label(($profile->{flags} =~ /-p\s+([0-9.]+)/) ? $1 : $DEFAULT_PERCENTILE);

            foreach my $vm (keys %$parsed) {
                my @vals = map { $_->{metrics}{physc}{$p_key} } @{$parsed->{$vm}};
                my @valid_vals = grep { defined $_ && looks_like_number($_) } @vals;
                if (@valid_vals) {
                    $current_baseline_results{$vm}{$profile->{name}} = sum0(@valid_vals) / scalar(@valid_vals);
                }
            }
        }
    }
    # --- Path B (Fallback): Re-use the "Pre-Peak" baseline from the most recent snapshot ---
    elsif ($most_recent_peak_snapshot) {
        $baseline_source_log = "Pre-Peak (re-used from snapshot ending " . $most_recent_peak_snapshot->{periodEndDate} . ")";
        print STDERR "  - Baseline Strategy: Post-Peak baseline contaminated or too short. Falling back to re-use Pre-Peak baseline.\n";
        %current_baseline_results = %{$most_recent_peak_snapshot->{results}{HistoricBaseline} || {}};
        # If we re-use this baseline, we must exclude its corresponding peak from the multiplier calculation.
        shift @event_history_final;
    }

    unless (%current_baseline_results) {
        warn "Warning: Could not determine a valid baseline. Cannot proceed with forecast.\n";
        return ({}, {});
    }

    # --- Step 3: Detect Active Events for Compounding ---
    my $today_for_recency = gmtime();
    my ($next_event_start, $next_event_end) = determine_event_period($event_config);
    my $forecast_date_obj = (defined $next_event_start && ref($next_event_start) eq 'Time::Piece') ? $next_event_start : $today_for_recency;
    my @active_events = detect_active_events($forecast_date_obj, $full_seasonality_config);

    # --- Step 4: Calculate Recency-Weighted Multiplier & Final Forecast ---
    my %final_forecasts;
    my %historic_data_for_csv; # For verbose reporting

    if (!@active_events) {
        print STDERR "  - No active seasonal events found for the forecast period. Using baseline as forecast.\n";
        # If no events are active, the forecast is simply the baseline.
        foreach my $vm_name (keys %current_baseline_results) {
            foreach my $profile (@profiles) {
                $final_forecasts{$vm_name}{$profile->{name}} = $current_baseline_results{$vm_name}{$profile->{name}};
            }
        }
        return (\%final_forecasts, \%historic_data_for_csv);
    }

    my $is_compound_run = (scalar(@active_events) > 1);
    my @dampening_factors;
    print "  - Detected " . scalar(@active_events) . " active event(s): " . join(", ", map { $_->{_eventName} } @active_events) . "\n";
    if ($is_compound_run) { print "  - Compounding mode enabled.\n"; }

    # Main processing loop: iterate through each VM that has a valid baseline.
    foreach my $vm_name (sort keys %current_baseline_results) {
        my $vm_forecasts = {};
        my %vm_historic_multipliers;
        my %vm_historic_residuals;

        # Gather historical data for all active events for this VM.
        foreach my $active_event_cfg (@active_events) {
            my $active_event_name = $active_event_cfg->{_eventName};

            my @history_for_this_event;
            foreach my $hist_event (@event_history_final) {
                if ($hist_event->{eventName} eq $active_event_name) {
                    push @history_for_this_event, $hist_event;
                }
            }

            # Only proceed to calculate multipliers if history exists for this event.
            if (@history_for_this_event) {
                if (defined $active_event_cfg->{interaction_dampening_factor}) {
                    push @dampening_factors, $active_event_cfg->{interaction_dampening_factor};
                }
                foreach my $profile (@profiles) {
                    my $p_name = $profile->{name};
                    my @multipliers;
                    my @residuals;
                    foreach my $hist_event (@history_for_this_event) {
                        my $hist_results = $hist_event->{results};
                        my $hist_peak = $hist_results->{HistoricPeak}{$vm_name}{$p_name};
                        if (defined $hist_results->{ClippingInfo}{$vm_name}{$p_name}{unclippedPeakEstimate} && looks_like_number($hist_results->{ClippingInfo}{$vm_name}{$p_name}{unclippedPeakEstimate})) {
                            $hist_peak = $hist_results->{ClippingInfo}{$vm_name}{$p_name}{unclippedPeakEstimate};
                        }
                        my $hist_base = $hist_results->{HistoricBaseline}{$vm_name}{$p_name};
                        my $hist_residual = $hist_results->{PeakResidual}{$vm_name}{$p_name};

                        if (defined $hist_peak && defined $hist_base && $hist_base > 0.001) {
                            push @multipliers, { value => $hist_peak / $hist_base, date => Time::Piece->strptime($hist_event->{periodEndDate}, '%Y-%m-%d') };
                        }
                        if (defined $hist_residual && looks_like_number($hist_residual)) {
                            push @residuals, { value => $hist_residual, date => Time::Piece->strptime($hist_event->{periodEndDate}, '%Y-%m-%d') };
                        }
                    }
                    next unless @multipliers;

                    my $event_multiplier = calculate_recency_weighted_average(\@multipliers, $today_for_recency, 365);
                    $vm_historic_multipliers{$p_name}{$active_event_name} = { 'multiplier' => $event_multiplier, 'history' => \@multipliers };
                    $vm_historic_residuals{$p_name}{$active_event_name} = \@residuals;

                }
            }
        }

        # Now, synthesize the final forecast for each profile.
        foreach my $profile (@profiles) {
            my $p_name = $profile->{name};
            my $current_baseline_val = $current_baseline_results{$vm_name}{$p_name};
            next unless (defined $current_baseline_val && looks_like_number($current_baseline_val));
            next unless (exists $vm_historic_multipliers{$p_name});

            my $final_multiplier = 1.0;
            foreach my $event_data (values %{$vm_historic_multipliers{$p_name}}) {
                $final_multiplier *= $event_data->{'multiplier'};
            }

            if ($is_compound_run && @dampening_factors) {
                my $dampening_to_apply = (sort { $a <=> $b } @dampening_factors)[0];
                $final_multiplier = 1 + (($final_multiplier - 1) * $dampening_to_apply);
            }

            my $volatility_buffer = 1.0;
            my $primary_event_cfg = (grep { $_->{_eventName} eq $event_name } @active_events)[0] || $active_events[0];
            if (($primary_event_cfg->{volatility_adjustment} // 0) == 1 && exists $vm_historic_multipliers{$p_name}{$primary_event_cfg->{_eventName}}) {
                my $primary_event_history_ref = $vm_historic_multipliers{$p_name}{$primary_event_cfg->{_eventName}}{'history'};
                my $confidence = $primary_event_cfg->{seasonal_confidence_level} // '0.95';
                $volatility_buffer = calculate_volatility_buffer($primary_event_history_ref, $confidence);
            }

            my $forecasted_residual = 0;
            my @all_residuals_for_profile;
            if (exists $vm_historic_residuals{$p_name}) {
                foreach my $event_name (keys %{$vm_historic_residuals{$p_name}}) {
                    push @all_residuals_for_profile, @{$vm_historic_residuals{$p_name}{$event_name}};
                }
            }
            if (@all_residuals_for_profile) {
                $forecasted_residual = calculate_recency_weighted_average(\@all_residuals_for_profile, $today_for_recency, 365) // 0;
            }

            my $trend_adjusted_baseline = $current_baseline_val * $trend_adjustment_factor;
            my $primary_forecast = $trend_adjusted_baseline * $final_multiplier * $volatility_buffer;
            my $combined_forecast = $primary_forecast + $forecasted_residual;
            my $amplification = $event_config->{peak_amplification_factor} // 1.0;
            my $final_recommendation = $combined_forecast * $amplification;

            $vm_forecasts->{$p_name} = $final_recommendation;

            $seasonal_debug_info{$vm_name}{$p_name} = {
                historical_multipliers => $vm_historic_multipliers{$p_name}{$event_name}{'history'},
                baseline             => $current_baseline_val,
                baseline_source      => $baseline_source_log,
                trend_factor         => $trend_adjustment_factor,
                multiplier           => $final_multiplier,
                volatility           => $volatility_buffer,
                forecasted_residual  => $forecasted_residual,
                amplification_factor => $amplification,
                forecast             => $final_recommendation,
                OutlierWarning       => _get_warning_for_vm_forecast($vm_name, \%outlier_warnings)
            };
        }
        $final_forecasts{$vm_name} = $vm_forecasts;
    }

    # Populate data for the verbose historic_snapshot.csv file.
    if ($most_recent_peak_snapshot) {
        foreach my $vm_name (keys %{$most_recent_peak_snapshot->{results}{HistoricPeak}}) {
            $historic_data_for_csv{$vm_name}{'HistoricPeak'} = $most_recent_peak_snapshot->{results}{HistoricPeak}{$vm_name} || {};
            $historic_data_for_csv{$vm_name}{'HistoricBaseline'} = $most_recent_peak_snapshot->{results}{HistoricBaseline}{$vm_name} || {};
        }
    }

    # Return both the forecasts and the historical data for verbose reporting.
    return (\%final_forecasts, \%historic_data_for_csv);
}

# ==============================================================================
# Helper subroutine to write the multi-file CSV output for seasonal forecasts.
# ==============================================================================
# This version safely enhances the seasonal forecast output:
# - It adds a new, rich format ONLY for the 'final_forecast.csv'.
# - The logic for 'current_baseline' and 'historic_snapshot' is UNCHANGED,
#   ensuring no regressions.
# - It is fully commented and robust.
# ==============================================================================
sub write_seasonal_csv_output {
    my ($type, $system_id, $event_name, $timestamp, $data_href) = @_;

    # Prevent creation of empty files
    if (!defined $data_href || !%{$data_href} || !scalar(keys %{$data_href})) {
        print STDERR "  - INFO: No data available for seasonal output type '$type'. Skipping file generation.\n";
        return;
    }

    # Sanitise identifiers for use in filenames.
    my $s_id_safe = $system_id;
    my $e_name_safe = $event_name;
    $s_id_safe =~ s/[^a-zA-Z0-9_.-]//g;
    $e_name_safe =~ s/[^a-zA-Z0-9_.-]//g;

    my $filename = File::Spec->catfile($output_dir, "nfit-profile.$s_id_safe.$e_name_safe.$type.$timestamp.csv");
    # Store filename for final notification
    push @generated_files, $filename;

    print "  - Generating output file: $filename\n";

    open my $fh, '>', $filename or do {
        warn "Warning: Could not open output file '$filename': $!";
        return;
    };

    # --- Check if this is the rich forecast report ---
    if ($type eq 'final_forecast') {
        # --- PATH A: Generate the new, rich final_forecast.csv ---

        # Build the enhanced header with all standard columns plus the new one.
        my @header = (
            "VM", "TIER", "Hint", "Pattern", "Pressure", "PressureDetail", "SMT",
            "Serial", "SystemType", "Pool Name", "Pool ID", $PEAK_PROFILE_NAME
        );
        push @header, (map { $_->{name} } @csv_visible_profiles);
        # Add the new SeasonalMultiplier column before the entitlement columns.
        push @header, ("SeasonalMultiplier", "Current - ENT");
        print $fh join(",", map { quote_csv($_) } @header) . "\n";

        foreach my $vm_name (sort keys %$data_href) {
            my $vm_data = $data_href->{$vm_name};
            my $report_data = $vm_data->{_report_data} || {};
            my $cfg_csv = $vm_config_data{$vm_name};

            # Gather standard fields for the row
            my $smt_out = $cfg_csv->{smt} // $default_smt_arg;
            my ($serial_out, $systype_out, $poolname_out, $poolid_out, $ent_out) = ('', '', '', '', '');
            if (defined $cfg_csv) {
                $serial_out = $cfg_csv->{serial} // ''; $systype_out = $cfg_csv->{systemtype} // '';
                $poolname_out = $cfg_csv->{pool_name} // ''; $poolid_out = $cfg_csv->{pool_id} // '';
                $ent_out = $cfg_csv->{entitlement} // '';
            }

            my @row = (
                $vm_name, "", $report_data->{hint} // "", $report_data->{pattern} // "",
                $report_data->{pressure} // "", $report_data->{pressure_detail} // "", $smt_out,
                $serial_out, $systype_out, $poolname_out, $poolid_out, "" # Peak is not applicable for a forecast
            );

            my $seasonal_multiplier_for_row = "N/A";
            foreach my $profile (@csv_visible_profiles) {
                my $p_name = $profile->{name};
                my $value = $vm_data->{$p_name} // '';
                push @row, (looks_like_number($value) ? sprintf("%.4f", $value) : $value);

                # Use the multiplier from the first available profile for the summary column
                if ($seasonal_multiplier_for_row eq 'N/A' && exists $report_data->{seasonal_debug}{$p_name}) {
                    $seasonal_multiplier_for_row = sprintf("%.2f", $report_data->{seasonal_debug}{$p_name}{multiplier});
                }
            }

            # Add the new column value and the final entitlement value
            push @row, $seasonal_multiplier_for_row;
            push @row, (looks_like_number($ent_out) ? sprintf("%.2f", $ent_out) : $ent_out);

            print $fh join(",", map { quote_csv($_) } @row) . "\n";
        }

    } else {
        # --- PATH B: Original logic for other file types (UNCHANGED) ---

        # --- Build header ---
        my @header;
        if ($type eq 'historic_snapshot') {
            # Specialised header for the detailed historic report.
            @header = ("VM", "MetricType");
        } else {
            # Standard header for 'current_baseline' and 'final_forecast'
            @header = (
                "VM", "TIER", "Hint", "Pattern", "Pressure", "PressureDetail", "SMT",
                "Serial", "SystemType", "Pool Name", "Pool ID", $PEAK_PROFILE_NAME
            );
        }
        push @header, map { $_->{name} } @csv_visible_profiles;

        # --- Write data rows ---
        foreach my $vm_name (sort keys %$data_href) {
            if ($type eq 'historic_snapshot') {
                # --- Special handling for the historic snapshot file ---
                my $hist_data = $data_href->{$vm_name} || {};
                my $peak_data = $hist_data->{HistoricPeak} || {};
                my $base_data = $hist_data->{HistoricBaseline} || {};
                my @peak_row = ($vm_name, "HistoricPeak");
                foreach my $profile (@csv_visible_profiles) {
                    my $value = $peak_data->{$profile->{name}} // '';
                    push @peak_row, (looks_like_number($value) ? sprintf("%.4f", $value) : $value);
                }
                print $fh join(",", map { quote_csv($_) } @peak_row) . "\n";
                my @base_row = ($vm_name, "HistoricBaseline");
                foreach my $profile (@csv_visible_profiles) {
                    my $value = $base_data->{$profile->{name}} // '';
                    push @base_row, (looks_like_number($value) ? sprintf("%.4f", $value) : $value);
                }
                print $fh join(",", map { quote_csv($_) } @base_row) . "\n";
            } else {
                # --- Standard handling for 'current_baseline' file ---
                my @row = ($vm_name);
                my $vm_data = $data_href->{$vm_name};
                foreach my $profile (@csv_visible_profiles) {
                    my $value = $vm_data->{$profile->{name}} // '';
                    push @row, (looks_like_number($value) ? sprintf("%.4f", $value) : $value);
                }
            }
        }

        # Add the new SeasonalMultiplier column before the entitlement columns.
        push @header, ("SeasonalMultiplier", "Current - ENT");
        if ($add_excel_formulas) {
            push @header, ("NFIT - ENT", "NETT", "NETT%");
        }
        print $fh join(",", map { quote_csv($_) } @header) . "\n";

        my $excel_row_num_counter = 1;
        foreach my $vm_name (sort keys %$data_href) {
            my $vm_data = $data_href->{$vm_name};
            my $report_data = $vm_data->{_report_data} || {};
            my $cfg_csv = $vm_config_data{$vm_name};

            # Gather standard fields for the row
            my $smt_out = $cfg_csv->{smt} // $default_smt_arg;
            my ($serial_out, $systype_out, $poolname_out, $poolid_out, $ent_out) = ('', '', '', '', '');
            if (defined $cfg_csv) {
                $serial_out = $cfg_csv->{serial} // ''; $systype_out = $cfg_csv->{systemtype} // '';
                $poolname_out = $cfg_csv->{pool_name} // ''; $poolid_out = $cfg_csv->{pool_id} // '';
                $ent_out = $cfg_csv->{entitlement} // '';
            }

            $excel_row_num_counter++;
            my @row = (
                $vm_name, "", $report_data->{hint} // "", $report_data->{pattern} // "",
                $report_data->{pressure} // "", $report_data->{pressure_detail} // "", $smt_out,
                $serial_out, $systype_out, $poolname_out, $poolid_out, "" # Peak is not applicable for a forecast
            );

            my $seasonal_multiplier_for_row = "N/A";
            foreach my $profile (@csv_visible_profiles) {
                my $vm_data = $data_href->{$vm_name};
                my $p_name = $profile->{name};
                my $value = $vm_data->{$p_name} // '';
                push @row, (looks_like_number($value) ? sprintf("%.4f", $value) : $value);

                # Use the multiplier from the first available profile for the summary column
                if ($seasonal_multiplier_for_row eq 'N/A' && exists $report_data->{seasonal_debug}{$p_name}) {
                    $seasonal_multiplier_for_row = sprintf("%.2f", $report_data->{seasonal_debug}{$p_name}{multiplier});
                }
            }
        }
    }
    close $fh;
}

# ==============================================================================
# Subroutine to calculate a recency-weighted average.
# This is a key statistical function used by the multiplicative model.
# ==============================================================================
sub calculate_recency_weighted_average {
    my ($data_points_aref, $reference_date_obj, $half_life_days) = @_;

    my $sum_weighted_values = 0;
    my $sum_weights = 0;
    my $lambda = log(2) / $half_life_days;

    foreach my $dp (@$data_points_aref) {
        my $value = $dp->{value};
        my $date_obj = $dp->{date};

        my $days_diff = ($reference_date_obj->epoch - $date_obj->epoch) / ONE_DAY;
        $days_diff = 0 if $days_diff < 0;

        my $weight = exp(-$lambda * $days_diff);
        $sum_weighted_values += $value * $weight;
        $sum_weights += $weight;
    }

    if ($sum_weights > 1e-9) {
        return $sum_weighted_values / $sum_weights;
    } else {
        # Fallback: if all weights are zero (e.g., very old data), return a simple average.
        my @values = map { $_->{value} } @$data_points_aref;
        return @values ? (sum0(@values) / scalar(@values)) : undef;
    }
}

# ==============================================================================
# Subroutine to calculate a statistical volatility buffer.
# This increases the forecast based on the standard deviation of historical
# seasonal multipliers to account for year-over-year variance.
# ==============================================================================
sub calculate_volatility_buffer {
    my ($multipliers_aref, $confidence_level) = @_;

    my @values = map { $_->{value} } @$multipliers_aref;

    # Standard deviation requires at least 2 data points.
    return 1.0 if scalar(@values) < 2;

    # --- Calculate Mean and Standard Deviation ---
    my $sum = sum0(@values);
    my $mean = $sum / scalar(@values);
    return 1.0 if $mean == 0; # Avoid division by zero if mean is zero.

    my $sum_sq_diff = 0;
    foreach my $val (@values) {
        $sum_sq_diff += ($val - $mean)**2;
    }
    my $std_dev = sqrt($sum_sq_diff / (scalar(@values) - 1));

    # --- Z-score for common confidence levels ---
    # This lookup table provides the one-sided Z-score for a given confidence level.
    my %z_scores = (
        '0.90' => 1.645,
        '0.95' => 1.960,
        '0.98' => 2.326,
        '0.99' => 2.576,
    );
    my $z_score = $z_scores{$confidence_level} // 1.960; # Default to 95%

    # The buffer is 1 + (a fraction of the coefficient of variation).
    # This adds a percentage uplift proportional to the historical volatility.
    my $volatility_buffer = 1.0 + ($z_score * ($std_dev / $mean));

    # Sanity check: don't let the buffer be less than 1 (non-reducing).
    return $volatility_buffer > 1.0 ? $volatility_buffer : 1.0;
}

# ==============================================================================
# Subroutine to detect all active, compoundable seasonal events for a given date.
# Returns:
# - An array of event configuration hashes, sorted by priority (desc).
# Notes:
# - CORRECTED to handle both fixed-date and recurring-period definitions.
# ==============================================================================
sub detect_active_events {
    my ($analysis_date_obj, $seasonality_config_href) = @_;

    my @active_events;
    return @active_events unless (defined $analysis_date_obj && ref($analysis_date_obj) eq 'Time::Piece');

    # *** FIX: Use UTC for consistent comparisons ***
    my $analysis_date_utc = gmtime($analysis_date_obj->epoch)->truncate(to => 'day');

    foreach my $event_name (keys %{$seasonality_config_href}) {
        my $event_config = $seasonality_config_href->{$event_name};
        next unless (($event_config->{allow_compounding} // '') =~ /^(true|1)$/i);

        # Determine the period for this event relative to the analysis date.
        my ($start_obj, $end_obj) = determine_event_period($event_config, $analysis_date_utc);

        my $is_active = 0;

        if (defined $event_config->{dates}) {
            my @date_ranges = split /\s*,\s*/, $event_config->{dates};
            foreach my $range (@date_ranges) {
                if ($range =~ /(\d{4}-\d{2}-\d{2}):(\d{4}-\d{2}-\d{2})/) {
                    my $start_obj = Time::Piece->strptime($1, '%Y-%m-%d')->truncate(to => 'day');
                    my $end_obj   = Time::Piece->strptime($2, '%Y-%m-%d')->truncate(to => 'day');
                    if ($analysis_date_utc >= $start_obj && $analysis_date_utc <= $end_obj) {
                        $is_active = 1;
                        last;
                    }
                }
            }
        }
        elsif (defined $event_config->{period} && lc($event_config->{period}) eq 'monthly') {
            # *** FIX: Use the robust helper for consistent logic ***
            my $day_of_period = $event_config->{day_of_period} // -1;
            my $duration_days = $event_config->{duration_days} // 7;
            my ($event_start_obj, $event_end_obj) = _get_recurring_monthly_period($analysis_date_utc, $day_of_period, $duration_days);

            # The check is now simpler because determine_event_period already did the work.
            if (defined $start_obj && $analysis_date_utc >= $start_obj && $analysis_date_utc <= $end_obj) {
                $is_active = 1;
            }
        }

        if ($is_active) {
            $event_config->{_eventName} = $event_name;
            push @active_events, $event_config;
        }
    }

    return sort { ($b->{priority} // 0) <=> ($a->{priority} // 0) } @active_events;
}

# ==============================================================================
# Helper function to validate data line format
# ==============================================================================
sub is_valid_data_line {
    my ($line) = @_;
    # Simply check if line starts with your timestamp format
    return $line && $line =~ /^\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2},/;
}

# ==============================================================================
# Efficiently gets the start and end timestamps from a large cache file
# by reading only the first and last data lines.
# ==============================================================================
sub _get_cache_date_range {
    my ($data_cache_file) = @_;
    return (undef, undef) unless (-f $data_cache_file && -s $data_cache_file);

    open my $fh, '<', $data_cache_file or die "Could not open $data_cache_file: $!";
    my ($start_ts_str, $end_ts_str);

    # Get first data line (line 2) - validate it's a proper data line
    my $header = <$fh>;  # Skip header
    while (my $line = <$fh>) {
        chomp $line;
        if ($line && is_valid_data_line($line)) {
            ($start_ts_str) = split ',', $line, 2;
            last;
        }
    }

    # Get last line by reading backwards line by line
    my $file_size = -s $data_cache_file;
    my $pos = $file_size;
    my $line_buffer = '';
    my $last_valid_line = '';

    # Read backwards in small chunks to build lines
    while ($pos > 0 && !$last_valid_line) {
        my $chunk_size = ($pos < 1024) ? $pos : 1024;
        $pos -= $chunk_size;

        seek $fh, $pos, 0;
        my $chunk;
        read $fh, $chunk, $chunk_size;

        # Prepend to our buffer
        $line_buffer = $chunk . $line_buffer;

        # Process complete lines from the end
        my @lines = split /\n/, $line_buffer;

        # If we're not at the start, the first line might be partial
        if ($pos > 0) {
            $line_buffer = shift @lines;  # Keep partial line for next iteration
        } else {
            $line_buffer = '';
        }

        # Check lines from end to start
        for my $line (reverse @lines) {
            if ($line && is_valid_data_line($line)) {
                $last_valid_line = $line;
                last;
            }
        }
    }

    if ($last_valid_line) {
        ($end_ts_str) = split ',', $last_valid_line, 2;
    }

    close $fh;

    # Parse timestamps
    my ($start_obj, $end_obj);
    eval { $start_obj = Time::Piece->strptime($start_ts_str, "%Y-%m-%d %H:%M:%S") if $start_ts_str; };
    if ($@) {
        warn "Warning: Could not parse start timestamp '$start_ts_str' from cache.";
        return (undef, undef);
    }
    eval { $end_obj = Time::Piece->strptime($end_ts_str, "%Y-%m-%d %H:%M:%S") if $end_ts_str; };
    if ($@) {
        warn "Warning: Could not parse end timestamp '$end_ts_str' from cache.";
        return (undef, undef);
    }

    return ($start_obj, $end_obj);
}

sub _get_warning_for_vm_forecast {
    my ($vm_name, $warnings_href) = @_;
    # This finds the most recent warning for a given VM, as the hash is keyed by snapshot date.
    foreach my $key (sort { $b cmp $a } keys %$warnings_href) {
        if (exists $warnings_href->{$key}{$vm_name}) {
            return $warnings_href->{$key}{$vm_name};
        }
    }
    return undef;
}

sub raw_init_vm_bucket {
    my ($store_hr, $vm) = @_;
    $store_hr->{$vm} //= {};
    die "raw_init_vm_bucket: VM bucket not HASH for $vm"
        unless ref($store_hr->{$vm}) eq 'HASH';
    return $store_hr->{$vm};
}

sub raw_set_profile_rows {
    my ($store_hr, $vm, $profile, $rows_aref) = @_;
    my $bucket = raw_init_vm_bucket($store_hr, $vm);
    $bucket->{$profile} = (ref($rows_aref) eq 'ARRAY') ? $rows_aref : [];
}

sub raw_get_profile_rows {
    my ($store_hr, $vm, $profile) = @_;
    return [] unless ref($store_hr) eq 'HASH';
    my $b = $store_hr->{$vm};
    return [] unless ref($b) eq 'HASH';
    my $a = $b->{$profile};
    return (ref($a) eq 'ARRAY') ? $a : [];
}

sub raw_get_last_state {
    my ($store_hr, $vm, $profile) = @_;
    my $rows = raw_get_profile_rows($store_hr, $vm, $profile);
    return @$rows ? $rows->[-1] : undef;
}

sub _safe_dig {
    my ($href, @path) = @_;
    my $cur = $href;
    for my $k (@path) {
        return undef unless ref($cur) eq 'HASH' && exists $cur->{$k};
        $cur = $cur->{$k};
    }
    return $cur;
}

# ==============================================================================
# SUBROUTINE: map_growth_confidence
# PURPOSE:    Maps a statistical p-value to a human-readable confidence level
#             for the CSV report and rationale log.
# ARGUMENTS:
#   1. $p_value (numeric): The Mann-Kendall p-value (or undef)
#   2. $method_used (string): The method string ('sen_slope', 'none', etc.)
# RETURNS:
#   - Confidence string: 'high', 'medium', 'low', or 'n/a'
# ==============================================================================
sub map_growth_confidence {
    my ($p_value, $method_used) = @_;

    # Default to 'n/a' (Not Applicable) if no growth was calculated or no p-value
    return 'n/a' if (!defined $method_used ||
                     $method_used eq 'none' ||
                     $method_used eq 'sen_slope_not_significant' ||
                     $method_used eq 'sen_slope_too_small' ||
                     !defined $p_value);

    # A p-value of 0 means the trend was extremely significant (p < 0.0001)
    if ($p_value == 0) {
        return 'high';
    }
    # Map based on standard statistical thresholds
    if ($p_value < 0.01) {
        return 'high';      # Very strong evidence (p < 1%)
    } elsif ($p_value < 0.05) {
        return 'medium';    # Conventional significance (p < 5%)
    } else {
        return 'low';       # Weak or no evidence (p >= 5%)
    }
}

# ==============================================================================
# SUBROUTINE: _write_standard_csv_report (Refactored for Assimilation Map)
# PURPOSE:    Encapsulates the entire CSV generation process. It now reads all
#             its data from the final, fully populated assimilation map.
# ==============================================================================
sub _write_standard_csv_report {
    my ($assimilation_map_ref, $report_type, $system_id, $file_timestamp, $is_multiplicative_run_flag, $is_recency_decay_run_flag, $is_predictive_peak_run_flag, $adaptive_runq_saturation_thresh) = @_;

    # Ensure we have data to process before creating a file.
    # The @vm_order array is still used to control the iteration order.
    return unless (@vm_order && ref($assimilation_map_ref) eq 'HASH');

    my $system_id_for_filename = $system_id || 'standard';
    $system_id_for_filename =~ s/[^a-zA-Z0-9_.-]//g;
    my $report_type_for_filename = $report_type;
    $report_type_for_filename =~ s/[^a-zA-Z0-9_.-]//g;

    my $output_filename = File::Spec->catfile($output_dir, "nfit-profile.$system_id_for_filename.$report_type_for_filename.$file_timestamp.csv");
    push @generated_files, $output_filename;

    open my $out_fh, '>', $output_filename or die "FATAL: Cannot open output file '$output_filename': $!";

    my @csv_visible_profiles = grep { $_->{csv_output} } @profiles;

    my @header_for_this_report = @output_header_cols_csv;
    my $formula_col_offset = 0;

    # Logic to add/remove model-specific columns remains the same...
    if ($is_multiplicative_run_flag) {
        @header_for_this_report = grep { $_ ne 'RunQ_Tactical' && $_ ne 'RunQ_Strategic' && $_ ne 'RunQ_Potential' && $_ ne 'RunQ_Source' } @header_for_this_report;
    }
    my ($ent_idx) = grep { $header_for_this_report[$_] eq 'Current - ENT' } 0..$#header_for_this_report;
    $ent_idx //= scalar(@header_for_this_report);
    if ($is_multiplicative_run_flag) {
        my @new_cols = ("SeasonalMultiplier", "Baseline_PhysC");
        # Remove all growth/runq columns for this model
        @header_for_this_report = grep {
            !/^(RunQ_Tactical|RunQ_Strategic|RunQ_Potential|RunQ_Source|ProjectionDays|GrowthMethod|GrowthConfidence|GrowthTrend|GrowthSignificance)$/
        } @header_for_this_report;
        # Find ent_idx again after grep
        ($ent_idx) = grep { $header_for_this_report[$_] eq 'Current - ENT' } 0..$#header_for_this_report;
        $ent_idx //= scalar(@header_for_this_report); # Fallback
        splice @header_for_this_report, $ent_idx, 0, @new_cols;
        $formula_col_offset = scalar(@new_cols);
    } elsif ($is_recency_decay_run_flag || $nfit_enable_windowed_decay || $nfit_decay_over_states) {
        # Add GrowthAdj (from G3) and GrowthAdj_Source
        my @new_cols = ("GrowthAdj", "GrowthAdj_Min", "GrowthAdj_Max", "GrowthAdj_Source", "ProjectionDays", "GrowthMethod", "GrowthConfidence", "GrowthTrend", "GrowthSignificance");
        splice @header_for_this_report, $ent_idx, 0, @new_cols;
        $formula_col_offset = scalar(@new_cols);
    } elsif ($is_predictive_peak_run_flag) {
        # Remove growth/runq columns for this model
        @header_for_this_report = grep {
            !/^(RunQ_Tactical|RunQ_Strategic|RunQ_Potential|RunQ_Source|ProjectionDays|GrowthMethod|GrowthConfidence|GrowthTrend|GrowthSignificance)$/
        } @header_for_this_report;
        # Find ent_idx again after grep
        ($ent_idx) = grep { $header_for_this_report[$_] eq 'Current - ENT' } 0..$#header_for_this_report;
        $ent_idx //= scalar(@header_for_this_report); # Fallback
        my @new_cols = ("TrueBaseline_P99W1", "PredictedPeak_P99W1", "ForecastSource");
        splice @header_for_this_report, $ent_idx, 0, @new_cols;
        $formula_col_offset = scalar(@new_cols);
    }

    print {$out_fh} join(",", map { quote_csv($_) } @header_for_this_report) . "\n";

    my $excel_row_num_counter = 1;

    foreach my $vm_name (@vm_order) {
        $excel_row_num_counter++;
        my @data_row_csv;

        # Get the complete, final data for this VM from the assimilation map.
        my $vm_map_ref = $assimilation_map_ref->{$vm_name};
        my $cfg = $vm_map_ref->{Configuration};
        my $core = $vm_map_ref->{CoreResults};
        my $hinting = $vm_map_ref->{Hinting};
        my $modifiers = $vm_map_ref->{CSVModifiers};

        # --- START: Hint-Aware Rationale Selection for CSV ---
        # Call generate_sizing_hint here, *once* per VM, to get the
        # authoritative hint. This is now safe, as the assimilation map is fully built.
        # We use a safe check for $vm_name in %vm_config_data.
        my ($hint_type_tier, $hint_pattern_shape, $pressure_bool, $pressure_detail_str);

        if (exists $vm_config_data{$vm_name}) {
            ($hint_type_tier, $hint_pattern_shape, $pressure_bool, $pressure_detail_str) =
                generate_sizing_hint($vm_map_ref, undef, $adaptive_runq_saturation_thresh);
        } else {
            # VM is not in config file; fall back to safe defaults
            ($hint_type_tier, $hint_pattern_shape, $pressure_bool, $pressure_detail_str) = ('G3', 'G', 0, 'ConfigMissing');
        }

        ## Deterministic fallback chain for profile selection for growth and for the RunQ modifier columns

        # Map the hint pattern (O, B, G) to the correct T3 planning profile
        my $hint_pattern = ($hinting->{AutoTier} =~ /^([A-Z])/) ? $1 : 'G';
        my %pattern_to_profile_map = ('O'=>'O3-95W15', 'B'=>'B3-95W15', 'G'=>'G3-95W15', 'P'=>'G3-95W15');

        my $source_profile_name = $pattern_to_profile_map{$hint_pattern} // 'G3-95W15';

        # --- Populate CSV row directly from the map ---
        push @data_row_csv, $vm_name, ($tier_override_for_csv{$vm_name} // "");
        # Use the hints previously generated
        push @data_row_csv, $hint_type_tier, $hint_pattern_shape;
        push @data_row_csv, ($pressure_bool ? "True" : "False"), $pressure_detail_str // "";
        push @data_row_csv, $cfg->{smt} // "";
        push @data_row_csv, $cfg->{serial_number} // "", $vm_config_data{$vm_name}{systemtype} // ""; # systemtype from old config is richer
        push @data_row_csv, $vm_config_data{$vm_name}{pool_name} // "", $cfg->{pool_id} // "";

        unless ($is_multiplicative_run_flag || $is_predictive_peak_run_flag) {
            # This now correctly fetches the single, authoritative values for the VM (selecting the same profile name as $source_profile_name)
            push @data_row_csv, (defined $modifiers->{RunQ_Tactical} ? sprintf("%+.3f", $modifiers->{RunQ_Tactical}) : "0.000");
            push @data_row_csv, (defined $modifiers->{RunQ_Strategic} ? sprintf("%+.3f", $modifiers->{RunQ_Strategic}) : "0.000");
            push @data_row_csv, (defined $modifiers->{RunQ_Potential} && looks_like_number($modifiers->{RunQ_Potential}) ? sprintf("%+.3f", $modifiers->{RunQ_Potential}) : ($modifiers->{RunQ_Potential} // "0.000"));
            push @data_row_csv, $modifiers->{RunQ_Source} // "";
        }

        # Push the Peak value, ensuring 3-digit precision
        my $peak_val = $core->{PeakValue};
        push @data_row_csv, (defined $peak_val && looks_like_number($peak_val)) ? sprintf("%.3f", $peak_val) : ($peak_val // "");

        # Push all profile values with enforced 3-digit precision
        foreach my $profile (@csv_visible_profiles) {
            my $val = $core->{ProfileValues}{$profile->{name}};
            my $formatted_val = (defined $val && looks_like_number($val)) ? sprintf("%.3f", $val) : ($val // "");
            push @data_row_csv, $formatted_val;
        }

        # Model-specific columns
        if ($is_multiplicative_run_flag) {
            my $s_data = $seasonal_debug_info{$vm_name}{$MANDATORY_PEAK_PROFILE_FOR_HINT} || {};
            push @data_row_csv, (defined $s_data->{multiplier} ? sprintf("%.2f", $s_data->{multiplier}) : "N/A");
            push @data_row_csv, (defined $s_data->{baseline} ? sprintf("%.3f", $s_data->{baseline}) : "N/A");
        } elsif ($is_recency_decay_run_flag || $nfit_enable_windowed_decay || $nfit_decay_over_states) {

            # 1. Determine the source profile for growth rationale.
            #    Priority: User TIER > AutoTier > Fallback to 'G'.
            my $user_tier_override_csv = $tier_override_for_csv{$vm_name} // "";
            my $auto_tier_csv = $hinting->{AutoTier} // "G";

            my $pattern_source_csv = ($user_tier_override_csv ne "") ? $user_tier_override_csv : $auto_tier_csv;
            my ($pattern_csv) = ($pattern_source_csv =~ /^([A-Z])/);
            $pattern_csv //= 'G'; # Default to 'G' if regex fails

            # 1. Map the pattern to the corresponding 95th percentile (T3) profile.
            my %pattern_to_profile_map_csv = ('O' => 'O3-95W15', 'B' => 'B3-95W15', 'G' => 'G3-95W15', 'P' => 'G3-95W15');
            my $GROWTH_RATIONALE_SOURCE_PROFILE = $pattern_to_profile_map_csv{$pattern_csv} // 'G3-95W15';

            # Growth Modifier Column Logic with Fallback
            # 2. Attempt to get growth rationale from primary source profile
            my $gr = _safe_dig($vm_map_ref, 'GrowthRationaleByProfile', $GROWTH_RATIONALE_SOURCE_PROFILE) || {};

            # 2a. Check if the primary profile had a computational failure
            my $is_computational_failure = 0;
            if (exists $gr->{skip_reason} && defined $gr->{skip_reason}) {
                # Check for insufficient data
                $is_computational_failure = 1 if ($gr->{skip_reason} =~ /Insufficient daily data points/i);
            } elsif (exists $gr->{hamed_rao_adjustment_factor} &&
                     defined $gr->{hamed_rao_adjustment_factor} &&
                     $gr->{hamed_rao_adjustment_factor} <= 0) {
                # Check for Hamed-Rao corruption
                $is_computational_failure = 1;
            } elsif (exists $gr->{method_used} &&
                     $gr->{method_used} ne 'none' &&
                     !defined $gr->{sen_slope}) {
                # Calculation attempted but critical metrics missing
                $is_computational_failure = 1;
            }

            # 2b. If computational failure and not already using G3-95W15, fallback to G3-95W15
            my $fallback_applied = 0;
            if ($is_computational_failure && $GROWTH_RATIONALE_SOURCE_PROFILE ne 'G3-95W15') {
                if ($verbose) {
                    warn sprintf(
                        "INFO: Growth calculation failed for %s on VM %s (reason: %s). " .
                        "Falling back to G3-95W15.\n",
                        $GROWTH_RATIONALE_SOURCE_PROFILE,
                        $vm_name,
                        $gr->{skip_reason} // 'computational error'
                    );
                }

                # Attempt fallback to G3-95W15
                my $fallback_gr = _safe_dig($vm_map_ref, 'GrowthRationaleByProfile', 'G3-95W15') || {};

                # Only use fallback if it succeeded
                my $fallback_failed = 0;
                if (exists $fallback_gr->{skip_reason} && $fallback_gr->{skip_reason} =~ /Insufficient daily data points/i) {
                    $fallback_failed = 1;
                } elsif (exists $fallback_gr->{hamed_rao_adjustment_factor} &&
                         defined $fallback_gr->{hamed_rao_adjustment_factor} &&
                         $fallback_gr->{hamed_rao_adjustment_factor} <= 0) {
                    $fallback_failed = 1;
                }

                if (!$fallback_failed) {
                    # Fallback succeeded
                    $gr = $fallback_gr;
                    $GROWTH_RATIONALE_SOURCE_PROFILE = 'G3-95W15';
                    $fallback_applied = 1;
                } else {
                    # Both failed - keep original failure for transparency
                    if ($verbose) {
                        warn sprintf(
                            "WARNING: Fallback to G3-95W15 also failed for VM %s. " .
                            "No growth adjustment will be applied.\n",
                            $vm_name
                        );
                    }
                }
            }

            # 3. Get the specific GrowthAdj for this source profile (for the 'GrowthAdj' column)
            my $source_profile_growth_adj = _safe_dig($vm_map_ref, 'Growth', 'adjustments', $GROWTH_RATIONALE_SOURCE_PROFILE) // 0;

            # 4. Get the VM-wide Min/Max adjustments (from all profiles)
            my $growth_min = $vm_map_ref->{Growth}{min_adj} // 0;
            my $growth_max = $vm_map_ref->{Growth}{max_adj} // 0;

            # 5. Format and push the new columns
            my $source_adj_str = sprintf("%.3f", $source_profile_growth_adj);
            my $min_str = sprintf("%.3f", $growth_min);
            my $max_str = sprintf("%.3f", $growth_max);

            push @data_row_csv, $source_adj_str, $min_str, $max_str, $GROWTH_RATIONALE_SOURCE_PROFILE;

            # 6. Populate all other rationale fields from the source profile's rationale
            #    (This logic is now sourced from $gr, not $source_profile_name)
            my $proj_days = $gr->{projection_days};
            if (defined $proj_days && looks_like_number($proj_days)) {
                push @data_row_csv, int($proj_days);  # Integer format
            } else {
                push @data_row_csv, 'n/a';  # No growth calculation performed
            }

            # GrowthMethod
            my $method = $gr->{method_used} // 'none';
            push @data_row_csv, $method;

            # GrowthConfidence
            my $p_value = $gr->{sen_p_value};
            push @data_row_csv, map_growth_confidence($p_value, $method);

            # GrowthTrend
            # Ensure 'trend' is 'none' if method wasn't a significant trend
            my $trend = $gr->{sen_trend} // 'none';
            if ($method eq 'sen_slope_not_significant' || $method eq 'sen_slope_too_small' || $method eq 'none') {
                $trend = 'none';
            }
            push @data_row_csv, $trend;

            # GrowthSignificance (the p-value itself)
            if (defined $p_value && $method ne 'none') {
                # Format p-value to 4 decimal places, handling p=0
                push @data_row_csv, ($p_value == 0) ? "0.0000" : sprintf("%.4f", $p_value);
            } else {
                push @data_row_csv, 'n/a';
            }
            # --- END: Growth Trend Columns ---

        } elsif ($is_predictive_peak_run_flag) {
            my $s_data = $seasonal_debug_info{$vm_name}{'P-99W1'} || {};
            my $source = $s_data->{FinalSource} || 'N/A';
            if ($source eq 'PeakPrediction') { $source = 'PredictedPeak'; }
            elsif ($source eq 'TrueBaseline') { $source = 'BaselineIsHigher'; }
            push @data_row_csv, (defined $s_data->{TrueBaseline} ? sprintf("%.3f", $s_data->{TrueBaseline}) : "N/A");
            push @data_row_csv, (defined $s_data->{PredictedPeak} ? sprintf("%.3f", $s_data->{PredictedPeak}) : "N/A");
            push @data_row_csv, $source;
        }

        # Entitlement and formula columns
        my $ent_out = $cfg->{entitlement};
        my $current_ent_display = (looks_like_number($ent_out)) ? sprintf("%.2f", $ent_out) : ($ent_out // "");
        push @data_row_csv, $current_ent_display; # Always add Current - ENT
        if ($add_excel_formulas) {
            my $nfit_ent_formula_str = generate_nfit_ent_formula($excel_row_num_counter, scalar(@csv_visible_profiles), $formula_col_offset);
            my $col_nfit_ent_letter = get_excel_col_name(scalar(@output_header_cols_csv) - 2 + $formula_col_offset);
            my $col_curr_ent_letter = get_excel_col_name(scalar(@output_header_cols_csv) - 3 + $formula_col_offset);
            my $nett_user_formula_str = sprintf("=(%s%d-%s%d)", $col_nfit_ent_letter, $excel_row_num_counter, $col_curr_ent_letter, $excel_row_num_counter);
            my $nett_perc_user_formula_str = sprintf("=IFERROR((%s%d-%s%d)/%s%d,\"\")", $col_nfit_ent_letter, $excel_row_num_counter, $col_curr_ent_letter, $excel_row_num_counter, $col_curr_ent_letter, $excel_row_num_counter);
            push @data_row_csv, $nfit_ent_formula_str, $nett_user_formula_str, $nett_perc_user_formula_str;
        }

         print {$out_fh} join(",", map { quote_csv($_) } @data_row_csv) . "\n";
    }

    if ($add_excel_formulas) {
        my %serials_map;
        foreach my $vm_name (@vm_order) {
            my $serial = $assimilation_map_ref->{$vm_name}{Configuration}{serial_number};
            $serials_map{$serial} = 1 if (defined $serial && $serial ne '');
        }
        my @sorted_serials = sort keys %serials_map;
        print_csv_footer($out_fh, $excel_row_num_counter, $physc_data_file, scalar(@csv_visible_profiles), \@sorted_serials, $formula_col_offset);
    }

    close $out_fh;
}

# ==============================================================================
# Subroutine to format a duration in seconds into a human-readable string.
# ==============================================================================
sub format_duration {
    my ($seconds) = @_;
    if ($seconds >= 3600) {
        my $hours = int($seconds / 3600);
        my $minutes = int(($seconds % 3600) / 60);
        return sprintf("%dh %dm", $hours, $minutes);
    }
    return sprintf("%.2fs", $seconds) if $seconds < 60;
    my $minutes = int($seconds / 60);
    my $remaining_seconds = $seconds % 60;
    return sprintf("%dm %.2fs", $minutes, $remaining_seconds);
}

# Helper function to add months to a Time::Piece object
sub add_months {
    my ($time_obj, $months) = @_;

    my $year = $time_obj->year;
    my $month = $time_obj->mon + $months;
    my $day = $time_obj->mday;

    # Handle year rollover
    while ($month > 12) {
        $month -= 12;
        $year++;
    }
    while ($month < 1) {
        $month += 12;
        $year--;
    }

    # Handle day overflow (e.g., Jan 31 + 1 month should be Feb 28/29)
    my $days_in_month = days_in_month($year, $month);
    if ($day > $days_in_month) {
        $day = $days_in_month;
    }

    return Time::Piece->strptime(sprintf('%04d-%02d-%02d', $year, $month, $day), '%Y-%m-%d');
}

# Helper function to get days in a month
sub days_in_month {
    my ($year, $month) = @_;
    my @days = (31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31);

    # Check for leap year
    if ($month == 2 && (($year % 4 == 0 && $year % 100 != 0) || $year % 400 == 0)) {
        return 29;
    }

    return $days[$month - 1];
}

# ==============================================================================
# Calculates the start and end dates for a recurring monthly event based on a
# given reference date. It correctly handles month-end boundaries and invalid
# day-of-month configurations.
# ==============================================================================
sub _get_recurring_monthly_period {
    my ($base_date_obj, $day_of_period, $duration_days) = @_;

    my $end_date_obj;
    if ($day_of_period == -1) {
        # Last day of month: get first day of next month, then subtract one day
        my $next_month = add_months($base_date_obj, 1);
        my $first_of_next = Time::Piece->strptime(sprintf('%04d-%02d-01', $next_month->year, $next_month->mon), '%Y-%m-%d');
        $end_date_obj = $first_of_next - ONE_DAY;
    } else {
        # Specific day of month, clamped to be valid for that month
        my $year  = $base_date_obj->year;
        my $month = $base_date_obj->mon;
        my $day   = $day_of_period;

        my $days_in_target_month = days_in_month($year, $month);
        if ($day > $days_in_target_month) {
            # Clamp to the last day of the month if config is invalid (e.g., day 31 in Feb)
            $day = $days_in_target_month;
        }
        $end_date_obj = Time::Piece->strptime(sprintf('%04d-%02d-%02d', $year, $month, $day), '%Y-%m-%d');
    }

    # This calculation remains the same
    my $start_date_obj = $end_date_obj - (ONE_DAY * ($duration_days - 1));
    return ($start_date_obj, $end_date_obj);
}

# ==============================================================================
# Dedicated reporter for the multiplicative_seasonal model. It creates a
# single, rich final_forecast.csv file with all standard columns, the new
# SeasonalMultiplier column, and correctly offset Excel formulas.
# ==============================================================================
sub _write_multiplicative_seasonal_report {
    my ($system_id, $event_name, $timestamp, $forecast_data_href, $assimilation_map_ref) = @_;

    my $s_id_safe = $system_id;
    my $e_name_safe = $event_name;
    $s_id_safe =~ s/[^a-zA-Z0-9_.-]//g;
    $e_name_safe =~ s/[^a-zA-Z0-9_.-]//g;

    my $filename = File::Spec->catfile($output_dir, "nfit-profile.$s_id_safe.$e_name_safe.final_forecast.$timestamp.csv");
    push @generated_files, $filename;
    print "  - Generating output file: $filename\n";

    open my $fh, '>', $filename or die "Warning: Could not open output file '$filename': $!";

    # Build the enhanced header with the new SeasonalMultiplier column
    my @header = (
        "VM", "TIER", "Hint", "Pattern", "Pressure", "PressureDetail", "SMT",
        "Serial", "SystemType", "Pool Name", "Pool ID", $PEAK_PROFILE_NAME
    );
    push @header, (map { $_->{name} } @profiles);
    push @header, ("SeasonalMultiplier", "Current - ENT", "NFIT - ENT", "NETT", "NETT%");
    print $fh join(",", map { quote_csv($_) } @header) . "\n";

    my $excel_row_num_counter = 1;
    foreach my $vm_name (sort keys %$forecast_data_href) {
        $excel_row_num_counter++;
        my $vm_data = $forecast_data_href->{$vm_name};
        my $report_data = $vm_data->{_report_data} || {};
        my $cfg_csv = $vm_config_data{$vm_name};

        # Get the complete, final data for this VM from the assimilation map.
        my $vm_map_ref = $assimilation_map_ref->{$vm_name};
        my $cfg = $vm_map_ref->{Configuration};
        my $core = $vm_map_ref->{CoreResults};
        my $hinting = $vm_map_ref->{Hinting};
        my $modifiers = $vm_map_ref->{CSVModifiers};

        # Gather standard fields for the row
        my $smt_out = $cfg_csv->{smt} // $default_smt_arg;
        my ($serial_out, $systype_out, $poolname_out, $poolid_out, $ent_out) = ('', '', '', '', '');
        if (defined $cfg_csv) {
            $serial_out = $cfg_csv->{serial} // ''; $systype_out = $cfg_csv->{systemtype} // '';
            $poolname_out = $cfg_csv->{pool_name} // ''; $poolid_out = $cfg_csv->{pool_id} // '';
            $ent_out = $cfg_csv->{entitlement} // '';
        }

        my @row = (
            $vm_name, "", $report_data->{hint} // "", $report_data->{pattern} // "",
            $report_data->{pressure} // "", $report_data->{pressure_detail} // "", $smt_out,
            $serial_out, $systype_out, $poolname_out, $poolid_out, ""
        );

        my $seasonal_multiplier_for_row = "N/A";
        foreach my $profile (@profiles) {
            my $p_name = $profile->{name};
            my $value = $vm_data->{$p_name} // '';
            push @row, (looks_like_number($value) ? sprintf("%.4f", $value) : $value);
            if ($seasonal_multiplier_for_row eq 'N/A' && exists $report_data->{seasonal_debug}{$p_name}) {
                $seasonal_multiplier_for_row = sprintf("%.2f", $report_data->{seasonal_debug}{$p_name}{multiplier});
            }
        }

        # Add the new SeasonalMultiplier and standard entitlement columns
        push @row, $seasonal_multiplier_for_row;
        push @row, (looks_like_number($ent_out) ? sprintf("%.2f", $ent_out) : $ent_out);

        # Generate the Excel formulas with the correct column offset
        my $nfit_ent_formula_str = generate_nfit_ent_formula($excel_row_num_counter, scalar(@profiles), 1); # <-- Pass offset of 1
        my $nett_user_formula_str = sprintf("=(%s-%s)", get_excel_col_name(13 + scalar(@profiles) + 2) . $excel_row_num_counter, get_excel_col_name(12 + scalar(@profiles) + 2) . $excel_row_num_counter);
        my $nett_perc_user_formula_str = sprintf("=IFERROR((%s-%s)/%s,\"\")", get_excel_col_name(13 + scalar(@profiles) + 2) . $excel_row_num_counter, get_excel_col_name(12 + scalar(@profiles) + 2) . $excel_row_num_counter, get_excel_col_name(12 + scalar(@profiles) + 2) . $excel_row_num_counter);
        push @row, $nfit_ent_formula_str, $nett_user_formula_str, $nett_perc_user_formula_str;

        print $fh join(",", map { quote_csv($_) } @row) . "\n";
    }
    close $fh;
}

# ==============================================================================
# Subroutine to log the rationale for a multiplicative seasonal forecast.
# It provides a clear, aligned, and explanatory summary of how the forecast
# was derived for each profile, matching the format of the standard log.
# ==============================================================================
sub log_multiplicative_seasonal_rationale
{
    my ($fh) = @_;

    # Ensure the script does not die if the log file handle is not valid.
    return unless $fh;

    # Iterate through each VM that has results, maintaining a consistent order.
    foreach my $vm_name (sort @vm_order)
    {
        # Check if there is seasonal debug information available for this VM.
        next unless exists $seasonal_debug_info{$vm_name};

        # Print a clear, top-level header for each VM in the log.
        print {$fh} "\n######################################################################\n";
        print {$fh} "# Rationale for VM: $vm_name\n";
        print {$fh} "######################################################################\n\n";
        print {$fh} "CPU Sizing Path: Multiplicative Seasonal Forecast (Event: $apply_seasonality_event)\n\n";

        # Iterate through each profile to log its specific forecast calculation.
        foreach my $profile (@profiles)
        {
            my $p_name = $profile->{name};
            next unless exists $seasonal_debug_info{$vm_name}{$p_name};

            my $s_data = $seasonal_debug_info{$vm_name}{$p_name};
            my $profile_desc = parse_profile_name_for_log($p_name);

            # Use a fixed width for labels to ensure consistent alignment of colons.
            my $label_width = 35;

            print {$fh} "======================================================================\n";
            printf {$fh} "%-${label_width}s : %s\n", "Profile Processed", $profile_desc;
            print {$fh} "----------------------------------------------------------------------\n";
            printf {$fh} "  %-${label_width}s : %.4f cores\n", "Current Baseline Value", $s_data->{baseline};
            printf {$fh} "  %-${label_width}s : %.4f\n", "Historical Multiplier", $s_data->{multiplier};

            # Add detailed breakdown of how the multiplier was derived
            if (exists $s_data->{historical_multipliers} && ref($s_data->{historical_multipliers}) eq 'ARRAY' && @{$s_data->{historical_multipliers}}) {
                printf {$fh} "  %-${label_width}s : %s\n", "  Multiplier Methodology", "Recency-weighted average of past events:";
                foreach my $hist_entry (@{$s_data->{historical_multipliers}}) {
                    my $hist_date = ref($hist_entry->{date}) ? $hist_entry->{date}->date : $hist_entry->{date};
                    printf {$fh} "  %-${label_width}s   %s: %.4f\n", "", $hist_date, $hist_entry->{value};
                }
            }

            printf {$fh} "  %-${label_width}s : %.4f\n", "Volatility Buffer", $s_data->{volatility};
            printf {$fh} "  %-${label_width}s : %.4f cores\n", "Forecasted Peak Residual", $s_data->{forecasted_residual};
            printf {$fh} "  %-${label_width}s : %.2f\n", "Peak Amplification Factor", $s_data->{amplification_factor};
            print {$fh} "  --------------------------------------------------------------------\n";
            print {$fh} "  Calculation : ((Baseline * Multiplier * Buffer) + Residual) * Amplification\n";
            print {$fh} "  --------------------------------------------------------------------\n";
            print {$fh} "  --------------------------------------------------------------------\n";
            printf {$fh} "  %-${label_width}s : %.4f cores\n", "Final Forecasted Value", $s_data->{forecast};
            # --- Print outlier warning if it exists ---
            if (defined $s_data->{OutlierWarning} && $s_data->{OutlierWarning} ne '') {
                print {$fh} "\n  --- Workload Volatility Alert ---\n";
                print {$fh} "  " . $s_data->{OutlierWarning} . "\n";
            }
            print {$fh} "======================================================================\n\n";
        }
    }
}

# ==============================================================================
# Subroutine to determine the correct seasonal analysis path.
# It checks if a multiplicative model has enough historical data to run. If not,
# it switches to the defined fallback event. This makes the tool resilient.
# It returns the name of the event that should ultimately be executed.
# ==============================================================================
sub determine_seasonal_analysis_path {
    my ($event_config, $system_cache_dir, $event_name) = @_;

    # This function is only relevant for models that have historical prerequisites.
    my $model_to_run = $event_config->{model} // '';
    return $event_name unless ($model_to_run eq 'multiplicative_seasonal' || $model_to_run eq 'predictive_peak');

    my $min_history_required = $event_config->{min_historical_years} || 1;

    # Read the unified history to count available snapshots for this event.
    my $unified_history = read_unified_history($system_cache_dir);
    my $history_count = 0;
    foreach my $month_data (values %$unified_history) {
        if (exists $month_data->{SeasonalEventSnapshots}{$event_name}) {
            $history_count++;
        }
    }

    if ($history_count >= $min_history_required) {
        print STDERR "INFO: Found $history_count historical snapshot(s) for event '$event_name'. Proceeding with forecast.\n";
        return $event_name;
    } else {
        my $fallback_event_name = $event_config->{fallback_event} // '';
        print STDERR "\nWARNING: Insufficient historical data for '$event_name' forecast.\n";
        print STDERR "  - Required historical snapshots: $min_history_required\n";
        print STDERR "  - Found: $history_count\n";

        if ($fallback_event_name ne '' && exists $seasonality_config->{$fallback_event_name}) {
            print STDERR "  - Executing fallback event: '$fallback_event_name'\n\n";
            return $fallback_event_name;
        } else {
            die "FATAL: Cannot run forecast for '$event_name' due to insufficient history, and no 'fallback_event' is configured.\n";
        }
    }
}

# ==============================================================================
# SUBROUTINE: _build_nfit_baseline_command
# PURPOSE:    Constructs the specific nfit command for calculating a baseline
#             for a seasonal model. It ensures a consistent command is built
#             and that any conflicting decay or growth flags from the profile
#             are removed. A baseline must be a pure measurement.
# ARGS:
#   1. $profile_flags_in (string): The raw flags from the profile config.
#   2. $baseline_start_str (string): The start date for the analysis.
#   3. $baseline_end_str (string): The end date for the analysis.
#   4. $system_cache_dir (string): Path to the target cache directory.
#   5. $enable_clipping_detection (boolean, optional): If true, adds the
#      --enable-clipping-detection flag to the command.
#   6. $is_generic_baseline    : if true, all decay and time filters are stripped.
#   7. $allow_growth_prediction: if true, growth predictions will be enabled.
#   8. $profile_name_for_label : profile name to record in the results cache (if specified)
# RETURNS:
#   - The fully constructed nfit command string.
# ==============================================================================
sub _build_nfit_baseline_command {
    my ($profile_flags_in, $baseline_start_str, $baseline_end_str, $system_cache_dir, $enable_clipping_detection, $is_generic_baseline, $allow_growth_prediction, $profile_name_for_label) = @_;

    my $profile_flags = $profile_flags_in; # Work on a copy.

    # --- NEW: Sanitise incoming flags to remove extraneous quotes from config files.
    # This is the primary fix for the "Unknown option" error.
    $profile_flags =~ s/^\s*"?|"?\s*$//g;

    # A baseline is a historical measurement, not a forecast. Growth prediction
    # should almost always be stripped, EXCEPT for the multiplicative model's
    # "CurrentBaseline", which needs to reflect the true current trend.
    unless ($allow_growth_prediction) {
        $profile_flags =~ s/--enable-growth-prediction\s*//g;
        $profile_flags =~ s/--growth-projection-days\s+\d+\s*//g;
        $profile_flags =~ s/--max-growth-inflation-percent\s+\d+\s*//g;
    }

    # The --enable-windowed-decay Model is a trending analysis:
    # Applying a trending model on top of a period that is supposed to be a static baseline measurement would be logically incorrect.
    $profile_flags =~ s/--enable-windowed-decay\s*//g;

    if ($is_generic_baseline) {
        $profile_flags =~ s/--(?:online|batch|no-weekends)\b\s*//g;
        $profile_flags =~ s/--decay\s+[\w-]+\s*//g;
        $profile_flags =~ s/--runq-decay\s+[\w-]+\s*//g;
        $profile_flags =~ s/--avg-method\s+\w+\s*//g;
    }
    $profile_flags =~ s/--decay-over-states\s*//g;

    # Construct the base command, ensuring --nmondir is always present.
    # the '-k' (Peak) flag is always included for statistical history.
    my $base_flags = "-q -k --nmondir \"$system_cache_dir\" $rounding_flags_for_nfit";

    # Only add date filters if they are actually defined.
    # They can be added independently - not required to have both.
    $base_flags .= " --startdate $baseline_start_str" if defined $baseline_start_str;
    $base_flags .= " --enddate $baseline_end_str" if defined $baseline_end_str;

    my $vm_filter_arg = defined($target_vm_name) ? " -vm \"$target_vm_name\"" : "";
    my $smt_flag = "--smt $default_smt_arg";

    # Assemble the final, correct command.
    my $command = "$nfit_script_path $base_flags $vm_filter_arg $smt_flag $profile_flags";

    # Add clipping detection if requested
    if ($enable_clipping_detection) {
        $command .= " --enable-clipping-detection";
    }

    # Append the profile label if provided. This is for metadata and does not affect the L2 cache key.
    if (defined $profile_name_for_label && $profile_name_for_label ne '') {
        $command .= " --profile-label '$profile_name_for_label'";
    }

    return $command;
}

# ==============================================================================
# Main orchestrator for the 'predictive_peak' model.
# This version is enhanced to use the new two-part residual forecasting method.
# ==============================================================================
sub calculate_predictive_peak_forecast {
    my ($system_cache_dir, $system_identifier, $event_name, $event_config, $full_seasonality_config, $adaptive_runq_saturation_thresh) = @_;

    # --- Step 1: Get Historical Peak and Residual Data ---
    # Calls the enhanced helper to get a hash containing both data series.
    my $historical_data_href = _get_historical_peak_data($system_cache_dir, $event_name, $event_config);

    # --- Step 2: Calculate the Predicted Peak and Residual for each profile ---
    # Calls the enhanced prediction engine to get forecasts for both series.
    print STDERR " -> Step 2/3: Performing linear regression to predict next peak intensity and residual.\n";
    my ($predicted_components_href, $debug_info_for_vms) =
        _calculate_peak_prediction($historical_data_href, $event_config);

    # --- Step 3: Calculate the Non-Peak Baseline (excluding all historical peaks) ---
    # This logic remains unchanged.
    my $true_baseline_results_href = _get_true_baseline_results($system_cache_dir, $event_name, $event_config, $full_seasonality_config);

    # --- Step 4: Synthesize Final Results ---
    print STDERR " -> Step 3/3: Synthesizing final forecast from Predicted Peak, Predicted Residual, and Baseline.\n";
    my %final_results;
    foreach my $vm_name (keys %{$true_baseline_results_href}) {
        foreach my $profile (@profiles) {
            my $p_name = $profile->{name};
            my $baseline_val = $true_baseline_results_href->{$vm_name}{$p_name};

            # Get the two predicted components from the prediction engine.
            my $predicted_peak_val = $predicted_components_href->{$vm_name}{$p_name}{peak};
            my $predicted_residual_val = $predicted_components_href->{$vm_name}{$p_name}{residual};

            # Combine the signal and volatility forecasts. Undefined values are treated as zero.
            my $combined_prediction;
            if (defined $predicted_peak_val) {
                $combined_prediction = ($predicted_peak_val // 0) + ($predicted_residual_val // 0);
            }

            # The final recommendation is the higher of the baseline or the combined prediction.
            my ($final_value, $source) = (0, 'N/A');
            if (defined $baseline_val && defined $combined_prediction) {
                if ($baseline_val > $combined_prediction) {
                    $final_value = $baseline_val;
                    $source = 'TrueBaseline';
                } else {
                    $final_value = $combined_prediction;
                    $source = 'PeakPrediction';
                }
            } elsif (defined $combined_prediction) {
                $final_value = $combined_prediction;
                $source = 'PeakPrediction';
            } elsif (defined $baseline_val) {
                $final_value = $baseline_val;
                # This case indicates that prediction failed (e.g., insufficient history).
                $source = 'BaselineOnly (NoPrediction)';
            }

            # Apply the optional amplification factor
            my $amplification = $event_config->{peak_amplification_factor} // 1.0;
            my $final_recommendation = $final_value * $amplification;

            $final_results{$vm_name}{$p_name} = $final_recommendation;

            # Store all components in the debug hash for comprehensive logging.
            $seasonal_debug_info{$vm_name}{$p_name} = {
                TrueBaseline       => $baseline_val,
                PredictedPeak      => $predicted_peak_val,
                PredictedResidual  => $predicted_residual_val,
                CombinedPrediction => $combined_prediction,
                FinalSource        => $source,
                AmplificationFactor  => $amplification,
                FinalForecast        => $final_recommendation,
                PredictionDebug    => $debug_info_for_vms->{$vm_name}{$p_name},
                OutlierWarning     => _get_warning_for_vm_forecast($vm_name, \%outlier_warnings)
            };
        }
    }
    return \%final_results;
}

# ==============================================================================
# Helper to run nfit and get a "True/Non-Peak Baseline" by excluding all historical peak periods.
#
# ==============================================================================
sub _get_true_baseline_results {
    my ($system_cache_dir, $event_name, $event_config, $full_seasonality_config) = @_;

    my $data_cache_file = File::Spec->catfile($system_cache_dir, '.nfit.cache.data');
    return {} unless -f $data_cache_file;

    # --- Stage 1: Build a hash of dates to exclude ---
    my @all_peak_periods = find_all_historical_periods($full_seasonality_config, $data_cache_file);

    my %days_to_exclude_hash;
    my $total_days_to_process = 0;

    # Pre-calculate total days for an accurate progress bar and validation
    foreach my $period (@all_peak_periods) {
        next unless (defined $period && ref($period) eq 'ARRAY' && @$period == 2);
        my ($start, $end) = @{$period};
        next if ($start > $end);
        $total_days_to_process += (int(($end->epoch - $start->epoch) / 86400) + 1);
    }

    if ($total_days_to_process > 0) {
        print STDERR "  - Identifying $total_days_to_process unique peak day(s) for exclusion filter...\n";

        # Scope all counters locally to prevent bugs across function calls
        my $days_processed = 0;
        my $last_reported_perc = -1;
        my $period_num = 0;

        foreach my $period (@all_peak_periods) {
            $period_num++;
            next unless (defined $period && ref($period) eq 'ARRAY' && @$period == 2);
            my ($start, $end) = @{$period};
            next if ($start > $end);

            my $current = Time::Piece->new($start->epoch);
            while ($current <= $end) {

                $days_to_exclude_hash{$current->strftime('%Y-%m-%d')} = 1;
                $current += ONE_DAY;

                $days_processed++;

                my $current_perc = int(($days_processed / $total_days_to_process) * 100);
                if ($current_perc > $last_reported_perc) {
                    printf STDERR "\r    Expanding days: %d%% (%d/%d)", $current_perc, $days_processed, $total_days_to_process;
                    $last_reported_perc = $current_perc;
                }
            }
        }
        print STDERR "\n";
    }

    # --- Stage 2: Create the filtered data using the efficient hash lookup ---
    my ($filtered_fh, $filtered_filename) = tempfile(UNLINK => 1);

    my $unique_days_to_exclude = scalar(keys %days_to_exclude_hash);
    if ($unique_days_to_exclude > 0) {
        print STDERR "  - Filtering $unique_days_to_exclude day(s) from baseline data...";

        open(my $cache_fh, '<', $data_cache_file) or die "Cannot open $data_cache_file: $!";
        my $header = <$cache_fh>;
        print $filtered_fh $header;

        my $line_count = 0;
        while (my $line = <$cache_fh>) {
            $line_count++;
            print STDERR "." if $line_count % 500000 == 0;

            my $line_date = substr($line, 0, 10);
            next if exists $days_to_exclude_hash{$line_date};

            print $filtered_fh $line;
        }
        print STDERR " done\n";

        close $cache_fh;
    } else {
        print STDERR "  - No peak periods to filter; using full data cache for baseline.\n";
        system("cp '$data_cache_file' '$filtered_filename'");
    }

    close $filtered_fh;

    # --- Stage 3: Run nfit on the filtered file for each profile ---
    my %baseline_results;
    my $profile_count = scalar(@profiles);
    my $profile_num = 0;
    foreach my $profile (@profiles) {
        $profile_num++;
        print STDERR "    - Establishing Generic Non-Peak Baseline for profile $profile_num/$profile_count: $profile->{name}\n";

        # For a "True Baseline", we need a generic, unfiltered result. Enable generic baseline (no decay or time filters)
        my $nfit_cmd = _build_nfit_baseline_command($profile->{flags}, undef, undef, $system_cache_dir, 0, 1, undef, $profile->{name});

        my $nfit_output = '';
        # Use open3 to capture STDOUT, but let STDERR pass through to the terminal
        my $stderr_arg = ">&=" . fileno(STDERR);
        my $pid_nfit = open3(undef, my $stdout_nfit, $stderr_arg, "$nfit_cmd --show-progress");
        while(my $line = <$stdout_nfit>) {
            $nfit_output .= $line;
        }
        waitpid($pid_nfit, 0);

        next if $?;

        my $parsed = parse_nfit_json_output($nfit_output);
        my $p_key = "P" . clean_perc_label( ($profile->{flags} =~ /-p\s+([0-9.]+)/) ? $1 : $DEFAULT_PERCENTILE );

        foreach my $vm (keys %$parsed) {
            # For aggregated/decay runs, nfit returns a single JSON object per VM
            my @states_for_vm = @{$parsed->{$vm}};
            next unless @states_for_vm;

            my @valid_p_values;
            foreach my $state_hash (@states_for_vm) {
                my $metric_val = $state_hash->{metrics}{physc}{$p_key};
                if (defined $metric_val && looks_like_number($metric_val)) {
                    push @valid_p_values, $metric_val;
                }
            }

            if (@valid_p_values) {
                my $aggregated_value = sum0(@valid_p_values) / scalar(@valid_p_values);
                $baseline_results{$vm}{$profile->{name}} = $aggregated_value;
            }
        }
    }
    return \%baseline_results;
}

# ==============================================================================
# SUBROUTINE: _get_historical_peak_data
# PURPOSE:    Reads the snapshot cache for a given event and returns time series
#             data for historical peaks and residuals. This version is enhanced
#             to use the 'unclippedPeakEstimate' from the ClippingInfo block
#             if it exists, ensuring the returned data is corrected for
#             historical saturation.
# ARGS:
#   1. $system_cache_dir (string): Path to the target cache directory.
#   2. $event_name (string): The name of the event to retrieve data for.
#   3. $event_config (hash ref): The configuration for the event.
# RETURNS:
#   - A hash reference containing two keys, 'peaks' and 'residuals', each
#     pointing to a hash of historical data series, structured by profile and VM.
# ==============================================================================
sub _get_historical_peak_data {
    my ($system_cache_dir, $event_name, $event_config) = @_;

    print STDERR " -> Step 1/3: Loading historical peak and residual data for event '$event_name'.\n";

    my $unified_history = read_unified_history($system_cache_dir);
    my @event_history;
    # Extract all historical snapshots for the specified event.
    foreach my $month_key (sort keys %$unified_history) {
        my $month_data = $unified_history->{$month_key};
        if (exists $month_data->{SeasonalEventSnapshots}{$event_name}) {
            my $snapshot = $month_data->{SeasonalEventSnapshots}{$event_name};
            $snapshot->{_month_key} = $month_key; # Add date context
            push @event_history, $snapshot;
        }
    }

    my $max_peaks = $event_config->{max_historical_peaks} // 12;
    if (@event_history > $max_peaks) {
        @event_history = @event_history[-$max_peaks..-1]; # Keep only the most recent N peaks.
    }

    my %peaks_by_profile;
    my %residuals_by_profile; # New hash for residual data.

    foreach my $event (@event_history) {
        my $date = $event->{_month_key} . "-01"; # Use the first of the month as the date for the time series.
        my $hist_results = $event->{results};
        my $peak_results = $event->{results}{'PeakValue'} || {};
        my $residual_results = $event->{results}{'PeakResidual'} || {};

        # Process PeakValue data.
        foreach my $vm (keys %$peak_results) {
            foreach my $profile (keys %{$peak_results->{$vm}}) {
                my $value = $peak_results->{$vm}{$profile};
                if (exists $hist_results->{ClippingInfo}{$vm}{$profile}{unclippedPeakEstimate}) {
                    $value = $hist_results->{ClippingInfo}{$vm}{$profile}{unclippedPeakEstimate};
                }
                if (defined $value && looks_like_number($value)) {
                    push @{$peaks_by_profile{$profile}{$vm}}, { value => $value, date => $date };
                }
            }
        }

        # Process PeakResidual data.
        foreach my $vm (keys %$residual_results) {
            foreach my $profile (keys %{$residual_results->{$vm}}) {
                my $value = $residual_results->{$vm}{$profile};
                if (defined $value && looks_like_number($value)) {
                    push @{$residuals_by_profile{$profile}{$vm}}, { value => $value, date => $date };
                }
            }
        }
    }

    # Return a hash containing both data series.
    return {
        peaks => \%peaks_by_profile,
        residuals => \%residuals_by_profile
    };
}

# ==============================================================================
# Core statistical engine for the predictive_peak model.
# This version orchestrates a dual forecast for both the peak signal and the residual.
# ==============================================================================
sub _calculate_peak_prediction {
    my ($historical_data_href, $event_config) = @_;

    my $peak_series_per_profile = $historical_data_href->{peaks} || {};
    my $residual_series_per_profile = $historical_data_href->{residuals} || {};

    my %predictions;
    my %debug_info;

    my $min_peaks = $event_config->{min_historical_peaks} // 3;

    # This anonymous subroutine is a reusable, local prediction engine.
    # It takes a time series and returns a single predicted next value.
    my $_predict_next_value = sub {
        my ($series_aref) = @_;

        return (undef, "Insufficient history")
            unless (defined $series_aref && ref($series_aref) eq 'ARRAY' && scalar(@$series_aref) >= 2);

        # Perform simple outlier trimming by removing the min and max values
        # if the series is long enough to support it.
        my @values = map { $_->{value} } @$series_aref;
        @values = sort { $a <=> $b } @values;
        if (@values > 5) {
            shift @values; # Remove lowest
            pop @values;   # Remove highest
        }

        # Build points for the linear regression.
        my @points_for_regression;
        for my $i (0..$#values) {
            push @points_for_regression, [$i, $values[$i]];
        }

        my $regression = calculate_manual_linear_regression(\@points_for_regression);
        unless ($regression) {
            return (undef, "Regression failed");
        }

        my $slope = $regression->{slope};
        my $intercept = $regression->{intercept};

        # Project one step into the future.
        my $projected_value = ($slope * scalar(@values)) + $intercept;

        my $debug_str = sprintf("Success (Slope: %.4f, Points used: %d)", $slope, scalar(@values));

        return ($projected_value, $debug_str);
    };

    foreach my $profile_name (keys %{$peak_series_per_profile}) {
        foreach my $vm_name (keys %{$peak_series_per_profile->{$profile_name}}) {

            my $peak_series_aref = $peak_series_per_profile->{$profile_name}{$vm_name} || [];

            # Skip if there is not enough primary peak data to forecast.
            if (scalar(@$peak_series_aref) < $min_peaks) {
                $debug_info{$vm_name}{$profile_name} = {
                    peak_status => "Skipped: Insufficient history (" . scalar(@$peak_series_aref) . "/$min_peaks)",
                    residual_status => "Skipped: Dependent on peak forecast"
                };
                next;
            }

            # --- Perform the two separate forecasts ---
            my ($predicted_peak, $peak_debug) = $_predict_next_value->($peak_series_aref);

            my $residual_series_aref = $residual_series_per_profile->{$profile_name}{$vm_name} || [];
            my ($predicted_residual, $residual_debug) = $_predict_next_value->($residual_series_aref);

            # Store the individual forecast components.
            $predictions{$vm_name}{$profile_name} = {
                peak     => $predicted_peak,
                residual => $predicted_residual,
            };

            # Store the debug information for logging and rationale.
            $debug_info{$vm_name}{$profile_name} = {
                peak_status     => $peak_debug,
                residual_status => $residual_debug,
            };
        }
    }

    # Return the predictions and diagnostics to the calling function.
    return (\%predictions, \%debug_info);
}

# ==============================================================================
# Helper to calculate standard deviation.
# ==============================================================================
sub _calculate_std_dev {
    my ($data_aref) = @_;
    my $n = scalar(@$data_aref);
    return 0 if $n < 2;
    my $mean = sum0(@$data_aref) / $n;
    my $sum_sq_diff = sum0(map { ($_ - $mean)**2 } @$data_aref);
    return sqrt($sum_sq_diff / ($n - 1));
}

# ==============================================================================
# Finds all historical peak periods defined in the seasonality config that
# fall within the date range of the available cache data.
# This version is optimised for performance and robustness by pre-compiling
# regexes, reducing object creation, and preventing runaway loops.
# ==============================================================================
sub find_all_historical_periods {
    my ($full_seasonality_config, $data_cache_file) = @_;

    # Determine the actual time span of the data in the cache.
    my ($cache_start, $cache_end) = _get_cache_date_range($data_cache_file);
    return [] unless $cache_start && $cache_end;

    my @periods;
    my $now = gmtime(); # Cache current time to avoid repeated calls.

    # Pre-compile the regex once, outside the loops.
    my $date_range_regex = qr/(\d{4}-\d{2}-\d{2}):(\d{4}-\d{2}-\d{2})/;

    foreach my $e_name (keys %$full_seasonality_config) {
        my $e_config = $full_seasonality_config->{$e_name};

        # --- Path for events defined with fixed 'dates' ---
        if (defined $e_config->{dates}) {
            my @date_ranges = split /\s*,\s*/, $e_config->{dates};

            foreach my $range (@date_ranges) {
                next unless $range =~ $date_range_regex;

                # Only create Time::Piece objects after a successful regex match.
                my $start = Time::Piece->strptime($1, '%Y-%m-%d');
                my $end = Time::Piece->strptime($2, '%Y-%m-%d');
                push @periods, [$start, $end];
            }

        # --- Path for events defined with a recurring 'period' ---
        } elsif (defined $e_config->{period} && $e_config->{period} eq 'monthly') {
            # Pre-extract config values to avoid repeated hash lookups inside the loop.
            my $day_of_period = $e_config->{day_of_period} // -1;
            my $duration_days = $e_config->{duration_days} // 7;

            my $current_iterator = Time::Piece->new($cache_start->epoch)->truncate(to => 'month');

            # Add runaway protection to prevent infinite loops on very large date ranges.
            my $max_iterations = 1000; # Sensible limit for ~83 years of monthly data.
            my $iteration_count = 0;

            while ($current_iterator <= $cache_end && $iteration_count < $max_iterations) {
                my ($start, $end) = _get_recurring_monthly_period($current_iterator, $day_of_period, $duration_days);

                # Add the period if it's valid, historical, and within the cache's time span.
                if ($start && $end && $end < $now && $end <= $cache_end) {
                    push @periods, [$start, $end];
                }

                $current_iterator = $current_iterator->add_months(1);
                $iteration_count++;
            }

            # Warn the user if the protection limit was reached.
            warn "Monthly iteration limit reached for event '$e_name'" if $iteration_count >= $max_iterations;
        }
    }

    return @periods;
}

# ==============================================================================
# Subroutine to log the rationale for a predictive_peak seasonal forecast.
# ==============================================================================
sub log_predictive_peak_rationale {
    my ($fh) = @_;
    return unless $fh;

    foreach my $vm_name (sort @vm_order) {
        next unless exists $seasonal_debug_info{$vm_name};

        print {$fh} "\n######################################################################\n";
        print {$fh} "# Rationale for VM: $vm_name\n";
        print {$fh} "######################################################################\n\n";
        print {$fh} "CPU Sizing Path: Predictive Peak Forecast (Event: $apply_seasonality_event)\n\n";

        foreach my $profile (@profiles) {
            my $p_name = $profile->{name};
            next unless exists $seasonal_debug_info{$vm_name}{$p_name};

            my $s_data = $seasonal_debug_info{$vm_name}{$p_name};
            my $profile_desc = parse_profile_name_for_log($p_name);
            my $label_width = 38;

            print {$fh} "======================================================================\n";
            printf {$fh} "%-${label_width}s : %s\n", "Profile Processed", $profile_desc;
            print {$fh} "----------------------------------------------------------------------\n";

            my $predicted_peak_val = $s_data->{PredictedPeak};

            if (!defined $predicted_peak_val) {
                # This block handles cases where prediction was skipped (e.g., insufficient history).
                my $debug_info = $s_data->{PredictionDebug}{peak_status} // "Insufficient history";
                printf {$fh} "  %-${label_width}s : SKIPPED (%s)\n", "Peak Trend Prediction", $debug_info;
                printf {$fh} "  %-${label_width}s : %.4f cores\n", "1. Non-Peak Baseline", ($s_data->{TrueBaseline} // 0);
                print {$fh} "  --------------------------------------------------------------------\n";
                printf {$fh} "  %-${label_width}s : %.4f cores (Source: Baseline Only)\n", "Final Recommendation", ($s_data->{FinalForecast} // 0);

            } else {
                # This block logs a successful, multi-step forecast.
                printf {$fh} "  1. %-${label_width}s : %.4f cores\n", "Non-Peak Baseline (Floor)", ($s_data->{TrueBaseline} // 0);
                printf {$fh} "  2. %-${label_width}s : %.4f cores\n", "Predicted Peak (from trend)", $predicted_peak_val;
                printf {$fh} "  3. %-${label_width}s : %.4f cores\n", "Predicted Peak Residual", ($s_data->{PredictedResidual} // 0);
                printf {$fh} "  4. %-${label_width}s : %.2f\n", "Peak Amplification Factor", ($s_data->{AmplificationFactor} // 1.0);
                print {$fh} "  --------------------------------------------------------------------\n";
                printf {$fh} "  Calculation : MAX( Baseline, (Predicted Peak + Predicted Residual) * Amplification )\n";
                print {$fh} "  --------------------------------------------------------------------\n";
                printf {$fh} "  %-${label_width}s : %.4f cores (Source: %s)\n", "Final Forecasted Value",
                    ($s_data->{FinalForecast} // 0),
                    ($s_data->{FinalSource} // 'N/A');
            }

            # --- Print outlier warning if it exists ---
            if (defined $s_data->{OutlierWarning} && $s_data->{OutlierWarning} ne '') {
                print {$fh} "\n  --- Workload Volatility Alert ---\n";
                print {$fh} "  " . $s_data->{OutlierWarning} . "\n";
            }

            print {$fh} "======================================================================\n\n";
        }
    }
}

# ==============================================================================
# Calculate linear regression (slope, intercept, R-squared) manually
# ==============================================================================
sub calculate_manual_linear_regression
{
    my ($points_aref) = @_;
    my $n = scalar @{$points_aref};

    return undef if $n < 2;

    my ($sum_x, $sum_y, $sum_xy, $sum_x_squared, $sum_y_squared) = (0, 0, 0, 0, 0);

    foreach my $point (@{$points_aref})
    {
        my ($x_val, $y_val) = @{$point};
        $sum_x += $x_val;
        $sum_y += $y_val;
        $sum_xy += $x_val * $y_val;
        $sum_x_squared += $x_val**2;
        $sum_y_squared += $y_val**2;
    }

    my $denominator_slope = ($n * $sum_x_squared) - ($sum_x**2);

    if (abs($denominator_slope) > $FLOAT_EPSILON)
    {
        my $slope_calc = (($n * $sum_xy) - ($sum_x * $sum_y)) / $denominator_slope;
        my $intercept_calc = ($sum_y - ($slope_calc * $sum_x)) / $n;
        return {
            slope     => $slope_calc,
            intercept => $intercept_calc,
            n_points  => $n,
        };
    }

    # Cannot reliably determine a linear trend.
    return undef;
}

# ==============================================================================
# SUBROUTINE: read_unified_history
## PURPOSE:   Reads history from either legacy file or partitioned directory.
#             Stitches partitioned files into a single "Virtual Monolith" hash
#             to ensure zero regression for downstream logic.
# ARGUMENTS:
#   1. $system_cache_dir (string): The path to a specific system's cache directory.
# RETURNS:
#   - A hash reference of the decoded JSON data.
#   - Returns an empty hash reference if the file does not exist, is empty,
#     or is corrupt, ensuring a safe default.
sub read_unified_history {
    my ($system_cache_dir) = @_;

    my $legacy_file   = File::Spec->catfile($system_cache_dir, $UNIFIED_HISTORY_FILE);
    my $partition_dir = File::Spec->catfile($system_cache_dir, '.nfit.history');

    my $history_data = {};

    # PATH A: Partitioned History (Preferred)
    if (-d $partition_dir) {
        opendir(my $dh, $partition_dir) or die "FATAL: Cannot open history dir: $!";
        my @files = grep { /^nfit\.hist\.\d{4}-\d{2}\.json$/ } readdir($dh);
        closedir($dh);

        my $json_decoder = JSON->new->allow_nonref;

        foreach my $file (@files) {
            my $path = File::Spec->catfile($partition_dir, $file);
            my $json_text = do {
                open my $fh, '<:encoding(utf8)', $path or next; # Skip unreadable
                local $/; <$fh>;
            };
            next unless $json_text;

            my $chunk = eval { $json_decoder->decode($json_text) };
            if ($@) {
                warn "Warning: Corrupt history partition $file: $@";
                next;
            }

            # Merge chunk into main hash (Structure: { "YYYY-MM": { ... } })
            my ($key) = keys %$chunk;
            if ($key) {
                $history_data->{$key} = $chunk->{$key};
            }
        }
        return $history_data;
    }

    # PATH B: Legacy Monolith (Fallback)
    if (-f $legacy_file && -s $legacy_file) {
        eval {
            my $json = JSON->new->allow_nonref;
            local $/;
            open my $fh, '<:encoding(utf8)', $legacy_file or die "Error: $!";
            my $json_text = <$fh>;
            close $fh;
            $history_data = $json->decode($json_text);
        };
        if ($@) {
            warn "Warning: Could not decode legacy history cache. Treating as empty. Error: $@";
            return {};
        }
        return $history_data;
    }

    # PATH C: New System (Empty)
    return {};
}

# ==============================================================================
# SUBROUTINE: write_unified_history
# PURPOSE:    Writes a new monthly entry to the unified history cache file.
#             This function handles file locking to prevent data corruption from
#             concurrent processes. It performs a safe read-modify-write of
#             the entire data structure.
# ARGUMENTS:
#   1. $system_cache_dir (string): The path to a system's cache directory.
#   2. $month_key (string): The key for the new entry (e.g., "2025-07").
#   3. $new_data_for_month_href (hash ref): The data for the new monthly entry.
# RETURNS:
#   - 1 on success, 0 on failure.
# ==============================================================================
sub write_unified_history {
    my ($system_cache_dir, $month_key, $new_data_for_month_href) = @_;

    my $history_file = File::Spec->catfile($system_cache_dir, $UNIFIED_HISTORY_FILE);

    # Acquire global lock
    my ($lock_fh, $lock_path);
    eval {
        ($lock_fh, $lock_path) = _acquire_history_lock($system_cache_dir);
    };
    if ($@) {
        warn "Warning: Skipping history write due to lock failure: $@";
        return 0;
    }
    # ------------------------------------------------

    my $success = 0;
    eval {
        # Read the existing history file first.
        my $history_data = read_unified_history($system_cache_dir);

        # Add or overwrite the data for the specified month.
        $history_data->{$month_key} = $new_data_for_month_href;

        # Write the entire updated data structure back to the file.
        my $json = JSON->new->pretty->canonical;
        my $json_text = $json->encode($history_data);

        open my $fh, '>:encoding(utf8)', $history_file or die "Could not open '$history_file' for writing: $!";
        print $fh $json_text;
        close $fh;
        $success = 1;
    };
    if ($@) {
        warn "Warning: An error occurred while writing to the unified history cache '$history_file': $@";
        $success = 0;
    }

    # Release the lock.
    close $lock_fh;

    # Note: We do NOT unlink the global lock file in the new architecture
    # to prevent race conditions on file creation. It remains as a sentinel.

    return $success;
}

# ==============================================================================
# SUBROUTINE: _build_residual_manifest
# PURPOSE:    Creates a manifest for the residual peak profile. The residual
#             profile is a special, ultra-sensitive profile (typically P99.9
#             with W1 window) used to detect volatile workload spikes.
#
# ARGUMENTS:
#   1. $event_config (hash ref): The event configuration containing the
#      residual_peak_profile flags string.
#
# RETURNS:
#   - Hash reference to the manifest, or undef if no residual profile defined.
# ==============================================================================
sub _build_residual_manifest {
    my ($event_config) = @_;

    # Check if a residual profile is configured
    my $residual_flags_str = $event_config->{residual_peak_profile};
    return undef unless (defined $residual_flags_str && $residual_flags_str ne '');

    # Create a temporary profile object from the flags string
    # This follows the same structure as profiles loaded from config
    my $temp_profile = {
        name  => 'ResidualPeakProfile',  # Fixed name for identification
        flags => $residual_flags_str
    };

    # Use the existing manifest builder with a single-element profile array
    my $manifest = build_transform_manifest([$temp_profile], $nfit_runq_avg_method_str);

    return $manifest;
}

# ==============================================================================
# SUBROUTINE: _execute_history_pass
# PURPOSE:    Executes a single nfit pass for history priming using a sanitised
#             manifest. This wrapper reduces code duplication and ensures
#             consistent command construction across all history passes.
#
# ARGUMENTS:
#   1. $system_cache_dir (string): Path to the system cache directory
#   2. $manifest_href (hash ref): The sanitised historical manifest
#   3. $start_date (Time::Piece): Start date for analysis
#   4. $end_date (Time::Piece): End date for analysis
#   5. $enable_clipping (boolean): Whether to enable clipping detection
#   6. $pass_name (string): Descriptive name for logging (e.g., "Baseline")
#
# RETURNS:
#   - Hash reference to parsed results (VM -> [states]), or empty hash on failure
# ==============================================================================
sub _execute_history_pass {
    my ($system_cache_dir, $manifest_href, $start_date, $end_date, $enable_clipping, $pass_name) = @_;

    # Validate inputs
    unless (ref($manifest_href) eq 'HASH') {
        warn "WARNING: _execute_history_pass received invalid manifest for $pass_name pass\n";
        return {};
    }

    # Write manifest to temporary file
    use File::Temp qw(tempfile);
    my ($fh_manifest, $manifest_filename) = tempfile(
        "nfit_seasonal_${pass_name}_XXXXXX",
        SUFFIX => '.json',
        UNLINK => 1,
        TMPDIR => 1
    );

    print $fh_manifest JSON->new->pretty->canonical->encode($manifest_href);
    close $fh_manifest;

    # Build the nfit command
    my $nfit_cmd = "$nfit_script_path --manifest $manifest_filename "
                 . "--nmondir \"$system_cache_dir\" "
                 . "--startdate " . $start_date->ymd . " "
                 . "--enddate " . $end_date->ymd . " "
                 . "--smt $default_smt_arg "
                 . "--runq-avg-method $nfit_runq_avg_method_str";

    # Add clipping detection if requested (typically only for peak analysis)
    $nfit_cmd .= " --enable-clipping-detection" if $enable_clipping;

    # Execute the command
    print STDERR "      - Executing $pass_name analysis: " . $start_date->ymd . " to " . $end_date->ymd . "\n";

    my $nfit_output = '';
    my $stderr_arg = ">&=" . fileno(STDERR);
    my $pid = open3(undef, my $stdout_fh, $stderr_arg, $nfit_cmd);

    while (my $line = <$stdout_fh>) {
        $nfit_output .= $line;
    }

    waitpid($pid, 0);
    my $exit_code = $? >> 8;

    # Check for failure
    if ($exit_code != 0) {
        warn "WARNING: nfit failed during $pass_name pass (exit code: $exit_code)\n";
        warn "Command: $nfit_cmd\n";
        return {};
    }

    # Parse and return results
    my $parsed_results = parse_nfit_json_output($nfit_output);

    return $parsed_results;
}

# ==============================================================================
# SUBROUTINE: _assemble_seasonal_snapshot
# PURPOSE:    Assembles the final SeasonalEventSnapshot structure from the
#             results of three nfit passes (baseline, peak, residual). This
#             function contains the battle-tested logic for extracting values,
#             calculating residuals, and building ClippingInfo blocks.
#
# ARGUMENTS:
#   1. $peak_results_href (hash ref): Parsed peak period results
#   2. $baseline_results_href (hash ref): Parsed baseline period results
#   3. $residual_results_href (hash ref): Parsed residual peak results (may be empty)
#   4. $event_config (hash ref): Event configuration
#   5. $event_name (string): Event name for metadata
#   6. $peak_end (Time::Piece): Peak end date for metadata
#   7. $profiles_aref (array ref): Reference to the profiles array
#
# RETURNS:
#   - Hash reference to the complete snapshot structure, or undef on failure
# ==============================================================================
sub _assemble_seasonal_snapshot {
    my ($peak_results_href, $baseline_results_href, $residual_results_href,
        $event_config, $event_name, $peak_end, $profiles_aref) = @_;

    # Validate that profiles array was passed correctly
    unless (ref($profiles_aref) eq 'ARRAY' && @$profiles_aref) {
        die "FATAL: _assemble_seasonal_snapshot requires a valid profiles array reference\n";
    }

    # Determine the correct key names based on model type
    my $model_type = $event_config->{model} // '';
    my ($peak_key, $baseline_key) = ('HistoricPeak', 'HistoricBaseline');
    if ($model_type eq 'predictive_peak' or $model_type eq 'adaptive_peak_forecasting') {
        ($peak_key, $baseline_key) = ('PeakValue', 'BaselineValue');
    }

    # Initialise result structures
    my %peak_period_results;
    my %baseline_period_results;
    my %clipping_info_results;
    my %residual_results_final;

    # ======================================================================
    # STEP 1: Extract baseline values for all VMs and profiles
    # ======================================================================
    # CRITICAL FIX: When nfit runs without decay (historical mode), it may
    # return multiple states per VM. Profiles with time filters (e.g. -online)
    # may appear in earlier states, whilst general profiles appear in later
    # states. We must search through ALL states to find each profile's value.
    # ======================================================================
    foreach my $vm_name (keys %$baseline_results_href) {
        my $states_aref = $baseline_results_href->{$vm_name};
        next unless ref($states_aref) eq 'ARRAY' && @$states_aref;

        # Extract values for each profile
        foreach my $profile (@$profiles_aref) {
            my $profile_name = $profile->{name};
            my $p_key = "P" . clean_perc_label(($profile->{flags} =~ /-p\s+([0-9.]+)/) ? $1 : $DEFAULT_PERCENTILE);

            # Search through all states (newest to oldest) to find this profile's value
            # We search backwards because the most recent state is usually most complete
            my $metric_val = undef;
            for (my $i = $#{$states_aref}; $i >= 0; $i--) {
                my $state = $states_aref->[$i];
                $metric_val = _safe_dig($state, 'metrics', 'physc', $profile_name, $p_key);
                last if defined $metric_val;  # Found it, stop searching
            }

            if (defined $metric_val) {
                $baseline_period_results{$vm_name}{$profile_name} = $metric_val;
            }
        }
    }

    # ======================================================================
    # STEP 2: Extract sanitised peak values for residual calculation
    # ======================================================================
    my %sanitised_peak_values;  # Used for residual calculation

    if (defined $event_config->{residual_peak_profile} &&
        $event_config->{residual_peak_profile} ne '' &&
        ref($residual_results_href) eq 'HASH' &&
        scalar keys %$residual_results_href) {

        foreach my $vm_name (keys %$residual_results_href) {
            my $states_aref = $residual_results_href->{$vm_name};
            next unless ref($states_aref) eq 'ARRAY' && @$states_aref;

            # Extract the residual profile's percentile
            my $residual_flags = $event_config->{residual_peak_profile};
            my $p_key = "P" . clean_perc_label(($residual_flags =~ /-p\s+([0-9.]+)/) ? $1 : $DEFAULT_PERCENTILE);

            # Search through all states (newest to oldest) to find the residual value
            my $metric_val = undef;
            for (my $i = $#{$states_aref}; $i >= 0; $i--) {
                my $state = $states_aref->[$i];
                $metric_val = _safe_dig($state, 'metrics', 'physc', 'ResidualPeakProfile', $p_key);
                last if defined $metric_val;  # Found it, stop searching
            }

            if (defined $metric_val) {
                $sanitised_peak_values{$vm_name} = $metric_val;
            }
        }
    }

    # ======================================================================
    # STEP 3: Extract peak values and clipping info for all profiles
    # ======================================================================
    # Same multi-state search logic as baseline extraction above.
    # ======================================================================
    foreach my $vm_name (keys %$peak_results_href) {
        my $states_aref = $peak_results_href->{$vm_name};
        next unless ref($states_aref) eq 'ARRAY' && @$states_aref;

        # Extract values for each profile
        foreach my $profile (@$profiles_aref) {
            my $profile_name = $profile->{name};
            my $p_key = "P" . clean_perc_label(($profile->{flags} =~ /-p\s+([0-9.]+)/) ? $1 : $DEFAULT_PERCENTILE);

            # Search through all states (newest to oldest) to find this profile's value
            my $standard_peak_value = undef;
            my $clipping_metrics = undef;
            my $source_state = undef;

            for (my $i = $#{$states_aref}; $i >= 0; $i--) {
                my $state = $states_aref->[$i];
                my $val = _safe_dig($state, 'metrics', 'physc', $profile_name, $p_key);
                if (defined $val) {
                    $standard_peak_value = $val;
                    $source_state = $state;
                    last;  # Found it, stop searching
                }
            }

            if (defined $standard_peak_value) {
                $peak_period_results{$vm_name}{$profile_name} = $standard_peak_value;
            }

            # Extract clipping information from the same state where we found the value
            if (defined $source_state) {
                $clipping_metrics = _safe_dig($source_state, 'metrics', 'physc', $profile_name, 'ClippingInfo');
            }

            if ($clipping_metrics && ref($clipping_metrics) eq 'HASH' && exists $clipping_metrics->{isClipped}) {
                my $unmet_demand_est = $clipping_metrics->{unmetDemandEstimate} // 0;
                my $max_cpu = _safe_dig($source_state, 'metadata', 'max_cpu') // 0;

                # Assemble ClippingInfo with the exact structure expected by consumers
                $clipping_info_results{$vm_name}{$profile_name} = {
                    isClipped               => $clipping_metrics->{isClipped},
                    clippingConfidence      => $clipping_metrics->{clippingConfidence} // 'unknown',
                    capacityLimit           => $max_cpu,
                    unmetDemandEstimate     => $unmet_demand_est,
                    unclippedPeakEstimate   => ($standard_peak_value // 0) + $unmet_demand_est,
                    platformSpecificMarkers => {
                        aix_runq_saturation => _safe_dig($clipping_metrics, 'platformMarkers', 'aix_runq_saturation') // 0
                    }
                };
            }

            # Calculate peak residual if we have sanitised peak data
            if (defined $standard_peak_value && exists $sanitised_peak_values{$vm_name}) {
                my $residual = $sanitised_peak_values{$vm_name} - $standard_peak_value;
                $residual_results_final{$vm_name}{$profile_name} = ($residual > 0) ? $residual : 0;
            }
        }
    }

    # ======================================================================
    # STEP 4: Validate and return
    # ======================================================================
    unless (scalar keys %peak_period_results && scalar keys %baseline_period_results) {
        warn "      - WARNING: Could not generate valid peak/baseline data for event '$event_name'. Skipping snapshot.\n";
        return undef;
    }

    # Assemble the final snapshot structure
    return {
        eventName     => $event_name,
        periodEndDate => $peak_end->ymd,
        generatedOn   => localtime()->datetime(),
        results       => {
            $peak_key      => \%peak_period_results,
            $baseline_key  => \%baseline_period_results,
            'ClippingInfo' => \%clipping_info_results,
            'PeakResidual' => \%residual_results_final
        }
    };
}

# ==============================================================================
# SUBROUTINE: _generate_seasonal_snapshot_for_period
# PURPOSE:    Performs single-pass analysis for a seasonal event's peak and
#             baseline periods using the manifest-driven Single Pass Engine
#             architecture. This function executes nfit three times (baseline,
#             peak, residual) instead of 3 * N times (where N = number of profiles).
#
#             This refactored version achieves the same I/O reduction as the
#             monthly analysis (typically 16x for a 16-profile configuration).
# ==============================================================================
sub _generate_seasonal_snapshot_for_period {
    my ($system_cache_dir, $event_config, $event_name, $peak_start, $peak_end, $baseline_start, $baseline_end) = @_;

    print STDERR "      - Using Single Pass Engine for seasonal event: $event_name\n";

    # ======================================================================
    # STEP 1: Build and sanitise the main manifest (all profiles)
    # ======================================================================
    my $tactical_manifest = build_transform_manifest(\@profiles, $nfit_runq_avg_method_str);
    my $main_historical_manifest = _sanitise_manifest_for_history($tactical_manifest);

    # ======================================================================
    # STEP 2: Build and sanitise the residual manifest (if configured)
    # ======================================================================
    my $residual_historical_manifest = undef;

    if (defined $event_config->{residual_peak_profile} && $event_config->{residual_peak_profile} ne '') {
        my $residual_tactical_manifest = _build_residual_manifest($event_config);
        if (defined $residual_tactical_manifest) {
            $residual_historical_manifest = _sanitise_manifest_for_history($residual_tactical_manifest);
        }
    }

    # ======================================================================
    # STEP 3: Execute three nfit passes
    # ======================================================================

    # Pass 1: Baseline period (no clipping detection needed)
    my $baseline_results = _execute_history_pass(
        $system_cache_dir,
        $main_historical_manifest,
        $baseline_start,
        $baseline_end,
        0,  # enable_clipping = false
        "Baseline"
    );

    # Pass 2: Standard peak period (with clipping detection)
    my $peak_results = _execute_history_pass(
        $system_cache_dir,
        $main_historical_manifest,
        $peak_start,
        $peak_end,
        1,  # enable_clipping = true
        "Peak"
    );

    # Pass 3: Residual peak period (only if configured)
    my $residual_results = {};
    if (defined $residual_historical_manifest) {
        $residual_results = _execute_history_pass(
            $system_cache_dir,
            $residual_historical_manifest,
            $peak_start,
            $peak_end,
            0,  # enable_clipping = false
            "Residual"
        );
    }

    # ======================================================================
    # STEP 4: Assemble the final snapshot structure
    # ======================================================================
    my $snapshot_result = _assemble_seasonal_snapshot(
        $peak_results,
        $baseline_results,
        $residual_results,
        $event_config,
        $event_name,
        $peak_end,
        \@profiles  # Pass profiles array explicitly to ensure correct scoping
    );

    return $snapshot_result;
}

# ==============================================================================
# SUBROUTINE: update_monthly_history (PRODUCTION FIX - Unified In-Memory)
# PURPOSE:    Orchestrates a robust, multi-phase strategy to create and update
#             the unified monthly history cache using a single in-memory hash
#             to prevent data corruption from conflicting file writes.
# ==============================================================================
sub update_monthly_history {
    my ($system_cache_dir, $system_identifier, $seasonality_config, $min_days_for_history, $adaptive_runq_saturation_thresh, $force_flag) = @_;
    print STDERR "\n--- Updating Unified Monthly History for system: $system_identifier ---\n";

    # In-Flight auto-migration to per-month history
    # We acquire the lock here to perform the check and potential migration atomically.
    {
        my ($lock_fh, $lock_path);
        eval { ($lock_fh, $lock_path) = _acquire_history_lock($system_cache_dir); };
        if (!$@) {
            if (-f File::Spec->catfile($system_cache_dir, '.nfit.history.json') &&
                !-d File::Spec->catfile($system_cache_dir, '.nfit.history')) {

                _migrate_history_to_partitioned($system_cache_dir);
            }
            close $lock_fh;
        }
    }
    # --------------------------------------------

    my $data_cache_file = File::Spec->catfile($system_cache_dir, '.nfit.cache.data');
    my ($cache_start_obj, $cache_end_obj) = _get_cache_date_range($data_cache_file);

    unless ($cache_start_obj && $cache_end_obj) {
        warn "Warning: Could not determine date range from cache file. Cannot update history.\n";
        return;
    }
    print STDERR "Data cache spans from " . $cache_start_obj->date . " to " . $cache_end_obj->date . "\n";

    # --- Step 1: Load existing history ONCE ---
    my $history_data = read_unified_history($system_cache_dir);
    my @months_to_process;

    # --- Step 2: Planning Stage - Discover unprocessed months ---
    print STDERR "Scanning cache for unprocessed or incomplete months...\n";
    my $iterator_month = Time::Piece->new($cache_start_obj->epoch)->truncate(to => 'month');
    while ($iterator_month <= $cache_end_obj) {
        my $month_key = $iterator_month->strftime('%Y-%m');
        my $month_start_obj = Time::Piece->new($iterator_month->epoch);
        my $month_end_obj   = Time::Piece->new($month_start_obj->epoch)->add_months(1) - ONE_DAY;
        my $effective_start_obj = ($cache_start_obj > $month_start_obj) ? $cache_start_obj : $month_start_obj;
        my $effective_end_obj   = ($cache_end_obj   < $month_end_obj)   ? $cache_end_obj   : $month_end_obj;
        my $days_in_period = ($effective_start_obj <= $effective_end_obj) ? (int(($effective_end_obj->epoch - $effective_start_obj->epoch) / 86400) + 1) : 0;

        my $should_process = 0;
        if (exists $history_data->{$month_key}) {
            my $stored_days = $history_data->{$month_key}{Metadata}{ProcessedDays} // 0;
            if ($days_in_period > $stored_days) {
                 print STDERR "  - INFO: Found more complete data for $month_key ($days_in_period > $stored_days days). Queueing for reprocessing.\n";
                 $should_process = 1;
            }
        } elsif ($days_in_period >= $min_days_for_history) {
            $should_process = 1;
        }

        if ($should_process) {
            push @months_to_process, { key => $month_key, start_obj => $effective_start_obj, end_obj => $effective_end_obj, days => $days_in_period };
        }
        $iterator_month = $iterator_month->add_months(1);
    }
    print STDERR "Scan complete.\n";

    # Pre-execution validation for the first month to be processed
    if (@months_to_process) {
        my $first_month_key = $months_to_process[0]{key};
        _validate_pre_update_conditions($system_cache_dir, $first_month_key, $force_flag);

        # Create checkpoint backup before any modifications
        print STDERR "\n--- Creating Pre-Tactical Checkpoint ---\n";
        _create_checkpoint_backup($system_cache_dir, 'pre-tactical', 1);
    }

    # --- Step 3: Execution Stage ---
    if (@months_to_process) {
        print STDERR "Identified " . scalar(@months_to_process) . " month(s) to process: " . join(", ", map { $_->{key} } @months_to_process) . "\n";

        my @all_historical_peaks = find_all_historical_periods($seasonality_config, $data_cache_file);

        # This single function call modifies the in-memory $history_data hash.
        _generate_monthly_and_seasonal_history(
            $system_cache_dir, \@months_to_process, $seasonality_config,
            \@all_historical_peaks, $history_data, $adaptive_runq_saturation_thresh
        );
    } else {
        print STDERR "No new months to process. History is up to date.\n";
    }


    # Track if this was a first run (no pre-existing history file)
    my $history_file = File::Spec->catfile($system_cache_dir, '.nfit.history.json');
    my $was_first_run = !(-f $history_file);

    # --- Phase 3: Enrich with Growth Rationale from L2 Cache ---
    my $was_enriched = _enrich_history_with_growth_rationale($system_cache_dir, $history_data);

    # --- Step 4: Final Write ---
    # The history file is written only ONCE at the end if any changes were made.
    if (@months_to_process || $was_enriched) {
        print "\n--- Finalising History Cache ---\n";
        print "  - Writing all updates to .nfit.history.json...\n";

        _write_full_history($system_cache_dir, $history_data);
        print "  - History cache successfully updated.\n";

        # If this was the first run, create a post-write backup to protect the newly created file
        if ($was_first_run && -f $history_file) {
            print "\n--- Creating Post-Initial-Write Backup ---\n";
            _create_checkpoint_backup($system_cache_dir, 'post-initial-write', 1);
        }
    } else {
        print "  - No changes to write. History cache is already current.\n";
    }
}

# ==============================================================================
# SUBROUTINE: _generate_monthly_and_seasonal_history (PRODUCTION FIX - COMBINED)
# PURPOSE:    Processes months to generate both generic analysis and snapshots,
#             modifying a single in-memory hash to prevent data corruption.
# ==============================================================================
sub _generate_monthly_and_seasonal_history {
    my ($system_cache_dir, $months_to_process_aref, $seasonality_config, $all_historical_peaks_aref, $history_data_href, $adaptive_runq_saturation_thresh) = @_;

    foreach my $month_job (@$months_to_process_aref) {
        my $month_key = $month_job->{key};
        my $start_str = $month_job->{start_obj}->strftime('%Y-%m-%d');
        my $end_str   = $month_job->{end_obj}->strftime('%Y-%m-%d');

        # --- PHASE 1: Generate MonthlyWorkloadAnalysis ---
        print "\n--- Processing Month: $month_key (Period: $start_str to $end_str) ---\n";
        my $workload_analysis = _generate_monthly_workload_analysis($system_cache_dir, $start_str, $end_str, $adaptive_runq_saturation_thresh);

        # --- Populate the in-memory hash ---
        $history_data_href->{$month_key} = {
            MonthlyWorkloadAnalysis => $workload_analysis,
            Metadata => {
                ProcessedStartDate => $start_str,
                ProcessedEndDate   => $end_str,
                ProcessedDays      => $month_job->{days},
                LastUpdated        => localtime()->datetime(),
            }
        };

        # --- PHASE 2: Generate SeasonalEventSnapshots for this month ---
        print "  - Discovering seasonal events for month: $month_key...\n";
        my %seasonal_snapshots_for_month;
        my %calculation_cache;

        foreach my $event_name (sort keys %$seasonality_config) {
            next if $event_name eq 'Global';
            my $event_config = $seasonality_config->{$event_name};
            next if (($event_config->{model} // '') eq 'recency_decay');

            my ($peak_start, $peak_end) = determine_event_period($event_config, $month_job->{start_obj});

            if ($peak_start && $peak_end && $peak_start <= $month_job->{end_obj} && $peak_end >= $month_job->{start_obj}) {
                print "    - Found active event: '$event_name'\n";
                my $baseline_days = $event_config->{baseline_period_days} // 16;
                my $potential_baseline_start = $peak_start - ($baseline_days * ONE_DAY);
                my $latest_preceding_peak_end;
                foreach my $p (@{$all_historical_peaks_aref}) {
                    if ($p->[1] < $peak_start && (!defined $latest_preceding_peak_end || $p->[1] > $latest_preceding_peak_end)) {
                        $latest_preceding_peak_end = $p->[1];
                    }
                }
                my $baseline_start_obj = $potential_baseline_start;
                if (defined $latest_preceding_peak_end && $potential_baseline_start <= $latest_preceding_peak_end) {
                    $baseline_start_obj = $latest_preceding_peak_end + ONE_DAY;
                    my $available_days = ($peak_start->epoch - $baseline_start_obj->epoch) / ONE_DAY;
                    print "      - WARNING: Baseline for this event truncated to " . int($available_days) . " day(s) to avoid overlap.\n";
                }
                my $baseline_end_obj = $peak_start - ONE_SECOND;
                my $model_type_for_key = $event_config->{model} // 'unknown';
                my $cache_key = $peak_start->date . ":" . $peak_end->date . ":" . $baseline_start_obj->date . ":" . $model_type_for_key;

                my $snapshot_results;
                if (exists $calculation_cache{$cache_key}) {
                    print "      - INFO: Reusing cached calculations for identical time period and model type.\n";
                    $snapshot_results = $calculation_cache{$cache_key};
                } else {
                    $snapshot_results = _generate_seasonal_snapshot_for_period($system_cache_dir, $event_config, $event_name, $peak_start, $peak_end, $baseline_start_obj, $baseline_end_obj);
                    $calculation_cache{$cache_key} = $snapshot_results;
                }
                if ($snapshot_results) {
                    $snapshot_results->{eventName} = $event_name;
                    $seasonal_snapshots_for_month{$event_name} = $snapshot_results;
                }
            }
        }

        if (scalar keys %seasonal_snapshots_for_month > 0) {
            $history_data_href->{$month_key}{SeasonalEventSnapshots} = \%seasonal_snapshots_for_month;
            $history_data_href->{$month_key}{Metadata}{LastUpdated} = localtime()->datetime();
        }
    }
}

# ==============================================================================
# SUBROUTINE: _generate_seasonal_snapshot_history (Phase 2)
# PURPOSE:    Processes a list of months to generate model-specific snapshots.
#             Implements intelligent caching to avoid redundant `nfit` calls
#             for events that share peak/baseline periods. Includes critical logic
#             to prevent baseline contamination from preceding peaks.
# ==============================================================================
sub _generate_seasonal_snapshot_history {
    my ($system_cache_dir, $months_to_process_aref, $seasonality_config, $all_historical_peaks_aref) = @_;

    print "\n--- Phase 2: Generating Seasonal Event Snapshots ---\n";
    foreach my $month_job (@$months_to_process_aref) {
        my $month_key   = $month_job->{key};
        my $start_obj   = $month_job->{start_obj};
        my $end_obj     = $month_job->{end_obj};

        print "  - Discovering events for month: $month_key...\n";

        my %seasonal_snapshots_for_month;
        my %calculation_cache; # In-memory cache for this month's run

        foreach my $event_name (sort keys %$seasonality_config) {
            next if $event_name eq 'Global';
            my $event_config = $seasonality_config->{$event_name};
            next if (($event_config->{model} // '') eq 'recency_decay'); # Skip non-priming models

            my ($peak_start, $peak_end) = determine_event_period($event_config, $start_obj);

            if ($peak_start && $peak_end && $peak_start <= $end_obj && $peak_end >= $start_obj) {
                print "    - Found active event: '$event_name'\n";

                my $baseline_days = $event_config->{baseline_period_days} // 16;
                my $potential_baseline_start = $peak_start - ($baseline_days * ONE_DAY);

                my $latest_preceding_peak_end;
                foreach my $p (@{$all_historical_peaks_aref}) {
                    if ($p->[1] < $peak_start && (!defined $latest_preceding_peak_end || $p->[1] > $latest_preceding_peak_end)) {
                        $latest_preceding_peak_end = $p->[1];
                    }
                }

                my $baseline_start_obj = $potential_baseline_start;
                if (defined $latest_preceding_peak_end && $potential_baseline_start <= $latest_preceding_peak_end) {
                    $baseline_start_obj = $latest_preceding_peak_end + ONE_DAY;
                    my $available_days = ($peak_start->epoch - $baseline_start_obj->epoch) / ONE_DAY;
                    print "      - WARNING: Baseline for this event truncated to " . int($available_days) . " day(s) to avoid overlap with a preceding peak.\n";
                }

                my $baseline_end_obj = $peak_start - ONE_SECOND;

                # --- FIX: Make the cache key model-specific ---
                my $model_type_for_key = $event_config->{model} // 'unknown';
                my $cache_key = $peak_start->date . ":" . $peak_end->date . ":" . $baseline_start_obj->date . ":" . $model_type_for_key;

                my $snapshot_results;
                if (exists $calculation_cache{$cache_key}) {
                    print "      - INFO: Reusing cached calculations for identical time period and model type.\n";
                    $snapshot_results = $calculation_cache{$cache_key};
                } else {
                    $snapshot_results = _generate_seasonal_snapshot_for_period(
                        $system_cache_dir, $event_config, $event_name,
                        $peak_start, $peak_end,
                        $baseline_start_obj, $baseline_end_obj
                    );
                    $calculation_cache{$cache_key} = $snapshot_results; # Cache the result
                }

                if ($snapshot_results) {
                    $snapshot_results->{eventName} = $event_name;
                    $seasonal_snapshots_for_month{$event_name} = $snapshot_results;
                }
            }
        }

        if (scalar keys %seasonal_snapshots_for_month > 0) {
            print "    - Writing snapshots for $month_key to history cache...\n";
            my $history_data = read_unified_history($system_cache_dir);

            # CRITICAL FIX: Ensure we don't lose MonthlyWorkloadAnalysis
            $history_data->{$month_key} //= {};

            # Preserve MonthlyWorkloadAnalysis and Metadata if they exist
            my $existing_workload = $history_data->{$month_key}{MonthlyWorkloadAnalysis};
            my $existing_metadata = $history_data->{$month_key}{Metadata};

            $history_data->{$month_key}{SeasonalEventSnapshots} = \%seasonal_snapshots_for_month;

            # Update metadata timestamp but preserve other fields
            if ($existing_metadata) {
                $history_data->{$month_key}{Metadata} = $existing_metadata;
            }
            $history_data->{$month_key}{Metadata}{LastUpdated} = localtime()->datetime();

            _write_full_history($system_cache_dir, $history_data);

        }
    }
}

# ==============================================================================
# SUBROUTINE: _validate_history_structure
# PURPOSE:    Performs structural validation on the history data structure before
#             writing to disk. This safety check prevents corruption by detecting
#             missing or empty blocks that would break downstream consumers.
#
# ARGUMENTS:
#   1. $history_href (hash ref): The complete history data structure
#
# RETURNS:
#   - 1 on success (structure is valid)
#   - Dies with detailed error message on validation failure
#
# VALIDATION CHECKS:
#   1. Required blocks exist (MonthlyWorkloadAnalysis, Metadata)
#   2. Blocks are not empty
#   3. Required fields are present in nested structures
#   4. Basic data type validation (hashes where expected, etc.)
# ==============================================================================
sub _validate_history_structure {
    my ($history_href) = @_;

    # Validate input type
    unless (ref($history_href) eq 'HASH') {
        die "FATAL: History structure validation failed - not a hash reference\n";
    }

    # Check that we have at least one month of data
    my @months = keys %$history_href;
    unless (@months) {
        die "FATAL: History structure validation failed - no monthly data present\n";
    }

    # Validate each month's structure
    foreach my $month_key (sort @months) {
        my $month_data = $history_href->{$month_key};

        # Check that month data is a hash
        unless (ref($month_data) eq 'HASH') {
            die "FATAL: History structure validation failed for $month_key - month data is not a hash\n";
        }

        # Check for required top-level blocks
        unless (exists $month_data->{MonthlyWorkloadAnalysis}) {
            die "FATAL: History structure validation failed for $month_key - missing MonthlyWorkloadAnalysis block\n";
        }

        unless (exists $month_data->{Metadata}) {
            die "FATAL: History structure validation failed for $month_key - missing Metadata block\n";
        }

        # Validate MonthlyWorkloadAnalysis is not empty
        my $workload_analysis = $month_data->{MonthlyWorkloadAnalysis};
        unless (ref($workload_analysis) eq 'HASH') {
            die "FATAL: History structure validation failed for $month_key - MonthlyWorkloadAnalysis is not a hash\n";
        }

        unless (scalar keys %$workload_analysis) {
            die "FATAL: History structure validation failed for $month_key - MonthlyWorkloadAnalysis is empty\n";
        }

        # Validate that each VM in MonthlyWorkloadAnalysis has required fields
        foreach my $vm_name (keys %$workload_analysis) {
            my $vm_data = $workload_analysis->{$vm_name};

            unless (ref($vm_data) eq 'HASH') {
                die "FATAL: History structure validation failed for $month_key VM $vm_name - VM data is not a hash\n";
            }

            # Check for required fields
            my @required_fields = qw(ProfileValues Hint Pattern Pressure);
            foreach my $field (@required_fields) {
                unless (exists $vm_data->{$field}) {
                    die "FATAL: History structure validation failed for $month_key VM $vm_name - missing required field: $field\n";
                }
            }

            # Validate ProfileValues is a non-empty hash
            unless (ref($vm_data->{ProfileValues}) eq 'HASH' && scalar keys %{$vm_data->{ProfileValues}}) {
                die "FATAL: History structure validation failed for $month_key VM $vm_name - ProfileValues is empty or invalid\n";
            }
        }

        # Validate Metadata structure
        my $metadata = $month_data->{Metadata};
        unless (ref($metadata) eq 'HASH') {
            die "FATAL: History structure validation failed for $month_key - Metadata is not a hash\n";
        }

        # Check for essential metadata fields
        unless (exists $metadata->{ProcessedDays}) {
            die "FATAL: History structure validation failed for $month_key - Metadata missing ProcessedDays\n";
        }

        # Optional: Validate SeasonalEventSnapshots structure if present
        if (exists $month_data->{SeasonalEventSnapshots}) {
            my $seasonal = $month_data->{SeasonalEventSnapshots};
            unless (ref($seasonal) eq 'HASH') {
                die "FATAL: History structure validation failed for $month_key - SeasonalEventSnapshots is not a hash\n";
            }

            # Validate each event's structure
            foreach my $event_name (keys %$seasonal) {
                my $event_data = $seasonal->{$event_name};

                unless (ref($event_data) eq 'HASH' && exists $event_data->{results}) {
                    die "FATAL: History structure validation failed for $month_key event $event_name - missing or invalid results block\n";
                }
            }
        }
    }

    # All validation checks passed
    return 1;
}

# ==============================================================================
# SUBROUTINE: _write_full_history (Partition Aware)
# PURPOSE:    Writes the in-memory history hash to the partitioned directory structure.
#             Creates the directory if missing. Enforces Global Lock.
#             Includes structural validation before writing to prevent corruption.
#             Refactored to use the standardised _acquire_history_lock for safety.
# ARGUMENTS:
#   1. $system_cache_dir (string): The path to a system's cache directory.
#   2. $history_data_href (hash ref): The complete history data structure.
# RETURNS:
#   - None
# ==============================================================================
sub _write_full_history {
    my ($system_cache_dir, $history_data_href) = @_;

    # 1. Validation
    # ======================================================================
    # CRITICAL SAFETY CHECK: Validate structure before writing
    # ======================================================================
    # This validation prevents corrupted or incomplete data from being
    # persisted to disk. It's better to fail fast with a clear error
    # than to write broken data that silently breaks downstream consumers.
    # ======================================================================
    eval { _validate_history_structure($history_data_href); };
    if ($@) {
        die "FATAL: Aborting history write due to validation failure:\n$@";
    }

    _add_lightweight_metadata($history_data_href, 'nfit-profile');

    # 2. Global Lock Acquisition
    my ($lock_fh, $lock_path);
    eval { ($lock_fh, $lock_path) = _acquire_history_lock($system_cache_dir); };
    if ($@) { die "FATAL: Could not acquire global history lock: $@"; }

    # 3. Directory Setup
    my $partition_dir = File::Spec->catfile($system_cache_dir, '.nfit.history');
    unless (-d $partition_dir) {
        make_path($partition_dir) or die "FATAL: Cannot create history directory: $!";
    }

    # 4. Write Partitions
    my $json_encoder = JSON->new->pretty->canonical;
    my $write_errors = 0;

    foreach my $month_key (keys %$history_data_href) {
        # Skip metadata/non-month keys if any creep in
        next unless $month_key =~ /^\d{4}-\d{2}$/;

        my $filename = "nfit.hist.${month_key}.json";
        my $filepath = File::Spec->catfile($partition_dir, $filename);

        # Wrap in month key for consistency
        my $payload = { $month_key => $history_data_href->{$month_key} };

        eval {
            open my $fh, '>:encoding(utf8)', $filepath or die "Open failed: $!";
            print $fh $json_encoder->encode($payload);
            close $fh;
        };
        if ($@) {
            warn "Error writing partition $filename: $@";
            $write_errors++;
        }
    }

    # 5. Legacy Cleanup (Soft)
    # If we successfully wrote partitions, we ensure no stale monolith exists
    # to confuse readers, though _migrate should have handled this.
    my $legacy_file = File::Spec->catfile($system_cache_dir, $UNIFIED_HISTORY_FILE);
    if (-f $legacy_file && !$write_errors) {
        rename($legacy_file, $legacy_file . ".migrated");
    }

    close $lock_fh;

    if ($write_errors) {
        die "FATAL: Errors occurred writing history partitions. Check logs.";
    }
}

# ==============================================================================
# SUBROUTINE: _create_checkpoint_backup (Directory Aware & Compressed)
# PURPOSE:    Creates an atomic, compressed snapshot of the history.
#             Handles both Legacy (File) and Modern (Directory) structures.
#             Uses Archive::Tar for cross-platform (AIX/Linux) compatibility.
# ARGUMENTS:
#   1. $system_cache_dir: Path to the staging cache
#   2. $checkpoint_type:  Label for the backup (e.g., 'pre-tactical')
#   3. $include_l2_cache: Boolean, whether to include L2 results
# RETURNS:
#   Path to the created backup file, or undef on failure.
# ==============================================================================
sub _create_checkpoint_backup {
    my ($system_cache_dir, $checkpoint_type, $include_l2_cache) = @_;

    # Derive serial number and paths
    my $serial = basename($system_cache_dir);
    my $root_dir = dirname(dirname($system_cache_dir));
    my $backup_root = File::Spec->catfile($root_dir, 'backups', $serial);

    make_path($backup_root) unless -d $backup_root;

    my $timestamp = localtime->strftime('%Y%m%d_%H%M%S');

    # Identify the source (Directory takes precedence over File)
    my $history_dir_source  = File::Spec->catfile($system_cache_dir, '.nfit.history');
    my $history_file_source = File::Spec->catfile($system_cache_dir, '.nfit.history.json');

    my $source_path;
    my $is_directory = 0;

    if (-d $history_dir_source) {
        $source_path = $history_dir_source;
        $is_directory = 1;
    } elsif (-f $history_file_source) {
        $source_path = $history_file_source;
        $is_directory = 0;
    } else {
        print "  - No existing history found to backup (First Run)\n";
        return undef;
    }

    # Generate unique backup filename (.tar.gz for dirs, .json.gz for files)
    my $extension = $is_directory ? "tar.gz" : "json.gz";
    my $backup_target = _get_next_available_filename(
        $backup_root,
        ".nfit.history.${timestamp}",
        $extension
    );

    # Perform the Backup
    print "  - Creating compressed checkpoint (${checkpoint_type}).\n";

    eval {
        if ($is_directory) {
            # Create a compressed tarball of the directory contents
            my $tar = Archive::Tar->new;

            # We cd into the directory to keep paths relative inside the tar
            my $cwd = Cwd::getcwd();
            chdir($system_cache_dir);

            # Add the folder (e.g., .nfit.history)
            # This recursively adds contents
            $tar->add_files(basename($source_path));

            # Write compressed file
            $tar->write($backup_target, COMPRESS_GZIP);

            chdir($cwd); # Restore PWD
        } else {
            # Gzip the single legacy file
            # We use IO::Zlib or just simple system gzip if we want to be lazy,
            # but let's stick to Perl for safety.
            # Actually, for a single file, copying then gzipping is safest
            # to avoid race conditions reading the source.

            require File::Copy;
            # Copy to .tmp first
            my $tmp_target = "$backup_target.tmp";
            File::Copy::copy($source_path, $tmp_target) or die "Copy failed: $!";

            # Compress in place
            system("gzip -f \"$tmp_target\""); # Standard on AIX/Linux
            rename("$tmp_target.gz", $backup_target);
        }
        print "  â Created: " . basename($backup_target) . "\n";
    };

    if ($@) {
        warn "Warning: Could not create backup at '$backup_target': $@";
        return undef;
    }

    # Backup L2 Cache (Standard Copy - these are transient/rebuildable)
    if ($include_l2_cache) {
        foreach my $cache_file ('.nfit.cache.results', '.nfit.cache.manifest') {
            my $source = File::Spec->catfile($system_cache_dir, $cache_file);
            next unless -f $source;

            # We don't compress L2 cache backups to keep this step fast,
            # as these files are smaller than history.
            my $ext = ($cache_file =~ /\.(\w+)$/) ? $1 : '';
            my $cache_backup = _get_next_available_filename($backup_root, "${cache_file}.${timestamp}", $ext);

            require File::Copy;
            File::Copy::copy($source, $cache_backup) or warn "Could not backup $cache_file: $!";
        }
    }

    _prune_old_backups($backup_root, 24);
    return $backup_target;
}

# ==============================================================================
# SUBROUTINE: _get_next_available_filename
# PURPOSE:    Generates a unique filename using rolling numbering (001, 002, etc.)
#             to prevent collisions, similar to CSV output collision prevention.
# ARGUMENTS:
#   1. $dir (string): Directory where file will be created
#   2. $base_name (string): Base filename without extension
#   3. $extension (string): File extension (without leading dot)
# RETURNS:
#   - Full path to available filename
# ==============================================================================
sub _get_next_available_filename {
    my ($dir, $base_name, $extension) = @_;

    my $counter = 1;
    my $candidate;

    do {
        my $suffix = sprintf(".%03d", $counter);
        if ($extension) {
            $candidate = File::Spec->catfile($dir, "${base_name}${suffix}.${extension}");
        } else {
            $candidate = File::Spec->catfile($dir, "${base_name}${suffix}");
        }
        $counter++;
    } while (-e $candidate && $counter < 1000); # Safety limit

    die "Could not generate unique filename after 999 attempts" if $counter >= 1000;

    return $candidate;
}

# ==============================================================================
# SUBROUTINE: _prune_old_backups
# PURPOSE:    Removes old backup files to prevent unbounded disk usage.
#             Keeps the N most recent backup files based on modification time.
# ARGUMENTS:
#   1. $backup_dir (string): Directory containing backups
#   2. $keep_count (integer): Number of recent backups to retain
# RETURNS:
#   - None
# ==============================================================================
sub _prune_old_backups {
    my ($backup_dir, $keep_count) = @_;

    opendir(my $dh, $backup_dir) or return;
    my @backups = grep { /^\.nfit\.history\.\d{8}_\d{6}\.\d{3}\.json$/ }
                  readdir($dh);
    closedir($dh);

    return if @backups <= $keep_count;

    # Sort by modification time (oldest first)
    my @sorted = map { $_->[0] }
                 sort { $a->[1] <=> $b->[1] }
                 map {
                     my $path = File::Spec->catfile($backup_dir, $_);
                     [$_, (stat($path))[9]]
                 } @backups;

    # Remove oldest files beyond keep_count
    my $remove_count = @sorted - $keep_count;
    for (my $i = 0; $i < $remove_count; $i++) {
        my $old_file = File::Spec->catfile($backup_dir, $sorted[$i]);
        unlink $old_file or warn "Could not remove old backup $old_file: $!";
        print "  - Pruned old backup: " . basename($old_file) . "\n";
    }
}

# ==============================================================================
# SUBROUTINE: _validate_pre_update_conditions
# PURPOSE:    Enforces strict pre-conditions before allowing --update-history to
#             proceed. Ensures both decay models have been executed for complete
#             results, and prevents accidental corruption of enriched months.
# ARGUMENTS:
#   1. $system_cache_dir (string): Path to the system's staging cache
#   2. $month_key (string): The month key being updated (e.g., "2025-09")
#   3. $force_flag (boolean): Whether --force flag was specified
# RETURNS:
#   - Dies with error message if validation fails (unless --force specified)
# ==============================================================================
sub _validate_pre_update_conditions {
    my ($system_cache_dir, $month_key, $force_flag) = @_;

    print "\n--- Pre-Update Validation ---\n";

    # Check 1: Verify both decay models have been executed
    my $l2_cache_file = File::Spec->catfile($system_cache_dir, '.nfit.cache.results');

    if (-f $l2_cache_file) {
        my $json = JSON->new->relaxed;
        my $l2_data;

        eval {
            open my $fh, '<:encoding(utf8)', $l2_cache_file
                or die "Could not open L2 cache: $!";
            my $json_text = do { local $/; <$fh> };
            close $fh;
            $l2_data = $json->decode($json_text);
        };

        if ($@) {
            warn "Warning: Could not read L2 cache for validation: $@\n";
        } else {
            my $has_decay_over_states = 0;
            my $has_windowed_decay = 0;

            # Scan L2 cache for analysisType values
            foreach my $l2_key (keys %$l2_data) {
                my $l2_results_aref = $l2_data->{$l2_key};
                next unless (ref($l2_results_aref) eq 'ARRAY' && @$l2_results_aref);

                my $first_result = $l2_results_aref->[0];
                my $analysis_type = $first_result->{analysisType} // '';

                $has_decay_over_states = 1 if $analysis_type eq 'hybrid_decay_aggregated';
                $has_windowed_decay = 1 if $analysis_type eq 'windowed_decay_aggregated';

                # Early exit if both found
                last if ($has_decay_over_states && $has_windowed_decay);
            }

            unless ($has_decay_over_states && $has_windowed_decay) {
                if ($force_flag) {
                    warn "â  WARNING: Both decay models not detected in L2 cache. " .
                         "Proceeding due to --force flag.\n";
                } else {
                    die "FATAL: Incomplete tactical analysis detected.\n" .
                        "Both --decay-over-states AND --enable-windowed-decay must be " .
                        "executed before --update-history.\n" .
                        "Current state: decay-over-states=$has_decay_over_states, " .
                        "windowed-decay=$has_windowed_decay\n" .
                        "Use --force to override this check.\n";
                }
            } else {
                print "  â Both decay models confirmed in L2 cache\n";
            }
        }
    } else {
        if ($force_flag) {
            warn "â  WARNING: L2 cache not found. Proceeding due to --force flag.\n";
        } else {
            die "FATAL: L2 results cache not found at '$l2_cache_file'.\n" .
                "You must run tactical analysis before --update-history.\n" .
                "Use --force to override this check.\n";
        }
    }

    # Check 2: Prevent overwriting already-enriched months
    my $history_data = read_unified_history($system_cache_dir);

    if (exists $history_data->{$month_key}) {
        if (exists $history_data->{$month_key}{SeasonalEventSnapshots}) {
            # Check if any snapshot has forecast enrichment
            my $snapshots = $history_data->{$month_key}{SeasonalEventSnapshots};
            foreach my $event_name (keys %$snapshots) {
                if (exists $snapshots->{$event_name}{forecast}) {
                    if ($force_flag) {
                        warn "â  WARNING: Month $month_key already enriched with forecasts. " .
                             "This will be overwritten. Proceeding due to --force flag.\n";
                    } else {
                        die "FATAL: Month $month_key already contains forecast enrichment.\n" .
                            "Re-running tactical analysis would corrupt forecast consistency.\n" .
                            "If intentional, use --force to override.\n" .
                            "Consider restoring from ROOT/backups/<serial>/ if this is an error.\n";
                    }
                    last;
                }
            }
        }
    }

    print "  â Pre-update validation passed\n";
}

# ==============================================================================
# SUBROUTINE: _add_lightweight_metadata
# PURPOSE:    Adds lightweight modification tracking metadata to each month entry
#             in the history cache, providing audit trails without the complexity
#             of cryptographic checksums.
# ARGUMENTS:
#   1. $history_data_href (hash ref): The complete history data structure
#   2. $updated_by (string): Tool name that performed the update
# RETURNS:
#   - None (modifies hash reference in place)
# ==============================================================================
sub _add_lightweight_metadata {
    my ($history_data_href, $updated_by) = @_;

    foreach my $month_key (keys %$history_data_href) {
        $history_data_href->{$month_key}{_metadata} = {
            last_updated => scalar(localtime->strftime('%Y-%m-%dT%H:%M:%S%z')),
            updated_by => $updated_by,
            toolkit_version => $VERSION,
            backup_recommendation => "Restore from ROOT/backups/<serial>/ if corruption suspected"
        };
    }
}

# ==============================================================================
# SUBROUTINE: _enrich_history_with_growth_rationale
# PURPOSE:    Performs a final enrichment pass on the unified history data (which is passed as an argument).
#             It robustly scans the nfit L2 Results Cache for growth-enabled
#             model runs, reads the embedded 'profileLabel' from the data, and
#             injects the 'growthRationale' block into the correct location,
#             keyed by the profile name.
# ==============================================================================
sub _enrich_history_with_growth_rationale {
    my ($system_cache_dir, $history_data) = @_;

    print "\n--- Phase 3: Enriching History with Growth Rationale from L2 Cache ---\n";

    my $l2_cache_file = File::Spec->catfile($system_cache_dir, '.nfit.cache.results');

    unless (-f $l2_cache_file) {
        print "  - INFO: L2 results cache not found. Skipping enrichment.\n";
        return 0; # Return false (was not modified)
    }

    # --- Step 1: Load both the history and the L2 cache ---
    my $l2_cache_data = eval {
        local $/;
        open my $fh, '<:encoding(utf8)', $l2_cache_file or die "Could not open L2 cache '$l2_cache_file': $!";
        my $json_text = <$fh>;
        close $fh;
        JSON->new->allow_nonref->decode($json_text);
    };
    if ($@ || !ref($l2_cache_data) eq 'HASH') {
        warn "Warning: Could not decode L2 results cache '$l2_cache_file'. Skipping enrichment.\n";
        return 0; # Return false (was not modified)
    }

    # --- Step 2: Scan L2 cache and build a structured hash of all available growth data ---
    # This hash is the single source of truth for all growth rationale.
    # Structure: {vm_name}{profile_name}{model_type} = rationale_hash
    my %growth_data_found;
    foreach my $l2_key (keys %$l2_cache_data) {
        my $l2_results_aref = $l2_cache_data->{$l2_key};
        next unless (ref($l2_results_aref) eq 'ARRAY' && @$l2_results_aref);

        my $first_result = $l2_results_aref->[0];
        my $analysis_type = $first_result->{analysisType} // '';
        my $model_type;

        if ($analysis_type eq 'hybrid_decay_aggregated') {
            $model_type = 'decay_over_states';
        } elsif ($analysis_type eq 'windowed_decay_aggregated') {
            $model_type = 'windowed_decay';
        } else {
            next; # Not a growth-enabled model result, skip this L2 cache entry.
        }

        foreach my $result (@$l2_results_aref) {
            my $vm = $result->{vmName};
            next unless $vm;

            # Instead of a top-level profileLabel, iterate the metrics.physc
            # block to find all profiles and their nested rationales.
            my $physc_metrics = _safe_dig($result, 'metrics', 'physc');
            next unless (ref($physc_metrics) eq 'HASH');

            foreach my $profile_name (keys %$physc_metrics) {
                my $profile_data = $physc_metrics->{$profile_name};

                # Check for the rationale (the trigger)
                if (ref($profile_data) eq 'HASH' && $profile_data->{growthRationale}) {
                    # --- Store the rationale AND its associated tactical values ---
                    $growth_data_found{$vm}{$profile_name}{$model_type} = {
                        rationale  => $profile_data->{growthRationale},
                        BaseValue  => $profile_data->{BaseValue},
                        FinalValue => $profile_data->{FinalValue}
                    };
                }
           }
        }
    }

    # --- Step 3: Iterate through history and inject the found growth data ---
    my $was_modified = 0;
    foreach my $month_key (keys %$history_data) {
        next unless (ref($history_data->{$month_key}) eq 'HASH' && $history_data->{$month_key}{MonthlyWorkloadAnalysis});
        my $workload_analysis = $history_data->{$month_key}{MonthlyWorkloadAnalysis};

        foreach my $vm_name (keys %$workload_analysis) {
            # Skip VMs for which no growth data was ever found.
            next unless exists $growth_data_found{$vm_name};

            my $vm_analysis = $workload_analysis->{$vm_name};

            # Inject all found growth data for this VM.
            foreach my $profile_name (keys %{$growth_data_found{$vm_name}}) {
                foreach my $model_type (keys %{$growth_data_found{$vm_name}{$profile_name}}) {

                    # --- Unpack the new data wrapper ---
                    my $new_data_wrapper = $growth_data_found{$vm_name}{$profile_name}{$model_type}; # This is the hash {rationale, BaseValue, FinalValue}
                    my $new_rationale    = $new_data_wrapper->{rationale};

                    # Ensure the nested structure exists for Rationale.
                    $vm_analysis->{growthRationale} //= {};
                    $vm_analysis->{growthRationale}{$profile_name} //= {};

                    # --- Create new block for Tactical Values ---
                    # This is the new block to store tactical results
                    $vm_analysis->{TacticalValues} //= {};
                    $vm_analysis->{TacticalValues}{$profile_name} //= {};

                    # Check if the Rationale data is new or different.
                    if (!exists $vm_analysis->{growthRationale}{$profile_name}{$model_type} ||
                        Dumper($vm_analysis->{growthRationale}{$profile_name}{$model_type}) ne Dumper($new_rationale))
                    {
                         $vm_analysis->{growthRationale}{$profile_name}{$model_type} = $new_rationale;
                         $was_modified = 1;
                    }

                    # --- Harvest BaseValue and FinalValue ---
                    # This archives the tactical model's input (BaseValue)
                    # and output (FinalValue) for long-term auditing.
                    my $base_val = $new_data_wrapper->{BaseValue};
                    my $final_val = $new_data_wrapper->{FinalValue};

                    if (!exists $vm_analysis->{TacticalValues}{$profile_name}{$model_type} ||
                        $vm_analysis->{TacticalValues}{$profile_name}{$model_type}{BaseValue} ne $base_val ||
                        $vm_analysis->{TacticalValues}{$profile_name}{$model_type}{FinalValue} ne $final_val)
                    {
                        # Add the tactical results for this profile AND model
                        $vm_analysis->{TacticalValues}{$profile_name}{$model_type} = {
                            BaseValue  => $base_val,
                            FinalValue => $final_val
                        };
                        $was_modified = 1;
                    }
                }
            }
        }
    }

    # --- Step 4: Write back to the history file ONLY if it was modified ---
    if ($was_modified) {
        print "  - INFO: Found new/updated growth rationale data in L2 cache. Updating history file...\n";
        _write_full_history($system_cache_dir, $history_data);
    } else {
        print "  - INFO: No new growth rationale data found in L2 cache. History is up to date.\n";
    }

    # Return the modification status to the caller
    return $was_modified;
}

# ==============================================================================
# HELPER SUBROUTINE for the enrichment process.
# PURPOSE:    A robust replication of nfit.pl's generate_canonical_key function.
#             It builds the precise key used to store results in the L2 cache for
#             a specific growth analysis run.
# ==============================================================================
sub _generate_l2_cache_key_for_nfit {
    my ($profile_obj, $model_type, $system_cache_dir) = @_;

    my @key_parts;
    my $flags = $profile_obj->{flags};

    # This simulates the arguments nfit.pl would receive from a typical
    # nfit-profile run that enables growth prediction.

    # --- Flags passed from nfit-profile to nfit ---
    push @key_parts, "--nmondir $system_cache_dir";
    push @key_parts, "--smt $default_smt_arg";
    push @key_parts, "--runq-avg-method $nfit_runq_avg_method_str";
    push @key_parts, "--peak"; # nfit-profile always adds this

    # --- Flags from the profile definition ---
    # Parse them from the string to handle them individually
    if ($flags =~ /--percentile\s+([0-9.]+)/ || $flags =~ /-p\s+([0-9.]+)/) { push @key_parts, "--percentile $1"; }
    if ($flags =~ /--window\s+(\d+)/ || $flags =~ /-w\s+(\d+)/) { push @key_parts, "--window $1"; }
    if ($flags =~ /--avg-method\s+(\w+)/) { push @key_parts, "--avg-method $1"; }
    if ($flags =~ /--decay\s+([\w-]+)/) { push @key_parts, "--decay $1"; }
    if ($flags =~ /--runq-decay\s+([\w-]+)/) { push @key_parts, "--runq-decay $1"; }
    if ($flags =~ /--filter-above-perc\s+([0-9.]+)/) { push @key_parts, "--filter-above-perc $1"; }
    if ($flags =~ /--online/) { push @key_parts, "--online"; }
    if ($flags =~ /--batch/) { push @key_parts, "--batch"; }
    if ($flags =~ /--no-weekends/) { push @key_parts, "--no-weekends"; }

    # --- Flags specific to the growth model ---
    if ($model_type eq 'decay_over_states') {
        push @key_parts, "--decay-over-states";
    } elsif ($model_type eq 'windowed_decay') {
        push @key_parts, "--enable-windowed-decay";
        push @key_parts, "--process-window-unit $nfit_window_unit_str";
        push @key_parts, "--process-window-size $nfit_window_size_val";
    }

    # Growth flags are present in the profile, so they are included automatically
    if ($flags =~ /--enable-growth-prediction/) {
        push @key_parts, "--enable-growth-prediction";
        if ($flags =~ /--growth-projection-days\s+(\d+)/) { push @key_parts, "--growth-projection-days $1"; }
        if ($flags =~ /--max-growth-inflation-percent\s+(\d+)/) { push @key_parts, "--max-growth-inflation-percent $1"; }
    }

    # Note: This replication assumes a standard set of flags passed from nfit-profile.
    # It prioritizes the flags that define the analysis type and profile uniqueness.
    # This is far more robust than the previous simple string concatenation.

    return join(" ", sort @key_parts);
}

# ==============================================================================
# SUBROUTINE: _assemble_monthly_analysis
# PURPOSE:    Assembles the MonthlyWorkloadAnalysis data structure from parsed
#             nfit JSON output. This function contains the battle-tested logic
#             for extracting profile values, calculating sizing hints, building
#             RunQ metrics, and assembling ClippingInfo blocks.
#
# ARGUMENTS:
#   1. $parsed_results_href (hash ref): Parsed JSON from nfit (VM -> [states])
#   2. $adaptive_runq_saturation_thresh (float): Adaptive saturation threshold
#
# RETURNS:
#   - Hash reference containing the complete MonthlyWorkloadAnalysis structure
#     keyed by VM name.
#
# EXTRACTED FROM:
#   This function was extracted from _generate_monthly_workload_analysis to
#   enable code reuse and preserve all recent bug fixes to ClippingInfo
#   assembly and RunQ metrics extraction.
# ==============================================================================
sub _assemble_monthly_analysis {
    my ($parsed_results_href, $adaptive_runq_saturation_thresh) = @_;

    # Validate inputs
    unless (ref($parsed_results_href) eq 'HASH') {
        die "FATAL: _assemble_monthly_analysis requires parsed results hash reference";
    }

    my %analysis_results;
    my %collated_results_table;  # Temporary table for hint generation

    # ======================================================================
    # STEP 1: Build collated results table for ALL profiles
    # ======================================================================
    # This step harvests all P-metrics for the MonthlyWorkloadAnalysis.ProfileValues
    # block, fixing the historical data regression.
    # It ALSO surgically extracts the max P-99W1 'Peak' metric, which is
    # a critical, separate input for the hint-generation logic in STEP 2.
    # ======================================================================
    foreach my $vm_name (keys %$parsed_results_href) {
        my $states_aref = $parsed_results_href->{$vm_name};
        next unless ref($states_aref) eq 'ARRAY' && @$states_aref;

        my @p99w1_peak_values_for_hinting; # Array to store P-99W1 'Peak' metrics

        # Iterate all configured profiles to get their specific P-metric
        foreach my $profile (@profiles) {
            my $profile_name = $profile->{name};

            # Find the P-metric for this profile (e.g., "P95")
            my ($p_val_num) = $profile->{flags} =~ /(?:-p|--percentile)\s+([0-9.]+)/;
            my $p_metric_key = "P" . clean_perc_label($p_val_num // $DEFAULT_PERCENTILE);

            my $metric_val = undef;

            # Iterate ALL states (newest to oldest)
            for (my $i = $#{$states_aref}; $i >= 0; $i--) {
                my $state = $states_aref->[$i];

                # A. Find the P-metric value (if we haven't already)
                $metric_val //= _safe_dig($state, 'metrics', 'physc', $profile_name, $p_metric_key);

                # B. If this is the P-99W1 profile, find its 'Peak' metric for hinting
                if ($profile_name eq $MANDATORY_PEAK_PROFILE_FOR_HINT) {
                    my $profile_peak_val = _safe_dig($state, 'metrics', 'physc', $profile_name, 'Peak');
                    push @p99w1_peak_values_for_hinting, $profile_peak_val if (defined $profile_peak_val && looks_like_number($profile_peak_val));
                }
            } # end state loop

            # Store the final P-metric value for the ProfileValues block
            if (defined $metric_val && looks_like_number($metric_val)) {
                $collated_results_table{$vm_name}{$profile_name} = $metric_val + 0;
            }
        } # end profile loop

        # Store the max P-99W1 Profile Peak for hint logic, using the original key
        if (@p99w1_peak_values_for_hinting) {
            $collated_results_table{$vm_name}{$PEAK_PROFILE_NAME} = max(@p99w1_peak_values_for_hinting);
        }
    }

    # ======================================================================
    # STEP 2: Generate hints and extract metrics for each VM
    # ======================================================================
    foreach my $vm_name (keys %collated_results_table) {
        my $vm_config_ref = $vm_config_data{$vm_name};
        my $states_aref = $parsed_results_href->{$vm_name};
        my $last_state_data = $states_aref->[-1] // {};

        # ------------------------------------------------------------------
        # Extract configuration metadata from the last state
        # ------------------------------------------------------------------
        my $max_cpu_from_state = _safe_dig($last_state_data, 'metadata', 'configuration', 'maxCpu') // 0;
        my $smt_from_state     = _safe_dig($last_state_data, 'metadata', 'configuration', 'smt') // $default_smt_arg;

        # ------------------------------------------------------------------
        # Build the VM map structure for sizing hint generation
        # ------------------------------------------------------------------
        my %vm_map = (
            Configuration => {
                vm_name     => $vm_name,
                max_cpu     => $max_cpu_from_state,
                smt         => $smt_from_state,
                entitlement => $vm_config_ref->{entitlement} // 0,
                %{$vm_config_ref || {}}
            },
            CoreResults => {
                ProfileValues => $collated_results_table{$vm_name}
            },
            RunQMetrics => _safe_dig($last_state_data, 'metrics', 'runq') || {}
        );

        # ------------------------------------------------------------------
        # Generate sizing hint using existing, proven logic
        # ------------------------------------------------------------------
        my ($hint, $pattern, $pressure, $pressure_detail, $rationale_text, $has_abs_pressure, $has_norm_pressure) =
            generate_sizing_hint(\%vm_map, undef, $adaptive_runq_saturation_thresh);

        # ------------------------------------------------------------------
        # Extract RunQ metrics from the last state
        # ------------------------------------------------------------------
        my $runq_metrics_block = $last_state_data->{metrics}{runq} || {};

        # ------------------------------------------------------------------
        # Build ClippingInfo for all profiles
        # ------------------------------------------------------------------
        # This logic iterates through all profiles and extracts clipping
        # detection results. The structure here has been carefully debugged
        # and must be preserved exactly.
        # ------------------------------------------------------------------
        my $clipping_info_for_vm = {};

        foreach my $profile_name (keys %{$last_state_data->{metrics}{physc} || {}}) {
            # Skip if this isn't a real profile (e.g., metadata keys)
            next unless ref($last_state_data->{metrics}{physc}{$profile_name}) eq 'HASH';

            # Extract clipping data from this profile's output
            my $clipping_metrics = _safe_dig($last_state_data, 'metrics', 'physc', $profile_name, 'ClippingInfo');

            # Try alternate path if first doesn't work (backwards compatibility)
            $clipping_metrics //= _safe_dig($last_state_data, 'metrics', 'physc', $profile_name, 'Clipping');

            # Only process if we found valid clipping data
            if ($clipping_metrics && ref($clipping_metrics) eq 'HASH' && exists $clipping_metrics->{isClipped}) {
                my $unmet_demand_est = $clipping_metrics->{unmetDemandEstimate} // 0;

                # Get the base value for this profile to calculate unclipped peak estimate
                my $profile_obj = (grep { $_->{name} eq $profile_name } @profiles)[0];
                next unless $profile_obj;  # Safety check

                my $p_key = "P" . clean_perc_label(($profile_obj->{flags} =~ /-p\s+([0-9.]+)/) ? $1 : $DEFAULT_PERCENTILE);
                my $base_peak_for_unclip = _safe_dig($last_state_data, 'metrics', 'physc', $profile_name, $p_key);

                # Get MaxCPU from VM config
                # IMPORTANT: ClippingInfo uses 'maxcpu' (lowercase), not 'max_cpu'
                my $maxcpu = $vm_config_ref->{maxcpu} // 0;

                # Assemble the ClippingInfo structure with exact key names
                # that downstream consumers (nfit-forecast, GUI) expect
                $clipping_info_for_vm->{$profile_name} = {
                    isClipped               => $clipping_metrics->{isClipped},
                    clippingConfidence      => $clipping_metrics->{clippingConfidence} // 'unknown',
                    capacityLimit           => $maxcpu,
                    unmetDemandEstimate     => $unmet_demand_est,
                    unclippedPeakEstimate   => ((defined $base_peak_for_unclip && looks_like_number($base_peak_for_unclip)) ? $base_peak_for_unclip : 0) + $unmet_demand_est,
                    platformSpecificMarkers => {
                        aix_runq_saturation => _safe_dig($clipping_metrics, 'platformMarkers', 'aix_runq_saturation') // 0
                    }
                };
            }

            # --- Harvest DailyProfileSeries Data ---
            # Extract the daily time-series map
            my $daily_series = _safe_dig($last_state_data, 'metrics', 'physc', $profile_name, 'DailySeries');
            if (ref($daily_series) eq 'HASH' && scalar(keys %$daily_series) > 0) {
                # Temporarily store in collated table for final assembly
                $collated_results_table{$vm_name}{_DailySeries}{$profile_name} = $daily_series;
            }
        }

        # ------------------------------------------------------------------
        # Assemble the final data structure for this VM
        # ------------------------------------------------------------------
        # CRITICAL: This structure must NOT include growthRationale or any
        # other predictive metadata. MonthlyWorkloadAnalysis is pure historical.
        # The growthRationale is harvested separately by the enrichment phase.
        # ------------------------------------------------------------------
        $analysis_results{$vm_name} = {
            Hint           => $hint,
            Pattern        => $pattern,
            Pressure       => $pressure,
            PressureDetail => $pressure_detail,
            RunQMetrics    => {
                # Extract the specific RunQ metrics needed by downstream logic
                AbsRunQ_P90  => _safe_dig($runq_metrics_block, 'absolute', $MANDATORY_PEAK_PROFILE_FOR_HINT, 'P90'),
                NormRunQ_P50 => _safe_dig($runq_metrics_block, 'normalized', $MANDATORY_PEAK_PROFILE_FOR_HINT, 'P50'),
                NormRunQ_P90 => _safe_dig($runq_metrics_block, 'normalized', $MANDATORY_PEAK_PROFILE_FOR_HINT, 'P90')
            },
            ProfileValues  => $collated_results_table{$vm_name},
            DailyProfileSeries => $collated_results_table{$vm_name}{_DailySeries} || {},
            ClippingInfo   => $clipping_info_for_vm
        };

        # Cleanup: Remove the temporary _DailySeries key from ProfileValues to keep the schema clean
        delete $analysis_results{$vm_name}{ProfileValues}{_DailySeries};
    }

    return \%analysis_results;
}

# ==============================================================================
# SUBROUTINE: _generate_monthly_workload_analysis
# PURPOSE:    Performs a full single-pass analysis for a given time period
#             (typically one month) using the manifest-driven Single Pass Engine
#             architecture, then assembles the comprehensive MonthlyWorkloadAnalysis
#             data structure.
#
#             This function has been refactored to use the SPE to achieve dramatic
#             I/O reduction (16x for a typical 16-profile configuration).
# ==============================================================================
sub _generate_monthly_workload_analysis {
    my ($system_cache_dir, $start_date_str, $end_date_str, $adaptive_runq_saturation_thresh) = @_;

    print STDERR "    - Using Single Pass Engine for monthly analysis\n";

    # ======================================================================
    # STEP 1: Build the tactical manifest
    # ======================================================================
    # Use the existing, battle-tested build_transform_manifest function.
    # This function handles all the complexity of profile parameter extraction,
    # transform key generation, and percentile de-duplication.
    # ======================================================================
    my $tactical_manifest = build_transform_manifest(\@profiles, $nfit_runq_avg_method_str);

    # ======================================================================
    # STEP 2: Create the PURE HISTORICAL manifest
    # ======================================================================
    # The sanitisation function is the primary data purity guardrail.
    # It strips all predictive elements (growth, decay) whilst preserving
    # the profile's fundamental measurement definition.
    # ======================================================================
    my $historical_manifest = _sanitise_manifest_for_history($tactical_manifest);

    # ======================================================================
    # STEP 3: Write manifest to temporary file
    # ======================================================================
    use File::Temp qw(tempfile);
    my ($fh_manifest, $manifest_filename) = tempfile(
        'nfit_history_manifest_XXXXXX',
        SUFFIX => '.json',
        UNLINK => 1,
        TMPDIR => 1
    );

    # Write the manifest using canonical JSON encoding for consistency
    print $fh_manifest JSON->new->pretty->canonical->encode($historical_manifest);
    close $fh_manifest;

    # ======================================================================
    # STEP 4: Build and execute the single nfit command
    # ======================================================================
    my $nfit_cmd = "$nfit_script_path --manifest $manifest_filename "
                 . "--nmondir \"$system_cache_dir\" "
                 . "--startdate $start_date_str "
                 . "--enddate $end_date_str "
                 . "--smt $default_smt_arg "
                 . "--runq-avg-method $nfit_runq_avg_method_str "
                 . "--enable-clipping-detection "  # Historical capacity measurement
                 . "--show-progress";

    # Execute nfit once for all profiles
    print STDERR "    - Executing nfit engine for date range: $start_date_str to $end_date_str\n";

    my $nfit_output = '';
    my $nfit_error  = '';

    # Capture both STDOUT (JSON) and STDERR (progress/errors)
    my $stderr_arg = ">&=" . fileno(STDERR);
    my $pid = open3(undef, my $stdout_fh, $stderr_arg, $nfit_cmd);

    while (my $line = <$stdout_fh>) {
        $nfit_output .= $line;
    }

    waitpid($pid, 0);
    my $exit_code = $? >> 8;

    # Check for execution failure
    if ($exit_code != 0) {
        die "FATAL: nfit engine failed during monthly analysis.\n"
            . "Exit code: $exit_code\n"
            . "Command: $nfit_cmd\n"
            . "Check STDERR output above for details.\n";
    }

    # ======================================================================
    # STEP 5: Parse the JSON output
    # ======================================================================
    # Use the existing, proven parse_nfit_json_output function.
    # It correctly handles multi-line JSON output where each line is a
    # separate VM result object.
    # ======================================================================
    my $parsed_results = parse_nfit_json_output($nfit_output);

    # Sanity check: ensure we got results
    unless (ref($parsed_results) eq 'HASH' && scalar keys %$parsed_results) {
        warn "WARNING: nfit returned no results for monthly analysis period $start_date_str to $end_date_str\n";
        return {};
    }

    print STDERR "    - Successfully processed " . scalar(keys %$parsed_results) . " VM(s)\n";

    # ======================================================================
    # STEP 6: Assemble the final MonthlyWorkloadAnalysis structure
    # ======================================================================
    # Call the extracted assembly logic which contains all the recent
    # bug fixes for ClippingInfo, RunQ metrics, and profile value extraction.
    # ======================================================================
    my $analysis_results = _assemble_monthly_analysis(
        $parsed_results,
        $adaptive_runq_saturation_thresh
    );

    return $analysis_results;
}

# ==============================================================================
# SUBROUTINE: detect_sampling_interval
# Robustly detects the NMON sampling interval from the .nfit.cache.data file.
# It reads the start of the cache, isolates timestamps for the first VM found,
# calculates the time difference between consecutive samples, and finds the mode
# of these deltas to determine the most likely interval.
#
# Note: This version introduces jitter bucketing: Before counting the frequency,
#  it checks if a delta is close to a standard interval (60, 300, 600, 900 seconds)
#  within a tolerance ($EPS = 3). If it is, it snaps the value to that standard
#  interval before incrementing the count.
# Returns:
#   - The detected interval in seconds (e.g., 60, 300), or undef on failure.
sub detect_sampling_interval {
    my ($data_cache_file) = @_;

    my $SAMPLES_TO_GATHER       = 50;
    my $MAX_LINES_TO_SCAN       = 5000;
    my $MIN_SAMPLES_FOR_DETECTION = 5;   # required count in a single interval bucket
    my @STD = (60, 300, 600, 900);
    my $EPS = 3;                          # jitter tolerance (seconds)

    return undef unless (-f $data_cache_file && -s $data_cache_file);

    open my $fh, '<:encoding(utf8)', $data_cache_file or do {
        warn "Warning: Could not open data cache '$data_cache_file' for interval detection: $!";
        return undef;
    };

    <$fh>; # Skip header

    my $target_vm_for_detection;
    my @timestamps;
    my $lines_scanned = 0;

    while (my $line = <$fh>) {
        $lines_scanned++;
        last if $lines_scanned > $MAX_LINES_TO_SCAN;

        my ($ts_str, $vm_name) = ($line =~ /^(\d{4}-\d{2}-\d{2}\s\d{2}:\d{2}:\d{2}),([^,]+)/);
        next unless ($ts_str && $vm_name);

        $target_vm_for_detection //= $vm_name;
        next unless $vm_name eq $target_vm_for_detection;

        eval {
            push @timestamps, Time::Piece->strptime($ts_str, "%Y-%m-%d %H:%M:%S")->epoch;
        };
        last if @timestamps >= $SAMPLES_TO_GATHER;
    }
    close $fh;

    # Need enough timestamps to produce at least MIN_SAMPLES_FOR_DETECTION deltas
    return undef if @timestamps < ($MIN_SAMPLES_FOR_DETECTION + 1);

    @timestamps = sort { $a <=> $b } @timestamps;

    my %delta_counts;
    for (my $i = 1; $i < @timestamps; $i++) {
        my $delta = $timestamps[$i] - $timestamps[$i-1];
        next unless ($delta > 10 && $delta < 1810);

        # Jitter bucketing: snap near-standards to the standard value
        my $bucket = $delta;
        my $best_d = $EPS + 1;
        for my $s (@STD) {
            my $d = abs($delta - $s);
            if ($d < $best_d) { $best_d = $d; $bucket = ($d <= $EPS) ? $s : $bucket; }
        }
        $delta_counts{$bucket}++;
    }

    return undef unless %delta_counts;

    # Find max frequency
    my $max_count = 0;
    foreach my $delta (keys %delta_counts) {
        $max_count = $delta_counts{$delta} if $delta_counts{$delta} > $max_count;
    }

    # Require minimum evidence in the winning bucket
    return undef if $max_count < $MIN_SAMPLES_FOR_DETECTION;

    # Collect all tied winners
    my @tied_deltas = grep { $delta_counts{$_} == $max_count } keys %delta_counts;

    # Deterministic tie-break: closest to a standard, then smallest delta
    if (@tied_deltas > 1) {
        my %distances;
        for my $delta (@tied_deltas) {
            my $min_dist = 1e9;
            foreach my $std (@STD) {
                my $dist = abs($delta - $std);
                $min_dist = $dist if $dist < $min_dist;
            }
            $distances{$delta} = $min_dist;
        }
        @tied_deltas = sort { $distances{$a} <=> $distances{$b} || $a <=> $b } @tied_deltas;
    }

    return $tied_deltas[0];
}

# ==============================================================================
# --- set_adaptive_thresholds ---
# Takes a detected sampling interval, snaps it to a standard value (e.g., 1, 5, 10 min),
# calculates the adaptive thresholds for saturation and efficiency, and logs the rationale.
# This function does NOT modify global state; it returns the calculated values.
#
# Returns:
#   - A list of three values:
#     1. The new saturation threshold (float).
#     2. The new target for efficiency calculation (float).
#     3. The new max downsizing percentage (as a factor, e.g., 0.05 for 5%).
sub set_adaptive_thresholds {
    my ($raw_interval_seconds, $log_fh) = @_;

    # --- Baseline (1-min) thresholds ---
    my $sat_thresh = $RUNQ_PRESSURE_P90_SATURATION_THRESHOLD;
    my $target_norm_runq = $DEFAULT_TARGET_NORM_RUNQ_FOR_EFFICIENCY_CALC;
    my $max_downsize_perc_factor = $MAX_EFFICIENCY_REDUCTION_PERCENTAGE;

    my $snapped_minutes = 1.0;
    my $detection_method_log = "cache-based detection using mode of deltas with epsilon bucketing";
    my $log_message;

    if (!defined $raw_interval_seconds) {
        $log_message = "No reliable interval found; defaulting to 1.0 min; thresholds at 1-min baseline.";
    } else {
        # Snap the detected raw seconds to the nearest standard interval with a tolerance
        my $epsilon = 5; # Allow +/- 5 seconds
        if    (abs($raw_interval_seconds - 60) <= $epsilon)  { $snapped_minutes = 1; }
        elsif (abs($raw_interval_seconds - 300) <= $epsilon) { $snapped_minutes = 5; }
        elsif (abs($raw_interval_seconds - 600) <= $epsilon) { $snapped_minutes = 10; }
        elsif (abs($raw_interval_seconds - 900) <= $epsilon) { $snapped_minutes = 15; }
        else {
             # If it doesn't snap cleanly, default to 1 min but log the anomaly.
            $log_message = sprintf("Unusual interval %ds detected; did not snap to a standard value. Defaulting to 1.0 min.", $raw_interval_seconds);
            $snapped_minutes = 1.0;
        }
    }

    if ($snapped_minutes > 1) {
        # It's a non-default interval, so calculate the adaptive thresholds.
        my $k    = $snapped_minutes;
        my $base = $RUNQ_PRESSURE_P90_SATURATION_THRESHOLD; # Follow the global 1-min baseline
        my $beta = 0.4; # Decay exponent, could be made configurable in the future
        # General formula: 1 + (base-1) * k^(-beta)
        # This removes the hard-coded '0.8' and derives it from the baseline (1.8 - 1.0)
        $sat_thresh = 1 + (($base - 1.0) * ($k**-$beta));
        # Downsizing thresholds: Use the lookup table for conservatism
        my %downsize_targets = ( 5 => 0.75, 10 => 0.72, 15 => 0.70 );
        my %downsize_caps    = ( 5 => 5,    10 => 5,    15 => 5 ); # Cap is 5% for all coarser intervals

        $target_norm_runq = $downsize_targets{$k} // $target_norm_runq;
        # NOTE: The cap is stored as a percentage and converted to a factor here.
        my $cap_perc = $downsize_caps{$k};
        if (defined $cap_perc) {
            $max_downsize_perc_factor = $cap_perc / 100.0;
        }

        $log_message = sprintf("Sampling interval detected %.1fs -> snapped to %.1f min.", $raw_interval_seconds, $snapped_minutes) if defined $raw_interval_seconds;
    } elsif (!defined $log_message) {
         $log_message = sprintf("Sampling interval detected %.1fs -> snapped to 1.0 min. Using baseline thresholds.", $raw_interval_seconds) if defined $raw_interval_seconds;
    }


    # --- Rationale Logging ---
    if ($log_fh) {
        print {$log_fh} "----------------------------------------------------------------------\n";
        print {$log_fh} "Adaptive Threshold Calculation\n";
        print {$log_fh} "  - Method                   : $detection_method_log\n";
        print {$log_fh} "  - Status                   : $log_message\n";
        print {$log_fh} "  - Saturation Threshold     : " . sprintf("%.2f", $sat_thresh) . " x LCPU\n";
        print {$log_fh} "  - Downsizing Target        : " . sprintf("%.2f", $target_norm_runq) . " / LCPU\n";
        print {$log_fh} "  - Downsizing Cap           : " . sprintf("%.1f%%", $max_downsize_perc_factor * 100) . "\n";
        print {$log_fh} "----------------------------------------------------------------------\n";
    }

    return ($sat_thresh, $target_norm_runq, $max_downsize_perc_factor);
}

# --- parse_vm_tier_overrides ---
# Parses a simple INI-style file to allow users to override the auto-detected
# tier for specific VMs, giving them fine-grained control.
sub parse_vm_tier_overrides {
    my ($filepath) = @_;
    my %overrides;
    return \%overrides unless (-f $filepath);

    print STDERR "  - VM Tier Overrides found: $filepath\n";
    open my $fh, '<:encoding(utf8)', $filepath or do {
        warn "Warning: Could not open VM tier override file '$filepath': $!. Skipping overrides.";
        return \%overrides;
    };

    my $current_vm = '';
    while (my $line = <$fh>) {
        chomp $line;
        $line =~ s/\s*[#;].*//;
        $line =~ s/^\s+|\s+$//g;
        next if $line eq '';

        if ($line =~ /^\s*\[\s*([^\]]+?)\s*\]\s*$/) {
            $current_vm = $1;
        } elsif ($current_vm ne '' && $line =~ /^\s*tier\s*=\s*(.+)$/i) {
            my $tier = uc($1);
            $tier =~ s/\s+//g;
            $overrides{$current_vm} = $tier;
            $current_vm = ''; # Reset after reading the tier
        }
    }
    close $fh;
    print STDERR "    - Loaded " . scalar(keys %overrides) . " VM-specific tier overrides.\n";
    return \%overrides;
}

# ==============================================================================
# SUBROUTINE: build_transform_manifest
# PURPOSE:    Parses all loaded profiles and aggregates their computational
#             requirements into a single "transform manifest". This manifest
#             identifies each unique data transformation needed for PhysC,
#             NormRunQ, and AbsRunQ, ensuring calculation parameters are
#             correctly isolated.
# ==============================================================================
# REPLACE the entire 'build_transform_manifest' subroutine with this corrected version.
# This version ensures strict isolation between all manifest entries.

sub build_transform_manifest {
    my ($profiles_ref, $global_runq_avg_method) = @_;
    my %manifest;

    # Helper to create a consistent transform key.
    my $_get_transform_key = sub {
        my ($metric, $method, $window, $decay, $filter, $time, $weekends) = @_;
        return join(":", $metric, $method, $window, $decay, $filter, $time, $weekends);
    };

    foreach my $profile (@$profiles_ref) {
        my $flags = $profile->{flags};
        my $p_name = $profile->{name};

        # --- Extract all relevant parameters from the profile's flags ---
        my ($p_perc)      = $flags =~ /-p\s+([0-9.]+)/;
        my ($w_min)       = $flags =~ /-w\s+(\d+)/;
        my ($avg_method)  = $flags =~ /--avg-method\s+(\w+)/;
        my ($decay)       = $flags =~ /--decay\s+([\w-]+)/;
        my ($runq_decay)  = $flags =~ /--runq-decay\s+([\w-]+)/;
        my ($filter_perc) = $flags =~ /--filter-above-perc\s+([0-9.]+)/;
        my ($rq_norm_str) = $flags =~ /--runq-norm-perc\s+"?([0-9.,\s]+)"?/;
        my ($rq_abs_str)  = $flags =~ /--runq-abs-perc\s+"?([0-9.,\s]+)"?/;
        my $time_filter = 'none';
        if ($flags =~ / -online\b/)     { $time_filter = 'online'; }
        elsif ($flags =~ / -batch\b/)   { $time_filter = 'batch'; }
        my $no_weekends = ($flags =~ / -no-weekends\b/) ? 1 : 0;

        # --- 1. PhysC Transform ---
        my $physc_method = $avg_method // $DEFAULT_AVG_METHOD;
        my $physc_decay = $decay // $DEFAULT_DECAY_LEVEL;
        my $physc_window = $w_min // 15;
        my $physc_filter = $filter_perc // '0';

        my $physc_key = $_get_transform_key->('PhysC', $physc_method, $physc_window, $physc_decay, $physc_filter, $time_filter, $no_weekends);

        # Find or create the transform. CRITICAL: Initialize 'profiles' with a NEW hash ref '{}'.
        $manifest{$physc_key} //= {
            metric => 'PhysC', method => $physc_method, window => $physc_window, decay => $physc_decay,
            filter_perc => $physc_filter, time_filter => $time_filter, no_weekends => $no_weekends,
            profiles => {}
        };

        # Add this profile's directives under the correct transform.
        if (defined $p_perc) {
            my $p_directives = $manifest{$physc_key}{profiles}{$p_name} //= {};
            push @{ $p_directives->{percentiles} }, $p_perc;

            $p_directives->{enable_growth} = 1 if ($flags =~ /--enable-growth-prediction\b/);
            $p_directives->{enable_clipping} = 1 if ($flags =~ /--enable-clipping-detection\b/);
            $p_directives->{calculate_peak} = 1 if ($p_name eq $MANDATORY_PEAK_PROFILE_FOR_HINT || $flags =~ /-k\b|--peak\b/);
        }

        # --- 2. RunQ Transforms ---
        my $runq_eff_decay = $runq_decay // $decay // 'medium';
        my $runq_method = ($global_runq_avg_method eq 'none') ? 'none' : ($global_runq_avg_method // $DEFAULT_NFIT_RUNQ_AVG_METHOD);
        my $runq_window = $w_min // 15;

        if (defined $rq_norm_str) {
            my @norm_percs = grep { looks_like_number($_) } split /[,\s]+/, $rq_norm_str;
            push @norm_percs, (25, 50, 75, 90);
            my %seen; @norm_percs = grep { !$seen{$_}++ } @norm_percs;

            if (@norm_percs) {
                my $norm_key = $_get_transform_key->('NormRunQ', $runq_method, $runq_window, $runq_eff_decay, '0', $time_filter, $no_weekends);
                $manifest{$norm_key} //= {
                    metric => 'NormRunQ', method => $runq_method, window => $runq_window, decay => $runq_eff_decay,
                    filter_perc => '0', time_filter => $time_filter, no_weekends => $no_weekends,
                    profiles => {}
                };
                if (@norm_percs) {
                    my $n_directives = $manifest{$norm_key}{profiles}{$p_name} //= {};
                    push @{ $n_directives->{percentiles} }, @norm_percs;
                }
            }
        }

        if (defined $rq_abs_str) {
            my @abs_percs = grep { looks_like_number($_) } split /[,\s]+/, $rq_abs_str;
            push @abs_percs, $p_perc if ($runq_perc_behavior_mode eq 'match' && defined $p_perc);
            my %seen; @abs_percs = grep { !$seen{$_}++ } @abs_percs;

            if (@abs_percs) {
                my $abs_key = $_get_transform_key->('AbsRunQ', $runq_method, $runq_window, $runq_eff_decay, '0', $time_filter, $no_weekends);
                $manifest{$abs_key} //= {
                    metric => 'AbsRunQ', method => $runq_method, window => $runq_window, decay => $runq_eff_decay,
                    filter_perc => '0', time_filter => $time_filter, no_weekends => $no_weekends,
                    profiles => {}
                };
                if (@abs_percs) {
                    my $a_directives = $manifest{$abs_key}{profiles}{$p_name} //= {};
                    push @{ $a_directives->{percentiles} }, @abs_percs;
                }
            }
        }
    }

    # Final pass to de-duplicate percentiles
    foreach my $key (keys %manifest) {
        foreach my $p_name (keys %{ $manifest{$key}{profiles} }) {
            if (exists $manifest{$key}{profiles}{$p_name}{percentiles}) {
                my $percs_ref = \@{ $manifest{$key}{profiles}{$p_name}{percentiles} };
                my %seen; @$percs_ref = grep { !$seen{$_}++ } @$percs_ref;
            }
        }
    }

    # remove profiles that never got valid percentiles:
    foreach my $key (keys %manifest) {
        foreach my $pname (keys %{ $manifest{$key}{profiles} }) {
            my $pd = $manifest{$key}{profiles}{$pname};
            delete $manifest{$key}{profiles}{$pname}
                unless (ref($pd) eq 'HASH' && exists $pd->{percentiles} && @{ $pd->{percentiles} });
        }
    }

    return \%manifest;
}

# ==============================================================================
# SUBROUTINE: _sanitise_manifest_for_history
# PURPOSE:    Transforms a "tactical" manifest (from build_transform_manifest)
#             into a "historical" manifest by surgically removing all predictive
#             and time-weighting directives whilst preserving the profile's
#             fundamental historical definition.
#
# ARGUMENTS:
#   1. $manifest_href (hash ref): The tactical manifest to sanitise.
#
# RETURNS:
#   - A hash reference to the sanitised manifest.
#
# CRITICAL DESIGN PRINCIPLES:
#   This function is the primary data purity guardrail for the history priming
#   process. It implements the rules defined in:
#   "nFit - Maintaining Cache Structure Purity.md"
#
#   What is REMOVED (predictive/time-weighting):
#     - decay keys (inter-period time-weighting)
#     - runq_decay keys (RunQ time-weighting)
#     - enable_growth directives (forward-looking predictions)
#
#   What is PRESERVED (profile definition/historical measurement):
#     - method (sma/ema/wma) - defines how noise is smoothed
#     - window - defines the smoothing window size
#     - time_filter (online/batch) - defines WHAT data to measure
#     - no_weekends - defines operational time boundaries
#     - filter_perc - defines data quality thresholds
#     - calculate_peak - historical maximum measurement
#     - enable_clipping - historical capacity limit detection
#
# RATIONALE:
#   A workload profile like "G3-95W15" is not just "P95". It is explicitly
#   defined as "the P95 of a 15-minute simple moving average, excluding
#   weekends, during online hours". This complete definition must be preserved
#   in the historical record, as it defines the measurement methodology.
#
#   Time-context filters (-online, -no-weekends) are NOT predictive. They
#   define WHAT is being measured (operational hours vs. all hours). Removing
#   them would corrupt the historical record by blending non-operational data
#   into profiles explicitly designed to exclude it.
# ==============================================================================
sub _sanitise_manifest_for_history {
    my ($manifest_href) = @_;

    # Validate input to catch programming errors early
    unless (ref($manifest_href) eq 'HASH') {
        die "FATAL: _sanitise_manifest_for_history requires a hash reference. Received: "
            . (defined $manifest_href ? ref($manifest_href) || 'scalar' : 'undef');
    }

    # Perform a true deep copy to avoid any possibility of modifying the
    # original tactical manifest. This is critical for maintainability.
    use Storable qw(dclone);
    my $sanitised_href = dclone($manifest_href);

    # Iterate through each transform in the manifest
    foreach my $transform_key (keys %$sanitised_href) {
        my $transform = $sanitised_href->{$transform_key};

        # Sanity check: ensure we're working with a proper transform structure
        next unless ref($transform) eq 'HASH';

        # ======================================================================
        # STEP 1: Remove predictive time-weighting (decay)
        # ======================================================================
        # Decay applies recency weighting across time periods, making older
        # data contribute less to the final metric. This is predictive logic
        # (assumes recent behaviour is more relevant for forecasting).
        #
        # For historical records, we want pure statistical aggregation where
        # all time periods in the analysis window are weighted equally.
        #
        # We DELETE the keys entirely rather than setting them to 'none' or 'low'
        # because:
        #   a) Absence of the key causes nfit to perform pure aggregation
        #   b) Even 'low' decay applies some time-weighting
        #   c) Explicit deletion makes the intent unambiguous
        # ======================================================================
        delete $transform->{decay};
        delete $transform->{runq_decay} if exists $transform->{runq_decay};

        # ======================================================================
        # STEP 2: PRESERVE smoothing method and window
        # ======================================================================
        # The method (sma/ema) and window define HOW the profile smooths noise
        # within each time period. This is NOT predictiveÃÂ¢Ã¢âÂ¬Ã¢â¬Âit's the measurement
        # methodology.
        #
        # Example: "G3-95W15" explicitly means "P95 of 15-minute SMA"
        #          Changing this would create a different metric entirely.
        #
        # NO CHANGES to: $transform->{method}, $transform->{window}
        # ======================================================================

        # ======================================================================
        # STEP 3: PRESERVE time-context filters
        # ======================================================================
        # Time-context filters define WHAT operational state is being measured:
        #   - time_filter (online/batch): Operational hours vs. batch windows
        #   - no_weekends: Weekday operations vs. full week
        #   - filter_perc: Data quality threshold (exclude idle periods)
        #
        # These are fundamental to the profile's definition and MUST be preserved.
        #
        # A profile defined with "-online -no-weekends" is explicitly measuring
        # "workload during operational business hours". Stripping these filters
        # would contaminate the measurement by including non-operational data.
        #
        # NO CHANGES to: $transform->{time_filter}, $transform->{no_weekends},
        #                $transform->{filter_perc}
        # ======================================================================

        # ======================================================================
        # STEP 4: Sanitise profile-level directives
        # ======================================================================
        next unless exists $transform->{profiles} && ref($transform->{profiles}) eq 'HASH';

        foreach my $profile_name (keys %{ $transform->{profiles} }) {
            my $directives = $transform->{profiles}{$profile_name};
            next unless ref($directives) eq 'HASH';

            # ------------------------------------------------------------------
            # Remove growth prediction (CRITICAL for data purity)
            # ------------------------------------------------------------------
            # Growth prediction is forward-looking and must never appear in
            # historical records. The history cache must contain only "what
            # actually happened", not "what we predict will happen".
            #
            # We use DELETE rather than setting to 0 to make the absence
            # explicit and unambiguous.
            # ------------------------------------------------------------------
            delete $directives->{enable_growth};

            # ------------------------------------------------------------------
            # PRESERVE historical measurement flags
            # ------------------------------------------------------------------
            # These directives capture factual historical measurements:
            #   - calculate_peak: The actual maximum value observed
            #   - enable_clipping: Detection of capacity saturation
            #
            # Both are measurements of "what happened" and belong in the
            # historical record.
            #
            # NO CHANGES to: $directives->{calculate_peak},
            #                $directives->{enable_clipping}
            # ------------------------------------------------------------------
        }
    }

    return $sanitised_href;
}

# ==============================================================================
# SUBROUTINE: run_single_pass_analysis
# PURPOSE:    Orchestrates the single-pass analysis for a given system cache.
#             It builds the manifest, assembles and executes the nfit command
#             with all necessary global flags, and returns the parsed results.
# ==============================================================================
sub run_single_pass_analysis {
    my ($system_cache_dir, $profiles_ref, $args_ref) = @_;

    # 1. Build the manifest using the function already added.
    _phase("Building transform manifest");
    my $transform_manifest = build_transform_manifest($profiles_ref, $args_ref->{runq_avg_method});

    # *** ADD THIS COMPLETE DIAGNOSTIC BLOCK TO PROVE THE CORRECTNESS OF THE MANIFEST***
#    {
#        warn "\n" . "=" x 70 . "\n";
#        warn "STEP 1: MANIFEST DEBUG IN nfit-profile.pl (After build_transform_manifest)\n";
#        warn "=" x 70 . "\n\n";
#
#        # Test profile to trace
#        my $test_profile = 'B3-95W15';
#
#        # Count PhysC transforms
#        my @physc_transforms = grep {
#            $transform_manifest->{$_}{metric} eq 'PhysC'
#        } keys %$transform_manifest;
#        warn "Total PhysC transforms created: " . scalar(@physc_transforms) . "\n\n";
#
#        # Check which transforms contain our test profile
#        my @transforms_with_test = grep {
#            $transform_manifest->{$_}{metric} eq 'PhysC' &&
#            exists $transform_manifest->{$_}{profiles}{$test_profile}
#        } keys %$transform_manifest;
#
#        warn "Transforms containing '$test_profile': " . scalar(@transforms_with_test) . "\n";
#
#        if (@transforms_with_test) {
#            foreach my $key (sort @transforms_with_test) {
#                my $prof_data = $transform_manifest->{$key}{profiles}{$test_profile};
#                my $has_pct = defined $prof_data->{percentiles};
#                my $pct_val = $has_pct ? "[" . join(",", @{$prof_data->{percentiles}}) . "]" : "UNDEFINED";
#                warn "  - $key\n";
#                warn "    Percentiles: $pct_val\n";
#            }
#        } else {
#            warn "  ERROR: '$test_profile' not found in ANY PhysC transform!\n";
#        }
#
#        warn "\n";
#
#        # Show summary of ALL PhysC transforms and their profiles
#        warn "Complete PhysC Transform Summary:\n";
#        warn "-" x 70 . "\n";
#        foreach my $key (sort @physc_transforms) {
#            my @profiles_in_transform = sort keys %{$transform_manifest->{$key}{profiles}};
#            warn "Transform: $key\n";
#            warn "  Profiles (" . scalar(@profiles_in_transform) . "): " . join(", ", @profiles_in_transform) . "\n";
#
#            # Check if percentiles are defined for each
#            my @with_pct = grep {
#                defined $transform_manifest->{$key}{profiles}{$_}{percentiles}
#            } @profiles_in_transform;
#            my @without_pct = grep {
#                !defined $transform_manifest->{$key}{profiles}{$_}{percentiles}
#            } @profiles_in_transform;
#
#            if (@without_pct) {
#                warn "  WARNING: Profiles without percentiles: " . join(", ", @without_pct) . "\n";
#            }
#        }
#
#        warn "\n" . "=" x 70 . "\n";
#        warn "END STEP 1 MANIFEST DEBUG\n";
#        warn "=" x 70 . "\n\n";
#    }
#    # *** END DIAGNOSTIC BLOCK ***

    # Check if any profile in the manifest requires growth prediction.
    my $any_profile_has_growth = 0;
    foreach my $transform (values %$transform_manifest) {
        foreach my $profile_directives (values %{ $transform->{profiles} }) {
            if ($profile_directives->{enable_growth}) {
                $any_profile_has_growth = 1;
                last;
            }
        }
        last if $any_profile_has_growth;
    }

    # 2. Create a temporary file for the manifest.
    my ($fh_manifest, $manifest_filename) = tempfile(UNLINK => 1);
    print $fh_manifest JSON->new->pretty->encode($transform_manifest);
    close $fh_manifest;

    # --- BEGIN STEP 1 DEBUGGING ---
#    print STDERR "--- DEBUG: TRANSFORM MANIFEST ---\n";
#    my $manifest_json_for_debug = JSON->new->pretty->encode($transform_manifest);
#    print STDERR $manifest_json_for_debug . "\n";
#    print STDERR "--- MANIFEST DEBUG END. SCRIPT WILL NOW EXIT. ---\n";
#    exit 0; # Exit early for debugging
    # --- END STEP 1 DEBUGGING ---

    # 3. Assemble the complete nfit command with all preserved global flags.
    my $command = $args_ref->{nfit_path} . " --manifest $manifest_filename";

    # Add required I/O and pass-through global flags.
    $command .= " --nmondir \"$system_cache_dir\"";
    $command .= " " . $args_ref->{rounding_flags} if $args_ref->{rounding_flags};
    $command .= " --smt $args_ref->{default_smt}" if defined $args_ref->{default_smt};
    $command .= " --runq-avg-method $args_ref->{runq_avg_method}" if defined $args_ref->{runq_avg_method};

    # Add optional filtering flags.
    $command .= " --startdate $args_ref->{start_date}" if defined $args_ref->{start_date};
    $command .= " --enddate $args_ref->{end_date}" if defined $args_ref->{end_date};
    $command .= " --vm \"$args_ref->{vm_name}\"" if defined $args_ref->{vm_name};

    # Add flags for specific analysis models, auto-enabling decay for growth
    my $decay_mode_set = 0;
    if ($args_ref->{enable_windowed_decay}) {
        $command .= " --enable-windowed-decay";
        $decay_mode_set = 1;
    }
    if ($args_ref->{decay_over_states}) {
        $command .= " --decay-over-states";
        $decay_mode_set = 1;
    }

    # If any profile needs growth, a decay model is required. Auto-enable one if not set.
    if ($any_profile_has_growth && !$decay_mode_set) {
        $command .= " --enable-windowed-decay";
        # Optionally print a notice to stderr that a default mode was activated.
        print STDERR "INFO: Auto-enabling --enable-windowed-decay as it is required for profile-level growth prediction.\n";
    }

    # Pass the global growth flag if needed, and other flags like clipping detection
    $command .= " --enable-growth-prediction" if $any_profile_has_growth;
    $command .= " --analysis-reference-date $args_ref->{analysis_reference_date}" if defined $args_ref->{analysis_reference_date};
    $command .= " --enable-clipping-detection" if $args_ref->{enable_clipping_detection}; # Pass this through

    # 4. Execute the nfit engine and capture its output.
     _phase("Executing nFit Engine.");
    my $raw_nfit_output = `$command`;
    my $exit_status = $? >> 8;

    if ($exit_status != 0) {
        die "FATAL: nfit single-pass engine failed for system '$system_cache_dir' with exit code $exit_status.\nCMD: $command\nOUTPUT: $raw_nfit_output";
    }

    # --- BEGIN STEP 2 DEBUGGING ---
    #print STDERR "--- DEBUG: RAW JSON OUTPUT FROM nfit.pl ---\n";
    #print STDERR "nfit.pl Exit Status: " . $exit_status . "\n";
    #print STDERR "Command Executed:\n$command\n\n";
    #print STDERR "Output Received:\n";
    #print STDERR $raw_nfit_output . "\n";
    #print STDERR "--- JSON DEBUG END. SCRIPT WILL NOW EXIT. ---\n";
    #exit 0; # Exit early for debugging
    # --- END STEP 2 DEBUGGING ---

    # 5. Parse and return the final results.
    return parse_nfit_json_output($raw_nfit_output);
}

# Helper for hash references (with safe fallback)
sub _safe_get {
    my ($href, $key, $fallback) = @_;
    return (ref($href) eq 'HASH' && exists $href->{$key}) ? $href->{$key} : $fallback;
}

# ==============================================================================
# SUBROUTINE: build_assimilation_map
# PURPOSE:    Acts as an anti-corruption layer by consuming the raw, nested JSON
#             output from the nfit.pl engine and transforming it into a stable,
#             predictable, and mostly flat Perl hash structure. This map becomes
#             the single source of truth for all subsequent consumer logic within
#             nfit-profile.pl.
# ARGUMENTS:
#   1. $parsed_nfit_results_href (hash ref): The raw, decoded Perl hash from nfit.pl.
#   2. $profiles_aref (array ref): A reference to the global @profiles array.
#   3. $adaptive_runq_saturation_thresh (float): The dynamically calculated threshold for detecting Absolute RunQ saturation.
#        This value is passed directly to 'generate_sizing_hint' to determine the VM's pressure status.
#
# RETURNS:
#   - A hash reference to the fully populated assimilation map.
# ==============================================================================
sub build_assimilation_map {
    my ($parsed_nfit_results_href, $profiles_aref, $adaptive_runq_saturation_thresh) = @_;

    my %assimilation_map;

    # Iterate through each VM returned by the nfit engine.
    foreach my $vm_name (sort keys %{$parsed_nfit_results_href}) {
        my @states_for_vm = @{ $parsed_nfit_results_href->{$vm_name} };
        next unless @states_for_vm;

        # The representative state is used to source common metadata and RunQ metrics.
        my $representative_state = $states_for_vm[-1];
        my $is_aggregated = (ref($representative_state) eq 'HASH' && ($representative_state->{analysisType} // '') =~ /aggregated/i);

        # Initialise the map entry for this VM with a predictable structure.
        my $vm_map = $assimilation_map{$vm_name} = {
            Configuration    => {},
            CoreResults      => { ProfileValues => {}, PeakValue => undef },
            Hinting          => {},
            RunQMetrics      => {},
            Growth           => { min_adj => 0.0, max_adj => 0.0, adjustment => 0.0, rationale => {} },
            RawNfitStates    => \@states_for_vm,
            SeasonalForecast => {},
            CSVModifiers     => {},
            # GrowthRationaleByProfile will store ALL rationales for logging
            GrowthRationaleByProfile => {},
        };

        # --- Block 1: Populate Configuration ---
        my $metadata_block = _safe_dig($representative_state, 'metadata') || {};
        $vm_map->{Configuration} = {
            smt           => _safe_dig($representative_state, 'metadata', 'smt'),
            entitlement   => _safe_dig($representative_state, 'metadata', 'entitlement'),
            max_cpu       => _safe_dig($representative_state, 'metadata', 'max_cpu'),
            virtual_cpus  => _safe_dig($representative_state, 'metadata', 'virtual_cpus'),
            is_capped     => _safe_dig($representative_state, 'metadata', 'capped'),
            pool_id       => _safe_dig($representative_state, 'metadata', 'pool_id'),
            pool_cpu      => _safe_dig($representative_state, 'metadata', 'pool_cpu'),
            serial_number => _safe_dig($representative_state, 'metadata', 'serial_number'),
            proc_type     => _safe_dig($representative_state, 'metadata', 'proc_type'),
            proc_version  => _safe_dig($representative_state, 'metadata', 'proc_version'),
            proc_clock    => _safe_dig($representative_state, 'metadata', 'proc_clock'),
        };

        # --- Block 2: Populate CoreResults (ProfileValues and PeakValue) ---
        if ($is_aggregated) {
            # For aggregated runs, data is already per-profile.
            # Growth is now calculated per profile inside nfit.pl.
            my $physc_metrics = _safe_dig($representative_state, 'metrics', 'physc') || {};
            my @growth_values_for_vm;

            foreach my $profile_name (keys %$physc_metrics) {
                my $profile_data = $physc_metrics->{$profile_name};

                # CRITICAL: Extract the growth-inclusive baseline for RunQ modifier processing.
                # BaseValue from nfit is ALWAYS pre-growth. FinalValue is BaseValue + GrowthAdj.
                my $base_val   = _safe_dig($profile_data, 'BaseValue');
                my $growth_adj = _safe_dig($profile_data, 'GrowthAdj');
                my $final_val  = _safe_dig($profile_data, 'FinalValue'); # Growth-inclusive
                my $rationale  = _safe_dig($profile_data, 'growthRationale');

                # Store the growth-inclusive FinalValue as the starting point for nfit-profile modifiers.
                # This ensures that profile metrics in the CSV include the GrowthAdj.
                # Fallback: If FinalValue is not available, compute it as BaseValue + GrowthAdj.
                my $growth_inclusive_value = $final_val;
                if (!defined $growth_inclusive_value || !looks_like_number($growth_inclusive_value)) {
                    my $base_numeric = (defined $base_val && looks_like_number($base_val)) ? $base_val : 0;
                    my $growth_numeric = (defined $growth_adj && looks_like_number($growth_adj)) ? $growth_adj : 0;
                    $growth_inclusive_value = $base_numeric + $growth_numeric;
                }
                $vm_map->{CoreResults}{ProfileValues}{$profile_name} = $growth_inclusive_value;

                # Store the pre-growth BaseValue separately for audit trail purposes
                $vm_map->{Growth}{base_values}{$profile_name} = $base_val // 0;

                # Store per-profile growth adjustment for later use
                my $growth_adj_for_profile = (defined $growth_adj && looks_like_number($growth_adj)) ? $growth_adj : 0;
                push @growth_values_for_vm, $growth_adj_for_profile;
                $vm_map->{Growth}{adjustments}{$profile_name} = $growth_adj_for_profile;

                # Harvest ALL rationales for audit logging
                if (ref($rationale) eq 'HASH' && scalar keys %$rationale) {
                    $vm_map->{GrowthRationaleByProfile}{$profile_name} = $rationale;
                }
            }

            # ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
            # Calculate min/max from non-zero growth values only
            # ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
            # RATIONALE: Zero growth means "no trend detected", not "minimum is zero".
            # Capacity planners need the actual range of predicted growth values.
            # - If all profiles have 0 growth â min=0, max=0 (correct: no growth)
            # - If some profiles have growth â show min/max of non-zero values
            # - Negative growth (declining trends) is preserved in the range
            # ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ

            # Filter to non-zero growth values (preserving negatives)
            my @non_zero_growth = grep { abs($_) > 1e-9 } @growth_values_for_vm;

            if (@non_zero_growth) {
                # Calculate range from actual growth predictions
                $vm_map->{Growth}{max_adj} = max(@non_zero_growth);
                $vm_map->{Growth}{min_adj} = min(@non_zero_growth);
            } else {
                # No growth detected on any profile
                $vm_map->{Growth}{max_adj} = 0.0;
                $vm_map->{Growth}{min_adj} = 0.0;
            }

            # The PeakValue comes from the raw peak tracker, not a specific profile's aggregated value.
            $vm_map->{CoreResults}{PeakValue} = _safe_dig($representative_state, 'metadata', 'peakPhyscFromLatestState');

       } else {
            # Logic for per-state runs remains correct.
            my %profile_sums;
            my %profile_counts;
            foreach my $profile (@$profiles_aref) {
                my ($p_val_num) = $profile->{flags} =~ /(?:-p|--percentile)\s+([0-9.]+)/;
                my $p_metric_key = "P" . clean_perc_label($p_val_num // $DEFAULT_PERCENTILE);
                foreach my $state (@states_for_vm) {
                    my $metric_val = _safe_dig($state, 'metrics', 'physc', $profile->{name}, $p_metric_key);
                    if (defined $metric_val && looks_like_number($metric_val)) {
                        $profile_sums{$profile->{name}} += $metric_val;
                        $profile_counts{$profile->{name}}++;
                    }
                }
            }
            foreach my $profile_name (keys %profile_sums) {
                if ($profile_counts{$profile_name} > 0) {
                    $vm_map->{CoreResults}{ProfileValues}{$profile_name} = $profile_sums{$profile_name} / $profile_counts{$profile_name};
                }
            }
            my @peak_values;
            foreach my $state (@states_for_vm) {
                my $peak_val = _safe_dig($state, 'metrics', 'physc', $MANDATORY_PEAK_PROFILE_FOR_HINT, 'Peak');
                push @peak_values, $peak_val if (defined $peak_val && looks_like_number($peak_val));
            }
            $vm_map->{CoreResults}{PeakValue} = max(@peak_values) if @peak_values;
        }

        # --- Block 3: Populate RunQMetrics (Comprehensive) ---
        # This loop iterates through ALL profiles to find and collate every RunQ
        # metric calculated by the nfit engine into a single, unified block.
        $vm_map->{RunQMetrics}{SourceProfile} = $MANDATORY_PEAK_PROFILE_FOR_HINT; # P-99W1 remains the default source
        foreach my $profile (@$profiles_aref) {
            my $p_name = $profile->{name};
            my $runq_norm_metrics = _safe_dig($representative_state, 'metrics', 'runq', 'normalized', $p_name) || {};
            my $runq_abs_metrics  = _safe_dig($representative_state, 'metrics', 'runq', 'absolute', $p_name) || {};
            foreach my $key (keys %$runq_norm_metrics) {
                $vm_map->{RunQMetrics}{"NormRunQ_$key"} //= $runq_norm_metrics->{$key};
            }
            foreach my $key (keys %$runq_abs_metrics) {
                $vm_map->{RunQMetrics}{"AbsRunQ_$key"} //= $runq_abs_metrics->{$key};
            }
        }

        # Calculate IQRC.
        my ($p25, $p50, $p75) = ($vm_map->{RunQMetrics}{'NormRunQ_P25'}, $vm_map->{RunQMetrics}{'NormRunQ_P50'}, $vm_map->{RunQMetrics}{'NormRunQ_P75'});
        if (defined $p50 && $p50 > $FLOAT_EPSILON && defined $p75 && defined $p25) {
            $vm_map->{RunQMetrics}{IQRC} = ($p75 - $p25) / $p50;
        }

        # --- Block 4: Populate Growth ---
        if ($is_aggregated) {
            # In this new logic, we intentionally leave Growth.adjustment and
            # Growth.rationale EMPTY.
            # They will be populated dynamically in the consumer functions
            # (log_profile_rationale and _write_standard_csv_report)
            # based on the correct profile context.
            $vm_map->{Growth}{adjustment} = 0.0; # Default to 0
            $vm_map->{Growth}{rationale}  = {};  # Default to empty
        }
    }
    return \%assimilation_map;
}

# ==============================================================================
# SUBROUTINE: _acquire_history_lock
# PURPOSE:    Acquires an EXCLUSIVE lock on the history infrastructure.
#             This enables safe read-modify-write operations across the
#             transition from Monolith -> Partitioned architecture.
# RETURNS:    ($fh, $lock_path) - Filehandle and path. Keep $fh open to hold lock.
# ==============================================================================
sub _acquire_history_lock {
    my ($system_cache_dir) = @_;

    my $lock_file = File::Spec->catfile($system_cache_dir, $HISTORY_LOCK_FILENAME);

    open my $lock_fh, '>', $lock_file
        or die "FATAL: Could not create lock file '$lock_file': $!";

    # Blocking lock - wait until acquired
    flock($lock_fh, LOCK_EX)
        or die "FATAL: Could not acquire exclusive lock on '$lock_file': $!";

    # Auto-flush to ensure lock intent is registered if we write pid later
    my $old_fh = select($lock_fh); $| = 1; select($old_fh);

    return ($lock_fh, $lock_file);
}

# ==============================================================================
# SUBROUTINE: _migrate_history_to_partitioned
# PURPOSE:    One-time migration of monolithic history file to partitioned layout.
#             Preserves data integrity by creating the directory, populating it,
#             and renaming the legacy file only upon success.
# ==============================================================================
sub _migrate_history_to_partitioned {
    my ($system_cache_dir) = @_;

    my $legacy_file = File::Spec->catfile($system_cache_dir, '.nfit.history.json');
    my $partition_dir = File::Spec->catfile($system_cache_dir, '.nfit.history');

    return unless -f $legacy_file; # Nothing to migrate

    # This function is called INSIDE a lock, so we don't re-acquire it here.

    print "  - Migrating legacy history to partitioned format.\n";

    # 1. Load Legacy Data
    my $json_text = do {
        open my $fh, '<:encoding(utf8)', $legacy_file or die "FATAL: Cannot read legacy history: $!";
        local $/; <$fh>;
    };
    my $data = JSON->new->decode($json_text);

    # 2. Create Directory
    unless (-d $partition_dir) {
        make_path($partition_dir) or die "FATAL: Cannot create history partition directory: $!";
    }

    # 3. Write Partition Files
    my $json_encoder = JSON->new->pretty->canonical;

    foreach my $month_key (keys %$data) {
        # We wrap the data in the month key to maintain the exact structure
        # { "YYYY-MM": { ... } } inside the file. This simplifies stitching.
        my $month_payload = { $month_key => $data->{$month_key} };

        # Naming Convention: nfit.hist.YYYY-MM.json
        my $filename = "nfit.hist.${month_key}.json";
        my $filepath = File::Spec->catfile($partition_dir, $filename);

        open my $fh, '>:encoding(utf8)', $filepath
            or die "FATAL: Cannot write partition file $filename: $!";
        print $fh $json_encoder->encode($month_payload);
        close $fh;
    }

    # 4. Rename Legacy File (The "Commit" operation)
    my $migrated_name = $legacy_file . ".migrated";
    rename($legacy_file, $migrated_name)
        or die "FATAL: Failed to rename legacy file after migration: $!";

    print "  - Migration complete. Legacy file renamed to .migrated.\n";
}

# --- usage_wrapper ---
# Generates and returns the usage/help message for the script.
sub usage_wrapper
{
    my $script_name = $0;
    $script_name =~ s{.*/}{}; # Get only script name, remove path
    return <<END_USAGE;
Usage: $script_name --physc-data <pc_file> [options]

Runs 'nfit' for multiple profiles. Applies RunQ modifiers and generates hints.
The RunQ modifier behavior (standard vs. additive-only) can be controlled
per profile via the 'runq_modifier_behavior' attribute in nfit.profiles.cfg.
Optionally passes flags to nfit for its internal windowed decay processing.
A detailed rationale log is written to $LOG_FILE_PATH.

Input Method (Provide one):
  --nmondir <directory>    : Path to a base directory containing system-specific caches
                             (e.g., './stage/'), or to a single cache directory
                             (e.g., './stage/12345ABC/').
  --mgsys <serial>         : Specifies a managed system. If --nmondir is absent,
                             implicitly uses the default base cache at './stage/'.
                             Can be combined with --nmondir to select a system
                             from a non-default base path.
  --default-smt, --smt <N> : Optional. Default SMT level (Default: $DEFAULT_SMT_VALUE_PROFILE).

RunQ Metric Configuration (for nfit calls):
--runq-norm-percentiles <list> : Global default for Normalised RunQ (Default: "$DEFAULT_RUNQ_NORM_PERCS").
                                   This list is combined with profile-specific settings and ensures P50,P90.
  --runq-abs-percentiles <list>  : Global default for Absolute RunQ (Default: "$DEFAULT_RUNQ_ABS_PERCS").
                                   This list is combined with profile-specific settings and ensures P90.
  --runq-perc-behavior <mode>    : Controls which AbsRunQ percentile is used for additive logic.
                                   'fixed' (Default): Always use AbsRunQ_P90.
                                   'match': Use AbsRunQ percentile matching the profile's PhysC -p <X> value.

Configuration Files:
  -config <vm_cfg_csv>       : Optional. VM configuration CSV file.
  --profiles-config <path>   : Optional. Profiles definition file (INI format).
                               Can contain 'runq_modifier_behavior = additive_only' per profile,
                               and profile-specific 'nfit_flags' including --runq-norm-perc/--runq-abs-perc.

Standard Analysis Filtering Options:
  -s, --startdate <YYYY-MM-DD> : Global start date for analysis (passed to nfit). Optional.
  -e, --enddate <YYYY-MM-DD>   : Global end date for analysis (passed to nfit). Optional.
  -vm, --lpar <name>           : Analyse only the specified VM/LPAR name (passed to nfit). Optional.

Seasonality and Business Cycle Analysis:
  This powerful feature uses 'etc/nfit.seasonality.cfg' to produce forecasts
  based on defined business events.

  --apply-seasonality <event>  : Generate a forecast using the specified seasonal model.
                                 For 'multiplicative_seasonal' models, this generates
                                 three CSV files (final, current_baseline, historic_snapshot).
                                 For 'recency_decay' models (e.g., 'month-end'), this
                                 anchors the analysis to the last peak to solve the
                                 "start-of-month" problem.

  --update-history             : Run in a special mode to populate the unified history cache.
                                 This command discovers and processes completed months in
                                 your data cache, generating both generic monthly analysis
                                 and specific seasonal event snapshots.
  --force                      : Overrides pre-execution validation checks. Use with caution.
  --min-history-days <N>       : When updating history, process partial months that have
                                 at least N days of data. (Default: 28).

  --reset-seasonal-cache     : Deletes the seasonal snapshot cache for a clean start.

Control nfit's Internal Windowed Recency Decay (Optional):
  --enable-windowed-decay                : Enable nfit's internal windowed processing.
  --decay-over-states                    : Enable the Hybrid State-Time Decay Model. This advanced mode
                                           applies recency decay to state-based results.
                                           (Mutually exclusive with --enable-windowed-decay).
  --process-window-unit <days|weeks>     : Unit for nfit's window size (Default: $DEFAULT_PROCESS_WINDOW_UNIT_FOR_NFIT).
  --process-window-size <N>              : Size of nfit's window in units (Default: $DEFAULT_PROCESS_WINDOW_SIZE_FOR_NFIT).
  --decay-half-life-days <N>             : Half-life for nfit's recency weighting (Default: $DEFAULT_DECAY_HALF_LIFE_DAYS_FOR_NFIT).
  --analysis-reference-date <YYYY-MM-DD> : Reference date for nfit's recency calculation
                                           (Default: nfit uses date of last record in its filtered NMON data).
                                           NOTE: Reference date must precede --enddate if both options are specified.
  --runq-avg-method <none|sma|ema>       : Averaging method for RunQ data within nfit before percentile.
                                           (Default: $DEFAULT_NFIT_RUNQ_AVG_METHOD). Uses main -w/--decay/--runq-decay from profile.

Rounding (Passed to nfit for its output):
  -r[=increment]             : Optional. nfit rounds results to NEAREST increment.
  -u[=increment]             : Optional. nfit rounds results UP to nearest increment.
                               (Default increment: $DEFAULT_ROUND_INCREMENT)
Other:
  --nfit-path <path>         : Optional. Path to the 'nfit' script.
  -h, --help                 : Display this help message.
  -v, --version              : Display script version and nfit version used.

CSV Output Options:
  --excel-formulas[=<true|false>] : Optional. Adds Excel-specific formula columns
                                    ("NFIT - ENT", "NETT", "NETT%") and a summary
                                    footer. If omitted, a clean data-only CSV
                                    is generated. (Default: false).

Output CSV Columns (Illustrative):
  VM,TIER,Hint,Pattern,Pressure,PressureDetail,SMT,
  Serial,SystemType,Pool Name,Pool ID,Peak,
  [Profile Names (values are potentially recency-weighted by nfit & RunQ-modified by nfit-profile)...],
  Current_ENT,NFIT_ENT_UserFormula,NETT_UserFormula,NETT_Perc_UserFormula

END_USAGE
}
