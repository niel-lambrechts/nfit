#!/usr/bin/env perl

# NAME     : nfit-profile
# AUTHOR   : Niël Lambrechts (https://github.com/niel-lambrechts)
# PURPOSE  : Runs 'nfit' multiple times with user-defined profiles.
#            Applies RunQ modifiers, generates hints, logs rationale, and aggregates to CSV.
# REQUIRES : Perl, nfit, Time::Piece, List::Util, IPC::Open3, version

use strict;
use warnings;
use Getopt::Long qw(GetOptions :config bundling);
use Cwd qw(abs_path);
use File::Basename qw(dirname basename);
use File::Spec qw(catfile);
use File::Path qw(make_path);
use File::Temp qw(tempfile);
use Time::Piece;
use Time::Seconds;
use List::Util qw(sum sum0 min max uniq); # sum0 is available from List::Util 1.33+
use Scalar::Util qw(looks_like_number);
use IPC::Open3;
use IO::Select;
use version;
use JSON;
use Fcntl qw(:DEFAULT :flock);
use constant ONE_SECOND => 1;
use Data::Dumper;
use Archive::Tar;
use IO::Zlib; # Standard in Perl core since 5.9.3

# --- Store original ARGV for logging ---
my @original_argv = @ARGV;

# --- Capture nfit-profile.pl start time ---
my $PROFILE_SCRIPT_START_TIME_EPOCH = time();
my $PROFILE_SCRIPT_START_TIME_STR = localtime($PROFILE_SCRIPT_START_TIME_EPOCH)->strftime("%Y-%m-%d %H:%M:%S %Z");

# --- Version ---
my $VERSION = '6.25.306.0';

# --- Seasonal Engine Version (Phase 4: Idempotency) ---
# Bump this constant whenever anchoring semantics, fingerprint logic, or
# model computation behaviour changes. This ensures stale cached results
# are correctly invalidated.
my $SEASONAL_ENGINE_VERSION = '1.0.0';

# --- Configuration ---
my $DEFAULT_AVG_METHOD     = 'ema';
my $DEFAULT_DECAY_LEVEL    = 'medium';
my $DEFAULT_WINDOW_MINUTES = 15;
my $DEFAULT_PERCENTILE = 95;
my $DEFAULT_ROUND_INCREMENT = 0.05;
my $DEFAULT_SMT            = 8;
my $DEFAULT_VM_CONFIG_FILE = "config-all.csv";
my $DEFAULT_PROFILES_CONFIG_FILE = "nfit.profiles.cfg";
my $DEFAULT_SMT_VALUE_PROFILE = 8;
my $DEFAULT_RUNQ_NORM_PERCS = "50,90"; # Global default for nfit-profile if not in profile's flags
my $DEFAULT_RUNQ_ABS_PERCS  = "90";    # Global default for nfit-profile if not in profile's flags
my $DEFAULT_NFIT_RUNQ_AVG_METHOD = "ema";

# This profile's CPU value is used for MaxCPU pressure checks,
# and its RunQ metrics will now be used for global RunQ pressure hints.
my $MANDATORY_PEAK_PROFILE_FOR_HINT = "P-99W1";

# Windowed Decay Defaults (for when nfit-profile instructs nfit to use its internal decay)
my $DEFAULT_PROCESS_WINDOW_UNIT_FOR_NFIT = "weeks";
my $DEFAULT_PROCESS_WINDOW_SIZE_FOR_NFIT = 1;
my $DEFAULT_DECAY_HALF_LIFE_DAYS_FOR_NFIT = 30;

# Heuristic Thresholds (for sizing hints and RunQ modifiers)
my $PATTERN_RATIO_THRESHOLD = 2.0;     # For O vs B pattern determination
my $HIGH_PEAK_RATIO_THRESHOLD = 5.0;   # For "Very Peaky" shape
my $LOW_PEAK_RATIO_THRESHOLD  = 2.0;   # For "Moderately Peaky" shape
my $LIMIT_THRESHOLD_PERC = 0.98;       # P99W1 vs MaxCPU for pressure detection

# RunQ Modifier Thresholds (for calculate_runq_modified_physc)
my $WORKLOAD_PRESSURE_NORM_P90_TRIGGER_THRESHOLD = 2.0; # NormRunQ P90 above this may indicate workload pressure (NOT adaptive)
my $RUNQ_PRESSURE_P90_SATURATION_THRESHOLD = 1.5;       # AbsRunQ P90 / (MaxCPU * SMT) above this indicates RunQ pressure (ADAPTIVE)
my $RUNQ_ADDITIVE_TOLERANCE_FACTOR = 1.8;               # Tolerate AbsRunQ up to this Factor x Base_Profile_PhysC's LCPU capacity (NOT adaptive)
my $ENTITLEMENT_BURST_ALLOWANCE_FACTOR = 0.25;          # Allow uncapped VMs to burst 25% over their entitlement before pressure is assessed
my $BURST_ALLOWANCE_MIN_FACTOR = 0.0;
my $BURST_ALLOWANCE_MAX_FACTOR = 0.5;                   # Clamp at 50%

# Internal constants for growth heuristics (not user-configurable initially)
my $GROWTH_MIN_HISTORICAL_PERIODS       = 5;    # Min number of windowed periods to attempt trend
my $GROWTH_MAX_CV_THRESHOLD             = 0.50; # Max Coefficient of Variation (StdDev/Mean); if > this, data too volatile
my $GROWTH_MIN_POSITIVE_SLOPE_THRESHOLD = 0.01; # Min slope (units/period) to consider as actual growth for inflation
my $GROWTH_MAX_PROJECTION_HISTORY_RATIO = 2.0;  # Max ratio of projection duration to history duration used for trend
my $DEFAULT_GROWTH_PROJECTION_DAYS         = 90;
my $DEFAULT_MAX_GROWTH_INFLATION_PERCENT   = 25;

# RunQ Volatility Confidence Factor Thresholds & Values (adjusts additive CPU based on P90/P50 RunQ ratio)
my $VOLATILITY_SPIKY_THRESHOLD = 0.5;    my $VOLATILITY_SPIKY_FACTOR = 0.70;    # Very stable/spiky, less confidence in adding CPU
my $VOLATILITY_MODERATE_THRESHOLD = 0.8; my $VOLATILITY_MODERATE_FACTOR = 0.85; # Moderately stable
my $RUNQ_PRESSURE_SATURATION_CONFIDENCE_FACTOR = 1.0; # Full confidence if RunQ saturation is high

# --- Advanced Efficiency Adjustment Tunable Parameters ---
# Guard Rail for High Existing Constraint (CPU Downsizing Skip)
my $BASE_PHYSC_VS_MAXCPU_THRESH_FOR_CONSTRAINT_GUARD = 0.90; # If Base PhysC > 90% of MaxCPU
my $RUNQ_PRESSURE_FOR_CONSTRAINT_GUARD_FACTOR = 0.80;      # And RunQ Pressure > (this factor * saturation_threshold)

# Dynamic Blending Weights for Efficient Target (P_efficient_target_raw vs. BasePhysC)
my $NORM_P50_LOW_THRESH_FOR_BLEND1 = 0.25;       # If NormP50 < this, give more weight to raw target
my $BLEND_WEIGHT_BASE_FOR_LOW_P50_1 = 0.70;      #   70% Base / 30% Raw Target
my $NORM_P50_MODERATE_THRESH_FOR_BLEND2 = 0.40;  # If NormP50 < this (but >= BLEND1), moderate blend
my $BLEND_WEIGHT_BASE_FOR_LOW_P50_2 = 0.75;      #   75% Base / 25% Raw Target (original idea)
# If NormP50 >= MODERATE_THRESH_FOR_BLEND2 (but still low enough for efficiency consideration),
# lean more heavily on BasePhysC by default (e.g., 85% Base / 15% Target)
my $BLEND_WEIGHT_BASE_DEFAULT_LOW_P50 = 0.85;

# Volatility-Sensitive Cap for MAX_EFFICIENCY_REDUCTION_PERCENTAGE
# These thresholds are for Volatility Ratio (P90/P50)
my $VOLATILITY_MODERATE_LOW_CAP_THRESH = 1.2;  # Volatility above this starts to reduce the reduction cap
my $VOLATILITY_MODERATE_MEDIUM_CAP_THRESH = 1.5; # Intermediate threshold
my $VOLATILITY_MODERATE_HIGH_CAP_THRESH = 1.8; # Volatility above this reduces cap more significantly

# Factors to scale down MAX_EFFICIENCY_REDUCTION_PERCENTAGE
my $REDUCTION_CAP_SCALE_FOR_MODERATE_VOLATILITY = 0.66; # e.g., 15% * 0.66 = ~10% max cut
my $REDUCTION_CAP_SCALE_FOR_MODERATE_MEDIUM_VOLATILITY = 0.50; # e.g., 15% * 0.50 = 7.5% max cut
my $REDUCTION_CAP_SCALE_FOR_MODERATE_HIGH_VOLATILITY = 0.33; # e.g., 15% * 0.33 = ~5% max cut

# --- Single-Thread-Dominated (STD) Workload Heuristics ---
# These constants are used to detect bursting workloads that are likely inefficient
# and should still be considered for strategic (but not tactical) downsizing.
my $STD_NORM_P90_THRESH = 0.5; # NormRunQ P90 must be below this to be considered non-concurrent.
my $STD_IQRC_THRESH     = 0.3; # Volatility must be below this (very steady).
my $STD_PHYSC_STABILITY_THRESH = 0.15; # PhysC P90/P50 ratio must be less than 1.15.
my $STD_LCPU_TIER1_MAX = 8;     # For small VMs (less dampening)
my $STD_LCPU_TIER2_MAX = 32;    # For medium VMs (base dampening)
                                # VMs > TIER2_MAX are large (more dampening)

# --- Dispatch-Bound Workload (DBW) Anomaly Heuristics ---
# Detects workloads with very low CPU usage but extremely high dispatch contention.
my $DBW_LOW_UTIL_FACTOR = 0.5; # BasePhysC must be less than 50% of Entitlement.
my $DBW_DSR_THRESHOLD = 10.0;  # Dispatch Stress Ratio must be > 10.0.
my $DBW_MEDIAN_PRESSURE_THRESHOLD = 1.5; # NormRunQ P50 must be > 1.5.

# --- Enhanced Efficiency Factor Constants (for calculate_runq_modified_physc) ---
my $VOLATILITY_CAUTION_THRESHOLD = 2.5; # If NormRunQ P90/P50 ratio >= this, skip efficiency reduction
my $NORM_P50_THRESHOLD_FOR_EFFICIENCY_CONSIDERATION = 0.5; # NormRunQ P50 must be below this to consider efficiency
my $MAX_EFFICIENCY_REDUCTION_PERCENTAGE  = 0.25; # Max % a profile can be reduced by efficiency logic (ADAPTIVE)
my $MIN_P50_DENOMINATOR_FOR_VOLATILITY = 0.1;    # Min P50 value to avoid division by zero in volatility calc
my $DEFAULT_TARGET_NORM_RUNQ_FOR_EFFICIENCY_CALC = 0.8; # Base, SMT-dependent adjustments in calc sub (ADAPTIVE)

# --- Hot Thread Workload (HTW) Additive Dampening Heuristics ---
# These constants are used to detect and dampen additive CPU for workloads
# that appear constrained (e.g., single-threaded) despite high normalized RunQ.
my $HOT_THREAD_WL_ENT_FACTOR = 0.80;    # BasePhysC < Entitlement * this_factor
my $HOT_THREAD_WL_MAXCPU_FACTOR = 0.25;   # OR BasePhysC < MaxCPU * this_factor
my $HOT_THREAD_WL_HIGH_NORM_P50_THRESHOLD = 3.0; # NormRunQ P50 > this_threshold
# RUNQ_PRESSURE_P90_SATURATION_THRESHOLD (1.8) is an existing constant, also used here.
my $HOT_THREAD_WL_IQRC_THRESHOLD = 1.0;    # NormRunQ_IQRC > this_threshold (tune based on data)
my $HOT_THREAD_WL_DETECTION_MIN_CONDITIONS_MET = 4; # Minimum number of detection conditions to be met

# Dynamic Dampening Multipliers for HTW
my $HOT_THREAD_WL_BASE_DAMPENING_FACTOR = 0.25; # Base factor for the dynamic dampening calculation
my $HOT_THREAD_WL_MIN_DYNAMIC_DAMPENING = 0.05; # Floor for the final dynamic dampening factor
my $HOT_THREAD_WL_MAX_DYNAMIC_DAMPENING = 0.75; # Ceiling for the final dynamic dampening factor

# Enhanced Safety Caps for Additive CPU (applied AFTER all other factors)
my $ADDITIVE_CPU_SAFETY_CAP_FACTOR_OF_BASE = 2.0; # Max additive CPU as a multiple of BasePhysC
my $ADDITIVE_CPU_SAFETY_CAP_ABSOLUTE = 0.5;       # Absolute maximum additive CPU in cores

my $POOL_CONSTRAINT_CONFIDENCE_FACTOR = 0.80; # Reduction factor for additive CPU if VM is in a non-default pool

# --- Built-in default Target NormRunQ values for the adaptive heuristic ---
my %DEFAULT_RUNQ_TARGETS = (
    'P'  => 0.25,
    'O1' => 0.30, 'O2' => 0.40, 'O3' => 0.50, 'O4' => 0.65,
    'G1' => 0.80, 'G2' => 0.90, 'G3' => 1.00, 'G4' => 1.10,
    'B1' => 1.20, 'B2' => 1.30, 'B3' => 1.40, 'B4' => 1.50,
);
my %custom_runq_targets;
my %tier_override_for_csv;

my $CACHE_STATES_FILE = ".nfit.cache.states";
my $DATA_CACHE_FILE   = ".nfit.cache.data";
my $UNIFIED_HISTORY_FILE = ".nfit.history.json";    # Unified history file

# --- Profile Definitions (Loaded from file) ---
my @profiles;
my @csv_visible_profiles;
my $PEAK_PROFILE_NAME = "Peak"; # Standardized name for the peak metric column in output
my @output_header_cols_csv;

# --- Argument Parsing ---
my $physc_data_file;
my $runq_data_file_arg;
my $vm_config_file_arg;
my $profiles_config_file_arg;
my $start_date_str;
my $end_date_str;
my $target_vm_name;
my $round_arg;                     # For nfit's -r (round to nearest)
my $roundup_arg;                   # For nfit's -u (round up)
my $default_smt_arg = $DEFAULT_SMT_VALUE_PROFILE;
my $runq_norm_perc_list_str = $DEFAULT_RUNQ_NORM_PERCS; # Global default for nfit-profile itself
my $runq_abs_perc_list_str  = $DEFAULT_RUNQ_ABS_PERCS;  # Global default for nfit-profile itself
my $runq_perc_behavior_mode = 'fixed';                  # Default behavior: use fixed P90 for RunQ. Alternative: 'match'.
my $help = 0;
my $show_version = 0;
my $script_dir = dirname(abs_path($0));

my $LOG_FILE_PATH = "$script_dir/output/nfit-profile.log";        # Default log file path
my $LOG_FILE_DIR = dirname(abs_path($LOG_FILE_PATH));
my $log_file_path_for_run = $LOG_FILE_PATH;

my $nfit_script_path = "$script_dir/nfit"; # Default path to nfit script
my $nfit_enable_windowed_decay = 0;        # Flag to instruct nfit to use its internal decay
my $nfit_window_unit_str = $DEFAULT_PROCESS_WINDOW_UNIT_FOR_NFIT;
my $nfit_window_size_val = $DEFAULT_PROCESS_WINDOW_SIZE_FOR_NFIT;
my $nfit_decay_half_life_days_val = $DEFAULT_DECAY_HALF_LIFE_DAYS_FOR_NFIT;
my $nfit_analysis_reference_date_str;
my $nfit_runq_avg_method_str = $DEFAULT_NFIT_RUNQ_AVG_METHOD; # 'none', 'sma', 'ema' for nfit's RunQ processing
my $nfit_decay_over_states = 0;
my $nmon_dir;
my $excel_formulas_flag = "false";
my $mgsys_filter;
my $verbose = 0;
my $force_update = 0;

# Seasonality related variables
my $apply_seasonality_event;
my $update_history_flag = 0; # New unified history update flag
my $min_history_days_arg;    # New flag for partial month processing
my $reset_seasonal_cache = 0;
my $seasonal_auto_flag = 0; # Automatic Seasonal Discovery Flag
my $DEFAULT_SEASONALITY_CONFIG_FILE = "nfit.seasonality.cfg";
my $seasonal_scope = 'latest'; # Phase 3: 'latest' (default) or 'all'

# Global flags for seasonality execution context
my $is_multiplicative_forecast_run = 0;
my $is_predictive_peak_model_run = 0;

# The Global Lock ensures atomic access to the history directory.
my $HISTORY_LOCK_FILENAME = ".nfit.history.lock";

GetOptions(
    'nmondir=s'                  => \$nmon_dir,
    'config=s'                   => \$vm_config_file_arg,
    'profiles-config=s'          => \$profiles_config_file_arg,
    'startdate|s=s'              => \$start_date_str,
    'enddate|e=s'                => \$end_date_str,
    'vm|lpar=s'                  => \$target_vm_name,
    'round|r:f'                  => \$round_arg,
    'roundup|u:f'                => \$roundup_arg,
    'default-smt|smt=i'          => \$default_smt_arg,
    'runq-norm-percentiles=s'    => \$runq_norm_perc_list_str,      # Global default list for NormRunQ
    'runq-abs-percentiles=s'     => \$runq_abs_perc_list_str,       # Global default list for AbsRunQ
    'runq-perc-behavior=s'       => \$runq_perc_behavior_mode,      # default: use 'AbsRunQ_P90' for all profiles ('match': match the RunQ percentile to the PhysC profile percentile)
    'help|h'                     => \$help,
    'nfit-path=s'                => \$nfit_script_path,
    'version'                    => \$show_version,
    'enable-windowed-decay'      => \$nfit_enable_windowed_decay,
    'process-window-unit=s'      => \$nfit_window_unit_str,
    'process-window-size=i'      => \$nfit_window_size_val,
    'decay-half-life-days=i'     => \$nfit_decay_half_life_days_val,
    'analysis-reference-date=s'  => \$nfit_analysis_reference_date_str,
    'runq-avg-method=s'          => \$nfit_runq_avg_method_str,
    'decay-over-states'          => \$nfit_decay_over_states,
    'excel-formulas=s'           => \$excel_formulas_flag,
    'mgsys|system|serial|host=s' => \$mgsys_filter,
    'apply-seasonality=s'        => \$apply_seasonality_event,
    'seasonal'                   => \$seasonal_auto_flag,
    'seasonal-scope=s'           => \$seasonal_scope,
    'update-history'             => \$update_history_flag,
    'min-history-days=i'         => \$min_history_days_arg,
    'reset-seasonal-cache'       => \$reset_seasonal_cache,
    "v|verbose+"                 => \$verbose,
    'force'                      => \$force_update,
) or die usage_wrapper();

print STDERR "nfit-profile version $VERSION\n" if (!defined $show_version);

unless($help or $show_version) {
    _phase("Initialisation");
};

# --- Validation ---
my $nfit_ver = "N/A"; # Store nfit version
# This block now runs every time to get the nfit version for logging
if (-x $nfit_script_path) {
	my $nfit_ver_output = `$nfit_script_path --version 2>&1`;
	my ($parsed_nfit_ver) = ($nfit_ver_output =~ /nfit version\s*([0-9.a-zA-Z-]+)/i);
	if (defined $parsed_nfit_ver) {
		$nfit_ver = $parsed_nfit_ver;
		# Check nfit version compatibility for certain features
		my $required_nfit_ver_for_windowing = "2.27.0";
		my $required_nfit_ver_for_runq_avg_and_decay = "2.28.0.4";

		if ($nfit_enable_windowed_decay && version->parse($nfit_ver) < version->parse($required_nfit_ver_for_windowing)) {
			print STDERR " [WARN] The `--enable-windowed-decay` option requires nfit version $required_nfit_ver_for_windowing or higher. Your nfit version ($nfit_ver) may not support this\n";
		}
		if (defined $nfit_runq_avg_method_str && $nfit_runq_avg_method_str ne 'none' && version->parse($nfit_ver) < version->parse($required_nfit_ver_for_runq_avg_and_decay)) {
			print STDERR " [WARN] The `--runq-avg-method (sma/ema)` option requires features from nfit version $required_nfit_ver_for_runq_avg_and_decay or higher. Your nfit version ($nfit_ver) behavior might differ for RunQ processing, especially if `--runq-decay` is intended\n";
		}
	} else {
		print STDERR "  [WARN] Unable to determine nfit version from output: $nfit_ver_output\n";
		if ($nfit_enable_windowed_decay || (defined $nfit_runq_avg_method_str && $nfit_runq_avg_method_str ne 'none')) {
			print STDERR " [WARN] Advanced nfit features are enabled but nfit version cannot be verified\n";
		}
	}
} else {
    print STDERR "  [ERROR] nFit engine ($nfit_script_path) not found or not executable\n";
    exit 2;
}

if ($show_version)
{
    print STDERR "nfit-profile version $VERSION\n";
    print STDERR "  nfit version $nfit_ver\n";
    exit 0;
}

if (defined $seasonal_scope && length $seasonal_scope) {
    $seasonal_scope = lc($seasonal_scope);
    if ($seasonal_scope ne 'latest' && $seasonal_scope ne 'all') {
        die "  [ERROR] Invalid --seasonal-scope '$seasonal_scope' (expected: latest|all)\n";
    }
}

# --- Validation for runq-perc-behavior ---
$runq_perc_behavior_mode = lc($runq_perc_behavior_mode);
unless ($runq_perc_behavior_mode eq 'fixed' || $runq_perc_behavior_mode eq 'match')
{
    die "  [ERROR] Invalid value for --runq-perc-behavior. Must be 'fixed' or 'match' instead of '$runq_perc_behavior_mode'.\n";
}

# --- Data Source Validation ---
# The script now operates exclusively on pre-built cache directories.
my $DEFAULT_BASE_STAGE_DIR = File::Spec->catfile($script_dir, 'stage');

if ($help)
{
    print STDERR usage_wrapper();
    exit 0;
}

# A cache source must be specified via --nmondir or --mgsys.
if (!defined $nmon_dir && !defined $mgsys_filter)
{
    print STDERR usage_wrapper();
    die "[ERROR] No data source cache specified. Please use --nmondir or --mgsys.\n";
}

# If --mgsys is provided without a base --nmondir, set --nmondir to the default.
# This allows the Smart Dispatcher to find the system-specific cache.
if (defined $mgsys_filter && !defined $nmon_dir)
{
    $nmon_dir = $DEFAULT_BASE_STAGE_DIR;
    # Only print the info message if it's a regular run, not a staging/snapshotting run.
    if (!$update_history_flag) {
        print STDERR "  ↳  Staging Directory: '$nmon_dir'\n";
    }
}

# Final check to ensure the base directory exists.
if (defined $nmon_dir && !-d $nmon_dir)
{
    die "[ERROR] The specified cache directory (--nmondir) was not found: '$nmon_dir'\n";
}


if ($default_smt_arg <= 0)
{
    die "[ERROR] --default-smt value must be a positive integer (e.g., 4, 8).\n";
}
if (! -x $nfit_script_path)
{
    die "[ERROR] Cannot find or execute 'nfit' script at '$nfit_script_path'. Use --nfit-path.\n";
}
if (defined($round_arg) && defined($roundup_arg))
{
    die "[ERROR] -round (-r) and -roundup (-u) options are mutually exclusive.\n";
}
if (defined $start_date_str && $start_date_str !~ /^\d{4}-\d{2}-\d{2}$/)
{
    die "[ERROR] Invalid startdate (-s) format '$start_date_str'. Use YYYY-MM-DD.\n";
}
if (defined $end_date_str && $end_date_str !~ /^\d{4}-\d{2}-\d{2}$/)
{
    die "[ERROR] Invalid enddate (-e) format '$end_date_str'. Use YYYY-MM-DD.\n";
}
if (defined $start_date_str && defined $end_date_str)
{
    my ($s_tp, $e_tp);
    eval { $s_tp = Time::Piece->strptime($start_date_str, "%Y-%m-%d"); };
    eval { $e_tp = Time::Piece->strptime($end_date_str, "%Y-%m-%d"); };

    if ($s_tp && $e_tp && $e_tp < $s_tp)
    {
        die "[ERROR] --enddate ($end_date_str) cannot be before --startdate ($start_date_str).\n";
    }
}

# Validate the relationship between enddate and analysis-reference-date
if (defined $end_date_str && defined $nfit_analysis_reference_date_str)
{
    my ($e_tp, $ref_tp);
    eval { $e_tp = Time::Piece->strptime($end_date_str, "%Y-%m-%d"); };
    eval { $ref_tp = Time::Piece->strptime($nfit_analysis_reference_date_str, "%Y-%m-%d"); };

    if ($e_tp && $ref_tp && $ref_tp > $e_tp)
    {
        die "[ERROR] --analysis-reference-date ($nfit_analysis_reference_date_str) cannot be after --enddate ($end_date_str).\n" .
            "       The reference date is used to calculate recency weights and should not be beyond the data cutoff.\n" .
            "       If you want to analyse data up to $end_date_str, either:\n" .
            "         1. Omit --analysis-reference-date (it will default to the last data point), or\n" .
            "         2. Set --analysis-reference-date to $end_date_str or earlier.\n";
    }
}

# Validations for nfit's windowed decay options, if enabled by nfit-profile
if ($nfit_enable_windowed_decay && $nfit_decay_over_states)
{
    die "[ERROR] --enable-windowed-decay and --decay-over-states are mutually exclusive analysis modes.\n";
}
if ($nfit_enable_windowed_decay)
{
    if ($nfit_window_unit_str ne "days" && $nfit_window_unit_str ne "weeks")
    {
        die "[ERROR] --process-window-unit must be 'days' or 'weeks'.\n";
    }
    if ($nfit_window_size_val < 1)
    {
        die "[ERROR] --process-window-size must be at least 1.\n";
    }
    if ($nfit_decay_half_life_days_val < 1)
    {
        die "[ERROR] --decay-half-life-days must be at least 1.\n";
    }
    if (defined $nfit_analysis_reference_date_str && $nfit_analysis_reference_date_str !~ /^\d{4}-\d{2}-\d{2}$/)
    {
        die "[ERROR] Invalid --analysis-reference-date format. Use YYYY-MM-DD.\n";
    }
    print STDERR "  Φ Enabled Windowed Decay processing (window size: $nfit_window_size_val $nfit_window_unit_str)\n";
}

# Validate nfit's RunQ averaging method, if specified
if (defined $nfit_runq_avg_method_str)
{
    $nfit_runq_avg_method_str = lc($nfit_runq_avg_method_str);
    unless ($nfit_runq_avg_method_str eq 'none' || $nfit_runq_avg_method_str eq 'sma' || $nfit_runq_avg_method_str eq 'ema')
    {
        die "[ERROR] The `--runq-avg-method` option requires 'none', 'sma', or 'ema'. Invalid option: '$nfit_runq_avg_method_str'.\n";
    }
}

# --- Excel Formula Validation ---
my $add_excel_formulas = 0; # Default to off
if (defined $excel_formulas_flag)
{
    # If flag is just --excel-formulas, $excel_formulas_flag will be empty string.
    # Default to 'true' if flag is present but no value is given.
    my $val = lc($excel_formulas_flag // 'true');
    if ($val eq 'true' || $val eq '1')
    {
        $add_excel_formulas = 1;
    }
    elsif ($val eq 'false' || $val eq '0')
    {
        $add_excel_formulas = 0;
    }
    else
    {
        die "[ERROR] Invalid value for --excel-formulas. Must be 'true', 'false', or omitted. Got '$excel_formulas_flag'.\n";
    }
}

my $output_dir = File::Spec->catfile($script_dir, 'output');
make_path($output_dir);
if (! -d $output_dir) {
    die "Unable to create output directory '$output_dir': $!";
}

my @generated_files; # Global array to store names of all generated files

# --- Smart Dispatcher Logic ---
# This block determines which cache directories to process. It can handle
# being pointed at a single cache directory or a parent directory containing
# multiple system-specific caches.

my @target_systems_to_process;
my $base_cache_dir = $nmon_dir; # Assume the provided dir is the base by default.

# First, check if the provided --nmondir is ITSELF a valid cache directory.
if (-f File::Spec->catfile($nmon_dir, '.nfit_stage_id'))
{
    # This is a singular run targeting a specific cache directory.
    print STDERR "  ↳  Single-cache mode enabled ($nmon_dir)\n";
    push @target_systems_to_process, undef; # 'undef' signals a single run.
    $base_cache_dir = dirname($nmon_dir); # The base is the parent of the cache dir.
}
else
{
    # If not, check if it's a PARENT directory containing multiple caches.
    opendir(my $dh, $nmon_dir) or die "Cannot open directory $nmon_dir: $!";
    my @subdirs = grep { -d File::Spec->catfile($nmon_dir, $_) && !/^\./ } readdir($dh);
    closedir($dh);

    my @found_serials;
    foreach my $subdir (@subdirs)
    {
        if (-f File::Spec->catfile($nmon_dir, $subdir, '.nfit_stage_id'))
        {
            push @found_serials, $subdir;
        }
    }

    if (@found_serials)
    {
        # This is a multi-system run.
        print STDERR "  ↳  Multi-cache mode enabled ($nmon_dir)\n";
        $base_cache_dir = $nmon_dir; # The provided dir is the base.

        # Apply --mgsys filter if provided, otherwise target all found systems.
        if (defined $mgsys_filter)
        {
            my %serials_from_args = map { $_ => 1 } split /,/, $mgsys_filter;
            @target_systems_to_process = grep { exists $serials_from_args{$_} } @found_serials;
        }
        else
        {
            @target_systems_to_process = @found_serials;
        }
    }
    else
    {
        # The directory is neither a cache itself, nor does it contain any caches.
        die "[ERROR] The directory '$nmon_dir' is not a valid nFit cache and does not contain any cache subdirectories.\n";
    }
}

# If a --vm filter was provided, we must refine our list of target systems.
# This only makes sense in a multi-system context where we can check each one.
if (scalar(@target_systems_to_process) > 1 && defined $target_vm_name)
{
    my %vms_to_find = map { $_ => 1 } split /,/, $target_vm_name;
    my %systems_with_target_vms;

    foreach my $serial (@target_systems_to_process)
    {
        my $states_file = File::Spec->catfile($base_cache_dir, $serial, $CACHE_STATES_FILE);
        next unless -f $states_file;

        my $json_text = do { open my $fh, '<:encoding(utf8)', $states_file or next; local $/; my $content = <$fh>; close $fh; $content; };
        next unless defined $json_text;

        my $states = eval { decode_json($json_text) };
        if ($@) { warn " [WARN] Could not decode JSON from '$states_file': $@. Skipping for VM discovery."; next; }

        foreach my $vm_in_state (keys %$states)
        {
            if (exists $vms_to_find{$vm_in_state})
            {
                $systems_with_target_vms{$serial} = 1;
                last; # Found a match for this system.
            }
        }
    }
    # The new target list is only the systems that contain the specified VMs.
    @target_systems_to_process = sort keys %systems_with_target_vms;
}

if (!@target_systems_to_process)
{
    die "[ERROR] No target managed systems could be identified for processing based on the provided filters.\n";
}

my $s = (scalar(@target_systems_to_process) != 1) ? 's' : '';
print STDERR "  ↳  Dispatcher detected " . scalar(@target_systems_to_process) . " managed system$s\n";

# --- Load Profile Definitions ---
# The log file will now be created inside the main processing loop for each system.
print STDERR "\n[+] Loading Profile Definitions\n";
my $profiles_config_path_to_load;
if (defined $profiles_config_file_arg)
{
    if (-f $profiles_config_file_arg)
    {
        $profiles_config_path_to_load = $profiles_config_file_arg;
    }
    else
    {
        die "[ERROR] Specified profiles config (--profiles-config) not found: $profiles_config_file_arg\n";
    }
}
else # Attempt to find default profiles config
{
    $profiles_config_path_to_load = "$script_dir/etc/$DEFAULT_PROFILES_CONFIG_FILE";
    unless (-f $profiles_config_path_to_load)
    {
        $profiles_config_path_to_load = "$script_dir/$DEFAULT_PROFILES_CONFIG_FILE"; # Try in script's root
    }
    unless (-f $profiles_config_path_to_load)
    {
        die "[ERROR] Default profiles config '$DEFAULT_PROFILES_CONFIG_FILE' not found in '$script_dir/etc/' or '$script_dir/'. Use --profiles-config.\n";
    }
}
print STDERR "  ↳  Profile configuration: $profiles_config_path_to_load\n";
@profiles = load_profile_definitions($profiles_config_path_to_load);
if (scalar @profiles == 0)
{
    die "[ERROR] No profiles loaded from '$profiles_config_path_to_load'.\n";
}

# Filter to only profiles with csv_output enabled
@csv_visible_profiles = grep { $_->{csv_output} } @profiles;

if (scalar @csv_visible_profiles < scalar @profiles) {
    my $invisible = scalar @profiles - scalar @csv_visible_profiles;
    print STDERR "    ✓ Loaded " . scalar(@profiles) . " workload profiles ($invisible invisible)\n";
} else {
    print STDERR "    ✓ Loaded " . scalar(@profiles) . " workload profiles\n";
}

# load custom VM tiers
my %vm_tier_overrides = %{parse_vm_tier_overrides("$script_dir/etc/nfit.vms.cfg")};

# --- Generate CSV header columns ---

# Define the order of columns for the CSV output
@output_header_cols_csv = (
    "VM", "TIER", "Hint", "Pattern", "Pressure", "PressureDetail", "SMT",
    "Serial", "SystemType", "Pool Name", "Pool ID",
    "RunQ_Tactical", "RunQ_Strategic", "RunQ_Potential", "RunQ_Source", $PEAK_PROFILE_NAME
);

# Add profile names as column headers (these will contain the RunQ-modified PhysC values)
push @output_header_cols_csv, map { $_->{name} } @csv_visible_profiles;

# Add "Current - ENT" which is always shown.
push @output_header_cols_csv, ("Current - ENT");
if ($add_excel_formulas) {
    push @output_header_cols_csv, ("NFIT - ENT", "NETT", "NETT%");
}

# --- Ensure the Mandatory P-99W1 Profile is Defined ---
# This check occurs after profiles are loaded.
my $mandatory_profile_is_present = 0;
foreach my $profile_entry (@profiles)
{
    if (defined $profile_entry->{name} && $profile_entry->{name} eq $MANDATORY_PEAK_PROFILE_FOR_HINT)
    {
        $mandatory_profile_is_present = 1;
        last;
    }
}

unless ($mandatory_profile_is_present)
{
    die "[ERROR] Mandatory profile \"$MANDATORY_PEAK_PROFILE_FOR_HINT\" is not defined in the profiles configuration file: '$profiles_config_path_to_load'.\n" .
    "  This profile is essential for core pressure detection logic in nfit-profile.\n" .
    "  Please add a profile named \"$MANDATORY_PEAK_PROFILE_FOR_HINT\" to your profiles configuration.\n";
}

# --- Load Seasonality Definitions ---
my $seasonality_config_path;
my $seasonality_config_file_arg; # Assume this could be a future flag
my $seasonality_config;

if (defined $seasonality_config_file_arg && -f $seasonality_config_file_arg) {
    $seasonality_config_path = $seasonality_config_file_arg;
} else {
    my $default_path_etc = "$script_dir/etc/$DEFAULT_SEASONALITY_CONFIG_FILE";
    my $default_path_root = "$script_dir/$DEFAULT_SEASONALITY_CONFIG_FILE";
    if (-f $default_path_etc) {
        $seasonality_config_path = $default_path_etc;
    } elsif (-f $default_path_root) {
        $seasonality_config_path = $default_path_root;
    }
}

if ($seasonality_config_path) {
    print STDERR "  ↳  Seasonality rules: $seasonality_config_path\n";
    # Replace Config::Tiny->read() with our custom, dependency-free parser.
    $seasonality_config = parse_seasonality_config($seasonality_config_path);
    unless (defined $seasonality_config) {
        # The custom sub will die on error, but this is a safeguard.
        die "[ERROR] Could not parse seasonality configuration file '$seasonality_config_path'.\n";
    }
    my $event_count = scalar(keys %$seasonality_config);
    print STDERR "    ✓ Loaded $event_count seasonal event definitions\n";
} elsif ($apply_seasonality_event || $update_history_flag) {
    # It's an error to request seasonality if the config file doesn't exist.
    die "  [ERROR] A seasonality operation was requested, but '$DEFAULT_SEASONALITY_CONFIG_FILE' could not be found in '$script_dir/etc/' or '$script_dir/'.\n";
}

# --- Determine Minimum Days for History Processing ---
my $min_days_for_history;
my $MIN_HISTORY_DAYS_DEFAULT = 28; # Hard-coded default

if (defined $min_history_days_arg) {
    $min_days_for_history = $min_history_days_arg; # Command line takes precedence
} elsif (defined $seasonality_config && exists $seasonality_config->{Global}{min_history_days}) {
    $min_days_for_history = $seasonality_config->{Global}{min_history_days}; # Config file is second
} else {
    $min_days_for_history = $MIN_HISTORY_DAYS_DEFAULT; # Fallback to default
}

# --- Validation for Seasonality and Decay Model Interaction ---
if (defined $apply_seasonality_event) {
    # Abort if the specified event doesn't exist in the loaded configuration.
    unless (exists $seasonality_config->{$apply_seasonality_event}) {
        die "  [ERROR] Seasonal event '$apply_seasonality_event' could not be found as a valid section in $seasonality_config_path\n";
    }

    my $event_config = $seasonality_config->{$apply_seasonality_event} // {};
    my $model_type = $event_config->{model} // '';

    # The multiplicative_seasonal model is a complete workflow and cannot be
    # combined with the top-level decay flags.
    if ($model_type eq 'multiplicative_seasonal' && ($nfit_enable_windowed_decay || $nfit_decay_over_states)) {
        die "  [ERROR] Incompatible arguments. The '--apply-seasonality' flag for a 'multiplicative_seasonal' event cannot be used with '--enable-windowed-decay' or '--decay-over-states'.\n";
    }
    # Automatically enable the correct decay model if needed
    elsif ($model_type eq 'recency_decay') {
        # Silently enable wndowed decay processing for recency_decay
        $nfit_enable_windowed_decay = 1; # Use the time-based windowed decay engine.
        $nfit_decay_over_states = 0;     # Ensure the state-based engine is disabled.
    }
}

# --- Locate and Load VM Configuration Data ---
# This data provides SMT, MaxCPU, Entitlement, etc., per VM.
my $vm_config_file_path = undef;
my $vm_config_found = 0;
my %vm_config_data;         # Stores parsed VM config: $vm_config_data{hostname}{key} = value
my %vm_config_col_idx;      # Maps column names (lowercase) to their index in the CSV
my $vm_config_header_count = 0; # Number of columns in VM config header

if (defined $vm_config_file_arg) # User specified a VM config file
{
    if (-f $vm_config_file_arg)
    {
        $vm_config_file_path = $vm_config_file_arg;
        $vm_config_found = 1;
        print STDERR "  ↳  VM configuration: $vm_config_file_path\n";
    }
    else
    {
        die "[ERROR] Specified VM configuration file (-config) not found: $vm_config_file_arg\n";
    }
}
elsif (!defined $apply_seasonality_event)
{
    # No VM config file specified, try default locations
    my $dp_etc = "$script_dir/etc/$DEFAULT_VM_CONFIG_FILE";  # Default path: script_dir/etc/
    my $dp_root = "$script_dir/$DEFAULT_VM_CONFIG_FILE"; # Alternative path: script_dir/
    if (-f $dp_etc)
    {
        $vm_config_file_path = $dp_etc;
        $vm_config_found = 1;
        print STDERR "  ↳  VM configuration: $vm_config_file_path\n";
    }
    elsif (-f $dp_root)
    {
        $vm_config_file_path = $dp_root;
        $vm_config_found = 1;
        print STDERR "  ↳  VM configuration: $vm_config_file_path\n";
    }
    else {
        print STDERR "    [WARN] Default VM configuration file not found ($DEFAULT_VM_CONFIG_FILE); VM metadata will be incomplete\n";
    }
}

if ($vm_config_found)
{
    open my $cfg_fh, '<:encoding(utf8)', $vm_config_file_path or die "[ERROR] Cannot open VM config file '$vm_config_file_path': $!\n";
    my $hdr = <$cfg_fh>; # Read header line
    unless (defined $hdr)
    {
        die "    [ERROR] Could not read header from VM config '$vm_config_file_path'\n";
    }
    chomp $hdr;
    $hdr =~ s/\r$//;        # Remove CR if present (Windows line endings)
    $hdr =~ s/^\x{FEFF}//;  # Remove BOM if present (UTF-8 Byte Order Mark)
    my @rhdrs = split /,/, $hdr;
    $vm_config_header_count = scalar @rhdrs;
    my %hmap; # Map lowercase header name to index
    for my $i (0 .. $#rhdrs)
    {
        my $cn = $rhdrs[$i];
        $cn =~ s/^\s*"?|"?\s*$//g; # Trim spaces and quotes from column name
        if ($cn ne '')
        {
            $hmap{lc($cn)} = $i; # Store lowercase column name
        }
    }

    # Check for required columns
    my @req_cols = qw(hostname serial systemtype procpool_name procpool_id entitledcpu maxcpu);
    my $has_smt_col = exists $hmap{'smt'}; # Check if SMT column exists
    unless (exists $hmap{'maxcpu'})
    {
        print STDERR "    [WARN] 'maxcpu' column not present in VM configuration file. This will affect MaxCPU capping logic\n";
    }
    if (!$has_smt_col)
    {
        print STDERR "    [WARN] 'SMT' column not present in VM configuration file. Using default SMT: $default_smt_arg for RunQ calculations\n";
    }

    foreach my $rc (@req_cols)
    {
        unless (exists $hmap{$rc})
        {
            die "    [ERROR] Required column '$rc' could not be found in VM configuration file ($vm_config_file_path).\n";
        }
        $vm_config_col_idx{$rc} = $hmap{$rc};
    }
    if ($has_smt_col) # If SMT column exists, store its index
    {
        $vm_config_col_idx{'smt'} = $hmap{'smt'};
    }

    # Read data lines from VM config
    while (my $ln = <$cfg_fh>)
    {
        chomp $ln;
        $ln =~ s/\r$//;
        next if $ln =~ /^\s*$/; # Skip empty lines

        # Attempt to parse CSV with quoted fields (handles commas within quotes)
        my @rvals = ($ln =~ /"([^"]*)"/g);
        if (scalar @rvals != $vm_config_header_count)
        { # Fallback to simple comma split if quote parsing fails or count mismatches
            @rvals = split /,/, $ln;
            if (scalar @rvals != $vm_config_header_count)
            {
                warn "    [WARN] Mismatched field count on VM config line $. Skipping: $ln\n";
                next;
            }
            # Trim whitespace for values from simple split
            $_ =~ s/^\s+|\s+$//g for @rvals;
        }
        # Else, if quote parsing worked, values in @rvals are already unquoted and trimmed by regex.

        my $hn = $rvals[ $vm_config_col_idx{'hostname'} ]; # Get hostname
        if (defined $hn && $hn ne '')
        {
            my $smt_v = $default_smt_arg; # Default SMT value
            if ($has_smt_col && defined $rvals[$vm_config_col_idx{'smt'}] && $rvals[$vm_config_col_idx{'smt'}] ne '')
            {
                my $sf = $rvals[$vm_config_col_idx{'smt'}];
                if ($sf =~ /(\d+)$/) # Extract trailing digits for SMT value (e.g., "SMT4" -> 4)
                {
                    $smt_v = $1;
                    if ($smt_v <= 0) # Validate SMT
                    {
                        warn "    [WARN] Invalid SMT '$sf' for '$hn'. Using default $default_smt_arg.\n";
                        $smt_v = $default_smt_arg;
                    }
                }
                else
                {
                    warn "    [WARN] Could not parse SMT '$sf' for '$hn'. Using default $default_smt_arg.\n";
                }
            }

            my $max_cpu_val = $rvals[$vm_config_col_idx{'maxcpu'}];
            # Validate MaxCPU value
            unless (defined $max_cpu_val && $max_cpu_val =~ /^[0-9.]+$/ && ($max_cpu_val+0) > 0)
            {
                $max_cpu_val = 0; # Default to 0 if invalid, meaning no effective MaxCPU cap from config
            }

            # Store parsed VM config data
            $vm_config_data{$hn} = {
                serial      => $rvals[$vm_config_col_idx{'serial'}],
                systemtype  => $rvals[$vm_config_col_idx{'systemtype'}],
                pool_name   => $rvals[$vm_config_col_idx{'procpool_name'}],
                pool_id     => $rvals[$vm_config_col_idx{'procpool_id'}],
                entitlement => $rvals[$vm_config_col_idx{'entitledcpu'}],
                maxcpu      => ($max_cpu_val + 0), # Store as number
                smt         => $smt_v,
            };
        }
        else # Hostname missing or empty
        {
            warn "    [WARN] Hostname field is missing in VM configuration file line $. Skipping.\n";
        }
    }
    close $cfg_fh;
    print STDERR "    ✓ Loaded " . scalar(keys %vm_config_data) . " VM configuration entries\n";
}
else # VM config file not found or not specified
{
    print STDERR "    [WARN] VM configuration file not loaded. MaxCPU capping logic will be affected, and SMT will use default\n";
}

# --- Construct Common Flags for nfit ---
# These flags are common to ALL nfit runs initiated by nfit-profile.
# Note: RunQ percentile flags (--runq-norm-perc, --runq-abs-perc) are now handled PER PROFILE run.
my $common_nfit_flags_base = "-q";
if ($nmon_dir)
{
    $common_nfit_flags_base .= " -k --nmondir \"$nmon_dir\"";
    # When using --nmondir, runq data comes from within the NMON files, so --runq-data is not used.
    # However, we still need to pass the RunQ averaging method to nfit if specified.
    if (defined $nfit_runq_avg_method_str)
    {
        $common_nfit_flags_base .= " --runq-avg-method \"$nfit_runq_avg_method_str\"";
    }
}
else # The original path using --physc-data
{
    $common_nfit_flags_base .= " -k --physc-data \"$physc_data_file\"";
    if (defined $runq_data_file_arg)
    {
        $common_nfit_flags_base .= " --runq-data \"$runq_data_file_arg\"";
        if (defined $nfit_runq_avg_method_str)
        {
            $common_nfit_flags_base .= " --runq-avg-method \"$nfit_runq_avg_method_str\"";
        }
    }
}
if (defined $start_date_str) # Global start date for all nfit runs
{
    $common_nfit_flags_base .= " -s $start_date_str";
}

# Common rounding flags (passed to nfit for its output formatting)
my $rounding_flags_for_nfit = ""; # These are applied by nfit itself
if (defined $round_arg)
{
    $rounding_flags_for_nfit .= " -r";
    if (length $round_arg && $round_arg !~ /^\s*$/) # Check if round_arg has a value (e.g. -r=0.1)
    {
        $rounding_flags_for_nfit .= "=$round_arg";
    }
}
elsif (defined $roundup_arg)
{
    $rounding_flags_for_nfit .= " -u";
    if (length $roundup_arg && $roundup_arg !~ /^\s*$/) # Check if roundup_arg has a value (e.g. -u=0.1)
    {
        $rounding_flags_for_nfit .= "=$roundup_arg";
    }
}
$common_nfit_flags_base .= $rounding_flags_for_nfit; # Add rounding to common flags if specified

# --- Main Logic: Run nfit Profiles ---
my %results_table; # Stores PhysC values from nfit for each profile: $results_table{vm_name}{profile_name}
my %runq_modifier_values;
my %nfit_growth_adjustments;
my %runq_uncapped_values;
my %runq_potential_values;
my %hint_tier_for_csv;
my %hint_pattern_for_csv;
my %hint_pressure_for_csv;
my @vm_order;      # To maintain CSV output order consistent with first nfit run that reports VMs
my %vm_seen;       # Tracks VMs seen to populate @vm_order correctly
my %primary_runq_metrics_captured_for_vm; # Tracks if global P50/P90 RunQ metrics captured for hints
my %source_profile_for_global_runq; # Which profile's output sourced the global RunQ P50/P90 for hints
my %per_profile_runq_metrics; # Stores ALL RunQ metrics (e.g. AbsRunQ_P80, AbsRunQ_P98) from EACH profile's nfit run
# Structure: $per_profile_runq_metrics{vm_name}{profile_name}{runq_metric_key}
my %per_profile_nfit_raw_results;
my %pressure_details_for_csv;
my %seasonal_debug_info;
my %outlier_warnings;

my %parsed_growth_adj_values;      # Stores GrowthAdj from nfit output
my %parsed_growth_adj_abs_values;  # Stores GrowthAdjAbs from nfit output
my $FLOAT_EPSILON = 1e-9;

my $LOG_FH;
my %open_log_files;

my $is_seasonal_run = $apply_seasonality_event || $update_history_flag;
my $o_nfit_analysis_reference_date_str = $nfit_analysis_reference_date_str;
my $o_start_date_str = $start_date_str;
my $o_end_date_str = $end_date_str;

# --- Main Processing Loop (iterates through systems for InfluxDB cache, runs once for standard file source modes) ---
foreach my $system_serial (@target_systems_to_process)
{
    my $system_analysis_start_time = time();

    # Restore user-supplied CLI date options for each managed system, to prevent
    # seasonal execution and per-event resets from leaking into subsequent systems.
    $start_date_str = $o_start_date_str;
    $end_date_str = $o_end_date_str;
    $nfit_analysis_reference_date_str = $o_nfit_analysis_reference_date_str;

    # To store the unique YYYYMMDD[-N] suffix
    my $unique_date_suffix;

    # Flag for the multiplicative seasonal model
    my $is_multiplicative_forecast_run = 0;
    my $is_predictive_peak_model_run = 0;
    my $historic_data_for_csv_href;

    my $effective_event_name;
    my $effective_config;
    my $exclusions;
    my ($start_date, $end_date);
    my ($effective_start_date_obj, $effective_end_date_obj);

    # First, determine the full path to the cache we are processing in this iteration.
    my $current_cache_path = defined($system_serial) ? File::Spec->catfile($base_cache_dir, $system_serial) : $nmon_dir;

    # Now, robustly determine the system identifier for logging.
    # Default to the directory name, but prefer the canonical name from the ID file.
    my $system_identifier = basename($current_cache_path); # Tier 2 Fallback identifier
    my $id_file_path = File::Spec->catfile($current_cache_path, '.nfit_stage_id');

    if (-f $id_file_path) {
        eval {
            open my $id_fh, '<:encoding(utf8)', $id_file_path;
            my $id_content = <$id_fh>;
            close $id_fh;

            # Tier 1 Attempt: Extract the system name from the file.
            if ($id_content && $id_content =~ /for system\s+(.+)/) {
                my $candidate_name = $1;
                $candidate_name =~ s/^\s+|\s+$//g; # Trim leading/trailing whitespace

                # --> ADDED: Validate the extracted name.
                # It must not be empty and must not contain illegal filename characters.
                if ($candidate_name ne '' && $candidate_name !~ /[\\\/:\*\?"<>\|]/) {
                    # Validation passed. Use the canonical name.
                    $system_identifier = $candidate_name;
                } else {
                    # Validation failed. The fallback (directory name) will be used.
                    warn " [WARN] Unusable system identifier ('$candidate_name') found in '$id_file_path'. Reverting to directory name.";
                }
            }
        };
        if ($@) {
            # This catches errors during file open/read. The fallback will be used.
            warn " [WARN] Could not read or parse '$id_file_path'. Using directory name for log. Error: $@";
        }
    }

    my $report_type_for_log = 'state-based'; # Default for the "no flags" forensic model
    if ($seasonal_auto_flag) {
        $report_type_for_log = 'seasonal';
    } elsif (defined $apply_seasonality_event && $apply_seasonality_event ne '') {
        $report_type_for_log = $apply_seasonality_event;
    } elsif ($nfit_decay_over_states) {
        $report_type_for_log = 'hybrid-state-decay';
    } elsif ($nfit_enable_windowed_decay) {
        $report_type_for_log = 'windowed-decay';
    }

    # Initialise L1 data cache
    my $data_cache_file = File::Spec->catfile($current_cache_path, $DATA_CACHE_FILE);

    # Calculate cache time-spam
    ($start_date, $end_date) = _get_cache_date_range($data_cache_file);

    #  --- Resolve effective analysis window (CLI -s/-e clamped to cache span) ---
    my $cli_start_obj = undef;
    my $cli_end_obj   = undef;

    if (defined $start_date_str) {
        $cli_start_obj = Time::Piece->strptime($start_date_str, "%Y-%m-%d");
    }
    if (defined $end_date_str) {
        $cli_end_obj = Time::Piece->strptime($end_date_str, "%Y-%m-%d");
    }

    ($effective_start_date_obj, $effective_end_date_obj) =
        _resolve_effective_window($start_date, $end_date, $cli_start_obj, $cli_end_obj);

    # Initialise the run end date to the effective end date by default
    my $run_end_date_obj = $effective_end_date_obj;

    if ($system_identifier && $system_identifier ne '.')
    {
        _phase("Processing Managed System: $system_identifier");

        if ($start_date && $end_date) {
            print STDERR "  ⧗ Data cache timespan: " . $start_date->strftime('%Y-%m-%d') . " - " . $end_date->strftime('%Y-%m-%d') . "\n";
        }

        # Determine and print the analysis type for clarity
        my $analysis_type_desc = 'Workload Profiling';
        if ($apply_seasonality_event) {
            my $event_config = $seasonality_config->{$apply_seasonality_event} // {};
            my $model_type = $event_config->{model} // '';

            if ($model_type eq 'multiplicative_seasonal') {
                $analysis_type_desc = "Seasonal Forecast (Multiplicative, event $apply_seasonality_event)";
            } elsif ($model_type eq 'recency_decay') {
                 $analysis_type_desc = "Seasonal Forecast (Recency-Decay, event $apply_seasonality_event)";
            } elsif ($model_type eq 'predictive_peak') {
                 $analysis_type_desc = "Seasonal Forecast (Predictive Peak, event $apply_seasonality_event)";
            } else {
                $analysis_type_desc = "Seasonal Forecast (event $apply_seasonality_event)";
            }
        } elsif ($nfit_decay_over_states) {
            $analysis_type_desc = "Hybrid State-Time Decay Analysis (--decay-over-states)";
        } elsif ($nfit_enable_windowed_decay) {
            $analysis_type_desc = "Time-Based Windowed Decay Analysis (--enable-windowed-decay)";
        } elsif ($seasonal_auto_flag) {
            $analysis_type_desc = 'Automatic Seasonal Discovery';
        }
        print STDERR "  ◆ Analysis Mode: $analysis_type_desc\n";

        # --- Generate unique, collision-resistant, paired filenames ---
        my $date_str = localtime->strftime('%Y%m%d');

        # Sanitise system identifiers for use in the filename.
        my $log_suffix_system = $system_identifier;
        $log_suffix_system =~ s/[^a-zA-Z0-9_.-]//g;
        my $log_suffix_report = $report_type_for_log;
        $log_suffix_report =~ s/[^a-zA-Z0-9_.-]//g;

        my $base_name_prefix = "nfit-profile.$log_suffix_system.$log_suffix_report";
        my $date_suffix = $date_str;
        my $counter = 0;
        # Check for collision on either file extension
        while (
            -f File::Spec->catfile($output_dir, "$base_name_prefix.$date_suffix.log") ||
            -f File::Spec->catfile($output_dir, "$base_name_prefix.$date_suffix.csv")
        ) {
            $counter++;
            $date_suffix = "$date_str-$counter";
        }
        $unique_date_suffix = $date_suffix; # Store for CSV naming
        $log_file_path_for_run = File::Spec->catfile($output_dir, "$base_name_prefix.$unique_date_suffix.log");
    }
    else
    {

        # Fallback to a default log path inside the output directory for single/non-serial runs.
        my $date_str = localtime->strftime('%Y%m%d');
        my $base_name_prefix = "nfit-profile.default_system.$report_type_for_log";
        my $date_suffix = $date_str;
        my $counter = 0;
        while (
            -f File::Spec->catfile($output_dir, "$base_name_prefix.$date_suffix.log") ||
            -f File::Spec->catfile($output_dir, "$base_name_prefix.$date_suffix.csv")
        ) {
            $counter++;
            $date_suffix = "$date_str-$counter";
        }
        $unique_date_suffix = $date_suffix; # Store for CSV naming
        $log_file_path_for_run = File::Spec->catfile($output_dir, "$base_name_prefix.$unique_date_suffix.log");
    }

    # --- Open Log File for this specific system ---
    my $log_fh_for_system = IO::File->new($log_file_path_for_run, '>:encoding(UTF-8)')
        or warn "[ERROR] Cannot open rationale log for '$system_serial' at '$log_file_path_for_run': $!. Rationale logging will be skipped for this system.\n";

    if ($log_fh_for_system)
    {
        $open_log_files{$system_identifier} = $log_fh_for_system;
        my $LOG_FH = $log_fh_for_system; # Use a lexical variable for printing the header

        $LOG_FH->autoflush(1); # Ensure immediate writing to log
        print {$LOG_FH} "======================================================================\n";
        print {$LOG_FH} "nFit Profile Rationale Log\n";
        print {$LOG_FH} "======================================================================\n";
        print {$LOG_FH} "nfit-profile.pl Run Started: $PROFILE_SCRIPT_START_TIME_STR\n";
        print {$LOG_FH} "nfit-profile.pl Version  : $VERSION\n";
        print {$LOG_FH} "nfit Version Used        : $nfit_ver\n";
        print {$LOG_FH} "----------------------------------------------------------------------\n";
        my @quoted_original_argv_log = map { $_ =~ /\s/ ? qq/"$_"/ : $_ } @original_argv;
        print {$LOG_FH} "Invocation: $0 " . join(" ", @quoted_original_argv_log) . "\n";
        print {$LOG_FH} "----------------------------------------------------------------------\n";
        print {$LOG_FH} "Key Global Settings for System: " . ($system_identifier // 'N/A') . "\n";
        print {$LOG_FH} "  - Profiles Config File       : $profiles_config_path_to_load\n";
        print {$LOG_FH} "  - VM Config File             : " . ($vm_config_file_path // "Not Provided/Default Attempted") . "\n";
        print {$LOG_FH} "  - Common Flags               : -q, -k, rounding, smt, runq-avg-method\n";
        print {$LOG_FH} "  - Dynamic Flags              : Date filters, RunQ percentiles, and Decay models (dynamic).\n";
        print {$LOG_FH} "  - RunQ Avg Method            : $nfit_runq_avg_method_str\n";
        print {$LOG_FH} "  - Default SMT for Profile    : $default_smt_arg\n";
        print {$LOG_FH} "======================================================================\n\n";
    }

    # --- Adaptive Threshold Initialisation for this system ---
    # This block is self-contained and does not mutate global variables.
    # It detects the interval and gets the appropriate thresholds for this system's run.
    my $detected_interval_secs = detect_sampling_interval($data_cache_file);

    # The set_adaptive_thresholds function returns the new values, which are stored
    # in local variables. This avoids modifying the global defaults.
    my ($adaptive_runq_saturation_thresh, $adaptive_target_norm_runq, $adaptive_max_efficiency_reduction) = set_adaptive_thresholds($detected_interval_secs, $log_fh_for_system);

    # --- Seasonality Engine: Main Controller ---
    # This block determines if a seasonal model should be applied and orchestrates the analysis.
    # This block handles all three modes:
    # 1. Updating a snapshot.
    # 2. Applying a recency_decay forecast.
    # 3. Applying a multiplicative_seasonal forecast.

    if ($update_history_flag) {
        # --- Mode 1: Update the Unified Monthly History Cache ---
        # This is a special run mode. Its only purpose is to analyse completed
        # months in the data cache and save the results to the new history file.
        # It does not produce a CSV output.
        # Call the new subroutine to handle the history generation.
        update_monthly_history($current_cache_path, $system_identifier, $seasonality_config, $min_days_for_history, $adaptive_runq_saturation_thresh, $force_update);

        # After updating the history, we skip the rest of the processing for this system.
        print STDERR "✓ History update complete for system $system_identifier\n";
        next; # Proceed to the next system in the loop.
    } elsif ($seasonal_auto_flag) {
        # --- Automatic Seasonal Discovery ---
        _execute_automatic_seasonal_discovery($seasonality_config, $current_cache_path, \@profiles, {
            system_identifier => $system_identifier,
            nfit_path => $nfit_script_path,
            rounding_flags => $rounding_flags_for_nfit,
            default_smt => $default_smt_arg,
            runq_avg_method => $nfit_runq_avg_method_str,
            runq_perc_behavior => $runq_perc_behavior_mode,
            adaptive_saturation_thresh => $adaptive_runq_saturation_thresh,
            adaptive_target_norm_runq => $adaptive_target_norm_runq,
            adaptive_max_efficiency_reduction => $adaptive_max_efficiency_reduction,
            sampling_interval => $detected_interval_secs,
            cache_start_date  => $start_date,
            cache_end_date    => $end_date,
            effective_start_date => $effective_start_date_obj,
            effective_end_date   => $effective_end_date_obj,
        }, $effective_end_date_obj, undef);
        next; # Skip standard processing
    } elsif ($apply_seasonality_event) {

        # Preserve explicit guard: adaptive_peak is owned by nfit-forecast
        my $requested_event_config = $seasonality_config->{$apply_seasonality_event} // {};
        my $requested_model_type   = $requested_event_config->{model} // '';
        if ($requested_model_type eq 'adaptive_peak') {
            die "[ERROR] The 'adaptive_peak' forecasting model can only be executed using the `nfit-forecast` program.\n" .
                "        Please use the command: ./nfit-forecast --nmondir $current_cache_path --apply-seasonality $apply_seasonality_event\n";
        }

        _execute_automatic_seasonal_discovery(
            $seasonality_config,
            $current_cache_path,
            \@profiles,
            {
                system_identifier => $system_identifier,
                nfit_path => $nfit_script_path,
                rounding_flags => $rounding_flags_for_nfit,
                default_smt => $default_smt_arg,
                runq_avg_method => $nfit_runq_avg_method_str,
                runq_perc_behavior => $runq_perc_behavior_mode,
                adaptive_saturation_thresh => $adaptive_runq_saturation_thresh,
                adaptive_target_norm_runq => $adaptive_target_norm_runq,
                adaptive_max_efficiency_reduction => $adaptive_max_efficiency_reduction,
                sampling_interval => $detected_interval_secs,
                cache_start_date  => $start_date,
                cache_end_date    => $end_date,
                effective_start_date => $effective_start_date_obj,
                effective_end_date   => $effective_end_date_obj,
            },
            $effective_end_date_obj,
            $apply_seasonality_event
        );

        next; # Skip standard processing for this system (matches --seasonal behaviour)
    }

    # --- Standard (non-seasonal) processing path ---
    # Executes the single-pass engine and any model-specific post-processing
    # for non-seasonal runs. Seasonal execution paths MUST `next` before this block.

    _phase("Standard Analysis (Non-Seasonal)");

    my $analysis_context = {
        analysis_start  => $system_analysis_start_time,
        start       => $effective_start_date_obj,
        end         => $run_end_date_obj,
        cache_start => $start_date,
        cache_end   => $end_date,
        interval    => $detected_interval_secs
    };

    # A single, clean call to the new single-pass engine.
    my $parsed_nfit_results = run_single_pass_analysis(
        $current_cache_path,
        \@profiles,
        {
            # ... (all necessary global arguments are passed in this hash) ...
            nfit_path               => $nfit_script_path,
            rounding_flags          => $rounding_flags_for_nfit,
            start_date              => $effective_start_date_obj->ymd,
            end_date                => $run_end_date_obj->ymd,
            vm_name                 => $target_vm_name,
            default_smt             => $default_smt_arg,
            runq_avg_method         => $nfit_runq_avg_method_str,
            enable_windowed_decay   => $nfit_enable_windowed_decay,
            decay_over_states       => $nfit_decay_over_states,
            analysis_reference_date => $nfit_analysis_reference_date_str,
            enable_growth_prediction => $nfit_enable_windowed_decay || $nfit_decay_over_states,
        }
    );

    _phase("Assimilating Engine Results");

    # This single function call replaces the previous complex assimilation logic.
    # It consumes the raw JSON from nfit and produces a clean, predictable map.
    # The global @profiles array is passed to assist with per-state averaging logic.
    my $assimilation_map_ref = build_assimilation_map($parsed_nfit_results, \@profiles, $adaptive_runq_saturation_thresh);

    # Capture Context Variables (Already present in scope)
    # $start_date (Time::Piece), $end_date (Time::Piece), $detected_interval_secs (Scalar)

    # The @vm_order array, which drives the main processing loop,
    # must be populated from the keys of our newly created assimilation map.
    @vm_order = sort keys %{$assimilation_map_ref};

    # --- DEBUG CHECKPOINT ---
    # As requested, this block will print the contents of the newly created
    # assimilation map and then halt execution. This allows for a clean
    # evaluation of the data structure before proceeding with further refactoring.
    #print STDERR "\n\n==================== nFit Profile Debug Checkpoint ====================\n\n";
    #print STDERR "--- STAGE 1: Assimilation Map Contents ---\n";
    #print STDERR "This map is the new single source of truth for all downstream logic.\n\n";
    #print STDERR Dumper($assimilation_map_ref);
    #print STDERR "\n================== End Debug. Halting Execution. ==================\n\n";
    #die "Exiting after assimilation map generation for review.";
    # --- END DEBUG CHECKPOINT ---

    # --- Rationale Logging and Hint Generation ---
    # This block now uses a conditional to call the correct logging subroutine
    # based on the analysis model that was run. It also now captures all hint
    # components for later use in the CSV report, improving efficiency
    if ($is_multiplicative_forecast_run) {
        # We pass the current assimilation map (initial_results) to the forecast engine.
        my ($forecast_results, $historic_data) = calculate_multiplicative_forecast(
            $current_cache_path,
            $system_identifier,
            $effective_event_name, # defined in the scope above (line 333)
            $effective_config,     # defined in the scope above (line 334)
            $seasonality_config,
            $assimilation_map_ref,  # Pass current results if needed
            $exclusions,
            $effective_start_date_obj,
            $effective_end_date_obj
        );

        # Guard against empty results from failed baseline calculation
        unless ($forecast_results && scalar keys %$forecast_results > 0) {
            warn "  [WARN] Multiplicative forecast returned no results for event '$effective_event_name'. Skipping.\n";
            # Skip the rest of the multiplicative processing
        }
        else {

            # Merge the forecast results into the main results table
            # (calculate_multiplicative_forecast already populates global %seasonal_debug_info)
            $historic_data_for_csv_href = $historic_data;

            # Calculate Horizon Metadata
            my ($next_start, $next_end) = determine_event_period($effective_config, $effective_end_date_obj, 'forecast');
            my $horizon_meta = undef;
            if ($next_end) {
                my $gen_time = Time::Piece->new();
                my $days_diff = int(($next_end->epoch - $gen_time->epoch) / 86400);
                $days_diff = 0 if $days_diff < 0;
                $horizon_meta = { target_date => $next_end->ymd, days => $days_diff };
            }

            # Store the multiplicative_seasonal forecast results to unified history
            _store_model_forecast_to_history(
                    $current_cache_path,
                    $effective_event_name,
                    'multiplicative_seasonal',
                    $forecast_results,
                    $effective_config,
                    $horizon_meta,
                    $analysis_context
                    );

            # Overwrite the main results table with the forecast results
            %results_table = %$forecast_results;

            # Populate the global vm_order array so the reporter knows which VMs to process
            @vm_order = sort keys %results_table;

            # Update assimilation map with forecast values for CSV reporter
            foreach my $vm_name (@vm_order) {
                $assimilation_map_ref->{$vm_name} //= {};
                $assimilation_map_ref->{$vm_name}{CoreResults} //= {};
                $assimilation_map_ref->{$vm_name}{CoreResults}{ProfileValues} = $forecast_results->{$vm_name};
            }

            # Path for the multiplicative seasonal model, which needs a unique log format.
            # We must populate the assimilation map with hints so the CSV writer can read them.
            foreach my $vm_name (@vm_order) {
                # Get the authoritative map entry for this VM
                my $vm_map_ref = $assimilation_map_ref->{$vm_name};

                # 1. Self-Reference & Metrics
                $vm_map_ref->{Configuration}{vm_name} = $vm_name;
                $vm_map_ref->{RunQMetrics} //= $per_profile_runq_metrics{$vm_name} || {};

                # 2. Generate Hints
                # Call generate_sizing_hint using the assimilation map (not a temp hash).
                my ($hint_type_tier, $hint_pattern_shape, $hint_pressure_bool, $pressure_detail_str, $rationale, $has_abs, $has_norm) =
                    generate_sizing_hint(
                        $vm_map_ref,
                        undef,  # No log file handle needed here
                        $adaptive_runq_saturation_thresh
                    );

                # 3. Store Hints
                $vm_map_ref->{Hinting}{AutoTier}       = $hint_type_tier;
                $vm_map_ref->{Hinting}{Pattern}        = $hint_pattern_shape;
                $vm_map_ref->{Hinting}{Pressure}       = $hint_pressure_bool;
                $vm_map_ref->{Hinting}{PressureDetail} = $pressure_detail_str // 'N/A';

                # 4. Populate legacy globals (for safety)
                $hint_tier_for_csv{$vm_name}      = $hint_type_tier;
                $hint_pattern_for_csv{$vm_name}   = $hint_pattern_shape;
                $hint_pressure_for_csv{$vm_name}  = $hint_pressure_bool;
                $pressure_details_for_csv{$vm_name} = $pressure_detail_str // 'N/A';

                # Populate Tier Override for CSV and Logic
                $vm_map_ref->{Hinting}{FinalTierForVM} = $vm_tier_overrides{$vm_name} // $hint_type_tier;
                $tier_override_for_csv{$vm_name}       = $vm_tier_overrides{$vm_name} // "";

                # 5. Calculate RunQ Modifiers (Tactical/Strategic/Potential)
                # Determine source profile (Logic borrowed from main loop)
                my $user_tier = $vm_tier_overrides{$vm_name} // "";
                my $pattern_source = ($user_tier ne "") ? $user_tier : $hint_type_tier;
                my ($pattern_char) = ($pattern_source =~ /^([A-Z])/);
                $pattern_char //= 'G';
                my %pat_map = ('O'=>'O3-95W15', 'B'=>'B3-95W15', 'G'=>'G3-95W15', 'P'=>'G3-95W15');
                my $runq_source_profile_name = $pat_map{$pattern_char} // 'G3-95W15';

                # 6. Iterate all profiles to apply modifiers
                foreach my $profile (@profiles) {
                    my $p_name = $profile->{name};

                    # Guard P-99W1 (Pure Signal)
                    # Ensure we do not apply RunQ modifiers to the statistical ceiling.
                    next if ($p_name eq $MANDATORY_PEAK_PROFILE_FOR_HINT);

                    my %pressure_flags = ( abs_pressure => $has_abs, norm_pressure => $has_norm );
                    my %adaptive_thresholds = ( saturation => $adaptive_runq_saturation_thresh, target => $adaptive_target_norm_runq, max_reduction => $adaptive_max_efficiency_reduction );

                    # Calculate modified value (Forecast + RunQ)
                    my ($adjusted_physc, $debug_info_ref) = calculate_runq_modified_physc(
                        $vm_name, $vm_map_ref, $profile, \%pressure_flags, \%adaptive_thresholds
                    );

                    # Update the CoreResults with the FINAL adjusted value
                    # This ensures the CSV shows the Forecast INCLUDING the RunQ modifier
                    if (looks_like_number($adjusted_physc)) {
                        $vm_map_ref->{CoreResults}{ProfileValues}{$p_name} = sprintf("%.4f", $adjusted_physc);
                    }

                    # If this is the source profile, save the modifiers for the CSV columns
                    if ($p_name eq $runq_source_profile_name) {
                        $vm_map_ref->{CSVModifiers}{RunQ_Tactical}  = $debug_info_ref->{'FinalAdditive'};
                        $vm_map_ref->{CSVModifiers}{RunQ_Strategic} = $debug_info_ref->{'RunQ_Strategic'};
                        $vm_map_ref->{CSVModifiers}{RunQ_Potential} = $debug_info_ref->{'RunQ_Potential'};
                        $vm_map_ref->{CSVModifiers}{RunQ_Source}    = $runq_source_profile_name;
                    }
                }

            }
            log_multiplicative_seasonal_rationale($open_log_files{$system_identifier}, $seasonality_config->{$apply_seasonality_event}, $analysis_context);
        }
    } elsif ($is_predictive_peak_model_run) {
        # Path for the new predictive peak model
        # The predictive_peak model uses linear regression on historical seasonal peaks.
        my $forecast_results = calculate_predictive_peak_forecast(
            $current_cache_path,
            $system_identifier,
            $effective_event_name,
            $effective_config,
            $seasonality_config,
            $adaptive_runq_saturation_thresh,
            $exclusions,
            $effective_start_date_obj,
            $effective_end_date_obj
        );

        # Determine Forecast Horizon for Metadata (anchored to effective end date)
        my $asof_end = $effective_end_date_obj // $end_date;
        my ($next_start, $next_end) = determine_event_period($effective_config, $asof_end, 'forecast');
        my $horizon_meta = undef;
        if ($next_end) {
            my $ref_time = (defined $asof_end && ref($asof_end) eq 'Time::Piece') ? $asof_end : Time::Piece->new();

            my $days_diff = int(($next_end->epoch - $ref_time->epoch) / 86400);
            $days_diff = 0 if $days_diff < 0;

            $horizon_meta = {
                target_date => $next_end->ymd,
                days        => $days_diff
            };
        }

        # Store the predictive_peak forecast results to unified history
        _store_model_forecast_to_history(
            $current_cache_path,
            $effective_event_name,
            'predictive_peak',
            $forecast_results,
            $effective_config,
            $horizon_meta,
            $analysis_context
        );

        # Overwrite the main results table with the forecast
        %results_table = %$forecast_results;

        # Populate the global vm_order array so the reporter knows which VMs to process.
        @vm_order = sort keys %results_table;

        # Generate and store the necessary hint/pressure data for each VM for the CSV.
        # fix: Hydrate the Assimilation Map Correctly ---
        foreach my $vm_name (@vm_order) {

            # 1. Update CoreResults in the map with the forecasted values
            $assimilation_map_ref->{$vm_name} //= {};
            $assimilation_map_ref->{$vm_name}{CoreResults}{ProfileValues} = $forecast_results->{$vm_name};

            # 2. Ensure self-reference exists
            $assimilation_map_ref->{$vm_name}{Configuration}{vm_name} = $vm_name;
            $assimilation_map_ref->{$vm_name}{RunQMetrics} //= $per_profile_runq_metrics{$vm_name} || {};

            # 3. Generate Hints using the MAP (Single Source of Truth)
            my ($hint_type_tier, $hint_pattern_shape, $hint_pressure_bool, $pressure_detail_str, $rationale, $has_abs, $has_norm) =
                generate_sizing_hint(
                    $assimilation_map_ref->{$vm_name},
                    undef,  # No log file handle needed here
                    $adaptive_runq_saturation_thresh
                );

            # 4. Store hints back into the map
            $assimilation_map_ref->{$vm_name}{Hinting}{AutoTier}       = $hint_type_tier;
            $assimilation_map_ref->{Hinting}{Pattern}                  = $hint_pattern_shape;
            $assimilation_map_ref->{$vm_name}{Hinting}{Pressure}       = $hint_pressure_bool;
            $assimilation_map_ref->{$vm_name}{Hinting}{PressureDetail} = $pressure_detail_str // 'N/A';
            $assimilation_map_ref->{Hinting}{FinalTierForVM} = $vm_tier_overrides{$vm_name} // $hint_type_tier;

            # 5. Populate legacy globals for safety
            $hint_tier_for_csv{$vm_name}      = $hint_type_tier;
            $hint_pattern_for_csv{$vm_name}   = $hint_pattern_shape;
            $hint_pressure_for_csv{$vm_name}  = $hint_pressure_bool;
            $pressure_details_for_csv{$vm_name} = $pressure_detail_str // 'N/A';
            # Populate Tier Override for CSV and Logic
            $tier_override_for_csv{$vm_name}       = $vm_tier_overrides{$vm_name} // "";

            # 6. Calculate RunQ Modifiers (Copy of Multiplicative Logic)
            my $user_tier = $vm_tier_overrides{$vm_name} // "";
            my $pattern_source = ($user_tier ne "") ? $user_tier : $hint_type_tier;
            my ($pattern_char) = ($pattern_source =~ /^([A-Z])/);
            $pattern_char //= 'G';
            my %pat_map = ('O'=>'O3-95W15', 'B'=>'B3-95W15', 'G'=>'G3-95W15', 'P'=>'G3-95W15');
            my $runq_source_profile_name = $pat_map{$pattern_char} // 'G3-95W15';

            my ($runq_source_profile_obj) = grep { $_->{name} eq $runq_source_profile_name } @profiles;

            if ($runq_source_profile_obj) {
                my %pressure_flags = ( abs_pressure => $has_abs, norm_pressure => $has_norm );
                my %adaptive_thresholds = ( saturation => $adaptive_runq_saturation_thresh, target => $adaptive_target_norm_runq, max_reduction => $adaptive_max_efficiency_reduction );

                my (undef, $debug_info_ref) = calculate_runq_modified_physc(
                    $vm_name, $assimilation_map_ref->{$vm_name}, $runq_source_profile_obj, \%pressure_flags, \%adaptive_thresholds
                );

                $assimilation_map_ref->{$vm_name}{CSVModifiers}{RunQ_Tactical}  = $debug_info_ref->{'FinalAdditive'};
                $assimilation_map_ref->{$vm_name}{CSVModifiers}{RunQ_Strategic} = $debug_info_ref->{'RunQ_Strategic'};
                $assimilation_map_ref->{$vm_name}{CSVModifiers}{RunQ_Potential} = $debug_info_ref->{'RunQ_Potential'};
                $assimilation_map_ref->{$vm_name}{CSVModifiers}{RunQ_Source}    = $runq_source_profile_name;
            }

            # 7. Iterate all profiles to apply modifiers (FIX: Was missing in manual path)
            foreach my $profile (@profiles) {
                my $p_name = $profile->{name};

                # --- GUARD: Skip RunQ Modifiers for P-99W1 ---
                next if ($p_name eq $MANDATORY_PEAK_PROFILE_FOR_HINT);

                my %pressure_flags = ( abs_pressure => $has_abs, norm_pressure => $has_norm );
                my %adaptive_thresholds = ( saturation => $adaptive_runq_saturation_thresh, target => $adaptive_target_norm_runq, max_reduction => $adaptive_max_efficiency_reduction );

                my ($adjusted_physc, $debug_info_ref) = calculate_runq_modified_physc(
                    $vm_name, $assimilation_map_ref->{$vm_name}, $profile, \%pressure_flags, \%adaptive_thresholds
                );

                if (looks_like_number($adjusted_physc)) {
                    $assimilation_map_ref->{$vm_name}{CoreResults}{ProfileValues}{$p_name} = sprintf("%.4f", $adjusted_physc);
                }
            }

            # 8. Update Peak Value logic
            my $baseline_peak = $seasonal_debug_info{$vm_name}{'P-99W1'}{'TrueBaseline'} // 0;
            my $predicted_peak = $seasonal_debug_info{$vm_name}{'P-99W1'}{'PredictedPeak'} // 0;

            # Store the effective peak back into the map
            $assimilation_map_ref->{$vm_name}{CoreResults}{ProfileValues}{$PEAK_PROFILE_NAME} =
                ($baseline_peak > $predicted_peak) ? $baseline_peak : $predicted_peak;
        }
        log_predictive_peak_rationale($open_log_files{$system_identifier}, $seasonality_config->{$apply_seasonality_event}, $analysis_context);
    } else {
        _phase("Applying modifiers and generating rationale");
        # --- FINAL POST-PROCESSING, HINT GENERATION, and RATIONALE LOGGING ---

        # --- Progress Bar Initialization ---
        my $total_vms = scalar @vm_order;
        my $total_prof = scalar @profiles;
        my $total_units_for_progress = $total_vms * $total_prof;
        my $done_units_for_progress = 0;
        my $vm_count_for_progress = 0;
        my $show_profile_progress_flag = ($verbose || -t STDERR);

        foreach my $vm_name (@vm_order) {
            $vm_count_for_progress++;

            # Get this VM's complete map entry.
            my $vm_map_ref = $assimilation_map_ref->{$vm_name};
            next unless (ref($vm_map_ref) eq 'HASH');
            $vm_map_ref->{Configuration}{vm_name} = $vm_name; # Self-reference for convenience

            # --- Phase 1: Generate Hints and Pressure Flags ---
            my ($hint_type_tier, $hint_pattern_shape, $pressure_bool, $pressure_detail_str, $pressure_rationale_text, $p99w1_has_abs_pressure, $p99w1_has_norm_pressure) =
                generate_sizing_hint(
                    $vm_map_ref,
                    $log_fh_for_system,
                    $adaptive_runq_saturation_thresh
                );

            # Store hint results in the map for use by other functions and the CSV writer.
            $vm_map_ref->{Hinting}{AutoTier} = $hint_type_tier;
            $vm_map_ref->{Hinting}{Pattern} = $hint_pattern_shape;
            $vm_map_ref->{Hinting}{Pressure} = $pressure_bool;
            $vm_map_ref->{Hinting}{PressureDetail} = $pressure_detail_str // 'N/A';
            $vm_map_ref->{Hinting}{P99W1_AbsRunQPressure} = $p99w1_has_abs_pressure;
            $vm_map_ref->{Hinting}{P99W1_NormRunQPressure} = $p99w1_has_norm_pressure;
            $vm_map_ref->{Hinting}{FinalTierForVM} = $vm_tier_overrides{$vm_name} // $hint_type_tier;
            $tier_override_for_csv{$vm_name} = $vm_tier_overrides{$vm_name} // "";

            # --- [NEW] Standardise Growth for Recency Decay ---
            # If this is a recency_decay run, we must ensure ALL profiles use the
            # growth adjustment from the stable "Source" profile (e.g. G3),
            # rather than their own potentially volatile/zero adjustment.
            my $seasonal_cfg = (defined $apply_seasonality_event && exists $seasonality_config->{$apply_seasonality_event})
                ? $seasonality_config->{$apply_seasonality_event}
                : {};
            if (($seasonal_cfg->{model} // '') eq 'recency_decay') {

                # 1. Determine Source Profile (Pattern/Tier Logic)
                #    Priority: UserOverride -> AutoHint -> Default 'G'
                my $user_tier = $vm_map_ref->{Hinting}{FinalTierForVM} // "";
                my $auto_tier = $vm_map_ref->{Hinting}{AutoTier} // "G";
                my $pattern_source = ($user_tier ne "") ? $user_tier : $auto_tier;

                my ($pattern_char) = ($pattern_source =~ /^([A-Z])/);
                $pattern_char //= 'G'; # Default to General

                # Map pattern to the standard Tier-3 planning profile
                my %pat_map = ('O'=>'O3-95W15', 'B'=>'B3-95W15', 'G'=>'G3-95W15', 'P'=>'G3-95W15');
                my $growth_source_profile = $pat_map{$pattern_char} // 'G3-95W15';

                # 2. Get the Standard Adjustment (from the Source Profile's growth)
                #    Growth adjustments were populated into {Growth}{adjustments} during assimilation.
                my $standard_growth_adj = $vm_map_ref->{Growth}{adjustments}{$growth_source_profile} // 0;

                # 3. Store metadata for Logging
                $vm_map_ref->{Growth}{StandardAdjustment} = {
                    Value  => $standard_growth_adj,
                    Source => $growth_source_profile
                };

                # 4. [CRITICAL] Pre-calculate CSVModifiers for recency_decay BEFORE the profile loop.
                #    The recency_decay model requires RunQ_Potential to be available, but CSVModifiers
                #    are normally populated in Phase 2 (after this block). We must calculate them here.
                my ($growth_source_profile_obj) = grep { $_->{name} eq $growth_source_profile } @profiles;

                if ($growth_source_profile_obj) {
                    my %pressure_flags_rd = (
                        abs_pressure  => $vm_map_ref->{Hinting}{P99W1_AbsRunQPressure}  // 0,
                        norm_pressure => $vm_map_ref->{Hinting}{P99W1_NormRunQPressure} // 0
                    );
                    my %adaptive_thresholds_rd = (
                        saturation    => $adaptive_runq_saturation_thresh,
                        target        => $adaptive_target_norm_runq,
                        max_reduction => $adaptive_max_efficiency_reduction
                    );

                    my (undef, $debug_info_ref_rd) = calculate_runq_modified_physc(
                        $vm_name, $vm_map_ref, $growth_source_profile_obj,
                        \%pressure_flags_rd, \%adaptive_thresholds_rd
                    );

                    # Populate CSVModifiers early so the profile loop can use them
                    $vm_map_ref->{CSVModifiers}{RunQ_Tactical}  = $debug_info_ref_rd->{'FinalAdditive'};
                    $vm_map_ref->{CSVModifiers}{RunQ_Strategic} = $debug_info_ref_rd->{'RunQ_Strategic'};
                    $vm_map_ref->{CSVModifiers}{RunQ_Potential} = $debug_info_ref_rd->{'RunQ_Potential'};
                    $vm_map_ref->{CSVModifiers}{RunQ_Source}    = $growth_source_profile;
                }

                # 5. Apply to ALL profiles (Update CoreResults)
                #    This ensures the CSV and the Logger see the standardised value (Base + StandardGrowth + RunQ).

                foreach my $prof (@profiles) {
                    my $p_name = $prof->{name};

                    # EXEMPTION: P-99W1 is a "Panic Profile" (Peak Analysis).
                    # It accepts NO modifiers (Growth or RunQ).
                    if ($p_name eq 'P-99W1') {
                         # Ensure P-99W1 stays as its raw Recency-Anchored value (no growth added)
                         next;
                    }

                    # Get the raw base value (pre-growth) directly from the Growth stash
                    my $base_val = $vm_map_ref->{Growth}{base_values}{$p_name};

                    if (defined $base_val && looks_like_number($base_val)) {
                        # A. Start with Base + StandardGrowth
                        my $final_val = $base_val + $standard_growth_adj;

                        # B. Add RunQ Modifier (Latent Demand / Potential)
                        #    CRITICAL: For recency_decay, we use RunQ_Potential (raw queue physics)
                        #    rather than RunQ_Tactical (change-managed, gated increment).
                        #    Rationale: recency_decay is a pure operational forecast answering
                        #    "What is actual demand now?" - it should not be subject to
                        #    change management gates that constrain monthly resizing plans.
                        my $runq_mod = $vm_map_ref->{CSVModifiers}{RunQ_Potential} // 0;

                        $final_val += $runq_mod;

                        # Update the authoritative CoreResults map
                        $vm_map_ref->{CoreResults}{ProfileValues}{$p_name} = sprintf("%.4f", $final_val);
                    }
                }
            }

            # Log the detailed pressure rationale text once per VM.
            if ($log_fh_for_system) {
                print {$log_fh_for_system} "\n######################################################################\n";
                print {$log_fh_for_system} "# Rationale for VM: $vm_name\n";
                print {$log_fh_for_system} "######################################################################\n\n";
                print {$log_fh_for_system} $pressure_rationale_text . "\n" if ($pressure_rationale_text);
            }

            # --- Phase 2: Calculate Modifiers (ONCE) and Final Profile Values ---

            # First, determine the single source profile for RunQ modifiers for this VM
            # GUARD: For recency_decay models, CSVModifiers were already calculated in the
            # dedicated block above. Skip recalculation to avoid overwriting with values
            # computed from the already-modified ProfileValues.
            my $seasonal_cfg_p2 = (defined $apply_seasonality_event && exists $seasonality_config->{$apply_seasonality_event})
                ? $seasonality_config->{$apply_seasonality_event}
              : {};
            my $skip_phase2_modifiers = (($seasonal_cfg_p2->{model} // '') eq 'recency_decay');

            # Priority: User TIER > AutoTier > Fallback to 'G'.
            my $user_tier_override_runq = $vm_map_ref->{Hinting}{FinalTierForVM} // "";
            my $auto_tier_runq = $vm_map_ref->{Hinting}{AutoTier} // "G";

            my $pattern_source_runq = ($user_tier_override_runq ne "") ? $user_tier_override_runq : $auto_tier_runq;
            my ($hint_pattern) = ($pattern_source_runq =~ /^([A-Z])/);
            $hint_pattern //= 'G'; # Default to 'G' if regex fails

            my %pattern_to_profile_map = ('O'=>'O3-95W15', 'B'=>'B3-95W15', 'G'=>'G3-95W15', 'P'=>'G3-95W15');
            my $runq_source_profile_name = $pattern_to_profile_map{$hint_pattern} // 'G3-95W15';

            # Find the profile object for the source profile
            my ($runq_source_profile_obj) = grep { $_->{name} eq $runq_source_profile_name } @profiles;

            # Only recalculate CSVModifiers if this is NOT a recency_decay run
            # (recency_decay already populated these values correctly)
            if ($runq_source_profile_obj && !$skip_phase2_modifiers) {
                my %pressure_flags = ( abs_pressure => $p99w1_has_abs_pressure, norm_pressure => $p99w1_has_norm_pressure );
                my %adaptive_thresholds = ( saturation => $adaptive_runq_saturation_thresh, target => $adaptive_target_norm_runq, max_reduction => $adaptive_max_efficiency_reduction );

                # Calculate modifiers ONLY for the source profile
                my (undef, $debug_info_ref) = calculate_runq_modified_physc(
                    $vm_name, $vm_map_ref, $runq_source_profile_obj, \%pressure_flags, \%adaptive_thresholds
                );

                # Store the authoritative modifiers for the CSV report
                $vm_map_ref->{CSVModifiers}{RunQ_Tactical}  = $debug_info_ref->{'FinalAdditive'};
                $vm_map_ref->{CSVModifiers}{RunQ_Strategic} = $debug_info_ref->{'RunQ_Strategic'};
                $vm_map_ref->{CSVModifiers}{RunQ_Potential} = $debug_info_ref->{'RunQ_Potential'};
                $vm_map_ref->{CSVModifiers}{RunQ_Source}    = $runq_source_profile_name;
            }

            # Now, iterate through all profiles to size them and log rationale
            foreach my $profile (@profiles) {
                my $profile_name = $profile->{name};
                $done_units_for_progress++;

                # Progress bar
                if ($show_profile_progress_flag) {
                    my $perc_done = ($total_units_for_progress > 0) ? (100.0 * $done_units_for_progress / $total_units_for_progress) : 0;
                    my $profile_name_for_progress = $profile->{name};
                    printf STDERR "\r    • Processing VM %d/%d (%s), Profile %d/%d (%s) [%.1f%%]...",
                        $vm_count_for_progress,
                        $total_vms,
                        $vm_name,
                        ($done_units_for_progress % $total_prof) || $total_prof,
                        $total_prof,
                        $profile_name_for_progress,
                        $perc_done;
                }

                my $base_physc_for_log = $vm_map_ref->{CoreResults}{ProfileValues}{$profile_name};
                next unless (defined $base_physc_for_log && looks_like_number($base_physc_for_log));

                if ($profile_name eq $MANDATORY_PEAK_PROFILE_FOR_HINT) {
                    log_peak_profile_rationale($log_fh_for_system, $vm_map_ref, $profile, $base_physc_for_log);
                } else {
                    # Recalculate for logging, but do NOT store these values for the CSV
                    my %pressure_flags = ( abs_pressure => $p99w1_has_abs_pressure, norm_pressure => $p99w1_has_norm_pressure );
                    my %adaptive_thresholds = ( saturation => $adaptive_runq_saturation_thresh, target => $adaptive_target_norm_runq, max_reduction => $adaptive_max_efficiency_reduction );

                    my ($adjusted_physc, $debug_info_ref) = calculate_runq_modified_physc(
                        $vm_name, $vm_map_ref, $profile, \%pressure_flags, \%adaptive_thresholds
                    );

                    # CRITICAL: For recency_decay models, ProfileValues was ALREADY correctly calculated
                    # in the dedicated block above (base + growth + runq_potential). We must NOT
                    # overwrite it here, as calculate_runq_modified_physc would double-apply modifications.
                    my $seasonal_cfg = (defined $apply_seasonality_event && exists $seasonality_config->{$apply_seasonality_event})
                        ? $seasonality_config->{$apply_seasonality_event}
                        : {};
                    my $is_recency_decay_model = (($seasonal_cfg->{model} // '') eq 'recency_decay');

                    my $final_csv_value;
                    if ($is_recency_decay_model) {
                        # Use the pre-calculated value (do NOT overwrite)
                        $final_csv_value = $vm_map_ref->{CoreResults}{ProfileValues}{$profile_name};
                    } else {
                        # Standard path: store the newly calculated value
                        $final_csv_value = looks_like_number($adjusted_physc) ? sprintf("%.3f", $adjusted_physc) : "N/A";
                        $vm_map_ref->{CoreResults}{ProfileValues}{$profile_name} = $final_csv_value;
                    }

                    my $raw_states_aref_for_log = $vm_map_ref->{RawNfitStates} || [];

                    log_profile_rationale(
                        $log_fh_for_system, $vm_map_ref, $profile,
                        $base_physc_for_log, $final_csv_value, $debug_info_ref,
                        $raw_states_aref_for_log,
                        $adaptive_runq_saturation_thresh
                    );
                }
            }

       }

        # --- Progress Bar Finalization ---
        if ($show_profile_progress_flag) {
            printf STDERR "\r    • Processing complete. (100.0%%)%s\n", ' ' x 70;
        }
    }

    # -- Reporting and Reset Block --
    # This block executes after all profiles have been run for the current system.

    # -- Reporting and Reset Block --
    # This block executes after all profiles have been run for the current system.
    _phase("Results");
    my $is_recency_decay_run = 0;

    # --- FIX: Prioritise Seasonality Name in Filename ---
    my $report_type_for_filename = 'state-based';

    if (defined $apply_seasonality_event && $apply_seasonality_event ne '') {
        $report_type_for_filename = $apply_seasonality_event;

        # Check if it is specifically recency decay
        my $event_config = $seasonality_config->{$apply_seasonality_event} // {};
        if (($event_config->{model} // '') eq 'recency_decay') {
            $is_recency_decay_run = 1;
        }
    }
    elsif ($nfit_decay_over_states) {
        $report_type_for_filename = 'hybrid-state-decay';
    } elsif ($nfit_enable_windowed_decay) {
        $report_type_for_filename = 'windowed-decay';
    }

    if ($is_seasonal_run && !$is_multiplicative_forecast_run) {
        my $event_config = $seasonality_config->{$apply_seasonality_event} // {};
        $is_recency_decay_run = (($event_config->{model} // '') eq 'recency_decay');
    }

   if ($is_multiplicative_forecast_run) {
        # This is the new path for the multiplicative model, which now uses the standard reporter.
        if (@vm_order) {
            _write_standard_csv_report($assimilation_map_ref, $report_type_for_filename, $system_identifier, $unique_date_suffix, 1, 0, 0, $adaptive_runq_saturation_thresh);

            # If verbose mode is on, generate the extra audit trail files.
            if ($verbose) {
                print STDERR "  ◆ Audit Trail\n";
                # The baseline results are the final values *before* the forecast was applied.
                # The %results_table was overwritten, so we can't re-use it.
                # However, the baseline is available inside the seasonal_debug_info hash.
                my %baseline_data_for_verbose;
                foreach my $vm (keys %seasonal_debug_info) {
                    foreach my $prof (keys %{$seasonal_debug_info{$vm}}) {
                        $baseline_data_for_verbose{$vm}{$prof} = $seasonal_debug_info{$vm}{$prof}{baseline};
                    }
                }
                # Use $apply_seasonality_event as it's in scope here.
                write_seasonal_csv_output("current_baseline", $system_identifier, $apply_seasonality_event, $unique_date_suffix, \%baseline_data_for_verbose);
                write_seasonal_csv_output("historic_snapshot", $system_identifier, $apply_seasonality_event, $unique_date_suffix, $historic_data_for_csv_href);
            }
        }
    }
    elsif ($is_seasonal_run) {
        # This path is for all non-multiplicative seasonal models (recency_decay, predictive_peak).
        my $event_config = $seasonality_config->{$apply_seasonality_event} // {};
        my $model_type = $event_config->{model} // '';

        if ($model_type eq 'recency_decay' || $model_type eq 'predictive_peak') {
            if (@vm_order) {
                # For recency_decay, pass a true flag to add its specific columns.
                # For predictive_peak, pass a false flag to generate a standard report.
                my $is_recency_flag = ($model_type eq 'recency_decay') ? 1 : 0;
                my $is_predictive_flag = ($model_type eq 'predictive_peak') ? 1 : 0;
                _write_standard_csv_report($assimilation_map_ref, $apply_seasonality_event, $system_identifier, $unique_date_suffix, 0, $is_recency_flag, $is_predictive_flag, $adaptive_runq_saturation_thresh);

                # Store recency_decay forecast results to unified history
                if ($model_type eq 'recency_decay')  {
                    my %recency_decay_results_for_history;

                    # 1. Extract core results for storage
                    foreach my $vm (keys %{$assimilation_map_ref}) {
                        my $core_res = $assimilation_map_ref->{$vm}{CoreResults}{ProfileValues};
                        if (ref($core_res) eq 'HASH') {
                            $recency_decay_results_for_history{$vm} = { %$core_res };
                        }
                    }

                    # 2. [REFACTORED] Horizon Metadata Calculation
                    # Use the deterministic event period relative to the analysis end date.
                    # This replaces the risky "scan for projection_days" and "localtime" logic.
                    my $horizon_meta = undef;

                    # Use the authoritative effective end date from the main scope
                    my $anchor_date = $effective_end_date_obj;

                    if (defined $anchor_date) {
                        # Identify the target event relative to the analysis anchor
                        my ($evt_start, $evt_end) = determine_event_period($event_config, $anchor_date, 'forecast');

                        if ($evt_start && $evt_end) {
                            my $days_diff = int(($evt_end->epoch - $anchor_date->epoch) / 86400);
                            $days_diff = 0 if $days_diff < 0;

                            $horizon_meta = {
                                target_date => $evt_end->ymd,
                                days        => $days_diff
                            };
                        }
                    }

                    # Note: We intentionally leave $horizon_meta as undef if the event period
                    # cannot be determined, rather than falling back to localtime() (Wall Clock).

                    _store_model_forecast_to_history(
                        $current_cache_path,
                        $apply_seasonality_event,
                        'recency_decay',
                        \%recency_decay_results_for_history,
                        $event_config,
                        $horizon_meta,
                        $analysis_context
                    );
                }
            } else {
                print STDERR "  [INFO] No VM forecast results were produced for seasonal event '$apply_seasonality_event' for model $model_type\n";
            }
        }
    } else {
        # This is the standard, non-seasonal run path. It now generates one report
        # per system before proceeding to the next.
        if (@vm_order) {
            _write_standard_csv_report($assimilation_map_ref, $report_type_for_filename, $system_identifier, $unique_date_suffix, 0, 0, 0, $adaptive_runq_saturation_thresh);
        }
    }

    # CRITICAL: Reset global data structures before processing the next system.
    @vm_order = ();
    %vm_seen = ();
    %results_table = ();
    %per_profile_runq_metrics = ();
    %primary_runq_metrics_captured_for_vm = ();
    %per_profile_nfit_raw_results = ();
    %seasonal_debug_info = ();

    # Report the duration for the completed system's analysis
    my $system_analysis_duration = time() - $system_analysis_start_time;
    print STDERR "  ✓ Processed system $system_identifier (elapsed " . format_duration($system_analysis_duration) . ")\n";

} # End foreach system

# --- Collect unique serials that were part of the output ---
my %serials_in_output_map;
if (%vm_config_data && @vm_order) { # Ensure vm_config_data was loaded and there are VMs to process
    foreach my $vm_name_in_order (@vm_order) {
        if (exists $vm_config_data{$vm_name_in_order} &&
            defined $vm_config_data{$vm_name_in_order}{serial} &&
            $vm_config_data{$vm_name_in_order}{serial} ne '') {
            $serials_in_output_map{$vm_config_data{$vm_name_in_order}{serial}} = 1;
        }
    }
}
my @sorted_unique_serials_list = sort keys %serials_in_output_map;
my $excel_row_num_counter = 1; # Excel rows are 1-based; header is row 1, so first data row is 2.

# --- Final Report Generation ---
# This block now only handles the default, non-seasonal run.
# All seasonal models now handle their own output from within the main system loop.
print STDERR "  ◆ Generated outputs\n";

# --- Script Footer ---
my $final_message = '';
if (@generated_files) {
    # Use List::Util::uniqstr if available, otherwise a simple hash works.
    my %seen;
    my @unique_files = grep { !$seen{$_}++ } @generated_files;

    $final_message .= "    ↳  CSV reports\n";
    foreach my $file (@unique_files) {
        $final_message .= "      ↳  $file\n";
    }
} elsif ($update_history_flag) {
    $final_message .= "    ↳  History\n      ↳  Monthly history updated\n";
} else {
    $final_message .= "    ↳  CSV reports\n      ↳  No output files were generated\n";
}
$final_message .= "    ↳  Rationale\n      ↳  $log_file_path_for_run\n";
print STDERR "$final_message\n";

# --- Report final script duration summary to STDERR ---
my $PROFILE_SCRIPT_END_TIME_EPOCH = time();
my $PROFILE_SCRIPT_DURATION = $PROFILE_SCRIPT_END_TIME_EPOCH - $PROFILE_SCRIPT_START_TIME_EPOCH;
print STDERR "  ⧗ Total execution time: " . format_duration($PROFILE_SCRIPT_DURATION) . "\n";

# --- Finalise and close all rationale log files ---
my $final_end_time_str = localtime($PROFILE_SCRIPT_END_TIME_EPOCH)->strftime("%Y-%m-%d %H:%M:%S %Z");
foreach my $system_id (keys %open_log_files) {
    my $log_fh = $open_log_files{$system_id};
    if ($log_fh) {
        print {$log_fh} "\n----------------------------------------------------------------------\n";
        print {$log_fh} "Analysis for System '$system_id' completed at: " . $final_end_time_str . "\n";
        # Note: A per-system duration would require storing start times in a hash as well.
        # This is sufficient to close the log with a final timestamp.
        print {$log_fh} "======================================================================\n";
        close $log_fh;
    }
}

exit 0;

# ==============================================================================
# Subroutines
# ==============================================================================

sub _phase {
    my ($msg) = @_;
    return unless ($verbose || -t STDERR);
    printf STDERR "\n▶ %s\n", $msg;
}

sub _task {
    my ($msg) = @_;
    return unless ($verbose || -t STDERR);
    printf STDERR "  • %s\n", $msg;
}

sub _subtask {
    my ($msg) = @_;
    return unless ($verbose || -t STDERR);
    printf STDERR "    • %s\n", $msg;
}

sub _step {
    my ($msg) = @_;
    return unless ($verbose || -t STDERR);
    printf STDERR "\n  [+] %s\n", $msg;
}

# --- get_excel_col_name ---
# Converts a 1-based column index to an Excel column name (e.g., 1 -> A, 27 -> AA).
sub get_excel_col_name {
    my ($idx) = @_;
    my $name = '';
    die "Column index must be positive" if (!defined $idx || $idx <= 0);
    while ($idx > 0) {
        my $mod = ($idx - 1) % 26;
        $name = chr(65 + $mod) . $name;
        $idx = int(($idx - $mod - 1) / 26); # Corrected logic for 1-based index progression
    }
    return $name;
}

# --- generate_nfit_ent_formula ---
# Generates the dynamic Excel formula for the "NFIT_ENT_UserFormula" column.
sub generate_nfit_ent_formula {
    my ($excel_row_num, $num_profiles, $column_offset) = @_;

	$column_offset //= 0;

    # First profile column is M (13th column).
    # Peak (L) is the 12th column. Profiles start after Peak.
    my $first_profile_excel_col_letter = get_excel_col_name(12 + 1);
    my $last_profile_excel_col_letter = get_excel_col_name(12 + $num_profiles);

    # The fixed array string for the MATCH function, as provided by the user.
    my $tier_match_array_str_for_formula = '{"P","G1","G2","G3","G4","O1","O2","O3","O4","B1","B2","B3","B4"}';

    # Dynamic column index for 'NFIT - Ent'.
    # 13 fixed leading columns (A-M) + num_profiles columns + 1 (for "NFIT - Ent" itself).
    my $entitlement_column_index = 13 + $num_profiles + 1 + $column_offset;

    # Using "A:AZ" as the VLOOKUP range as requested for stability.
    my $vlookup_range_for_peer_ent = "A:AZ";

    my $formula_body = sprintf(
        'IF(ISNUMBER(SEARCH("PowerHA Standby", I%d)),VLOOKUP(VLOOKUP(A%d, PowerHA!A:B, 2, FALSE),%s, %d, FALSE) * $L$258, CEILING(INDEX(%s%d:%s%d, MATCH(B%d, %s, 0)), 0.05))',
        $excel_row_num,                            # For I%d (SystemType)
        $excel_row_num,                            # For A%d (VM Name for inner VLOOKUP)
        $vlookup_range_for_peer_ent,               # Range for outer VLOOKUP (e.g., A:AZ)
        $entitlement_column_index,                 # Dynamic column index for Current_ENT of the peer
        $first_profile_excel_col_letter, $excel_row_num, # For M%d (start of profile data range)
        $last_profile_excel_col_letter,  $excel_row_num, # For e.g. Y%d (end of profile data range)
        $excel_row_num,                            # For C%d (Hint column, containing the tier string like "G3")
        $tier_match_array_str_for_formula          # For {"P","G1",...} array
    );
    return "=" . $formula_body; # Excel formulas start with "="
}

# --- print_csv_footer ---
# Prints the summary footer section with labels and Excel formulas.
# Make sure Time::Piece is used if not already at the top of your script for strftime
# use Time::Piece; # Already in the full script you provided.
# use List::Util qw(sum min max); # Already in the full script.
# Ensure get_excel_col_name and quote_csv are defined elsewhere or within this sub's scope.
# It accepts an offset to correctly calculate column letters when extra
# columns (like SeasonalMultiplier) are present in the report.
sub print_csv_footer {
    my ($fh, $last_data_row, $nmon_physc_file, $num_profiles, $sorted_unique_serials_list_ref, $col_offset) = @_;
    $col_offset //= 0; # Default to 0 if not provided

    my @sorted_unique_serials = @$sorted_unique_serials_list_ref;
    my $count_of_unique_serials = scalar(@sorted_unique_serials);
    my $loop_count_for_serials = ($count_of_unique_serials == 0) ? 1 : $count_of_unique_serials;

    # --- Calculate dynamic column letters based on script's output structure ---
    my $col_serial_letter = get_excel_col_name(8);
    my $col_system_type_letter = get_excel_col_name(9);

    # Apply the offset to all subsequent column calculations
    my $idx_current_ent = 12 + $num_profiles + 1 + $col_offset;
    my $col_current_ent_letter = get_excel_col_name($idx_current_ent);
    my $idx_nfit_ent_user_formula = 12 + $num_profiles + 2 + $col_offset;
    my $col_nfit_ent_user_formula_letter = get_excel_col_name($idx_nfit_ent_user_formula);

    # --- Get NMON physc data file modification timestamp ---
    my $file_timestamp_str = "N/A";
    if (defined $nmon_physc_file && -f $nmon_physc_file) {
        my $mtime_epoch = (stat($nmon_physc_file))[9];
        if (defined $mtime_epoch) {
            $file_timestamp_str = localtime($mtime_epoch)->strftime("%Y-%m-%d %H:%M:%S");
        } else {
            $file_timestamp_str = "Timestamp N/A (stat fetch failed for $nmon_physc_file)";
        }
    } else {
        $file_timestamp_str = "Timestamp N/A (File not found or not provided)";
    }

    # --- Define starting row for footer elements ---
    print $fh "\n"; # Blank line after main data
    my $footer_start_row = $last_data_row + 2;

    my $row_data_age = $footer_start_row;
    my $row_timestamp = $footer_start_row + 1;
    my $row_as_is_nfit_labels = $footer_start_row + 3;
    my $row_ent_col_headers = $footer_start_row + 4;
    my $row_unique_serials_start = $footer_start_row + 5;

    my $empty = "";
    my @csv_row;

    # --- Row 1 of Footer: Data Age ---
    my $col_letter_data_age_sum_current_incl = get_excel_col_name(23 + $col_offset);
    my $col_letter_data_age_sum_nfit_incl = get_excel_col_name(24 + $col_offset);
    my $col_letter_data_age_delta_incl = get_excel_col_name(25 + $col_offset);
    my $col_letter_data_age_perc_incl = get_excel_col_name(26 + $col_offset);

    my $formula_sum_current_ent_incl_sby = sprintf("=SUM(%s2:%s%d)", $col_current_ent_letter, $col_current_ent_letter, $last_data_row);
    my $formula_sum_nfit_ent_incl_sby = sprintf("=SUM(%s2:%s%d)", $col_nfit_ent_user_formula_letter, $col_nfit_ent_user_formula_letter, $last_data_row);
    my $formula_delta_incl_sby = sprintf("=%s%d-%s%d", $col_letter_data_age_sum_nfit_incl, $row_data_age, $col_letter_data_age_sum_current_incl, $row_data_age);
    my $formula_perc_incl_sby = sprintf("=IFERROR(%s%d/%s%d,\"\")", $col_letter_data_age_delta_incl, $row_data_age, $col_letter_data_age_sum_current_incl, $row_data_age);

    @csv_row = ($empty) x (22 + $col_offset);
    $csv_row[0] = "Data Age";
    $csv_row[21 + $col_offset] = "Incl. SBY";
    push @csv_row, $formula_sum_current_ent_incl_sby, $formula_sum_nfit_ent_incl_sby, $formula_delta_incl_sby, $formula_perc_incl_sby;
    print $fh join(",", map { quote_csv($_) } @csv_row) . "\n";

    # --- Row 2 of Footer: Timestamp ---
    my $col_letter_ts_sum_current_excl = $col_letter_data_age_sum_current_incl;
    my $col_letter_ts_sum_nfit_excl    = $col_letter_data_age_sum_nfit_incl;
    my $col_letter_ts_delta_excl       = $col_letter_data_age_delta_incl;
    my $col_letter_ts_perc_excl        = $col_letter_data_age_perc_incl;

    my $formula_sum_current_ent_excl_sby = sprintf("=SUMIFS(%s\$2:%s\$%d, %s\$2:%s\$%d, \"<>*PowerHA Standby*\")", $col_current_ent_letter, $col_current_ent_letter, $last_data_row, $col_system_type_letter, $col_system_type_letter, $last_data_row);
    my $formula_sum_nfit_ent_excl_sby = sprintf("=SUMIFS(%s\$2:%s\$%d, %s\$2:%s\$%d, \"<>*PowerHA Standby*\")", $col_nfit_ent_user_formula_letter, $col_nfit_ent_user_formula_letter, $last_data_row, $col_system_type_letter, $col_system_type_letter, $last_data_row);
    my $formula_delta_excl_sby = sprintf("=%s%d-%s%d", $col_letter_ts_sum_nfit_excl, $row_timestamp, $col_letter_ts_sum_current_excl, $row_timestamp);
    my $formula_perc_excl_sby = sprintf("=IFERROR(%s%d/%s%d,\"\")", $col_letter_ts_delta_excl, $row_timestamp, $col_letter_ts_sum_current_excl, $row_timestamp);

    @csv_row = ($empty) x (22 + $col_offset);
    $csv_row[0] = $file_timestamp_str;
    $csv_row[21 + $col_offset] = "Excl. SBY";
    push @csv_row, $formula_sum_current_ent_excl_sby, $formula_sum_nfit_ent_excl_sby, $formula_delta_excl_sby, $formula_perc_excl_sby;
    print $fh join(",", map { quote_csv($_) } @csv_row) . "\n";

    print $fh "\n";

    # --- Row: AS-IS NFIT Labels ---
    @csv_row = ($empty) x (23 + $col_offset);
    $csv_row[17 + $col_offset] = "AS-IS";
    $csv_row[18 + $col_offset] = "NFIT";
    $csv_row[20 + $col_offset] = "AS-IS";
    $csv_row[21 + $col_offset] = "NFIT";
    print $fh join(",", map { quote_csv($_) } @csv_row) . "\n";

    # --- Row: ENT Column Headers and other labels ---
    @csv_row = ($empty) x (22 + $col_offset);
    $csv_row[0] = "ENT"; $csv_row[1] = "ENT-NOVIO"; $csv_row[2] = "ENT-HA"; $csv_row[3] = "ENT-NFIT";
    $csv_row[4] = "NFIT-ENT-NO-VIO"; $csv_row[5] = "NFIT-ENT-NO-POWERHA-STANDBY";
    $csv_row[6] = "NFIT-ENT-NO-POWERHA-SBY-NO-VIO"; $csv_row[7] = "NFIT-ENT-POWERHA-SBY-AS-IS";
    $csv_row[8] = "NFIT-ENT-POWERHA-SBY-AS-IS-NOVIO";
    $csv_row[12] = "PowerHA SBY% TGT"; $csv_row[13] = "0.25";

    my $largest_frame_formula_as_is = sprintf("=LET(sys,%s\$2:%s\$%d,type,%s\$2:%s\$%d,ent,%s\$2:%s\$%d,rows,FILTER(HSTACK(sys,ent),NOT(type=\"VIO Server\")),uniqSys,UNIQUE(INDEX(rows,,1)),sums,BYROW(uniqSys,LAMBDA(s,SUM(FILTER(INDEX(rows,,2),INDEX(rows,,1)=s)))),XLOOKUP(MAX(sums),sums,uniqSys))", $col_serial_letter, $col_serial_letter, $last_data_row, $col_system_type_letter, $col_system_type_letter, $last_data_row, $col_current_ent_letter, $col_current_ent_letter, $last_data_row);
    my $largest_frame_formula_nfit = sprintf("=LET(sys,%s\$2:%s\$%d,type,%s\$2:%s\$%d,ent,%s\$2:%s\$%d,rows,FILTER(HSTACK(sys,ent),NOT(type=\"VIO Server\")),uniqSys,UNIQUE(INDEX(rows,,1)),sums,BYROW(uniqSys,LAMBDA(s,SUM(FILTER(INDEX(rows,,2),INDEX(rows,,1)=s)))),XLOOKUP(MAX(sums),sums,uniqSys))", $col_serial_letter, $col_serial_letter, $last_data_row, $col_system_type_letter, $col_system_type_letter, $last_data_row, $col_nfit_ent_user_formula_letter, $col_nfit_ent_user_formula_letter, $last_data_row);
    my $largest_powerha_formula_as_is = sprintf("=LET(sys,%s\$2:%s\$%d,type,%s\$2:%s\$%d,ent,%s\$2:%s\$%d,pharows,FILTER(HSTACK(sys,ent),ISNUMBER(SEARCH(\"PowerHA Primary\",type))),uniqSysPHA,UNIQUE(INDEX(pharows,,1)),sumsPHA,BYROW(uniqSysPHA,LAMBDA(s,SUM(FILTER(INDEX(pharows,,2),INDEX(pharows,,1)=s)))),XLOOKUP(MAX(sumsPHA),sumsPHA,uniqSysPHA,\"\"))", $col_serial_letter, $col_serial_letter, $last_data_row, $col_system_type_letter, $col_system_type_letter, $last_data_row, $col_current_ent_letter, $col_current_ent_letter, $last_data_row);
    my $largest_powerha_formula_nfit = sprintf("=LET(sys,%s\$2:%s\$%d,type,%s\$2:%s\$%d,ent,%s\$2:%s\$%d,pharows,FILTER(HSTACK(sys,ent),ISNUMBER(SEARCH(\"PowerHA Primary\",type))),uniqSysPHA,UNIQUE(INDEX(pharows,,1)),sumsPHA,BYROW(uniqSysPHA,LAMBDA(s,SUM(FILTER(INDEX(pharows,,2),INDEX(pharows,,1)=s)))),XLOOKUP(MAX(sumsPHA),sumsPHA,uniqSysPHA,\"\"))", $col_serial_letter, $col_serial_letter, $last_data_row, $col_system_type_letter, $col_system_type_letter, $last_data_row, $col_nfit_ent_user_formula_letter, $col_nfit_ent_user_formula_letter, $last_data_row);

    $csv_row[16 + $col_offset] = "Largest Frame";
    $csv_row[17 + $col_offset] = $largest_frame_formula_as_is;
    $csv_row[18 + $col_offset] = $largest_frame_formula_nfit;
    $csv_row[19 + $col_offset] = "Largest PowerHA";
    $csv_row[20 + $col_offset] = $largest_powerha_formula_as_is;
    $csv_row[21 + $col_offset] = $largest_powerha_formula_nfit;
    print $fh join(",", map { quote_csv($_) } @csv_row) . "\n";

    # --- Per-Serial Summary Rows ---
    for (my $i = 0; $i < $loop_count_for_serials; $i++) {
        my $current_formula_row = $row_unique_serials_start + $i;
        my @csv_row_serial_summary;

        if ($i == 0) {
            my $formula_unique_serials = sprintf("=UNIQUE(%s\$2:%s\$%d)", $col_serial_letter, $col_serial_letter, $last_data_row);
            push @csv_row_serial_summary, $formula_unique_serials;
        } else {
            push @csv_row_serial_summary, $empty;
        }

        # Formulas for columns B-J
        push @csv_row_serial_summary, sprintf("=IFERROR(SUMIFS(\$%s\$2:\$%s\$%d, \$%s\$2:\$%s\$%d, A%d),\"\")", $col_current_ent_letter, $col_current_ent_letter, $last_data_row, $col_serial_letter, $col_serial_letter, $last_data_row, $current_formula_row);
        push @csv_row_serial_summary, sprintf("=IFERROR(SUMIFS(\$%s\$2:\$%s\$%d, \$%s\$2:\$%s\$%d, A%d, \$%s\$2:\$%s\$%d, \"<>VIO Server\"),\"\")", $col_current_ent_letter, $col_current_ent_letter, $last_data_row, $col_serial_letter, $col_serial_letter, $last_data_row, $current_formula_row, $col_system_type_letter, $col_system_type_letter, $last_data_row);
        push @csv_row_serial_summary, sprintf("=IFERROR(SUMIFS(\$%s\$2:\$%s\$%d, \$%s\$2:\$%s\$%d, A%d, \$%s\$2:\$%s\$%d, \"*PowerHA Primary*\"),\"\")", $col_current_ent_letter, $col_current_ent_letter, $last_data_row, $col_serial_letter, $col_serial_letter, $last_data_row, $current_formula_row, $col_system_type_letter, $col_system_type_letter, $last_data_row);
        push @csv_row_serial_summary, sprintf("=IFERROR(SUMIFS(\$%s\$2:\$%s\$%d, \$%s\$2:\$%s\$%d, A%d),\"\")", $col_nfit_ent_user_formula_letter, $col_nfit_ent_user_formula_letter, $last_data_row, $col_serial_letter, $col_serial_letter, $last_data_row, $current_formula_row);
        push @csv_row_serial_summary, sprintf("=IFERROR(SUMIFS(\$%s\$2:\$%s\$%d, \$%s\$2:\$%s\$%d, A%d, \$%s\$2:\$%s\$%d, \"<>VIO Server\"),\"\")", $col_nfit_ent_user_formula_letter, $col_nfit_ent_user_formula_letter, $last_data_row, $col_serial_letter, $col_serial_letter, $last_data_row, $current_formula_row, $col_system_type_letter, $col_system_type_letter, $last_data_row);
        push @csv_row_serial_summary, sprintf("=IFERROR(SUMIFS(\$%s\$2:\$%s\$%d, \$%s\$2:\$%s\$%d, A%d, \$%s\$2:\$%s\$%d, \"<>*PowerHA Standby*\"),\"\")", $col_nfit_ent_user_formula_letter, $col_nfit_ent_user_formula_letter, $last_data_row, $col_serial_letter, $col_serial_letter, $last_data_row, $current_formula_row, $col_system_type_letter, $col_system_type_letter, $last_data_row);
        push @csv_row_serial_summary, sprintf("=IFERROR(SUMIFS(\$%s\$2:\$%s\$%d, \$%s\$2:\$%s\$%d, A%d, \$%s\$2:\$%s\$%d, \"<>*PowerHA Standby*\", \$%s\$2:\$%s\$%d, \"<>VIO Server\"),\"\")", $col_nfit_ent_user_formula_letter, $col_nfit_ent_user_formula_letter, $last_data_row, $col_serial_letter, $col_serial_letter, $last_data_row, $current_formula_row, $col_system_type_letter, $col_system_type_letter, $last_data_row, $col_system_type_letter, $col_system_type_letter, $last_data_row);
        push @csv_row_serial_summary, sprintf("=IFERROR(SUMPRODUCT((\$%s\$2:\$%s\$%d=A%d)*IF(ISNUMBER(SEARCH(\"PowerHA Standby\",\$%s\$2:\$%s\$%d)),\$%s\$2:\$%s\$%d,\$%s\$2:\$%s\$%d)),\"\")", $col_serial_letter, $col_serial_letter, $last_data_row, $current_formula_row, $col_system_type_letter, $col_system_type_letter, $last_data_row, $col_current_ent_letter, $col_current_ent_letter, $last_data_row, $col_nfit_ent_user_formula_letter, $col_nfit_ent_user_formula_letter, $last_data_row);
        push @csv_row_serial_summary, sprintf("=IFERROR(SUMPRODUCT((\$%s\$2:\$%s\$%d=A%d)*(\$%s\$2:\$%s\$%d<>\"VIO Server\")*IF(ISNUMBER(SEARCH(\"PowerHA Standby\",\$%s\$2:\$%s\$%d)),\$%s\$2:\$%s\$%d,\$%s\$2:\$%s\$%d)),\"\")", $col_serial_letter, $col_serial_letter, $last_data_row, $current_formula_row, $col_system_type_letter, $col_system_type_letter, $last_data_row, $col_system_type_letter, $col_system_type_letter, $last_data_row, $col_current_ent_letter, $col_current_ent_letter, $last_data_row, $col_nfit_ent_user_formula_letter, $col_nfit_ent_user_formula_letter, $last_data_row);

        if ($i == 0) {
            my $num_main_formulas = scalar(@csv_row_serial_summary);
            my $padding_needed = (16 + $col_offset) - $num_main_formulas;
            push @csv_row_serial_summary, ($empty) x $padding_needed if $padding_needed > 0;

            my $col_R_header_cell = get_excel_col_name(18 + $col_offset) . $row_ent_col_headers;
            my $col_S_header_cell = get_excel_col_name(19 + $col_offset) . $row_ent_col_headers;
            my $col_U_header_cell = get_excel_col_name(21 + $col_offset) . $row_ent_col_headers;

            push @csv_row_serial_summary, "Largest Frame ENT (Excl. VIO)";
            push @csv_row_serial_summary, sprintf("=IFERROR(SUMIFS(\$%s\$2:\$%s\$%d, \$%s\$2:\$%s\$%d, %s, \$%s\$2:\$%s\$%d, \"<>VIO Server\"),\"\")", $col_current_ent_letter, $col_current_ent_letter, $last_data_row, $col_serial_letter, $col_serial_letter, $last_data_row, $col_R_header_cell, $col_system_type_letter, $col_system_type_letter, $last_data_row);
            push @csv_row_serial_summary, sprintf("=IFERROR(SUMIFS(\$%s\$2:\$%s\$%d, \$%s\$2:\$%s\$%d, %s, \$%s\$2:\$%s\$%d, \"<>VIO Server\"),\"\")", $col_nfit_ent_user_formula_letter, $col_nfit_ent_user_formula_letter, $last_data_row, $col_serial_letter, $col_serial_letter, $last_data_row, $col_S_header_cell, $col_system_type_letter, $col_system_type_letter, $last_data_row);
            push @csv_row_serial_summary, "Largest PowerHA ENT";
            push @csv_row_serial_summary, sprintf("=IFERROR(SUMIFS(\$%s\$2:\$%s\$%d, \$%s\$2:\$%s\$%d, %s, \$%s\$2:\$%s\$%d, \"*PowerHA Primary*\"),\"\")", $col_current_ent_letter, $col_current_ent_letter, $last_data_row, $col_serial_letter, $col_serial_letter, $last_data_row, $col_U_header_cell, $col_system_type_letter, $col_system_type_letter, $last_data_row);
            push @csv_row_serial_summary, sprintf("=IFERROR(SUMIFS(\$%s\$2:\$%s\$%d, \$%s\$2:\$%s\$%d, %s, \$%s\$2:\$%s\$%d, \"*PowerHA Primary*\"),\"\")", $col_nfit_ent_user_formula_letter, $col_nfit_ent_user_formula_letter, $last_data_row, $col_serial_letter, $col_serial_letter, $last_data_row, $col_U_header_cell, $col_system_type_letter, $col_system_type_letter, $last_data_row);
        }
        print $fh join(",", map { quote_csv($_) } @csv_row_serial_summary) . "\n";
    }

    # --- Rows after per-serial summary (Frame Evac, etc.) ---
    my $actual_row_unique_serials_end = $row_unique_serials_start + $loop_count_for_serials - 1;
    my $row_after_serials_block = $row_unique_serials_start + $loop_count_for_serials;

    my $cell_largest_frame_asis_val = get_excel_col_name(18 + $col_offset) . $row_ent_col_headers;
    my $cell_largest_frame_nfit_val = get_excel_col_name(19 + $col_offset) . $row_ent_col_headers;
    my $cell_largest_pha_asis_val   = get_excel_col_name(21 + $col_offset) . $row_ent_col_headers;
    my $cell_largest_pha_nfit_val   = get_excel_col_name(22 + $col_offset) . $row_ent_col_headers;

    # Row: Frame Evac - Max Required
    @csv_row = ($empty) x (22 + $col_offset);
    $csv_row[16 + $col_offset] = "Frame Evac - Max Required";
    $csv_row[17 + $col_offset] = sprintf("=MAX(%s,%s)", $cell_largest_frame_asis_val, $cell_largest_pha_asis_val);
    $csv_row[21 + $col_offset] = sprintf("=MAX(%s,%s)", $cell_largest_frame_nfit_val, $cell_largest_pha_nfit_val);
    print $fh join(",", map { quote_csv($_) } @csv_row) . "\n";

    # Row: Frame Evac - Required per frame
    my $current_print_row_for_evac_max = $row_after_serials_block;
    my $cell_max_req_as_is_val = get_excel_col_name(18 + $col_offset) . $current_print_row_for_evac_max;
    my $cell_max_req_nfit_val  = get_excel_col_name(22 + $col_offset) . $current_print_row_for_evac_max;

    @csv_row = ($empty) x (22 + $col_offset);
    $csv_row[16 + $col_offset] = "Frame Evac - Required per frame";
    $csv_row[17 + $col_offset] = sprintf("=IFERROR(%s/COUNTA(UNIQUE(\$%s\$2:\$%s\$%d)),\"N/A\")", $cell_max_req_as_is_val, $col_serial_letter, $col_serial_letter, $last_data_row);
    $csv_row[21 + $col_offset] = sprintf("=IFERROR(%s/COUNTA(UNIQUE(\$%s\$2:\$%s\$%d)),\"N/A\")", $cell_max_req_nfit_val, $col_serial_letter, $col_serial_letter, $last_data_row);
    print $fh join(",", map { quote_csv($_) } @csv_row) . "\n";

    # --- Total Row for Per-Serial Summary ---
    @csv_row = ();
    push @csv_row, "Total";
    for my $col_idx (2..10) {
        my $col_letter = get_excel_col_name($col_idx);
        if ($count_of_unique_serials > 0) {
            push @csv_row, sprintf("=SUM(%s%d:%s%d)", $col_letter, $row_unique_serials_start, $col_letter, $actual_row_unique_serials_end);
        } else {
            push @csv_row, "0";
        }
    }
    my $current_cols = scalar(@csv_row);
    push @csv_row, ($empty) x ((22 + $col_offset) - $current_cols) if (22 + $col_offset) > $current_cols;
    print $fh join(",", map { quote_csv($_) } @csv_row) . "\n";
}


# ==============================================================================
# Subroutine to format nfit flags for display
# ==============================================================================
sub format_nfit_flags_for_display {
    my ($profile_name, $profile_specific_flags, $runq_perc_flags, $runq_behavior) = @_;
    my @output_lines;

    my $temp_profile_flags = $profile_specific_flags; # Work on a copy

    my @core_fit_parts;
    my @decay_parts;
    my @growth_parts;
    my @other_parts; # For flags not specifically categorized

    # Helper sub-subroutine to extract and remove a flag pattern
    # Arguments:
    #   1. Regex for the flag and its potential value (e.g., qr/-p\s+[^\s]+/)
    #   2. Array reference to store the extracted flag string
    #   3. Scalar reference to the string of flags to be processed (will be modified)
    sub _extract_flag {
        my ($flag_regex, $parts_array_ref, $flags_string_ref) = @_;
        if ($$flags_string_ref =~ s/($flag_regex)//) {
            my $extracted_part = $1;
            $extracted_part =~ s/^\s+|\s+$//g; # Trim whitespace
            push @$parts_array_ref, $extracted_part if $extracted_part;
        }
    }

    # --- Core Fit Parameters ---
    _extract_flag(qr/--percentile\s+(?:\"[^\"]*\"|\'[^\']*\'|[^\s\"]+)|-p\s+(?:\"[^\"]*\"|\'[^\']*\'|[^\s\"]+)/, \@core_fit_parts, \$temp_profile_flags);
    _extract_flag(qr/--process-window-size\s+(?:\"[^\"]*\"|\'[^\']*\'|[^\s\"]+)|-w\s+(?:\"[^\"]*\"|\'[^\']*\'|[^\s\"]+)/, \@core_fit_parts, \$temp_profile_flags);
    _extract_flag(qr/--filter-above-perc\s+(?:\"[^\"]*\"|\'[^\']*\'|[^\s\"]+)/, \@core_fit_parts, \$temp_profile_flags);
    # Add more related flags here if needed, e.g.:
    # _extract_flag(qr/--filter-metric\s+[^\s]+/, \@core_fit_parts, \$temp_profile_flags);
    # _extract_flag(qr/--filter-limit\s+[^\s]+/, \@core_fit_parts, \$temp_profile_flags);

    # --- Decay Options ---
    _extract_flag(qr/--decay\s+(?:\"[^\"]*\"|\'[^\']*\'|[^\s\"]+)/, \@decay_parts, \$temp_profile_flags);
    _extract_flag(qr/--runq-decay\s+(?:\"[^\"]*\"|\'[^\']*\'|[^\s\"]+)/, \@decay_parts, \$temp_profile_flags);
    # Add more related flags here, e.g.:
    # _extract_flag(qr/--ema-period\s+[^\s]+/, \@decay_parts, \$temp_profile_flags);
    # _extract_flag(qr/--sma-period\s+[^\s]+/, \@decay_parts, \$temp_profile_flags);

    # --- Growth Prediction ---
    _extract_flag(qr/--enable-growth-prediction\b/, \@growth_parts, \$temp_profile_flags); # \b for word boundary
    _extract_flag(qr/--max-growth-inflation-percent\s+(?:\"[^\"]*\"|\'[^\']*\'|[^\s\"]+)/, \@growth_parts, \$temp_profile_flags);
    _extract_flag(qr/--growth-period-days\s+(?:\"[^\"]*\"|\'[^\']*\'|[^\s\"]+)/, \@growth_parts, \$temp_profile_flags);

    if (@core_fit_parts) {
        push @output_lines, "  Core         : " . join(" ", @core_fit_parts);
    }
    if (@decay_parts) {
        push @output_lines, "  Decay        : " . join(" ", @decay_parts);
    }
    if (@growth_parts) {
        push @output_lines, "  Growth       : " . join(" ", @growth_parts);
    }

    # --- RunQ Percentiles (these are from the already processed $runq_perc_flags) ---
    my $trimmed_runq_perc_flags = $runq_perc_flags;
    $trimmed_runq_perc_flags =~ s/^\s+|\s+$//g; # Trim
    if ($trimmed_runq_perc_flags) {
        push @output_lines, "  RunQ Percs   : " . $trimmed_runq_perc_flags;
    }

    # --- Other/Remaining Profile Flags ---
    # Any flags left in $temp_profile_flags are considered "Other"
    $temp_profile_flags =~ s/^\s+|\s+$//g; # Trim remaining
    my @remaining_flags = split(/\s+/, $temp_profile_flags); # Split remaining by space
    @remaining_flags = grep { $_ ne "" } @remaining_flags; # Filter out empty strings
    if (@remaining_flags) {
        # Reconstruct to handle flags that might have been split from their values if not perfectly matched above
        # This simplistic split might not be perfect if un-extracted flags had quoted spaces.
        # For robust handling of complex "Other" flags, more sophisticated parsing of $temp_profile_flags would be needed.
        # However, ideally, most common flags are explicitly extracted above.
        push @output_lines, "  Other Args   : " . join(" ", @remaining_flags);
    }

    # --- RunQ Behavior ---
    # This comes directly from the profile config, not from the flag strings
    if (defined $runq_behavior && $runq_behavior ne 'default') {
        push @output_lines, "  RunQBehavior : $runq_behavior";
    }

    return join("\n", @output_lines);
}

# --- parse_profile_name_for_log ---
# Parses common nfit-profile profile name patterns for a more descriptive log output.
# Adheres to Allman style and includes comments.
sub parse_profile_name_for_log
{
    my ($profile_name_str) = @_;

    my $description = $profile_name_str; # Default to original name if no pattern matches
    my @parts;

    # Regex to capture common patterns like O3-95W15, P-99W1, G2-BatchSpecial etc.
    # This regex looks for: TypeChar [TierNum] - Percentile W WindowNum [SuffixLetters]
    if ($profile_name_str =~ /^([OBGP])(?:-?(\d+))?-?(\d{2,3})(?:W(\d+))?([A-Z]*)?$/i)
    {
        my $type_char   = uc($1);
        my $tier_num    = $2; # Optional
        my $perc_val    = $3;
        my $win_val     = $4; # Optional
        my $suffix_char = $5; # Optional

        my $type_desc = "Unknown Type"; # Default for safety
        if ($type_char eq 'O')
        {
            $type_desc = "Online";
        }
        elsif ($type_char eq 'B')
        {
            $type_desc = "Batch";
        }
        elsif ($type_char eq 'G')
        {
            $type_desc = "General";
        }
        elsif ($type_char eq 'P')
        {
            $type_desc = "Peak";
        }

        if (defined $tier_num && $tier_num ne "")
        {
            $type_desc .= " (Tier $tier_num)";
        }
        push @parts, $type_desc;

        if (defined $perc_val)
        {
            push @parts, "$perc_val" . "th Percentile";
        }
        if (defined $win_val)
        {
            push @parts, "$win_val-minute Window";
        }
        if (defined $suffix_char && $suffix_char ne "")
        {
            push @parts, "Variant '$suffix_char'";
        }

        $description = join(", ", @parts);
    }
    elsif (lc($profile_name_str) eq "peak") # Handle specific "Peak" profile name
    {
        $description = "Absolute Peak Value";
    }
    # Add more 'elsif' blocks here for other distinct profile naming conventions if needed.

    return "$profile_name_str ($description)";
}

# ==============================================================================
# SUBROUTINE: log_peak_profile_rationale
# PURPOSE:    Logs a simplified, explicit rationale for the P-99W1 profile,
#             clarifying that it is an unmodified, pure measurement.
# ==============================================================================
sub log_peak_profile_rationale {
    my ($fh, $vm_map_ref, $profile_ref, $base_physc) = @_;
    return unless $fh;

    my $vm_name = $vm_map_ref->{Configuration}{vm_name};
    my $profile_desc = parse_profile_name_for_log($profile_ref->{name});

    print {$fh} "\n======================================================================\n";
    printf {$fh} "VM Name                                      : %s\n", $vm_name;
    printf {$fh} "Profile Processed                            : %s\n", $profile_desc;
    print {$fh} "----------------------------------------------------------------------\n";
    print {$fh} "Measurement Type: Pure Peak (Statistical Ceiling)\n\n";
    print {$fh} "  - The P-99W1 profile is a special case used for peak analysis.\n";
    print {$fh} "  - It represents the smoothed (1-min SMA) 99.75th percentile of the\n";
    print {$fh} "    unfiltered historical data.\n";
    print {$fh} "  - NO growth, RunQ, or forecasting modifiers are applied to this value.\n\n";
    printf {$fh} "  Final Unmodified Value                       : %s cores\n", ($base_physc // 'N/A');
    print {$fh} "======================================================================\n\n";
}

# --- log_profile_rationale ---
# Logs the detailed rationale for how a profile's PhysC value was adjusted.
# Incorporates a summary-first approach and clearer narrative for planners.
# Adheres to Allman style and includes comments.
sub log_profile_rationale
{
    # This function is now refactored to read from the assimilation map.
    my ($fh, $vm_map_ref, $profile_ref, $base_physc_for_log, $final_csv_value_for_profile, $debug_info_ref, $raw_nfit_states_aref, $adaptive_runq_saturation_thresh) = @_;


    # Declare and initialise variables for STD rationale block
    my $eff_p_base_numeric = (defined $base_physc_for_log && looks_like_number($base_physc_for_log)) ? ($base_physc_for_log + 0) : undef;

    # Ensure script doesn't die if log handle isn't valid
    return unless $fh;

    # --- Unpack all required values from the map and arguments ---
    my $vm_name = $vm_map_ref->{Configuration}{vm_name};
    my $profile_obj = $profile_ref;
    my $base_physc_for_profile = $base_physc_for_log;
    my $calc_debug_info_ref = $debug_info_ref;

    # Source from map's Configuration block
    my $cfg = $vm_map_ref->{Configuration};
    my $smt_val = $cfg->{smt};
    my $entitlement_val = $cfg->{entitlement};
    my $lpar_max_cpu_cfg_val_from_config = $cfg->{max_cpu};
    my $curr_ent_numeric = (defined $entitlement_val && looks_like_number($entitlement_val)) ? ($entitlement_val + 0) : undef;

    # Source from map's RunQMetrics block
    my $runq = $vm_map_ref->{RunQMetrics};
    my $runq_metrics_source_profile_name_for_this_calc = $runq->{SourceProfile};
    my $normP50_for_this_calc = $runq->{'NormRunQ_P50'};
    my $normP90_for_this_calc = $runq->{'NormRunQ_P90'};
    my $abs_runq_value_used_for_calc = $calc_debug_info_ref->{AbsRunQValueUsedForCalc};

    # Source other values
    my $profile_rq_behavior = $profile_ref->{runq_behavior};


    # --- Determine if a seasonal run occurred to select the correct logging path ---
    my $event_config = defined($apply_seasonality_event) ? ($seasonality_config->{$apply_seasonality_event} // {}) : {};

    # --- Get profile name and metric key from the profile object ---
	my $model_type = $event_config->{model} // '';
	my $profile_being_adjusted = $profile_obj->{name}; # Get name from the object
	my $profile_desc = parse_profile_name_for_log($profile_being_adjusted);
	my $profile_physc_perc_val_num;
	if (defined $profile_obj->{flags} && $profile_obj->{flags} =~ /(?:-p|--percentile)\s+([0-9.]+)/) {
		$profile_physc_perc_val_num = $1 + 0;
	}
	my $p_metric_key = "P" . clean_perc_label($profile_physc_perc_val_num // $DEFAULT_PERCENTILE);

    # --- PATH A: Multiplicative Seasonal Model has its own log format ---
    if ($model_type eq 'multiplicative_seasonal' && exists $seasonal_debug_info{$vm_name}{$profile_being_adjusted}) {
        my $s_data = $seasonal_debug_info{$vm_name}{$profile_being_adjusted};
        print {$fh} "\n======================================================================\n";
        printf {$fh} "VM Name                                : %s\n", $vm_name;
        printf {$fh} "Profile Processed                      : %s\n", $profile_desc;
        print {$fh} "----------------------------------------------------------------------\n";
        print {$fh} "  CPU Forecasting Model: Multiplicative Seasonal Forecast\n\n";
        printf {$fh} "  - Current Baseline Value      : %.4f cores\n", $s_data->{baseline};
        printf {$fh} "  - Historical Multiplier       : %.4f\n", $s_data->{multiplier};
        printf {$fh} "  - Volatility Buffer           : %.4f\n", $s_data->{volatility};
        print {$fh} "  - Calculation                 : Baseline * Multiplier * Volatility\n";
        printf {$fh} "  Final Forecasted Value       : %.4f cores\n", $s_data->{forecast};
        print {$fh} "======================================================================\n\n";
        return;
    }

    # --- PATH B: Standard Rationale (now also used by recency_decay model) ---
    my $na = 'N/A'; # Consistent N/A string for display
    my $abs_runq_key_reported_in_log = $calc_debug_info_ref->{AbsRunQKeyUsed} // 'AbsRunQ_P90 (default)';

    # --- Top Summary Block ---
    my $profile_description_log = parse_profile_name_for_log($profile_being_adjusted);

    # Use the unrounded final value from debug_info for precise change calculation
    my $final_recommendation_unrounded_str = $calc_debug_info_ref->{'FinalAdjustedPhysC'} // $na;
    my $final_recommendation_unrounded_num = ($final_recommendation_unrounded_str ne $na && $final_recommendation_unrounded_str =~ /^-?[0-9.]+$/)
    ? ($final_recommendation_unrounded_str + 0) : undef;

    my $base_physc_val_num = looks_like_number($base_physc_for_profile) ? $base_physc_for_profile + 0 : undef;

    my $net_change_str = $na;
    if (defined $base_physc_val_num && defined $final_recommendation_unrounded_num) {
        my $delta = $final_recommendation_unrounded_num - $base_physc_val_num;
        my $perc_change_str = (abs($base_physc_val_num) > $FLOAT_EPSILON) ? sprintf(" (Change: %s%.1f%%)", ($delta >=0 ? "+" : ""), ($delta / $base_physc_val_num) * 100) : "";
        $net_change_str = sprintf("%s%.4f cores%s", ($delta >=0 ? "+" : ""), abs($delta), $perc_change_str);
    }

#    print {$fh} "\n======================================================================\n";
#    printf {$fh} "VM Name                                      : %s\n", $vm_name;
#    printf {$fh} "Profile Processed                            : %s\n", $profile_description_log;
#    print {$fh} "----------------------------------------------------------------------\n";
    if ($model_type eq 'recency_decay') {
		# --- PATH B: Log the Recency Decay Rationale (Standardised) ---
		my $profile_desc = parse_profile_name_for_log($profile_being_adjusted);

		print {$fh} "======================================================================\n";
		printf {$fh} "VM Name                             : %s\n", $vm_name;
		printf {$fh} "Profile Processed                   : %s\n", $profile_desc;
		print {$fh} "----------------------------------------------------------------------\n";
		print {$fh} "  CPU Forecasting Model: Recency-Anchored Decay (Seasonal: '$apply_seasonality_event')\n\n";

		# EXEMPTION: Special Handling for P-99W1
		if ($profile_being_adjusted eq 'P-99W1') {
			print {$fh} "  - The P-99W1 profile is a special case used for peak analysis.\n";
			print {$fh} "  - It represents the smoothed 99.75th percentile of the unfiltered\n";
			print {$fh} "    historical data (weighted by recency).\n";
			print {$fh} "  - NO growth, RunQ, or forecasting modifiers are applied to this value.\n\n";

			# Print Final Value
			printf {$fh} "  - Final nfit Value (Unrounded)    : %s cores\n", ($vm_map_ref->{CoreResults}{ProfileValues}{$profile_being_adjusted} // "N/A");
		}
		else {
			# 1. Retrieve Standardised Growth Info
			#    We rely on the data stored in the main loop: $vm_map_ref->{Growth}{StandardAdjustment}
			my $growth_info   = $vm_map_ref->{Growth}{StandardAdjustment} || {};
			my $growth_adj    = $growth_info->{Value} // 0;
			my $growth_source = $growth_info->{Source}; # Retrieve the source name (e.g., G3-95W15)

			# 2. Retrieve Base Value (Pre-Growth) from storage
			my $base_val_unrounded = $vm_map_ref->{Growth}{base_values}{$profile_being_adjusted};

			# Fallback for Base Value if not explicitly stored
			if (!defined $base_val_unrounded) {
				 $base_val_unrounded = (looks_like_number($base_physc_for_profile))
					? $base_physc_for_profile - $growth_adj
					: "N/A";
			}

            # 3. Retrieve RunQ Modifier (Potential - ungated queue physics)
			#    For recency_decay, we use RunQ_Potential which represents the raw
			#    queue-implied demand without change management gates.
			my $runq_mod = $vm_map_ref->{CSVModifiers}{RunQ_Potential} // 0;
			my $runq_label = "RunQ Modifier (Potential)";

			# --- Output ---
			print {$fh} "  - This model solves the 'Start-of-Month' problem and includes nfit's\n";
			print {$fh} "    standard growth prediction.\n\n";
			printf {$fh} "  - Analysis Reference Date         : %s\n", ($nfit_analysis_reference_date_str // "N/A");

			printf {$fh} "  - Base Value (Recency-Anchored)   : %.4f cores\n", $base_val_unrounded if (looks_like_number($base_val_unrounded));

			# Print Growth (Standardised)
			printf {$fh} "  - Growth Adjustment               : %+.4f cores (Standardised Source: %s)\n", $growth_adj, ($growth_source // "N/A");

			# Print RunQ (Signed - Positive or Negative)
			if ($runq_mod != 0) {
				 printf {$fh} "  - %-32s: %+.4f cores\n", $runq_label, $runq_mod;
			}

			print {$fh} "  --------------------------------------------------------------------\n";
			printf {$fh} "  - Final nfit Value (Unrounded)    : %s cores\n", ($vm_map_ref->{CoreResults}{ProfileValues}{$profile_being_adjusted} // "N/A");
		}

		print {$fh} "======================================================================\n\n";
		return;
	}

    # --- Section A: nfit Raw State Analysis ---
    # CRITICAL: Track pre-growth and post-growth values separately for audit transparency
    # base_physc_for_log now contains the growth-inclusive value (FinalValue from nfit).
    # Retrieve the true pre-growth BaseValue from the Growth.base_values storage.
    my $pre_growth_base_physc = $vm_map_ref->{Growth}{base_values}{$profile_being_adjusted};
    # Fallback: If base_values not available, compute from current value minus growth adj
    if (!defined $pre_growth_base_physc || !looks_like_number($pre_growth_base_physc)) {
        my $nfit_growth_adj = $vm_map_ref->{Growth}{adjustments}{$profile_being_adjusted} // 0;
        $pre_growth_base_physc = (looks_like_number($base_physc_for_log) ? $base_physc_for_log : 0) - $nfit_growth_adj;
    }
    my $nfit_growth_adj = $vm_map_ref->{Growth}{adjustments}{$profile_being_adjusted} // 0;
    my $post_growth_base_physc = (looks_like_number($pre_growth_base_physc) ? $pre_growth_base_physc : 0) + $nfit_growth_adj;

    if (ref($raw_nfit_states_aref) eq 'ARRAY' && @$raw_nfit_states_aref) {
        print {$fh} "Section A: nfit Raw State Analysis & Base Value Calculation\n";
        printf {$fh} "  - nfit reported the following configuration states for this profile:\n";

        my $first_result = $raw_nfit_states_aref->[0];
        my $is_aggregated = ($first_result->{analysisType} || '') =~ /aggregated/;

        if ($is_aggregated) {
            # Use the already-parsed configuration and base value from the assimilation map.
            my $config = $vm_map_ref->{Configuration} || {};
            my $metric_val = $pre_growth_base_physc; # This is the correct pre-growth base value.

            printf {$fh} "    - %-39s: Ent=%.2f, MaxCPU=%.2f, SMT=%d, BaseValue=%.4f\n",
                "Aggregated Result",
                $config->{entitlement} // 0,
                $config->{max_cpu} // 0,
                $config->{smt} // 0,
                (defined($metric_val) && looks_like_number($metric_val)) ? $metric_val : 0;
			my $state_count = $first_result->{state}{stateCount} // 1;
            printf {$fh} "    - Aggregation Method                     : Time-weighted decay model applied across %d configuration states.\n", $state_count;

       } else {
            # Standard logging for non-aggregated results
            foreach my $state_res (@$raw_nfit_states_aref) {
                my $state_id_str = $state_res->{state}{id} // 'N/A';
                my $config = $state_res->{metadata}{configuration} || {};
                my $metric_val = $state_res->{metrics}{physc}{$p_metric_key};

                printf {$fh} "    - %-26s: Ent=%.2f, MaxCPU=%.2f, SMT=%d, %s=%s\n",
                    $state_id_str,
                    $config->{entitlement} // 0,
                    $config->{maxCpu} // 0,
                    $config->{smt} // 0,
                    $p_metric_key,
                    defined($metric_val) ? sprintf("%.4f", $metric_val) : $na;
            }
            if (@$raw_nfit_states_aref > 1) {
                printf {$fh} "    - Aggregation Method                      : Simple Average of %d states was used.\n", scalar(@$raw_nfit_states_aref);
            } else {
                printf {$fh} "    - Aggregation Method                      : Direct value from a single state was used.\n";
            }
        }
    }

    printf {$fh} "Initial Base PhysC for Profile               : %s cores (Aggregated value from nfit)\n",
        (defined($pre_growth_base_physc) && looks_like_number($pre_growth_base_physc))
            ? sprintf("%.4f", $pre_growth_base_physc) : $na;
    printf {$fh} "Final nfit-profile Recommendation            : %s cores (Unrounded: %s)\n",
        ($final_csv_value_for_profile // $na), $final_recommendation_unrounded_str;
    printf {$fh} "Net Adjustment by nfit-profile               : %s\n", $net_change_str;
    print {$fh} "======================================================================\n";

    # --- Section A.1: Comprehensive Growth Adjustment Rationale ---
    # Retrieve growth rationale from the assimilation map (harvested during map building)
    my $gd = $vm_map_ref->{GrowthRationaleByProfile}{$profile_being_adjusted} || {};

    # Display growth section if adjustment is non-zero OR if rationale exists
    if (abs($nfit_growth_adj) > $FLOAT_EPSILON || (ref($gd) eq 'HASH' && scalar keys %$gd > 0)) {
        print {$fh} "Section A.1: nfit GrowthAdj (Theil-Sen Robust Trend)\n";
        printf {$fh} "  - nfit GrowthAdj Applied                   : %s%.4f cores\n",
            ($nfit_growth_adj >= 0 ? "+" : ""), $nfit_growth_adj;

        # Helper for consistent boolean/skipped formatting
        my $format_check_result = sub {
            my $val = shift;
            return "Skipped" if (!defined $val || $val eq 'Skipped');
            return $val ? "Passed" : "Failed";
        };

        # --- Print Adaptive Projection Metadata (if available) ---
        if (defined $gd->{projection_days}) {
            print {$fh} "  - Rationale for GrowthAdj (from nfit):\n";
            printf {$fh} "    - Projection Horizon    : %d days (%s)\n",
                $gd->{projection_days}, $gd->{projection_days_source} // 'unknown';
            printf {$fh} "    - Analysis Window       : %d days\n",
                $gd->{analysis_days} if defined $gd->{analysis_days};
            printf {$fh} "    - Sampling Interval     : %d minutes (average)\n",
                $gd->{avg_sampling_interval_mins} if defined $gd->{avg_sampling_interval_mins};

            # Calculate and display extrapolation ratio for adaptive projections
            if (($gd->{projection_days_source} // '') eq 'adaptive' &&
                defined $gd->{analysis_days} && $gd->{analysis_days} > 0) {
                my $ratio = sprintf("%.2f", $gd->{projection_days} / $gd->{analysis_days});
                printf {$fh} "    - Extrapolation Ratio   : %s:1 (projection:analysis)\n", $ratio;
            }
        }

        # --- Detailed Rationale for Robust Trend Analysis (if growth was calculated) ---
        print {$fh} "    - Rationale for GrowthAdj (Theil-Sen):\n";

        # Check if we have Theil-Sen specific fields (indicates v6.25+ cache)
        my $has_theil_sen_data = (defined $gd->{sen_slope} || defined $gd->{mk_variant});

        if ($has_theil_sen_data) {
            # NEW FORMAT: Comprehensive Theil-Sen rationale
            printf {$fh} "    1. Method         : %s (trend analysis on %d %s samples)\n",
                $gd->{method_used} // 'Theil-Sen',
                $gd->{sample_points} // ($gd->{num_hist_periods} // 0),
                $gd->{aggregation_basis} // 'daily';

            printf {$fh} "    2. Volatility Check : CV = %.4f (Limit: < %.2f) [Result: %s]\n",
                $gd->{volatility_cv} // ($gd->{stats_cv} // 0),
                $GROWTH_MAX_CV_THRESHOLD,
                $format_check_result->($gd->{volatility_check_passed} // $gd->{cv_check_passed});

            printf {$fh} "    3. Trend Analysis   : Theil-Sen Estimator (Sen's Slope)\n";
            printf {$fh} "       - Slope          : %+.6f cores/day\n", $gd->{sen_slope} // 0;
            printf {$fh} "       - Trend          : %s\n", $gd->{sen_trend} // 'none';

            printf {$fh} "    4. Significance     : Mann-Kendall Test (Variant: %s)\n",
                $gd->{mk_variant} // 'standard';
            printf {$fh} "       - p-value        : %.4f (Significant if < 0.05)\n",
                $gd->{sen_p_value} // 1.0;
            printf {$fh} "       - Kendall's Tau  : %.4f\n", $gd->{sen_tau} // 0;

            # Print result or skip reason
            if ($gd->{skip_reason}) {
                printf {$fh} "    5. Result         : Growth skipped. Reason: %s\n",
                    $gd->{skip_reason};
            } elsif (defined $gd->{growth_adj}) {
                my $capping_msg = (defined $gd->{was_capped} && $gd->{was_capped} eq '1')
                    ? sprintf(" (CAPPED from %.4f by %.0f%% limit)",
                              $gd->{original_growth_adj} // $gd->{growth_adj},
                              $gd->{cap_percent_applied} // $DEFAULT_MAX_GROWTH_INFLATION_PERCENT)
                    : "";
                printf {$fh} "    5. Result         : Trend is significant. GrowthAdj (%.4f) applied.%s\n",
                    $gd->{growth_adj}, $capping_msg;
            }

            # Print OLS comparison
            printf {$fh} "    6. OLS Comparison : Slope = %+.6f, R-squared = %.4f\n",
                $gd->{ols_slope} // 0, $gd->{ols_r2} // 0;

            # --- Hamed-Rao Correction Diagnostics (if present) ---
            printf {$fh} "    7. Mann-Kendall Test Details\n";
            if (defined $gd->{mk_variant} && $gd->{mk_variant} eq 'hamed-rao') {
                print {$fh} "    - Hamed-Rao Correction Diagnostics:\n";
                printf {$fh} "      - Adjustment Factor : %.4f\n",
                    $gd->{mk_adjustment_factor} // 1;
                printf {$fh} "      - Effective N       : %.0f (Reduced from %d due to autocorrelation)\n",
                    $gd->{mk_effective_n} // ($gd->{sample_points} // 0),
                    $gd->{sample_points} // 0;
                printf {$fh} "      - Lag-1 Autocorr    : %.4f\n",
                    $gd->{lag1_autocorr} // 0;
            }

            # Display MK variant and routing information
                if (my $mk_variant = $gd->{mk_variant}) {
                    if ($mk_variant eq 'standard_mk_trend_dominant') {
                        print {$fh} "    - Test Variant       : Standard MK (strong trend detected, |τ| ≥ 0.4)\n";
                        if (my $reason = $gd->{hamed_rao_skip_reason}) {
                            print {$fh} "    - Note               : $reason\n";
                        }
                    } elsif ($mk_variant eq 'standard_mk_via_hamed_rao_fallback') {
                        print {$fh} "    - Test Variant       : Standard MK (Hamed-Rao fallback)\n";
                        if (my $reason = $gd->{hamed_rao_fallback_reason}) {
                            print {$fh} "    - Fallback Reason    : $reason\n";
                        }
                        if (my $af = $gd->{hamed_rao_adjustment_factor}) {
                            print {$fh} "    - Attempted Adj Factor: $af\n";
                        }
                    } elsif ($mk_variant eq 'hamed-rao') {
                        # Already displayed above in existing section
                    } elsif ($mk_variant eq 'standard') {
                        print {$fh} "    - Test Variant       : Standard Mann-Kendall\n";
                    }
                }

                # Display autocorrelation max lag when available
                if (my $K = $gd->{hamed_rao_autocorr_max_lag}) {
                    print {$fh} "    - Max Lag (K)        : $K\n";
                }

        } elsif (defined $gd->{slope_check_passed} && $gd->{slope_check_passed} eq '1') {
            # LEGACY FORMAT: Old OLS-based rationale (pre-v6.25 cache)
            my $proj_val_str = looks_like_number($gd->{projected_val})
                ? sprintf("%.4f", $gd->{projected_val}) : "N/A";
            printf {$fh} "        4. Projection                    : Trend projected to %s cores over %d days.\n",
                $proj_val_str, $DEFAULT_GROWTH_PROJECTION_DAYS;

            my $inflation_str = looks_like_number($gd->{inflation_perc})
                ? sprintf("%.2f", $gd->{inflation_perc}) : "N/A";
            my $capping_msg = (defined $gd->{was_capped} && $gd->{was_capped} eq '1')
                ? "CAPPED" : "not capped";
            printf {$fh} "        5. Inflation                         : Calculated inflation is %s%%. This was %s to the max of %d%%.\n",
                $inflation_str, $capping_msg, $DEFAULT_MAX_GROWTH_INFLATION_PERCENT;
        } else {
            # No growth calculated or insufficient data
            print {$fh} "    - Rationale details not available (pre-v6.25 cache or no growth calculated).\n";
        }

        # Subtotal after growth adjustment
        printf {$fh} "  - Subtotal (Growth-Adjusted Base)            : %.4f cores\n",
            $post_growth_base_physc;
        print {$fh} "======================================================================\n";
    }
    print {$fh} "\n";

    # --- Section B: Key Inputs & Configuration ---

    print {$fh} "Section B: Key Inputs & Configuration for Modifier Logic\n";
    printf {$fh} "  1. Key RunQ Metrics (source: %s, state: %s)\n", $runq_metrics_source_profile_name_for_this_calc, "Most Recent";
    printf {$fh} "     - AbsRunQ for Upsizing (%s)    : %s threads\n", $abs_runq_key_reported_in_log, ($abs_runq_value_used_for_calc // $na);
    printf {$fh} "     - NormRunQ P25                          : %s\n", (looks_like_number($calc_debug_info_ref->{'NormRunQ_P25_Val'}) ? sprintf("%.4f", $calc_debug_info_ref->{'NormRunQ_P25_Val'}) : $na);
    printf {$fh} "     - NormRunQ P50                          : %.4f\n", ($normP50_for_this_calc // ($calc_debug_info_ref->{'NormRunQ_P50_Val'} // $na) );
    printf {$fh} "     - NormRunQ P75                          : %.4f\n", ($calc_debug_info_ref->{'NormRunQ_P75_Val'} // $na);
    printf {$fh} "     - NormRunQ P90                          : %.4f\n", ($normP90_for_this_calc // $na);

    my $iqrc_val_for_log_A_sec = looks_like_number($calc_debug_info_ref->{'NormRunQ_IQRC_Val'}) ? sprintf("%.2f", $calc_debug_info_ref->{'NormRunQ_IQRC_Val'}) : $na;
    printf {$fh} "     - NormRunQ IQRC (Volatility)            : %s", $iqrc_val_for_log_A_sec;
    my $iqrc_interpretation_log_A_sec = $na;
    if ($iqrc_val_for_log_A_sec ne $na && $iqrc_val_for_log_A_sec =~ /^-?[0-9.]+$/)
    {
        my $iqrc_num_A_sec = $iqrc_val_for_log_A_sec + 0;
        if    ($iqrc_num_A_sec < 0.3)  { $iqrc_interpretation_log_A_sec = "Very steady"; }
        elsif ($iqrc_num_A_sec <= 0.6) { $iqrc_interpretation_log_A_sec = "Moderate variability"; }
        elsif ($iqrc_num_A_sec <= 1.0) { $iqrc_interpretation_log_A_sec = "High variability"; }
        else                           { $iqrc_interpretation_log_A_sec = "Very bursty/erratic"; }
        printf {$fh} " (%s)\n", $iqrc_interpretation_log_A_sec;
    } else {
        print {$fh} "\n";
    }

    my $is_runq_pressure_C_log_sec = ($calc_debug_info_ref->{'IsRunQPressure'} // "False") eq "True";
    my $is_workload_pressure_C_log_sec = ($calc_debug_info_ref->{'IsWorkloadPressure'} // "False") eq "True";
    printf {$fh} "  2. VM Configuration & Profile Behavior\n";
    printf {$fh} "     - SMT                                   : %s\n", ($smt_val // $na);
	my $entitlement_display_A_log_sec = (defined $entitlement_val && looks_like_number($entitlement_val)) ? $entitlement_val : $na;
    printf {$fh} "     - Current Entitlement                   : %s cores\n", $entitlement_display_A_log_sec;
    my $lpar_max_cpu_display_A_log_sec = ($lpar_max_cpu_cfg_val_from_config > 0) ? sprintf("%.2f", $lpar_max_cpu_cfg_val_from_config) : $na;
    printf {$fh} "     - LPAR MaxCPU                           : %s cores\n", $lpar_max_cpu_display_A_log_sec;
    printf {$fh} "     - Profile RunQ Behaviour                : %s\n", ($profile_rq_behavior // $na);
	printf {$fh} "  3. Pressure Assessment Summary\n";
    # Enhanced line for Overall LPAR RunQ Pressure
    my $abs_runq_source_str = "$runq_metrics_source_profile_name_for_this_calc " . ($calc_debug_info_ref->{AbsRunQKeyUsed} // '');

    # Correctly retrieve the rationale string from the debug info hash
    my $pressure_basis_str = $calc_debug_info_ref->{'PressureBasisRationale'} // "MaxCPU";
    my $lpar_pressure_reason;

    if (($calc_debug_info_ref->{'IsRunQPressure'} // "False") eq "True") {
        $lpar_pressure_reason = sprintf("Ratio %.2f > %.2f (Pressure detected)", ($calc_debug_info_ref->{'RunQPressure_P90_Val'} // 0), $adaptive_runq_saturation_thresh);
    } else {
        $lpar_pressure_reason = sprintf("Ratio %.2f <= %.2f (No pressure detected)", ($calc_debug_info_ref->{'RunQPressure_P90_Val'} // 0), $adaptive_runq_saturation_thresh);
    }
    printf {$fh} "     - Overall LPAR RunQ Pressure            : %s (Source: %s)\n", (($calc_debug_info_ref->{'IsRunQPressure'} // "False") eq "True" ? "True" : "False"), $abs_runq_source_str;
    printf {$fh} "         Basis for Pressure Calc             : %s\n", $pressure_basis_str;
    printf {$fh} "         Reason for Pressure Flag            : %s\n", $lpar_pressure_reason;

    # Enhanced line for Normalised Workload Pressure
    my $norm_runq_source_str = "$runq_metrics_source_profile_name_for_this_calc";
    printf {$fh} "     - Normalised Workload Pressure          : %s (Source: %s; Reason: %s)\n\n",
        (($calc_debug_info_ref->{'IsWorkloadPressure'} // "False") eq "True" ? "True" : "False"),
        $norm_runq_source_str,
        ($calc_debug_info_ref->{'WorkloadPressureReason'} // "N/A");

    if (defined $calc_debug_info_ref->{'ReasonForNoModification'} && $calc_debug_info_ref->{'ReasonForNoModification'} ne '')
    {
        printf {$fh} "CPU Modification Path Skipped: %s\n", $calc_debug_info_ref->{'ReasonForNoModification'};
    }
    else
    {
        # --- Section C: CPU Downsizing (Efficiency Assessment) ---
        print {$fh} "Section C: CPU Downsizing (Efficiency Assessment)\n";
        my $downsizing_reason_B_log = $calc_debug_info_ref->{'DownsizingReason'} // "Not calculated or N/A."; # Use renamed key
        my $downsizing_factor_B_log = $calc_debug_info_ref->{'DownsizingFactor'} // "1.00"; # Use renamed key
		my $physc_after_downsizing_B_log = $calc_debug_info_ref->{'DownsizedPhysC'} // $na;

        if ($downsizing_reason_B_log =~ /Single-Threaded Dominant \(STD\) Workload Pattern Detected/) {
            # --- Path 1: STD Pattern was detected ---
            printf {$fh} "  - Overall Status                           : Tactical Downsizing Skipped (Entitlement Floor Guard)\n";
            printf {$fh} "  - Guardrail Rationale                      : Base PhysC (%.4f) > Entitlement (%.2f)\n", ($eff_p_base_numeric // 0), ($curr_ent_numeric // 0);
            printf {$fh} "  - Heuristic Override                       : High-Confidence STD workload pattern was detected.\n";
            printf {$fh} "      - Confidence Checks                    : %s\n", ($calc_debug_info_ref->{'STDConfidenceChecks'} // 'N/A');
            print  {$fh} "  - Strategic Signal Calculation (RunQ_Strategic):\n";
            my $runq_uncapped_val = $calc_debug_info_ref->{'RunQ_Strategic'} // 'N/A';
            my $potential_downsizing = (looks_like_number($calc_debug_info_ref->{'EffActualReductionCores'}) ? $calc_debug_info_ref->{'EffActualReductionCores'} : 0);
            printf {$fh} "      - Potential Downsizing                 : %.4f cores\n", $potential_downsizing;
            printf {$fh} "      - Dampening Tier                       : %s\n", ($calc_debug_info_ref->{'STDDampeningTier'} // 'N/A');
            my $final_dampening = $calc_debug_info_ref->{'STDFinalDampeningFactor'} // 0;
            printf {$fh} "      - Final Dampening Factor Applied       : %.0f%%\n", ($final_dampening * 100);
            printf {$fh} "      - Final RunQ_Strategic Value           : %.4f cores\n", (looks_like_number($runq_uncapped_val) ? $runq_uncapped_val : 0);

        } elsif ($downsizing_reason_B_log =~ /workload does not match STD pattern/) {
            # --- Path 2: Bursting, but STD Pattern was NOT detected ---
            printf {$fh} "  - Overall Status                           : Tactical Downsizing Skipped (Entitlement Floor Guard)\n";
            printf {$fh} "  - Guardrail Rationale                      : Base PhysC (%.4f) > Entitlement (%.2f)\n", ($eff_p_base_numeric // 0), ($curr_ent_numeric // 0);
            printf {$fh} "  - Heuristic Assessment                     : Workload does NOT match STD pattern. No override applied.\n";
            printf {$fh} "      - Confidence Checks                    : %s\n", ($calc_debug_info_ref->{'STDConfidenceChecks'} // 'N/A');

        } else {
            # --- Path 3: Standard logging for all other downsizing scenarios ---
            printf {$fh} "  - Overall Status                           : %s\n", $downsizing_reason_B_log;
        }

        printf {$fh} "  - Final Downsizing Factor                  : %s\n", $downsizing_factor_B_log;

        ## --- END: Enhanced Section C Rationale Logging ---

        # Conditionally print detailed analytical breakdown for downsizing
        if ($downsizing_reason_B_log =~ /^Analytical/ &&
            defined $calc_debug_info_ref->{'EffPEfficientTarget'} && # Internal keys can remain Eff...
            defined $calc_debug_info_ref->{'EffCondNormP50Met'} &&
            defined $calc_debug_info_ref->{'EffCondVolatilityMet'})
        {
            printf {$fh} "  - Detailed Analytical Path for Downsizing:\n";
            printf {$fh} "     a. Initial Condition Checks for Downsizing Path:\n";
            my $eff_cond_norm_p50_met_str_B = $calc_debug_info_ref->{'EffCondNormP50Met'} ? "YES (Low P50)" : "NO (P50 not low enough)";
            printf {$fh} "        - NormRunQ P50                       : %-5s (Condition: < %.2f for consideration? %s)\n",
            ($normP50_for_this_calc // $na),
            $NORM_P50_THRESHOLD_FOR_EFFICIENCY_CONSIDERATION,
            $eff_cond_norm_p50_met_str_B;

            my $eff_cond_volatility_met_str_B = $calc_debug_info_ref->{'EffCondVolatilityMet'} ? "YES (Not excessively volatile)" : "NO (Too volatile)";
            printf {$fh} "        - Workload Volatility                : %-5s (NormP90 %.2f / NormP50 %.2f. Condition: < %.2f to proceed? %s)\n",
            ($calc_debug_info_ref->{'EffVolatilityRatio'} // $na),
            ($normP90_for_this_calc ne $na ? ($normP90_for_this_calc+0):0), # Ensure numeric for sprintf
            ($normP50_for_this_calc ne $na ? ($normP50_for_this_calc+0):0),
            $VOLATILITY_CAUTION_THRESHOLD,
            $eff_cond_volatility_met_str_B;
            print {$fh} "\n";

            printf {$fh} "     b. Calculating Raw Efficient PhysC Target (Theoretical Minimum if RunQ was at Target Norm):\n";
            printf {$fh} "        - Base PhysC for Profile             : %s cores\n", ($calc_debug_info_ref->{'EffPBase'} // $na);
            printf {$fh} "        - AbsRunQ Metric Used                : %s (value: %.2f threads)\n", $abs_runq_key_reported_in_log, ($abs_runq_value_used_for_calc // $na);
            printf {$fh} "        - SMT Value                          : %s\n", ($calc_debug_info_ref->{'EffSMTValue'} // $na);
            my $smt_txt = defined $calc_debug_info_ref->{'EffSMTValue'}
                          ? $calc_debug_info_ref->{'EffSMTValue'} : $na;
            my $tgt_val = $calc_debug_info_ref->{'EffTargetNormRunQ'};
            my $tgt_txt = (defined $tgt_val && looks_like_number($tgt_val))
                          ? sprintf('%.2f', $tgt_val) : $na;
            printf {$fh} "        - Target NormRunQ for SMT%-11s : %s (internal heuristic for optimal queue/LCPU)\n", $smt_txt, $tgt_txt;
            printf {$fh} "        - Raw Efficient PhysC Target         : %s / (SMT * Target NormRunQ)\n", $abs_runq_key_reported_in_log;
            printf {$fh} "                                               %s / (%s * %.2f) = %s cores\n",
            ($abs_runq_value_used_for_calc // $na),
            ($calc_debug_info_ref->{'EffSMTValue'} // $na),
            ($calc_debug_info_ref->{'EffTargetNormRunQ'} // $na),
            ($calc_debug_info_ref->{'EffPEfficientTargetRaw'} // $na);
            print {$fh} "\n";

            printf {$fh} "     c. Blending Raw Target with Observed Base PhysC (Applying Confidence):\n";
            printf {$fh} "        - Blending Weights                   : %.0f%% Base PhysC / %.0f%% Raw Target\n",
            defined $calc_debug_info_ref->{'EffBlendWeightBase'} ? (($calc_debug_info_ref->{'EffBlendWeightBase'} // 0) * 100) : 0,
            defined $calc_debug_info_ref->{'EffBlendWeightTarget'} ? (($calc_debug_info_ref->{'EffBlendWeightTarget'} // 0) * 100) : 0;
            printf {$fh} "        - Blending Rationale                 : %s\n", ($calc_debug_info_ref->{'EffBlendReason'} // $na);
            printf {$fh} "        - Blended Efficient Target           : (Base PhysC * Weight) + (Raw Target * Weight)\n";
            printf {$fh} "                                               (%s * %.2f) + (%s * %.2f) = %s cores\n",
            ($calc_debug_info_ref->{'EffPBase'} // $na),
            ($calc_debug_info_ref->{'EffBlendWeightBase'} // 0.0),
            ($calc_debug_info_ref->{'EffPEfficientTargetRaw'} // $na),
            ($calc_debug_info_ref->{'EffBlendWeightTarget'} // 0.0),
            ($calc_debug_info_ref->{'EffPEfficientTarget'} // $na);
            print {$fh} "\n";

            printf {$fh} "     d. Determining Potential CPU Downsizing (Based on Blended Target):\n";
            my $eff_comp_base_vs_target_met_str_B = defined($calc_debug_info_ref->{'EffComparisonBaseVsTargetMet'})
            ? ($calc_debug_info_ref->{'EffComparisonBaseVsTargetMet'} ? "YES" : "NO") : $na;
            printf {$fh} "        - Comparison                         : Base PhysC (%s) > Blended Efficient Target (%s)? %s\n",
            ($calc_debug_info_ref->{'EffPBase'} // $na),
            ($calc_debug_info_ref->{'EffPEfficientTarget'} // $na),
            $eff_comp_base_vs_target_met_str_B;

            if (defined $calc_debug_info_ref->{'EffComparisonBaseVsTargetMet'} && $calc_debug_info_ref->{'EffComparisonBaseVsTargetMet'})
            {
                printf {$fh} "        - Potential CPU Downsize             : %s - %s = %s cores\n",
                ($calc_debug_info_ref->{'EffPBase'} // $na),
                ($calc_debug_info_ref->{'EffPEfficientTarget'} // $na),
                ($calc_debug_info_ref->{'EffPotentialReduction'} // $na);
                printf {$fh} "        - Max Downsize Cap %%                 : %.1f%% (Reason: %s)\n",
                ($calc_debug_info_ref->{'EffMaxAllowableReductionPerc'} eq $na ? ($MAX_EFFICIENCY_REDUCTION_PERCENTAGE*100) : ($calc_debug_info_ref->{'EffMaxAllowableReductionPerc'} +0) ),
                ($calc_debug_info_ref->{'EffReductionCapReason'} // $na);
                printf {$fh} "        - Max Allowable Downsize             : %s cores (Base PhysC * Max Downsize Cap %%)\n",
                ($calc_debug_info_ref->{'EffMaxAllowableReductionCores'} // $na);
                printf {$fh} "        - Actual CPU Downsized By            : %s cores (min of Potential and Max Allowable)\n",
                ($calc_debug_info_ref->{'EffActualReductionCores'} // $na);
            }
            else
            {
                printf {$fh} "        - No potential for downsizing based on Blended Target, or reduction was zero.\n";
            }
            print {$fh} "\n";

            printf {$fh} "     e. Final Downsizing Factor Calculation:\n";
            printf {$fh} "        - Calculated Factor                  : (Base PhysC - Actual Reduction) / Base PhysC\n";
            my $eff_p_base_val_for_div_B = ($calc_debug_info_ref->{'EffPBase'} ne $na && ($calc_debug_info_ref->{'EffPBase'} + 0) != 0)
            ? ($calc_debug_info_ref->{'EffPBase'} + 0) : 1.0;
            my $eff_p_base_display_for_div_B = ($eff_p_base_val_for_div_B == 1.0 && ($calc_debug_info_ref->{'EffPBase'} eq $na || ($calc_debug_info_ref->{'EffPBase'} + 0) == 0))
            ? "$eff_p_base_val_for_div_B (adj for display)" : ($calc_debug_info_ref->{'EffPBase'} // $na);
            printf {$fh} "                                               (%s - %s) / %s = %s\n",
            ($calc_debug_info_ref->{'EffPBase'} // $na),
            ($calc_debug_info_ref->{'EffActualReductionCores'} // "0.0000"),
            $eff_p_base_display_for_div_B,
            ($calc_debug_info_ref->{'EffCalculatedFactor'} // $na);
        }
        printf {$fh} "  => PhysC after Downsizing                  : %s cores\n\n", $physc_after_downsizing_B_log;

        # --- Section D: CPU Upsizing (Additive CPU) ---
        print {$fh} "Section D: CPU Upsizing (Additive CPU)\n";
        my $apply_additive_C_log_sec = $is_runq_pressure_C_log_sec || $is_workload_pressure_C_log_sec;

		printf {$fh} "  - Additive Logic Triggered                 : %s\n", ($apply_additive_C_log_sec ? "Yes" : "No");

        # --- Tier-Aware Scaling (if applied) ---
        if (exists $calc_debug_info_ref->{'TierScalingFactor'} &&
            $calc_debug_info_ref->{'TierScalingFactor'} ne '1.00') {

            printf {$fh} "  - Tier Scaling Applied                     : Tier %s (Factor: %sx)\n",
                   ($calc_debug_info_ref->{'TierNumber'} // 'N/A'),
                   ($calc_debug_info_ref->{'TierScalingFactor'} // '1.00');
            printf {$fh} "      Before Tier Scaling                    : %.4f cores\n",
                   (looks_like_number($calc_debug_info_ref->{'AdditiveCPU_PreTierScaling'})
                    ? $calc_debug_info_ref->{'AdditiveCPU_PreTierScaling'} : 0);
            printf {$fh} "      After Tier Scaling                     : %.4f cores\n",
                   (looks_like_number($calc_debug_info_ref->{'AdditiveCPU_PostTierScaling'})
                    ? $calc_debug_info_ref->{'AdditiveCPU_PostTierScaling'} : 0);
            printf {$fh} "      Rationale                              : %s\n",
                   ($calc_debug_info_ref->{'TierScalingRationale'} // 'N/A');
        }

        if ($apply_additive_C_log_sec)
        {
            printf {$fh} "    Details of Upsizing Calculation:\n";
            printf {$fh} "    - Base for Upsizing                      : %s cores (PhysC after Downsizing)\n", $physc_after_downsizing_B_log;
            printf {$fh} "    - Effective LCPUs at Base                : %s threads\n", ($calc_debug_info_ref->{'EffectiveLCPUsAtBase'} // $na);
            printf {$fh} "    - Excess Threads Calculated              : %s threads\n", ($calc_debug_info_ref->{'ExcessThreads'} // $na);
            printf {$fh} "    - Raw Additive CPU                       : %s cores (excess threads / SMT)\n", ($calc_debug_info_ref->{'RawAdditive'} // $na);
            printf {$fh} "    - Entitlement-Based Cap                  : %s cores (Max Additive Cap: %s)\n",
            (defined $calc_debug_info_ref->{'HotThreadWLDampenedAdditiveFrom'} && $calc_debug_info_ref->{'HotThreadWLDampeningApplied'} eq "True"
                ? $calc_debug_info_ref->{'HotThreadWLDampenedAdditiveFrom'}
                : ($calc_debug_info_ref->{'CappedRawAdditive'} // $na) # Show original value before HTW if HTW was applied
            ), ($calc_debug_info_ref->{'MaxAdditiveCap'} // $na);

            # Log enhanced burst and small entitlement handling details
            printf {$fh} "  Pressure Basis Rationale                   : %s\n", ($calc_debug_info_ref->{'PressureBasisRationale'} // "N/A");
            printf {$fh} "  Burst Allowance Used                       : %s\n", ($calc_debug_info_ref->{'BurstAllowanceUsed'} // "N/A");
            printf {$fh} "  Small Entitlement Handler Active           : %s\n", ($calc_debug_info_ref->{'SmallEntitlementHandler'} // "No");
            printf {$fh} "  Effective LCPUs for Pressure Calc          : %s\n", ($calc_debug_info_ref->{'EffectiveLCPUsForPressure'} // "N/A");

            if (defined $calc_debug_info_ref->{'HotThreadWLDampeningApplied'} && $calc_debug_info_ref->{'HotThreadWLDampeningApplied'} eq "True")
            {
                printf {$fh} "    - Hot Thread Dampening                : Applied\n";
                printf {$fh} "        Conditions Summary                : %s\n", ($calc_debug_info_ref->{'HotThreadWLConditionsString'} // $na);
                printf {$fh} "        Dynamic Dampen Factor             : %s\n", ($calc_debug_info_ref->{'HotThreadWLDynamicFactor'} // $na);
                printf {$fh} "        Additive (Before HTW)             : %s cores -> (After HTW): %s cores\n",
                ($calc_debug_info_ref->{'HotThreadWLDampenedAdditiveFrom'} // $na),
                ($calc_debug_info_ref->{'HotThreadWLDampenedAdditiveTo'} // $na);
            }
            elsif (defined $calc_debug_info_ref->{'HotThreadWLDampeningApplied'}) # Checked but not applied
            {
                printf {$fh} "    - Hot Thread Dampening                   : Not Applied (Details: %s)\n", ($calc_debug_info_ref->{'HotThreadWLConditionsString'} // "Conditions not met");
            }

            printf {$fh} "    - Volatility Factor Applied              : %s (Reason: %s)\n", ($calc_debug_info_ref->{'VoltFactor'} // $na), ($calc_debug_info_ref->{'VoltFactorReason'} // $na);
            printf {$fh} "    - Pool Factor Applied                    : %s\n", ($calc_debug_info_ref->{'PoolFactor'} // $na);
            if (defined $calc_debug_info_ref->{'DBWDampeningApplied'} && $calc_debug_info_ref->{'DBWDampeningApplied'} ne 'False') {
            printf {$fh} "    - Dispatch-Bound (DBW) Anomaly Dampening : %s\n", $calc_debug_info_ref->{'DBWDampeningApplied'};
            }

            if (defined $calc_debug_info_ref->{'AdditiveSafetyCapApplied'} && $calc_debug_info_ref->{'AdditiveSafetyCapApplied'} =~ /^True/)
            {
                printf {$fh} "    - Final Additive Safety Cap              : %s\n", $calc_debug_info_ref->{'AdditiveSafetyCapApplied'};
            }
        }
		printf {$fh} "  => Final Additive CPU                      : %s cores\n", ($calc_debug_info_ref->{'FinalAdditive'} // "0.0000");
		printf {$fh} "  => PhysC after Upsizing                    : %s cores\n\n", ($calc_debug_info_ref->{'PreMaxCpuCapRec'} // $na);


        # --- Section E: Maximum CPU Sizing Sanity Checks ---
        print {$fh} "Section E: Maximum CPU Sizing Sanity Checks\n";
        printf {$fh} "  - Recommendation before Max Sanity Check   : %s cores\n", ($calc_debug_info_ref->{'PreMaxCpuCapRec'} // $na);
        printf {$fh} "  - LPAR MaxCPU (from VM config)             : %s cores\n", $lpar_max_cpu_display_A_log_sec;
        printf {$fh} "  - Entitlement (for forecast multiplier)    : %s cores\n", $entitlement_display_A_log_sec;
        printf {$fh} "  - Forecast Multiplier Used                 : %s\n", ($calc_debug_info_ref->{'ForecastMultiplier'} // $na);
        printf {$fh} "  - Effective MaxCPU Sanity Limit            : %s cores\n", ($calc_debug_info_ref->{'EffectiveMaxCPUCap'} // $na);
        printf {$fh} "  - Limited by MaxCPU Sanity Check?          : %s\n", ($calc_debug_info_ref->{'CappedByMaxCPU'} // $na);
        printf {$fh} "  => Recommendation after Max Sanity Check   : %s cores\n\n", ($final_recommendation_unrounded_str // $na);
    } # End else for ReasonForNoModification (main calculation block)

    # --- Footer Block (Bottom Summary) ---
    print {$fh} "----------------------------------------------------------------------\n";
    printf {$fh} "Overall Summary for Profile: %s\n", $profile_being_adjusted;
    printf {$fh} "  - Initial Base PhysC (pre-growth)        : %.4f cores\n", $pre_growth_base_physc;
    printf {$fh} "  - nfit GrowthAdj (Theil-Sen)             : %s%.4f cores\n",
        ($nfit_growth_adj >= 0 ? "+" : ""), $nfit_growth_adj;
    printf {$fh} "  - nfit-profile Downsizing                : %s%.4f cores (Factor: %s)\n",
        (($calc_debug_info_ref->{'DownsizedPhysC'} // 0) - $post_growth_base_physc >= 0 ? "+" : ""),
        abs(($calc_debug_info_ref->{'DownsizedPhysC'} // 0) - $post_growth_base_physc),
        ($calc_debug_info_ref->{'DownsizingFactor'} // $na);

    my $downsizing_summary_reason_footer = $calc_debug_info_ref->{'DownsizingReason'} // "N/A";
    printf {$fh} "    - CPU Downsizing                         : Factor %s -> PhysC became %s cores. (Summary: %s)\n",
        ($calc_debug_info_ref->{'DownsizingFactor'} // $na),
        ($calc_debug_info_ref->{'DownsizedPhysC'} // $na),
        $downsizing_summary_reason_footer;

    my $additive_final_val_footer = $calc_debug_info_ref->{'FinalAdditive'} // "0.0000";
    my $additive_reason_summary_footer;

    if ($calc_debug_info_ref->{'IsRunQPressure'} eq "True" || $calc_debug_info_ref->{'IsWorkloadPressure'} eq "True") {
        $additive_reason_summary_footer = "Pressure detected";
        my @pressure_types_footer;
        if ($calc_debug_info_ref->{'IsRunQPressure'} eq "True") { push @pressure_types_footer, "Overall LPAR"; }
        if ($calc_debug_info_ref->{'IsWorkloadPressure'} eq "True") { push @pressure_types_footer, "Normalized Workload"; }
        if (@pressure_types_footer) { $additive_reason_summary_footer .= " (" . join(", ", @pressure_types_footer) . ")";}

        if (defined $calc_debug_info_ref->{'HotThreadWLDampeningApplied'} && $calc_debug_info_ref->{'HotThreadWLDampeningApplied'} eq "True")
        {
            $additive_reason_summary_footer .= "; HTW Dampened";
        }
        if (defined $calc_debug_info_ref->{'AdditiveSafetyCapApplied'} && $calc_debug_info_ref->{'AdditiveSafetyCapApplied'} =~ /^True/) {
            $additive_reason_summary_footer .= "; Safety Capped";
        }
        # Add note if additive ended up zero despite pressure
        if (abs(($additive_final_val_footer // 0)+0) < $FLOAT_EPSILON && ($calc_debug_info_ref->{'IsRunQPressure'} eq "True" || $calc_debug_info_ref->{'IsWorkloadPressure'} eq "True")) {
            $additive_reason_summary_footer .= "; Final Additive Zero (due to caps/dampening)";
        }
    }
    else {
        # This path is taken only when no pressure flags were set.
        if (defined $calc_debug_info_ref->{'ExcessThreads'} && $calc_debug_info_ref->{'ExcessThreads'} =~ /No excess threads/i) {
             $additive_reason_summary_footer = "No excess threads calculated for additive.";
         }
        else {
             $additive_reason_summary_footer = "No significant pressure detected.";
        }
    }
    printf {$fh} "    - CPU Upsizing (Additive)                : %s cores. (Reason: %s)\n",
        $additive_final_val_footer, $additive_reason_summary_footer;

    printf {$fh} "  - nfit-profile Upsizing (Additive)         : %s%.4f cores\n",
        ($additive_final_val_footer >= 0 ? "+" : ""), $additive_final_val_footer;

    my $lpar_cap_applied_summary_footer = "Not applied or N/A";
    if (defined $calc_debug_info_ref->{'CappedByMaxCPU'})
    {
        $lpar_cap_applied_summary_footer = $calc_debug_info_ref->{'CappedByMaxCPU'} eq "True"
        ? "Applied (Effective Limit: " . ($calc_debug_info_ref->{'EffectiveMaxCPUCap'} // $na) . ")"
        : "Not Applied (Effective Limit: " . ($calc_debug_info_ref->{'EffectiveMaxCPUCap'} // $na) . ")";
    }
    printf {$fh} "  - Max CPU Sanity Check                   : %s\n", $lpar_cap_applied_summary_footer;
    print {$fh} "  --------------------------------------------------------------------\n";
    printf {$fh} "  Final Recommended Value : %s cores (Unrounded: %s)\n",
        ($final_csv_value_for_profile // $na), # This is rounded for CSV
        $final_recommendation_unrounded_str;    # This is unrounded from calc
    print {$fh} "======================================================================\n\n";

}
# End of log_profile_rationale

# --- Helper subroutine to parse and collate percentile lists for nfit calls ---
# Takes an existing list of percentiles (as a comma-separated string) and an array
# of numeric percentiles that must be ensured to be present in the final list.
# Returns a sorted, unique array of percentile strings, formatted for nfit.
sub parse_and_collate_percentiles
{
    my ($existing_perc_list_str, @ensure_these_percs_numeric) = @_;
    my %final_percs_map; # Use a hash to store unique percentiles to avoid duplicates

    # Add percentiles from the existing string (e.g., from profile flags or global nfit-profile default)
    if (defined $existing_perc_list_str && $existing_perc_list_str ne '')
    {
        my @raw_list = split /,\s*/, $existing_perc_list_str;
        foreach my $p_str (@raw_list)
        {
            next if $p_str eq ''; # Skip empty strings that might result from split (e.g. "80,,90")

            # Validate if it looks like a percentile number, then format it consistently for nfit
            if ($p_str =~ /^[0-9.]+$/ && $p_str >= 0 && $p_str <= 100)
            {
                my $p_label = sprintf("%.2f", $p_str + 0); # Normalize format (e.g., "90.00")
                $p_label =~ s/\.?0+$//;                    # Clean trailing ".00" (e.g., "90")
                $p_label = "0" if $p_label eq "" && abs(($p_str+0) - 0) < 0.001; # Handle case of "0.00" -> "0"
                $final_percs_map{$p_label} = 1; # Add to hash (value 1 is arbitrary, key is what matters)
            }
            else
            {
                # If it's not a simple number, it might be an invalid value.
                # nfit will ultimately validate it. For now, include as is.
                # Alternatively, one could issue a warning here:
                # warn " [WARN] Non-standard percentile string '$p_str' found in list '$existing_perc_list_str'. Passing to nfit as is.\n";
                $final_percs_map{$p_str} = 1;
            }
        }
    }

    # Add percentiles that must be ensured (e.g., P90 for AbsRunQ, P50/P90 for NormRunQ)
    foreach my $p_num (@ensure_these_percs_numeric)
    {
        my $p_label = sprintf("%.2f", $p_num); # Format, e.g., 90 -> "90.00", 98.5 -> "98.50"
        $p_label =~ s/\.?0+$//;                # Clean to "90", "98.5"
        $p_label = "0" if $p_label eq "" && abs($p_num - 0) < 0.001; # "0.00" -> "0"
        $final_percs_map{$p_label} = 1; # Add/overwrite in hash to ensure it's present
    }

    # Return a numerically sorted list of unique percentile strings
    my @sorted_keys = sort {
        # Robust sort: treat as numbers if possible, otherwise string compare
        my $is_a_num = ($a =~ /^[0-9.]+$/); # Check if $a looks like a number
        my $is_b_num = ($b =~ /^[0-9.]+$/); # Check if $b looks like a number
        if ($is_a_num && $is_b_num) { return ($a+0) <=> ($b+0); } # Both are numbers, numeric sort
        elsif ($is_a_num) { return -1; } # Numbers come before non-numbers
        elsif ($is_b_num) { return 1;  } # Non-numbers come after numbers
        else { return $a cmp $b; }       # Both are non-numbers (e.g. invalid values), string compare
    } keys %final_percs_map; # Get unique keys from hash and sort them

    return @sorted_keys;
}

# --- quote_csv ---
# Ensures a string is properly quoted for CSV output, escaping internal double quotes.
sub quote_csv {
    my ($field) = @_;
    if (!defined $field) # Handle undefined fields as empty strings
    {
        $field = '';
    }
    $field =~ s/"/""/g; # Escape any double quotes within the field by doubling them
    return qq/"$field"/; # Enclose the entire field in double quotes
}

# --- load_profile_definitions ---
# Loads profile configurations from the specified INI-like file.
# Each section [Profile Name] defines a profile.
# Keys: nfit_flags (mandatory), runq_modifier_behavior (optional, default: 'default').
# This version correctly handles multi-line values for the 'nfit_flags' key.
sub load_profile_definitions {
    my ($filepath) = @_;
    my @loaded_profiles_list;
    my $current_profile = undef;
    my $last_key = '';

    open my $fh, '<:encoding(utf8)', $filepath or die "[ERROR] Cannot open profiles config file '$filepath': $!\n";

    while (my $line = <$fh>) {
        chomp $line;
        $line =~ s/\s*[#;].*//;    # Remove comments
        $line =~ s/^\s+|\s+$//g; # Trim whitespace
        next if $line eq '';

        if ($line =~ /^\s*\[\s*([^\]]+?)\s*\]\s*$/) { # New section
            my $section_name = $1;
            $current_profile = { name => $section_name, flags => '', runq_behavior => 'default', csv_output => 1 };
            push @loaded_profiles_list, $current_profile;
            $last_key = '';
        } elsif ($current_profile && $line =~ /^\s*([^=]+?)\s*=\s*(.*)$/) { # New key-value pair
            my $key = lc($1);
            my $value = $2;
            $key =~ s/^\s+|\s+$//g;
            $value =~ s/^\s+|\s+$//g;
            $last_key = $key;

            if ($key eq 'nfit_flags') {
                $current_profile->{flags} = $value;
            } elsif ($key eq 'runq_modifier_behavior') {
                $current_profile->{runq_behavior} = $value;
            } elsif ($key eq 'csv_output') {
                $current_profile->{csv_output} = ($value =~ /^(true|1|yes)$/i) ? 1 : 0;
            }
        } elsif ($current_profile && $last_key eq 'nfit_flags') { # Continuation of nfit_flags
            $current_profile->{flags} .= " $line";
        }
    }
    close $fh;

    # Final validation and cleanup
    my @valid_profiles;
    foreach my $p_ref (@loaded_profiles_list) {
        # Clean up any leading/trailing whitespace from concatenated flags
        $p_ref->{flags} =~ s/^\s+|\s+$//g if defined $p_ref->{flags};

        if (defined $p_ref->{flags} && $p_ref->{flags} ne '') {
            # (existing validation logic for mandatory P-99W1, etc.)
            push @valid_profiles, $p_ref;
        } else {
            warn " [WARN] Profile '[$p_ref->{name}]' in '$filepath' is missing 'nfit_flags'. Skipping.\n";
        }
    }
    return @valid_profiles;
}

# --- calculate_graduated_burst ---
# Returns graduated burst allowance based on entitlement size
# Small entitlements get higher burst allowance to reflect real-world burst capability
sub calculate_graduated_burst {
    my ($entitlement) = @_;

    if ($entitlement < 0.25) {
        return 0.50;  # 50% burst for micro-partitions
    } elsif ($entitlement < 0.50) {
        return 0.40;  # 40% burst for very small
    } elsif ($entitlement < 1.00) {
        return 0.30;  # 30% burst for small
    } else {
        return 0.25;  # 25% standard burst
    }
}

# --- calculate_structural_availability_factor ---
# For small entitlements, estimates the effective availability of vCPU threads
# based on entitlement size. Scales from 0.5 at Ent=0.1 to 0.95 at Ent=0.9
sub calculate_structural_availability_factor {
    my ($entitlement) = @_;

    # Ensure entitlement is in valid range
    return 0.5 if $entitlement <= 0.1;
    return 0.95 if $entitlement >= 0.9;

    # Linear scaling: 0.5 + (0.45 * entitlement)
    return 0.5 + (0.45 * $entitlement);
}

# --- apply_minimum_lcpu_floor ---
# Applies a minimum LCPU floor based on SMT level
# Prevents unrealistic near-zero LCPU calculations for micro-partitions
sub apply_minimum_lcpu_floor {
    my ($effective_lcpus, $smt_used) = @_;

    # Ensure at least 1 full hardware thread for micro-partitions
    # Scale slightly with SMT to be platform-aware
    my $min_floor = max(1, int($smt_used / 4));  # At SMT=8, floor=2; at SMT=4, floor=1

    return max($min_floor, $effective_lcpus);
}

# ==============================================================================
# SUBROUTINE: apply_tier_aware_upsizing_scaling
# PURPOSE:    Applies tier-number-aware scaling to upsizing (additive CPU)
#             adjustments. Higher tiers (1-2) receive more aggressive responses
#             to RunQ pressure due to lower latency tolerance. Tier 3 is the
#             baseline. Tier 4 receives slightly moderated adjustments.
#
# ARGUMENTS:
#   1. $additive_cpu (float): The calculated additive CPU before tier scaling
#   2. $final_tier (string): The tier identifier (e.g., "G3", "O1", "P")
#
# RETURNS:
#   - Hash containing:
#       scaled_value: The tier-adjusted additive CPU value
#       scaling_factor: The multiplier applied
#       tier_number: The extracted tier number
#       rationale: Human-readable explanation for logging
# ==============================================================================
sub apply_tier_aware_upsizing_scaling {
    my ($additive_cpu, $final_tier) = @_;

    # Default result (no scaling)
    my %result = (
        scaled_value   => $additive_cpu,
        scaling_factor => 1.00,
        tier_number    => 'N/A',
        rationale      => 'No tier-based scaling applied',
    );

    # Only scale positive values (upsizing adjustments)
    return \%result if (!defined $additive_cpu || $additive_cpu <= 0);

    # Extract tier number from tier identifier
    # Handles formats: G3, O1, B4, P (P is treated as tier 1)
    my $tier_num;
    if (defined $final_tier && $final_tier =~ /(\d)$/) {
        $tier_num = $1;
    } elsif (defined $final_tier && $final_tier eq 'P') {
        $tier_num = 1;  # P-tier is equivalent to tier 1
    } else {
        return \%result;  # Cannot determine tier, no scaling
    }

    # Tier-based scaling factors
    my %tier_upsizing_factors = (
        1 => 1.30,  # Tier 1: +30% aggressive response (low latency tolerance)
        2 => 1.15,  # Tier 2: +15% enhanced response
        3 => 1.00,  # Tier 3: Baseline STANDARD correction
        4 => 0.95,  # Tier 4: Slightly moderated (still substantial)
    );

    # Apply scaling if tier is valid
    if (exists $tier_upsizing_factors{$tier_num}) {
        my $factor = $tier_upsizing_factors{$tier_num};
        my $scaled = $additive_cpu * $factor;

        $result{scaled_value}   = $scaled;
        $result{scaling_factor} = $factor;
        $result{tier_number}    = $tier_num;

        # Generate detailed rationale
        if ($factor > 1.0) {
            my $increase_pct = ($factor - 1.0) * 100;
            $result{rationale} = sprintf(
                "Tier %d: Enhanced upsizing (+%.0f%%) for lower latency tolerance. Base: %.3f -> Scaled: %.3f cores",
                $tier_num, $increase_pct, $additive_cpu, $scaled
            );
        } elsif ($factor < 1.0) {
            my $decrease_pct = (1.0 - $factor) * 100;
            $result{rationale} = sprintf(
                "Tier %d: Moderated upsizing (-%.0f%%) for standard efficiency. Base: %.3f -> Scaled: %.3f cores",
                $tier_num, $decrease_pct, $additive_cpu, $scaled
            );
        } else {
            $result{rationale} = sprintf(
                "Tier %d: Standard baseline correction (no scaling). Value: %.3f cores",
                $tier_num, $scaled
            );
        }
    }

    return \%result;
}

# --- calculate_runq_modified_physc (with enhanced efficiency logic and detailed debug output) ---
# Calculates the final PhysC value for a profile after applying efficiency factors
# and RunQ-driven additive CPU adjustments.
# Takes the raw PhysC from nfit, RunQ metrics, SMT, entitlement, MaxCPU, etc.
# Returns the adjusted PhysC value and a hash of debug information for logging.
# --- calculate_runq_modified_physc (refactor: decoupled C & D, no early-returns, adds RunQ_Strategic) ---
#
sub calculate_runq_modified_physc
{
    my ($vm_name, $vm_map_ref, $profile_ref, $pressure_flags_href, $adaptive_thresholds_href) = @_;

    # --- Unpack all required values from the assimilation map and arguments ---
    my $profile_name = $profile_ref->{name};
    my $profile_runq_behavior_setting = $profile_ref->{runq_behavior} // 'default';

    # Core values from the map
    my $selected_tier_physc_value_str = $vm_map_ref->{CoreResults}{ProfileValues}{$profile_name};
    my $pure_p99w1_physc              = $vm_map_ref->{CoreResults}{ProfileValues}{$MANDATORY_PEAK_PROFILE_FOR_HINT};

    # Configuration from the map
    my $cfg_ref                 = $vm_map_ref->{Configuration};
    my $smt_used                = $cfg_ref->{smt};
    my $current_entitlement_str = $cfg_ref->{entitlement};
    my $max_cpu_config_str      = $cfg_ref->{max_cpu};
    my $vcpu_for_lpar_numeric   = $cfg_ref->{virtual_cpus};
    my $pool_cpu_for_lpar_numeric = $cfg_ref->{pool_cpu};
    my $is_in_non_default_pool  = (defined $cfg_ref->{pool_id} && $cfg_ref->{pool_id} != 0);

    # RunQ Metrics from the map
    my $runq_metrics_ref = $vm_map_ref->{RunQMetrics};
    my $norm_runq_p25_str = $runq_metrics_ref->{'NormRunQ_P25'};
    my $norm_runq_p50_str = $runq_metrics_ref->{'NormRunQ_P50'};
    my $norm_runq_p75_str = $runq_metrics_ref->{'NormRunQ_P75'};
    my $norm_runq_p90_str = $runq_metrics_ref->{'NormRunQ_P90'};

    # Determine which AbsRunQ percentile value to use based on profile behaviour.
    my $abs_runq_key_to_use = "AbsRunQ_P90"; # Default to fixed
    if ($runq_perc_behavior_mode eq 'match' && $profile_ref->{flags} =~ /-p\s+([0-9.]+)/) {
        $abs_runq_key_to_use = "AbsRunQ_P" . clean_perc_label($1);
    }
    my $abs_runq_p_value_str = $runq_metrics_ref->{$abs_runq_key_to_use};
    my $abs_runq_key_for_debug = $abs_runq_key_to_use;

    # Pressure flags and adaptive thresholds passed in from the main loop
    my $p99w1_overall_vm_has_abs_runq_pressure  = $pressure_flags_href->{abs_pressure};
    my $p99w1_overall_vm_has_norm_runq_pressure = $pressure_flags_href->{norm_pressure};
    my $adaptive_runq_saturation_thresh   = $adaptive_thresholds_href->{saturation};
    my $adaptive_target_norm_runq         = $adaptive_thresholds_href->{target};
    my $adaptive_max_efficiency_reduction = $adaptive_thresholds_href->{max_reduction};

    # The tier for all modifier calculations must come from the profile being processed,
    # not the global VM hint, to ensure logical consistency.
    my $final_tier = $profile_ref->{name};
    $final_tier =~ s/-.*//; # Extract the tier part (e.g., "G2-98W10" -> "G2")

    my %debug_info;
    my $na_str = "N/A";


    # Defensive normalisation: seasonal/bucket execution paths may call into
    # RunQ logic without a tier PhysC string populated.
    $selected_tier_physc_value_str = $na_str
        unless defined($selected_tier_physc_value_str) && length($selected_tier_physc_value_str);

    # --- Initialize all debug fields to sensible defaults or N/A ---
    $debug_info{'AbsRunQKeyUsed'} = $abs_runq_key_for_debug // 'N/A (key not provided)';
    $debug_info{'AbsRunQValueUsedForCalc'} = $abs_runq_p_value_str;
    $debug_info{'BasePhysC'} = $selected_tier_physc_value_str // $na_str;
    $profile_runq_behavior_setting //= 'default';

    my $base_physc = ($selected_tier_physc_value_str ne $na_str && $selected_tier_physc_value_str =~ /^-?[0-9.]+$/)
        ? ($selected_tier_physc_value_str + 0)
        : undef;

    # Efficiency related fields - meticulously initialized
    $debug_info{'DownsizingReason'} = "Efficiency calculation not initiated or skipped by initial guards.";
    $debug_info{'DownsizingFactor'} = "1.00"; # final sprintf form later
    $debug_info{'EffCondNormP50Met'} = undef;
    $debug_info{'EffCondVolatilityMet'} = undef;
    $debug_info{'EffVolatilityRatio'} = $na_str;
    $debug_info{'EffPBase'} = (defined $base_physc) ? sprintf("%.4f", $base_physc) : $na_str;
    $debug_info{'EffSMTValue'} = $smt_used // $na_str;
    $debug_info{'EffTargetNormRunQ'} = $na_str;
    $debug_info{'EffPEfficientTargetRaw'} = $na_str;
    $debug_info{'EffBlendReason'} = "Blending not applied or not applicable.";
    $debug_info{'EffBlendWeightBase'} = $na_str;
    $debug_info{'EffBlendWeightTarget'} = $na_str;
    $debug_info{'EffPEfficientTarget'} = $na_str;
    $debug_info{'EffComparisonBaseVsTargetMet'} = undef;
    $debug_info{'EffPotentialReduction'} = $na_str;
    $debug_info{'EffMaxAllowableReductionPerc'} = $adaptive_max_efficiency_reduction * 100;
    $debug_info{'EffReductionCapReason'} = "Default reduction cap applied.";
    $debug_info{'EffMaxAllowableReductionCores'} = $na_str;
    $debug_info{'EffActualReductionCores'} = $na_str;
    $debug_info{'EffCalculatedFactor'} = "1.0000";
    $debug_info{'EffFinalFactorApplied'} = "1.00";

    # General fields
    $debug_info{'DownsizedPhysC'} = defined($base_physc) ? sprintf("%.4f", $base_physc) : $na_str;
    $debug_info{'RunQ_Strategic'} = "0.0000";
    $debug_info{'RunQ_Potential'} = "0.0000";
    $debug_info{'RunQPressure_P90_Val'} = 0; $debug_info{'IsRunQPressure'} = "False";
    $debug_info{'IsWorkloadPressure'} = "False"; $debug_info{'WorkloadPressureReason'} = "Conditions not met or N/A inputs";
    $debug_info{'EffectiveLCPUsAtBase'} = $na_str; $debug_info{'ExcessThreads'} = $na_str;
    $debug_info{'RawAdditive'} = "0.0000"; $debug_info{'MaxAdditiveCap'} = "0.0000"; $debug_info{'CappedRawAdditive'} = "0.0000";
    $debug_info{'VoltFactorReason'} = "Default (no overriding condition met or additive not applied)";
    $debug_info{'VoltFactor'} = "1.00";
    $debug_info{'PoolFactor'} = "1.00";
    $debug_info{'FinalAdditive'} = "0.0000";
    $debug_info{'PreMaxCpuCapRec'} = $debug_info{'DownsizedPhysC'};
    $debug_info{'LPARMaxCPUConfig'} = ($max_cpu_config_str ne "" && $max_cpu_config_str =~ /^[0-9.]+$/ && ($max_cpu_config_str+0) > 0)
        ? ($max_cpu_config_str+0) : $na_str;
    $debug_info{'EntitlementForForecast'} = (defined $current_entitlement_str && $current_entitlement_str ne "" && $current_entitlement_str =~ /^-?[0-9.]+$/)
        ? ($current_entitlement_str + 0) : 0;
    $debug_info{'ForecastMultiplier'} = $na_str; $debug_info{'EffectiveMaxCPUCap'} = $na_str;
    $debug_info{'CappedByMaxCPU'} = $na_str;
    $debug_info{'FinalAdjustedPhysC'} = $debug_info{'BasePhysC'};
    $debug_info{'ReasonForNoModification'} = "";

    my $curr_ent_numeric = $debug_info{'EntitlementForForecast'};
    my $eff_p_base_numeric = defined($base_physc) ? $base_physc : undef;

    # NOTE: Growth adjustment is now pre-applied in ProfileValues (stored as FinalValue).
    # The base_physc value retrieved from ProfileValues already includes GrowthAdj.
    # This ensures all subsequent calculations (downsizing, upsizing, capping) work with
    # the growth-adjusted baseline, maintaining proper audit trail transparency.
    # The manual addition that was previously here has been removed to prevent double-counting.
    $debug_info{'EffPBase'} = defined($eff_p_base_numeric) ? sprintf("%.4f", $eff_p_base_numeric) : $na_str;

    unless (defined $base_physc)
    {
        $debug_info{'ReasonForNoModification'} = "BasePhysC for profile not numeric or N/A";
        $debug_info{'FinalAdjustedPhysC'} = $selected_tier_physc_value_str // $na_str;
        return ($selected_tier_physc_value_str // $na_str, \%debug_info);
    }

    my $norm_p50_numeric = ($norm_runq_p50_str ne $na_str && $norm_runq_p50_str =~ /^-?[0-9.]+$/) ? ($norm_runq_p50_str + 0) : undef;
    my $norm_p90_numeric = ($norm_runq_p90_str ne $na_str && $norm_runq_p90_str =~ /^-?[0-9.]+$/) ? ($norm_runq_p90_str + 0) : undef;
    my $abs_runq_p_numeric  = ($abs_runq_p_value_str ne $na_str && $abs_runq_p_value_str  =~ /^-?[0-9.]+$/) ? ($abs_runq_p_value_str + 0)  : undef;
    $debug_info{'NormRunQ_P90_Val'} = (defined $norm_p90_numeric) ? sprintf("%.2f", $norm_p90_numeric) : $na_str;

    # Calculate Effective LCPUs for Pressure
    my ($effective_lcpus_for_pressure_calc, $pressure_basis_rationale);
    my $max_cpu_for_lpar_numeric = (defined $max_cpu_config_str && looks_like_number($max_cpu_config_str)) ? ($max_cpu_config_str + 0) : 0;
    my $base_burst_factor = 0.50;
    my $evr_tier_rationale = "Conservative/Standard (EVR < 5x)";
    if (defined $curr_ent_numeric && $curr_ent_numeric > 0 && defined $vcpu_for_lpar_numeric && $vcpu_for_lpar_numeric > 0) {
        my $evr = $vcpu_for_lpar_numeric / $curr_ent_numeric;
        if ($evr > 10) {
            $base_burst_factor = 3.00;
            $evr_tier_rationale = sprintf("Aggressive (EVR %.1fx > 10x)", $evr);
        } elsif ($evr >= 5) {
            $base_burst_factor = 1.50;
            $evr_tier_rationale = sprintf("Moderate (EVR %.1fx >= 5x)", $evr);
        }
    }
    my $pool_constraint_multiplier = 1.0;
    my $pool_constraint_rationale = "Default Pool (ID 0), no constraint applied";
    if ($is_in_non_default_pool && $pool_cpu_for_lpar_numeric > 0 && $curr_ent_numeric > 0) {
        my $vm_share_of_pool = $curr_ent_numeric / $pool_cpu_for_lpar_numeric;
        $pool_constraint_multiplier = 1.0 - (0.3 * min(1.0, $vm_share_of_pool * 2));
        $pool_constraint_multiplier = max(0.7, $pool_constraint_multiplier);
        $pool_constraint_rationale = sprintf("User Pool (Share %.1f%%), multiplier of %.2f applied", $vm_share_of_pool*100, $pool_constraint_multiplier);
    }
    my $final_burst_allowance = $base_burst_factor * $pool_constraint_multiplier;
    my $theoretical_lcpus = ($curr_ent_numeric * (1 + $final_burst_allowance)) * $smt_used;

    # handle potentially undefined P-99W1 values
    my $p99w1_physc_val_str = looks_like_number($pure_p99w1_physc) ? sprintf("%.3f", $pure_p99w1_physc) : $na_str;
    my $p99w1_physc_val = ($p99w1_physc_val_str ne $na_str && $p99w1_physc_val_str =~ /^-?[0-9.]+\z/)
        ? ($p99w1_physc_val_str + 0)
        : 0;
    my $capacity_headroom = ($max_cpu_for_lpar_numeric > 0) ? (1 - ($p99w1_physc_val / $max_cpu_for_lpar_numeric)) : 0;
    my $headroom_confidence_modifier = 1.0;
    my $headroom_rationale = "High Headroom (>30%), full confidence in observed peak";
    if ($capacity_headroom < 0.10) {
        $headroom_confidence_modifier = 0.75;
        $headroom_rationale = sprintf("Low Headroom (%.1f%%), low confidence in further bursting", $capacity_headroom * 100);
    } elsif ($capacity_headroom < 0.30) {
        $headroom_confidence_modifier = 0.90;
        $headroom_rationale = sprintf("Medium Headroom (%.1f%%), medium confidence", $capacity_headroom * 100);
    }
    my $confidence_adjusted_peak_lcpus = ($p99w1_physc_val * $smt_used) * $headroom_confidence_modifier;
    my $absolute_ceiling_lcpus = $max_cpu_for_lpar_numeric > 0 ? ($max_cpu_for_lpar_numeric * $smt_used) : ($vcpu_for_lpar_numeric * $smt_used);
    $effective_lcpus_for_pressure_calc = min($absolute_ceiling_lcpus, max($theoretical_lcpus, $confidence_adjusted_peak_lcpus));
    $pressure_basis_rationale = sprintf(
        "\n   1. Theoretical Capacity (Config)\n".
        "      - Base Burst Factor                    : %.0f%% (Reason: %s)\n".
        "      - Pool Constraint                      : %.2fx (Reason: %s)\n".
        "      => Theoretical LCPUs                   : %.2f Ent * (1 + %.2f Final Burst) * %d SMT = %.1f\n".
        "   2. Observed Capacity (Evidence-based)\n".
        "      - Observed Peak (P-99W1)               : %.2f cores (Headroom: %.1f%%)\n".
        "      - Confidence Modifier                  : %.2fx (Reason: %s)\n".
        "      => Conf-Adjusted LCPUs                 : (%.2f Cores * %d SMT) * %.2f = %.1f\n".
        "   3. Final Decision\n".
        "      => Effective LCPUs                     : min(%.1f Max LCPUs, max(%.1f Theoretical, %.1f Observed)) = %.1f",
        $base_burst_factor*100, $evr_tier_rationale,
        $pool_constraint_multiplier, $pool_constraint_rationale,
        $curr_ent_numeric, $final_burst_allowance, $smt_used, $theoretical_lcpus,
        $p99w1_physc_val, $capacity_headroom*100,
        $headroom_confidence_modifier, $headroom_rationale,
        $p99w1_physc_val, $smt_used, $headroom_confidence_modifier, $confidence_adjusted_peak_lcpus,
        $absolute_ceiling_lcpus, $theoretical_lcpus, $confidence_adjusted_peak_lcpus, $effective_lcpus_for_pressure_calc
    );

    my $norm_p25_numeric = ($norm_runq_p25_str ne $na_str && $norm_runq_p25_str =~ /^-?[0-9.]+$/) ? ($norm_runq_p25_str + 0) : undef;
    my $norm_p75_numeric = ($norm_runq_p75_str ne $na_str && $norm_runq_p75_str =~ /^-?[0-9.]+$/) ? ($norm_runq_p75_str + 0) : undef;
    my $normrunq_iqrc_val = undef;
    if (defined $norm_p25_numeric && defined $norm_p50_numeric && defined $norm_p75_numeric)
    {
        my $p50_denominator_for_iqrc = (abs($norm_p50_numeric) > $MIN_P50_DENOMINATOR_FOR_VOLATILITY)
            ? $norm_p50_numeric
            : (($norm_p50_numeric >= 0) ? $MIN_P50_DENOMINATOR_FOR_VOLATILITY : -$MIN_P50_DENOMINATOR_FOR_VOLATILITY);
        if (abs($p50_denominator_for_iqrc) > $FLOAT_EPSILON)
        {
            $normrunq_iqrc_val = ($norm_p75_numeric - $norm_p25_numeric) / $p50_denominator_for_iqrc;
        }
        elsif (abs($norm_p75_numeric - $norm_p25_numeric) < $FLOAT_EPSILON) { $normrunq_iqrc_val = 0.0; }
        else { $normrunq_iqrc_val = 999.0; }
    }
    $debug_info{'NormRunQ_P25_Val'} = (defined $norm_p25_numeric) ? sprintf("%.2f", $norm_p25_numeric) : $na_str;
    $debug_info{'NormRunQ_P50_Val'} = (defined $norm_p50_numeric) ? sprintf("%.2f", $norm_p50_numeric) : $na_str;
    $debug_info{'NormRunQ_P75_Val'} = (defined $norm_p75_numeric) ? sprintf("%.2f", $norm_p75_numeric) : $na_str;
    $debug_info{'EffectiveLCPUsForPressure'} = sprintf("%.1f", $effective_lcpus_for_pressure_calc);
    $debug_info{'EffectiveLCPUsAtBase'} = sprintf("%.4f", $effective_lcpus_for_pressure_calc);
    my $runq_pressure_p_val = ($effective_lcpus_for_pressure_calc > 0) ? (($abs_runq_p_numeric // 0) / $effective_lcpus_for_pressure_calc) : 0;
    $debug_info{'NormRunQ_IQRC_Val'} = (defined $normrunq_iqrc_val) ? sprintf("%.3f", $normrunq_iqrc_val) : $na_str;
    $debug_info{'RunQPressure_P90_Val'} = sprintf("%.4f", $runq_pressure_p_val);
    $debug_info{'PressureBasisRationale'} = $pressure_basis_rationale;
    $debug_info{'BurstAllowanceUsed'} = sprintf("%.2f%% (EVR-based, Pool-adjusted)", $final_burst_allowance * 100);
    $debug_info{'SmallEntitlementHandlerActive'} = ($curr_ent_numeric < 1.0 && $max_cpu_for_lpar_numeric > $curr_ent_numeric) ? "Yes" : "No";

    # =========================
    # Section C: Strategic Efficiency / Downsizing (ALWAYS RUNS)
    # =========================
    my $efficiency_factor_numeric = 1.00;
    $debug_info{'DownsizingFactor'} = sprintf("%.2f", $efficiency_factor_numeric);
    my $base_adjusted_physc = (defined $eff_p_base_numeric ? $eff_p_base_numeric : 0) * $efficiency_factor_numeric;
    $debug_info{'DownsizedPhysC'} = sprintf("%.4f", $base_adjusted_physc);
    $debug_info{'DownsizingReason'} = "Default (No CPU downsizing applied or eligible).";
    if (defined $norm_p50_numeric && defined $norm_p90_numeric && defined $abs_runq_p_numeric && $smt_used > 0 && defined $eff_p_base_numeric)
    {
        $debug_info{'EffCondNormP50Met'} = ($norm_p50_numeric < $NORM_P50_THRESHOLD_FOR_EFFICIENCY_CONSIDERATION);
        my $effective_p50_for_volatility = ($norm_p50_numeric > 0.0001) ? max($norm_p50_numeric, $MIN_P50_DENOMINATOR_FOR_VOLATILITY) : $MIN_P50_DENOMINATOR_FOR_VOLATILITY;
        my $volatility_ratio = (defined $norm_p90_numeric && $effective_p50_for_volatility > 0.0001) ? ($norm_p90_numeric / $effective_p50_for_volatility) : 1.0;
        $debug_info{'EffVolatilityRatio'} = sprintf("%.2f", $volatility_ratio);
        $debug_info{'EffCondVolatilityMet'} = ($volatility_ratio < $VOLATILITY_CAUTION_THRESHOLD);
        if (!$debug_info{'EffCondVolatilityMet'}) {
            $debug_info{'DownsizingReason'} = sprintf("Skipped CPU Downsizing: Workload volatile (NormP90/P50 ratio %.2f >= %.2f). No analytical reduction.", $volatility_ratio, $VOLATILITY_CAUTION_THRESHOLD);
        }
        elsif (!$debug_info{'EffCondNormP50Met'}) {
            $debug_info{'DownsizingReason'} = sprintf("Skipped CPU Downsizing: NormRunQ P50 (%.2f) not below threshold (%.2f) for efficiency consideration. No analytical reduction.", $norm_p50_numeric, $NORM_P50_THRESHOLD_FOR_EFFICIENCY_CONSIDERATION);
        }
        else {
            my $target_norm_runq_eff_calc;
            my $adaptive_target_reason;
            my $tier_to_check = $final_tier;
            if (exists $custom_runq_targets{$tier_to_check}) {
                $target_norm_runq_eff_calc = $custom_runq_targets{$tier_to_check};
                $adaptive_target_reason = "User-defined target for tier '$tier_to_check' from nfit.profiles.cfg";
            } elsif (exists $DEFAULT_RUNQ_TARGETS{$tier_to_check}) {
                $target_norm_runq_eff_calc = $DEFAULT_RUNQ_TARGETS{$tier_to_check};
                $adaptive_target_reason = "Built-in default target for tier '$tier_to_check'";
            } else {
                my ($pattern) = ($final_tier =~ /^([OGBP])/);
                my $base_tier = $pattern // 'G';
                $target_norm_runq_eff_calc = $DEFAULT_RUNQ_TARGETS{$base_tier} // $DEFAULT_TARGET_NORM_RUNQ_FOR_EFFICIENCY_CALC;
                $adaptive_target_reason = "Fallback target for base pattern '$base_tier'";
            }
            $debug_info{'AdaptiveTargetNormRunQReason'} = "$adaptive_target_reason: ${target_norm_runq_eff_calc}";
            my $p_efficient_target_raw = ($smt_used * $target_norm_runq_eff_calc > 0.0001) ? ($abs_runq_p_numeric / ($smt_used * $target_norm_runq_eff_calc)) : $eff_p_base_numeric + 1;
            $debug_info{'EffPEfficientTargetRaw'} = sprintf("%.4f", $p_efficient_target_raw);
            $debug_info{'EffTargetNormRunQ'} = sprintf("%.2f", $target_norm_runq_eff_calc);
            my $base_physc_weight = $BLEND_WEIGHT_BASE_DEFAULT_LOW_P50;
            my $efficient_target_weight = 1.0 - $base_physc_weight;
            my $blending_details_str = sprintf("Default low P50 blend (%.0f%% Base / %.0f%% Target).", $base_physc_weight*100, $efficient_target_weight*100);
            if ($norm_p50_numeric < $NORM_P50_LOW_THRESH_FOR_BLEND1) {
                $base_physc_weight = $BLEND_WEIGHT_BASE_FOR_LOW_P50_1;
                $efficient_target_weight = 1.0 - $base_physc_weight;
                $blending_details_str = sprintf("NormP50 (%.2f) < %.2f, using more aggressive blend (%.0f%% Base / %.0f%% Target).", $norm_p50_numeric, $NORM_P50_LOW_THRESH_FOR_BLEND1, $base_physc_weight*100, $efficient_target_weight*100);
            } elsif ($norm_p50_numeric < $NORM_P50_MODERATE_THRESH_FOR_BLEND2) {
                $base_physc_weight = $BLEND_WEIGHT_BASE_FOR_LOW_P50_2;
                $efficient_target_weight = 1.0 - $base_physc_weight;
                $blending_details_str = sprintf("NormP50 (%.2f) < %.2f, using moderate blend (%.0f%% Base / %.0f%% Target).", $norm_p50_numeric, $NORM_P50_MODERATE_THRESH_FOR_BLEND2, $base_physc_weight*100, $efficient_target_weight*100);
            }
            $debug_info{'EffBlendWeightBase'}   = sprintf("%.2f", $base_physc_weight);
            $debug_info{'EffBlendWeightTarget'} = sprintf("%.2f", $efficient_target_weight);
            $debug_info{'EffBlendReason'}       = $blending_details_str;
            my $blended_efficient_target = ($eff_p_base_numeric * $base_physc_weight) + ($p_efficient_target_raw * $efficient_target_weight);
            $debug_info{'EffPEfficientTarget'}  = sprintf("%.4f", $blended_efficient_target);
            $debug_info{'EffComparisonBaseVsTargetMet'} = ($eff_p_base_numeric > $blended_efficient_target);
            if ($debug_info{'EffComparisonBaseVsTargetMet'})
            {
                my $potential_reduction_cores = $eff_p_base_numeric - $blended_efficient_target;
                $debug_info{'EffPotentialReduction'} = sprintf("%.4f", $potential_reduction_cores);
                $debug_info{'RunQ_Potential'} = sprintf("%.4f", -1 * $potential_reduction_cores);
                my $base_max_reduction_factor;
                my $size_aware_cap_reason;
                if ($eff_p_base_numeric < 1.0) {
                    $base_max_reduction_factor = 0.30;
                    $size_aware_cap_reason = sprintf("BasePhysC (%.2f) < 1.0, using %.0f%% cap", $eff_p_base_numeric, $base_max_reduction_factor * 100);
                } elsif ($eff_p_base_numeric < 4.0) {
                    $base_max_reduction_factor = 0.25;
                    $size_aware_cap_reason = sprintf("BasePhysC (%.2f) < 4.0, using %.0f%% cap", $eff_p_base_numeric, $base_max_reduction_factor * 100);
                } else {
                    $base_max_reduction_factor = 0.20;
                    $size_aware_cap_reason = sprintf("BasePhysC (%.2f) >= 4.0, using %.0f%% cap", $eff_p_base_numeric, $base_max_reduction_factor * 100);
                }
                my $current_max_reduction_perc_val = $base_max_reduction_factor;
                my $volatility_reason = "";
                if ($volatility_ratio > $VOLATILITY_MODERATE_HIGH_CAP_THRESH) {
                    $current_max_reduction_perc_val *= $REDUCTION_CAP_SCALE_FOR_MODERATE_HIGH_VOLATILITY;
                    $volatility_reason = sprintf(" (scaled by %.2fx for high volatility)", $REDUCTION_CAP_SCALE_FOR_MODERATE_HIGH_VOLATILITY);
                } elsif ($volatility_ratio > $VOLATILITY_MODERATE_MEDIUM_CAP_THRESH) {
                    $current_max_reduction_perc_val *= $REDUCTION_CAP_SCALE_FOR_MODERATE_MEDIUM_VOLATILITY;
                    $volatility_reason = sprintf(" (scaled by %.2fx for medium volatility)", $REDUCTION_CAP_SCALE_FOR_MODERATE_MEDIUM_VOLATILITY);
                } elsif ($volatility_ratio > $VOLATILITY_MODERATE_LOW_CAP_THRESH) {
                    $current_max_reduction_perc_val *= $REDUCTION_CAP_SCALE_FOR_MODERATE_VOLATILITY;
                    $volatility_reason = sprintf(" (scaled by %.2fx for moderate volatility)", $REDUCTION_CAP_SCALE_FOR_MODERATE_VOLATILITY);
                }
                $debug_info{'EffReductionCapReason'} = $size_aware_cap_reason . $volatility_reason;
                $debug_info{'EffMaxAllowableReductionPerc'} = $current_max_reduction_perc_val * 100;
                my $max_allowable_reduction_cores = $eff_p_base_numeric * $current_max_reduction_perc_val;
                $debug_info{'EffMaxAllowableReductionCores'} = sprintf("%.4f", $max_allowable_reduction_cores);
                my $actual_reduction_cores = min($potential_reduction_cores, $max_allowable_reduction_cores);
                $actual_reduction_cores = max(0, $actual_reduction_cores);
                $debug_info{'EffActualReductionCores'} = sprintf("%.4f", $actual_reduction_cores);
                $debug_info{'RunQ_Strategic'} = sprintf("%.4f", -1 * ($actual_reduction_cores+0));
                if ($actual_reduction_cores > 0.0001) {
                    my $new_physc_after_reduction = $eff_p_base_numeric - $actual_reduction_cores;
                    $efficiency_factor_numeric = ($eff_p_base_numeric > 0.0001) ? ($new_physc_after_reduction / $eff_p_base_numeric) : 1.00;
                    my $min_expected_eff_factor = 1 - $current_max_reduction_perc_val;
                    if ($efficiency_factor_numeric < ($min_expected_eff_factor - 0.001) ) { $efficiency_factor_numeric = $min_expected_eff_factor; }
                    $efficiency_factor_numeric = 1.00 if $efficiency_factor_numeric > 1.00;
                    $efficiency_factor_numeric = max(0, $efficiency_factor_numeric);
                    $debug_info{'EffCalculatedFactor'}   = sprintf("%.4f", $efficiency_factor_numeric);
                    $debug_info{'EffFinalFactorApplied'} = sprintf("%.2f",  $efficiency_factor_numeric);
                    $debug_info{'DownsizingReason'}      = sprintf("Analytical CPU Downsizing (using blended target & dynamic cap): Reduction of %.4f cores applied.", $actual_reduction_cores);
                } else {
                    $debug_info{'EffFinalFactorApplied'} = "1.00";
                    $debug_info{'EffCalculatedFactor'}   = "1.0000";
                    $debug_info{'DownsizingReason'} = sprintf("Analytical CPU Downsizing (using blended target & dynamic cap): Base_PhysC %.4f, Blended_Target %.4f. Calculated reduction (%.4f) negligible or zero. No adjustment from this path.", $eff_p_base_numeric, $blended_efficient_target, $actual_reduction_cores // 0.0);
                }
            } else {
                $debug_info{'EffFinalFactorApplied'} = "1.00";
                $debug_info{'EffCalculatedFactor'}   = "1.0000";
                $debug_info{'DownsizingReason'} = sprintf("Analytical CPU Downsizing: Base_PhysC %.4f not greater than Blended_Efficient_Target_PhysC %.4f. No reduction. Blending Reason: %s", $eff_p_base_numeric, $blended_efficient_target, $blending_details_str);
                $debug_info{'RunQ_Potential'} = "0.0000";
            }
        }
    } else {
        $debug_info{'DownsizingReason'} = "Key metrics (NormP50/P90, AbsRunQ) N/A for full analytical CPU downsizing check. No efficiency adjustment applied.";
    }

    my $skip_downsizing_reason;
    if (($p99w1_overall_vm_has_abs_runq_pressure && $p99w1_overall_vm_has_abs_runq_pressure) || ($p99w1_overall_vm_has_norm_runq_pressure && $p99w1_overall_vm_has_norm_runq_pressure)) {
        $skip_downsizing_reason = sprintf("VM's %s profile shows RunQ pressure.", $MANDATORY_PEAK_PROFILE_FOR_HINT);
    } elsif ($profile_runq_behavior_setting eq 'additive_only') {
       $skip_downsizing_reason = "Profile runq_behavior=additive_only.";
    } elsif (defined $curr_ent_numeric && $curr_ent_numeric > 0 && defined $eff_p_base_numeric && ($eff_p_base_numeric > $curr_ent_numeric)) {

        # --- Enhanced STD Pattern Detection with Multiple Confidence Boosters ---

        # 1. Dynamically determine the source profile for PhysC stability metrics.
        #    Priority: User TIER > AutoTier > Fallback to 'G'.
        my $user_tier_override_std = $vm_map_ref->{Hinting}{FinalTierForVM} // "";
        my $auto_tier_std = $vm_map_ref->{Hinting}{AutoTier} // "G";

        my $pattern_source_std = ($user_tier_override_std ne "") ? $user_tier_override_std : $auto_tier_std;
        my ($pattern) = ($pattern_source_std =~ /^([A-Z])/);
        $pattern //= 'G'; # Default to 'G' if regex fails

        my %pattern_to_profile_map = ('O' => 'O3-95W15', 'B' => 'B3-95W15', 'G' => 'G3-95W15', 'P' => 'G3-95W15');
        my $source_profile_base = $pattern_to_profile_map{$pattern} // 'G3-95W15';

        my $p50_profile_name = $source_profile_base; $p50_profile_name =~ s/-95W/-50W/;

        # 2. Look up PhysC P50 and P95 from the assimilation map's CoreResults.
        my $physc_p50 = $vm_map_ref->{CoreResults}{ProfileValues}{$p50_profile_name};
        my $physc_p95 = $vm_map_ref->{CoreResults}{ProfileValues}{$source_profile_base};

        # 3. Calculate PhysC stability using the P95/P50 ratio.
        my $physc_stability_ratio = (defined $physc_p50 && looks_like_number($physc_p50) && $physc_p50 > 0.01) ? ((looks_like_number($physc_p95) ? $physc_p95 : 0) / $physc_p50) : 999;
        my $is_physc_stable = ($physc_stability_ratio < (1.0 + $STD_PHYSC_STABILITY_THRESH));

        # 4. Core STD pattern detection (all 3 conditions must be met).
        my $is_std_pattern = (defined $norm_p90_numeric && $norm_p90_numeric < $STD_NORM_P90_THRESH) && (defined $normrunq_iqrc_val && $normrunq_iqrc_val < $STD_IQRC_THRESH) && $is_physc_stable;
        $debug_info{'STDConfidenceChecks'} = sprintf("NormP90<%.1f(%.2f), IQRC<%.1f(%.3f), PhysC_Stable(P95/P50)<%.2f(%.2f)", $STD_NORM_P90_THRESH, ($norm_p90_numeric // 0), $STD_IQRC_THRESH, ($normrunq_iqrc_val // 0), (1.0 + $STD_PHYSC_STABILITY_THRESH), $physc_stability_ratio);

        # If pattern matched: This is a high-confidence inefficient workload.
        if ($is_std_pattern) {
            my $actual_reduction_num = (defined $debug_info{'EffActualReductionCores'} && looks_like_number($debug_info{'EffActualReductionCores'})) ? ($debug_info{'EffActualReductionCores'} + 0) : 0;

            # 5. Impact-based dampening.
            my $spinning_thread_impact_ratio = 1.0 / max(1, $effective_lcpus_for_pressure_calc);
            my $graduated_dampening_factor;
            my $tier_rationale;

            if ($spinning_thread_impact_ratio >= 0.20) {
                # High impact
                $graduated_dampening_factor = 0.85;
                $tier_rationale = sprintf("High Impact (%.1f%% of VM capacity)", $spinning_thread_impact_ratio * 100);
            } elsif ($spinning_thread_impact_ratio >= 0.10) {
                # Medium impact
                $graduated_dampening_factor = 0.70;
                $tier_rationale = sprintf("Medium Impact (%.1f%% of VM capacity)", $spinning_thread_impact_ratio * 100);
            } else {
                $graduated_dampening_factor = 0.30;
                $tier_rationale = sprintf("Low Impact (%.1f%% of VM capacity)", $spinning_thread_impact_ratio * 100);
            }
            my $dampened_reduction = $actual_reduction_num * $graduated_dampening_factor;
            $debug_info{'RunQ_Strategic'} = sprintf("%.4f", -$dampened_reduction);
            $skip_downsizing_reason = "Single-Threaded Dominant (STD) Workload Pattern Detected. Tactical downsizing skipped; updating strategic RunQ_Strategic recommendation.";
            $debug_info{'STDDampeningTier'} = sprintf("%s, L_eff=%.1f", $tier_rationale, $effective_lcpus_for_pressure_calc);
            $debug_info{'STDFinalDampeningFactor'} = $graduated_dampening_factor;
        } else {
            # PATTERN NOT MATCHED: This is a genuine burst. Block all downsizing.
            $skip_downsizing_reason = sprintf("Base PhysC (%.4f) > Entitlement (%.2f) and workload does not match STD pattern.", $eff_p_base_numeric, $curr_ent_numeric);
            $debug_info{'RunQ_Strategic'} = "0.0000";
        }
    }
    if (defined $skip_downsizing_reason) {
        $base_adjusted_physc = $eff_p_base_numeric;
        $efficiency_factor_numeric = 1.0;
        $debug_info{'DownsizingReason'} = "Skipped Applying Downsizing: " . $skip_downsizing_reason;
    } else {
        $base_adjusted_physc = $eff_p_base_numeric * $efficiency_factor_numeric;
        if ($efficiency_factor_numeric < 1 - 1e-4) {
            $debug_info{'DownsizingReason'} = "Analytical CPU Downsizing applied.";
        }
    }

    # Now, definitively set the final factor that was actually used.
    $debug_info{'DownsizingFactor'} = sprintf("%.2f", $efficiency_factor_numeric);
    $debug_info{'DownsizedPhysC'} = sprintf("%.4f", $base_adjusted_physc);

    # =========================
    # Section D: Tactical Additive CPU
    # =========================
    my $tier_scaling_factor = 1.00;
    # Default to Tier 3 (baseline, 1.00x scaling) if the tier cannot be determined.
    # This is a safe fallback that prevents uninitialised value warnings.
    my $tier_num_for_scaling = 3;
    my $is_runq_pressure = ($runq_pressure_p_val > $adaptive_runq_saturation_thresh);
    $debug_info{'IsRunQPressure'} = $is_runq_pressure ? "True" : "False";
    my $is_workload_pressure_calc = 0;
    my $workload_pressure_reason_str_calc = "Workload pressure conditions not met or inputs N/A.";
    my $min_absrunq_for_workload_pressure_check = $smt_used > 0 ? $smt_used : 1.0;
    if (defined $norm_p90_numeric) {
        if ($norm_p90_numeric > $WORKLOAD_PRESSURE_NORM_P90_TRIGGER_THRESHOLD) {
            if (defined $abs_runq_p_numeric && $abs_runq_p_numeric >= $min_absrunq_for_workload_pressure_check) {
                $is_workload_pressure_calc = 1;
                $workload_pressure_reason_str_calc = sprintf("NormRunQ P90 (%.2f) > threshold (%.2f) AND AbsRunQ (%s=%.2f) >= SMT-based min threshold (%.2f)", $norm_p90_numeric, $WORKLOAD_PRESSURE_NORM_P90_TRIGGER_THRESHOLD, $debug_info{AbsRunQKeyUsed}, $abs_runq_p_numeric, $min_absrunq_for_workload_pressure_check);
            } else {
                $workload_pressure_reason_str_calc = sprintf("NormRunQ P90 (%.2f) > threshold (%.2f), BUT AbsRunQ (%s=%.2f) < SMT-based min threshold (%.2f). Workload Pressure NOT flagged.", $norm_p90_numeric, $WORKLOAD_PRESSURE_NORM_P90_TRIGGER_THRESHOLD, $debug_info{AbsRunQKeyUsed}, $abs_runq_p_numeric // $na_str, $min_absrunq_for_workload_pressure_check);
            }
        } else {
            $workload_pressure_reason_str_calc = sprintf("NormRunQ P90 (%.2f) <= threshold (%.2f)", $norm_p90_numeric, $WORKLOAD_PRESSURE_NORM_P90_TRIGGER_THRESHOLD);
        }
    } else {
        $workload_pressure_reason_str_calc = "NormRunQ P90 N/A";
    }
    $debug_info{'IsWorkloadPressure'} = $is_workload_pressure_calc ? "True" : "False";
    $debug_info{'WorkloadPressureReason'} = $workload_pressure_reason_str_calc;


    # --- RunQ Design Philosophy ---
    # These three signals provide complementary views of CPU pressure at different
    # planning horizons, cleanly separating statistical measurement from business
    # policy. All values are ADDITIVE (delta cores), not absolute totals.
    #
    # RunQ_Potential: Pure Statistical Demand (Measurement)
    #   - Formula: excess_threads / SMT
    #   - No caps, no policy, no constraints applied
    #   - Answers: "What is the raw, unconstrained CPU demand that is not being met?"
    #   - Purpose: Baseline workload signal for trend analysis and anomaly detection
    #   - Time Horizon: Instantaneous snapshot of current pressure
    #
    # RunQ_Strategic: Architecturally Feasible Demand (Measurement)
    #   - Formula: min(RunQ_Potential, MaxCPU - BasePhysC)
    #   - Capped ONLY by available architectural headroom before hitting MaxCPU
    #   - No entitlement constraints, no tier policy, no volatility dampening
    #   - Answers: "What is the maximum demand that could be met within the VM's
    #              current architectural limits?"
    #   - Purpose: Identify VMs that are architecturally constrained (hitting MaxCPU)
    #   - Time Horizon: Quarterly planning cycle (3-6 months)
    #   - Diagnostic: If Tactical > Strategic, MaxCPU must be raised to implement
    #                 the recommendation
    #
    # RunQ_Tactical: Actionable Monthly Increment (Recommendation)
    #   - Formula: Raw demand passed through all safety gates and policy adjustments
    #   - Applies: EntCap (tier-aware), volatility factor, pool constraints,
    #              tier scaling, final adaptive safety cap
    #   - Answers: "What is the single, safe, incremental change I should make
    #              this month to begin addressing the pressure?"
    #   - Purpose: Safe, implementable monthly entitlement adjustment
    #   - Time Horizon: Monthly operational cycle (30 days)
    #
    # CRITICAL DESIGN PRINCIPLES:
    #   1. Potential and Strategic are MEASUREMENTS (no business policy)
    #   2. Tactical is a RECOMMENDATION (all business policy applied)
    #   3. Never contaminate Potential/Strategic with tier classifications or
    #      operational constraints (entitlement) - these are statistical signals
    #   4. All three signals are additive deltas for direct comparability
    #   5. The relationship Tactical â‰¤ Strategic â‰¤ Potential should hold under
    #      normal conditions (violations indicate architectural constraints)
    # =============================================================================

    my $apply_additive_logic = ($is_runq_pressure || $is_workload_pressure_calc);
    my $additive_cpu = 0.0;
    my $raw_additive_cpu = 0.0;
    my $max_additive_cap_sliding = 0.0;
    my $capped_raw_additive_val = 0.0;
    my $volatility_confidence_factor = 1.0;
    my $pool_confidence_factor = 1.0;
    my $excess_threads = 0;

    # The additive logic path is only entered if significant pressure has been detected.
    if ($apply_additive_logic && defined $smt_used && $smt_used > 0)
    {
        # Calculate how many threads are waiting above a tolerated ceiling.
        # The tolerance factor allows for a healthy, non-problematic queue before recommending upsizing.

        # --- Tier-Aware Tolerance Factor ---
        # Select the tolerance for calculating "excess threads" based on the VM's tier.
        # Performance-critical tiers have a much lower tolerance for queuing.
        my $tolerance_factor = $RUNQ_ADDITIVE_TOLERANCE_FACTOR; # Default to 1.80

        if ($final_tier eq 'P') {
            # Extreme Performance (e.g., VIO Servers) - virtually no tolerance for queuing.
            $tolerance_factor = 1.10;
        } elsif ($final_tier =~ /^O1$/) {
            # High Performance (e.g., Core OLTP DBs) - very low tolerance.
            $tolerance_factor = 1.25;
        } elsif ($final_tier =~ /^O2$|^G1$|^G2$/) {
            # Balanced Performance (e.g., Web Apps, General DBs) - moderate tolerance.
            $tolerance_factor = 1.50;
        }
        # Efficiency Tiers (G3, G4, B1-B4) will use the default 1.80.

        my $capacity_threshold_for_excess = $tolerance_factor * $effective_lcpus_for_pressure_calc;

        $excess_threads = (defined $abs_runq_p_numeric ? $abs_runq_p_numeric : 0) - $capacity_threshold_for_excess;
        if ($excess_threads > 0)
        {
            # Convert the excess threads into a raw CPU core requirement. This is the unconstrained demand.
            $debug_info{'ExcessThreads'} = sprintf( "\n         1. Observed Run Queue (%s) : %.2f threads\n". "         2. Tolerated Capacity               : %.1f LCPUs * %.2f (Tolerance Factor) = %.2f threads\n". "         => Excess Threads to Service        : %.2f - %.2f = %.2f", $debug_info{AbsRunQKeyUsed}, ($abs_runq_p_numeric // 0.0), $effective_lcpus_for_pressure_calc, $tolerance_factor, $capacity_threshold_for_excess, ($abs_runq_p_numeric // 0.0), $capacity_threshold_for_excess, $excess_threads);
            $raw_additive_cpu = $excess_threads / $smt_used if $smt_used > 0;

            # Determine the business policy scaling factor based on the VM's final tier.
            if (defined $final_tier && $final_tier =~ /(\d)$/) {
                $tier_num_for_scaling = $1;
            } elsif (defined $final_tier && $final_tier eq 'P') {
                $tier_num_for_scaling = 1;
            }

            if (defined $tier_num_for_scaling) {
                my %tier_upsizing_factors = (
                    1 => 1.30, 2 => 1.15, 3 => 1.00, 4 => 0.95
                );
                $tier_scaling_factor = $tier_upsizing_factors{$tier_num_for_scaling} // 1.00;
            }

            # --- Apply the Tier-Aware Entitlement-Based Cap (EntCap) ---
            # This is the first major safety gate. It limits the upsizing recommendation to a proportion
            # of the VM's current size, enforcing incremental change. Performance tiers get more aggressive caps.
            # IMPORTANT: This cap is applied ONLY to the TACTICAL path.
            my ($max_add_abs1, $max_add_perc2, $max_add_perc3, $max_add_perc_else);

            # Determine which tier group the VM falls into
            if ($final_tier =~ /^[PO]1$|^[PO]2$|^G1$|^G2$/) {
                # Performance Tiers (Aggressive Response)
                $max_add_abs1      = 2.0;  # Allow up to +2.0 cores for small critical VMs
                $max_add_perc2     = 1.50; # 150% of Ent
                $max_add_perc3     = 1.00; # 100% of Ent
                $max_add_perc_else = 0.75; # 75% of Ent
            } else {
                # Efficiency Tiers (Conservative, Original Logic)
                $max_add_abs1      = 1.0;  # 1.0 core
                $max_add_perc2     = 1.00; # 100% of Ent
                $max_add_perc3     = 0.75; # 75% of Ent
                $max_add_perc_else = 0.50; # 50% of Ent
            }

            if ($curr_ent_numeric < 1.0)      { $max_additive_cap_sliding = $max_add_abs1; }
            elsif ($curr_ent_numeric < 2.0)   { $max_additive_cap_sliding = $curr_ent_numeric * $max_add_perc2; }
            elsif ($curr_ent_numeric < 4.0)   { $max_additive_cap_sliding = $curr_ent_numeric * $max_add_perc3; }
            else                               { $max_additive_cap_sliding = $curr_ent_numeric * $max_add_perc_else; }
            if ($curr_ent_numeric == 0 && $max_additive_cap_sliding == 0) { $max_additive_cap_sliding = $max_add_abs1; }

            # The EntCap is applied to the raw demand to begin the TACTICAL calculation path.
            $capped_raw_additive_val = min($raw_additive_cpu, $max_additive_cap_sliding);

            # --- Heuristic Checks for Pathological Workloads (HTW, DBW, etc.) ---
            # These checks identify workloads where simply adding CPU is inefficient or incorrect.
            $debug_info{'HotThreadWLDampeningApplied'} = "False"; $debug_info{'HotThreadWLConditionsString'} = "N/A";
            $debug_info{'HotThreadWLDynamicFactor'} = "N/A"; $debug_info{'HotThreadWLDampenedAdditiveFrom'} = "N/A";
            $debug_info{'HotThreadWLDampenedAdditiveTo'} = "N/A";
            if ($capped_raw_additive_val > $FLOAT_EPSILON)
            {
                my @htw_conditions_met_details; my $htw_condition_count = 0;
                if ($is_workload_pressure_calc) { push @htw_conditions_met_details, "HighNormP90"; $htw_condition_count++; }
                my $cond2_underutilized = 0;
                if (defined $base_physc && $base_physc > $FLOAT_EPSILON) {
                    my $underutilized_vs_ent = (defined $curr_ent_numeric && $curr_ent_numeric > $FLOAT_EPSILON && $base_physc < ($curr_ent_numeric * $HOT_THREAD_WL_ENT_FACTOR));
                    my $underutilized_vs_maxcpu = (defined $max_cpu_for_lpar_numeric && $max_cpu_for_lpar_numeric > $FLOAT_EPSILON && $base_physc < ($max_cpu_for_lpar_numeric * $HOT_THREAD_WL_MAXCPU_FACTOR));
                    if ($underutilized_vs_ent || $underutilized_vs_maxcpu) {
                        $cond2_underutilized = 1;
                        my $detail_ent_str = $underutilized_vs_ent ? sprintf("BaseP(%.2f)<Ent(%.2f)*%.1f", $base_physc, $curr_ent_numeric // 0, $HOT_THREAD_WL_ENT_FACTOR) : "";
                        my $detail_max_str = $underutilized_vs_maxcpu ? sprintf("BaseP(%.2f)<MaxP(%.2f)*%.1f", $base_physc, $max_cpu_for_lpar_numeric // 0, $HOT_THREAD_WL_MAXCPU_FACTOR) : "";
                        push @htw_conditions_met_details, "UnderutilizedCap(" . join(" or ", grep { $_ ne "" } $detail_ent_str, $detail_max_str) . ")";
                        $htw_condition_count++;
                    }
                }
                if (defined $norm_p50_numeric && $norm_p50_numeric > $HOT_THREAD_WL_HIGH_NORM_P50_THRESHOLD) { push @htw_conditions_met_details, sprintf("HighNormP50(%.2f>%.1f)", $norm_p50_numeric, $HOT_THREAD_WL_HIGH_NORM_P50_THRESHOLD); $htw_condition_count++; }
                if (!$is_runq_pressure && defined $debug_info{'RunQPressure_P90_Val'} && $debug_info{'RunQPressure_P90_Val'} ne $na_str) { push @htw_conditions_met_details, sprintf("NoLPARRunQSat(AbsPVal:%.2f<%.1f)", ($debug_info{'RunQPressure_P90_Val'} + 0), $RUNQ_PRESSURE_P90_SATURATION_THRESHOLD); $htw_condition_count++; }
                if (defined $normrunq_iqrc_val && abs($normrunq_iqrc_val) > $HOT_THREAD_WL_IQRC_THRESHOLD) { push @htw_conditions_met_details, sprintf("HighIQRC(%.2f>%.1f)", $normrunq_iqrc_val, $HOT_THREAD_WL_IQRC_THRESHOLD); $htw_condition_count++; }
                my $hot_thread_wl_conditions_met_str = @htw_conditions_met_details ? join("; ", @htw_conditions_met_details) : "No specific conditions met";
                if ($htw_condition_count >= $HOT_THREAD_WL_DETECTION_MIN_CONDITIONS_MET)
                {
                    my $util_ratio = (defined $max_cpu_for_lpar_numeric && $max_cpu_for_lpar_numeric > $FLOAT_EPSILON && defined $base_physc) ? ($base_physc / $max_cpu_for_lpar_numeric) : 1.0;
                    my $util_damp_multiplier = max(0.1, min(1.0, $util_ratio));
                    my $iqrc_damp_multiplier = (defined $normrunq_iqrc_val) ? min(1.0, 1.0 / (1.0 + abs($normrunq_iqrc_val))) : 1.0;
                    my $base_physc_severity_multiplier = (defined $base_physc && $base_physc > $FLOAT_EPSILON) ? min(1.0, $base_physc / 1.0) : 0.1;
                    my $calculated_dynamic_damp_factor = $HOT_THREAD_WL_BASE_DAMPENING_FACTOR * $util_damp_multiplier * $iqrc_damp_multiplier * $base_physc_severity_multiplier;
                    my $final_dynamic_dampening_factor_calc = max($HOT_THREAD_WL_MIN_DYNAMIC_DAMPENING, min($HOT_THREAD_WL_MAX_DYNAMIC_DAMPENING, $calculated_dynamic_damp_factor));
                    my $original_additive_val = $capped_raw_additive_val;
                    $capped_raw_additive_val *= $final_dynamic_dampening_factor_calc;
                    $debug_info{'HotThreadWLDampeningApplied'} = "True";
                    $debug_info{'HotThreadWLConditionsString'} = $hot_thread_wl_conditions_met_str;
                    $debug_info{'HotThreadWLDynamicFactor'} = sprintf("%.4f (Base:%.2f UtilM:%.2f IqrcM:%.2f SevM:%.2f -> RawCalc:%.4f)", $final_dynamic_dampening_factor_calc, $HOT_THREAD_WL_BASE_DAMPENING_FACTOR, $util_damp_multiplier, $iqrc_damp_multiplier, $base_physc_severity_multiplier, $calculated_dynamic_damp_factor);
                    $debug_info{'HotThreadWLDampenedAdditiveFrom'} = sprintf("%.4f", $original_additive_val);
                    $debug_info{'HotThreadWLDampenedAdditiveTo'}   = sprintf("%.4f", $capped_raw_additive_val);
                    $debug_info{'CappedRawAdditive'} = sprintf("%.4f", $capped_raw_additive_val);
                } else {
                    $debug_info{'HotThreadWLDampeningApplied'} = "False";
                    $debug_info{'HotThreadWLConditionsString'} = sprintf("Conditions not met for HTW dampening (%d/5 met: %s).", $htw_condition_count, $hot_thread_wl_conditions_met_str);
                }
            }
            if ($is_runq_pressure) {
                $volatility_confidence_factor = $RUNQ_PRESSURE_SATURATION_CONFIDENCE_FACTOR;
                $debug_info{'VoltFactorReason'} = sprintf("RunQPressure Saturation (Factor set to %.2f)", $RUNQ_PRESSURE_SATURATION_CONFIDENCE_FACTOR);
            } elsif ($is_workload_pressure_calc && defined $norm_p50_numeric && defined $norm_p90_numeric && $norm_p90_numeric > 0.01) {
                my $volatility_ratio_for_factor = ($norm_p50_numeric > 0.01) ? ($norm_p90_numeric / $norm_p50_numeric) : 999;
                if ($volatility_ratio_for_factor < $VOLATILITY_SPIKY_THRESHOLD) { $volatility_confidence_factor = $VOLATILITY_SPIKY_FACTOR; }
                elsif ($volatility_ratio_for_factor < $VOLATILITY_MODERATE_THRESHOLD){ $volatility_confidence_factor = $VOLATILITY_MODERATE_FACTOR; }
                else { $volatility_confidence_factor = 1.0; }
                $debug_info{'VoltFactorReason'} = sprintf("Calculated (NormRQ P90/P50 ratio %.2f for WorkloadPressure -> Factor %.2f)", $volatility_ratio_for_factor, $volatility_confidence_factor);
            } else {
                $debug_info{'VoltFactorReason'} = "Additive logic applied, but conditions for specific Volatility Factor adjustment not met (e.g., WorkloadPressure False or P50/P90 N/A for ratio). Using default factor.";
            }
            $additive_cpu = $capped_raw_additive_val * $volatility_confidence_factor;
            if ($is_in_non_default_pool && $additive_cpu > 0) {
                $pool_confidence_factor = $POOL_CONSTRAINT_CONFIDENCE_FACTOR;
                $additive_cpu *= $pool_confidence_factor;
            }

            # --- FINAL SIGNAL ASSIGNMENT ---
            my $sanity_bound = (defined $max_cpu_for_lpar_numeric && $max_cpu_for_lpar_numeric > 0) ? ($max_cpu_for_lpar_numeric * 2.0) : 10.0;

            # 1. Potential is the pure, unscaled, raw demand.
            $debug_info{'RunQ_Potential'} = $raw_additive_cpu;

            # 2. RunQ_Strategic: Architecturally feasible demand, capped only by MaxCPU.
            # formula: min(RunQ_Potential, MaxCPU - BasePhysC)
            my $strategic_val = $raw_additive_cpu;
            if ($max_cpu_for_lpar_numeric > 0) {
                my $available_headroom = $max_cpu_for_lpar_numeric - $base_physc;
                $strategic_val = min($raw_additive_cpu, $available_headroom);
            }
            $debug_info{'RunQ_Strategic'} = $strategic_val;

            # 3. Check for pathological state (Signal Inversion)
            if ($raw_additive_cpu > $sanity_bound) {
                # Handle pathological state (Signal Inversion)
                $debug_info{'RunQ_Potential'} = "ANOMALY (XRQ)";
                $debug_info{'RunQ_Strategic'} = $max_cpu_for_lpar_numeric;
                $debug_info{'ReasonForSignalInversion'} = sprintf(
                    "Raw Additive CPU (%.2f) exceeded sanity bound (%.2f), indicating a probable system backlog or malfunction.",
                    $raw_additive_cpu, $sanity_bound
                );
            }

      } else {
            $debug_info{'ExcessThreads'} = sprintf("0.0000 (No excess above tolerated capacity of %.2f from %s; AbsRunQ %s was %.2f)", $capacity_threshold_for_excess, ($pressure_basis_rationale =~ /Burst-Tolerant/ ? "Entitlement+Burst" : "MaxCPU"), $debug_info{AbsRunQKeyUsed}, ($abs_runq_p_numeric // $na_str));
            $raw_additive_cpu = 0.0;
            $capped_raw_additive_val = 0.0;
            $additive_cpu = 0.0;
            $debug_info{'VoltFactorReason'} = "No excess threads, so no additive CPU calculated.";
            $debug_info{'RunQ_Strategic'} = "0.0000";
            $debug_info{'RunQ_Potential'} = "0.0000";
        }
    } else {
        $debug_info{'ExcessThreads'} = "N/A (Additive logic not applied as no significant pressure detected or missing inputs)";
        $debug_info{'VoltFactorReason'} = "Additive logic not applied.";
    }
    $debug_info{'RawAdditive'}       = sprintf("%.4f", $raw_additive_cpu);
    $debug_info{'MaxAdditiveCap'}    = sprintf("%.4f", $max_additive_cap_sliding);
    $debug_info{'CappedRawAdditive'} = sprintf("%.4f", $capped_raw_additive_val);
    $debug_info{'VoltFactor'}        = sprintf("%.2f", $volatility_confidence_factor);
    $debug_info{'PoolFactor'}        = sprintf("%.2f", $pool_confidence_factor);
    my $dbw_dampening_applied = "False";
    my $cond1_low_util = (defined $base_physc && $base_physc < ($curr_ent_numeric * $DBW_LOW_UTIL_FACTOR));
    my $dsr = 0;
    if (defined $base_physc && $base_physc > 0.01 && defined $abs_runq_p_numeric) { $dsr = $abs_runq_p_numeric / ($base_physc * $smt_used); }
    my $cond2_high_dsr = ($dsr > $DBW_DSR_THRESHOLD);
    my $cond3_high_median_pressure = (defined $norm_p50_numeric && $norm_p50_numeric > $DBW_MEDIAN_PRESSURE_THRESHOLD);
    if ($cond1_low_util && $cond2_high_dsr && $cond3_high_median_pressure) {
        my $original_additive = $additive_cpu;
        $debug_info{'RunQ_Potential'} = "ANOMALY (DBW)";
        $additive_cpu *= 0.25;
        $dbw_dampening_applied = sprintf("True (75%% dampening applied).\n                                               Additive CPU reduced from %.4f to %.4f.\n                                               Conditions met: Low Util (BaseP %.2f < %.2f Ent * %.2f), High DSR (%.1f > %.1f), High Median Pressure (NormP50 %.2f > %.1f)", $original_additive, $additive_cpu, $base_physc, $curr_ent_numeric, $DBW_LOW_UTIL_FACTOR, $dsr, $DBW_DSR_THRESHOLD, $norm_p50_numeric, $DBW_MEDIAN_PRESSURE_THRESHOLD);
    }
    $debug_info{'DBWDampeningApplied'} = $dbw_dampening_applied;

    # This is the final and most important safety cap. It prevents recommendations from violating
    # the VM's hard limits (MaxCPU) while allowing for "Intelligent Headroom" based on pressure severity.
    # --- Adaptive Safety Hard-Cap with Intelligent Headroom ---
    my $original_additive_before_safety_cap = $additive_cpu;
    my $additive_safety_cap_applied_reason = "Not applied";

    # Step 1: Calculate Pressure Severity to determine Virtual Headroom
    my $pressure_ratio = ($effective_lcpus_for_pressure_calc > 0) ? (($excess_threads // 0) / $effective_lcpus_for_pressure_calc) : 0;
    my $virtual_headroom_factor = 0;
    if ($apply_additive_logic) { # Only apply headroom if there's pressure
        if ($pressure_ratio >= 1.0) {
            $virtual_headroom_factor = 0.50; # 50% for Severe Pressure
        } else {
            $virtual_headroom_factor = 0.25; # 25% for Significant Pressure
        }
    }
    my $virtual_max_cpu = $max_cpu_for_lpar_numeric * (1 + $virtual_headroom_factor);

    # Step 2: Calculate the Primary Dynamic Cap using Virtual Headroom
    my $cap_from_ent = $curr_ent_numeric * 0.40;
    my $headroom = ($virtual_max_cpu > $curr_ent_numeric) ? ($virtual_max_cpu - $curr_ent_numeric) : 0;
    my $cap_from_headroom = $headroom * 0.50;
    my $primary_cap = ($headroom > 0) ? min($cap_from_ent, $cap_from_headroom) : $cap_from_ent;

    # The Pressure Gate moderates the cap for borderline cases.
    # If pressure is only moderate, we apply a small dampening to the cap.
    # For significant or severe pressure, we apply the full cap without dampening.
    my $pressure_gate_modifier = 1.0;
    if ($pressure_ratio < 0.5) {
        $pressure_gate_modifier = 0.75;
    }

    my $gated_cap = $primary_cap * $pressure_gate_modifier;
    my $relative_floor = 0.02 * max($curr_ent_numeric, 1.0);
    my $final_safety_cap_value = max($relative_floor, $gated_cap);
    $final_safety_cap_value = min($final_safety_cap_value, 2.0);
    my $cap_rationale = sprintf("min(EntCap=%.2f, HeadCap=%.2f) * PresGate=%.2f -> floor/ceil -> %.4f", $cap_from_ent, $cap_from_headroom, $pressure_gate_modifier, $final_safety_cap_value);
    if ($additive_cpu > $final_safety_cap_value) {
        $additive_cpu = $final_safety_cap_value;
        $additive_safety_cap_applied_reason = sprintf("True: Additive CPU hard-capped to %.4f (%s)", $additive_cpu, $cap_rationale);
    } else {
        $additive_safety_cap_applied_reason = sprintf("False (Additive %.4f within adaptive hard cap of %.4f from %s)", $original_additive_before_safety_cap, $final_safety_cap_value, $cap_rationale);
    }
    $debug_info{'AdditiveSafetyCapApplied'} = $additive_safety_cap_applied_reason;

    # --- Final Tier-Aware Scaling for the Tactical Recommendation ---
    # This is the last step, applying the business policy (tier importance) to the final, safety-vetted additive value.
    if ($tier_scaling_factor != 1.00 && $additive_cpu > 0) {
        my $pre_scaled_additive = $additive_cpu;
        $additive_cpu *= $tier_scaling_factor;

        # Reconstruct the rationale for logging
        my $increase_pct = ($tier_scaling_factor - 1.0) * 100;
        my $rationale_text = sprintf(
            "Tier %d: Enhanced upsizing (+%.0f%%) for lower latency tolerance. Base: %.3f -> Scaled: %.3f cores",
            $tier_num_for_scaling, $increase_pct, $pre_scaled_additive, $additive_cpu
        );

        $debug_info{'TierScalingFactor'}        = sprintf("%.2f", $tier_scaling_factor);
        $debug_info{'TierNumber'}               = $tier_num_for_scaling;
        $debug_info{'TierScalingRationale'}     = $rationale_text;
        $debug_info{'AdditiveCPU_PreTierScaling'}  = sprintf("%.4f", $pre_scaled_additive);
        $debug_info{'AdditiveCPU_PostTierScaling'} = sprintf("%.4f", $additive_cpu);
    }

    my $final_tactical_modifier = 0;
    if ($additive_cpu > $FLOAT_EPSILON) {
        $final_tactical_modifier = $additive_cpu;
    } elsif ($efficiency_factor_numeric < (1.0 - $FLOAT_EPSILON)) {
        my $actual_reduction = (defined $debug_info{'EffActualReductionCores'} && looks_like_number($debug_info{'EffActualReductionCores'})) ? ($debug_info{'EffActualReductionCores'} + 0) : 0;
        $final_tactical_modifier = -1 * $actual_reduction;
    }
    $debug_info{'RunQ_Tactical'} = $final_tactical_modifier;
    $debug_info{'FinalAdditive'} = sprintf("%.4f", $final_tactical_modifier);

    # =========================
    # Final Synthesis + MaxCPU
    # =========================
    my $calculated_demand = $base_adjusted_physc + $additive_cpu;
    my $runq_modified_rec = $calculated_demand;
    $debug_info{'PreMaxCpuCapRec'} = sprintf("%.4f", $runq_modified_rec);
    my $forecast_multiplier_val = 1.25;
    if ($curr_ent_numeric < 0.5)    { $forecast_multiplier_val = 2.5; }
    elsif ($curr_ent_numeric < 1.0) { $forecast_multiplier_val = 2.0; }
    elsif ($curr_ent_numeric < 2.0) { $forecast_multiplier_val = 1.75; }
    elsif ($curr_ent_numeric < 4.0) { $forecast_multiplier_val = 1.5; }
    $debug_info{'ForecastMultiplier'} = $forecast_multiplier_val;
    my $effective_max_cpu_cap_val = ($max_cpu_for_lpar_numeric > 0) ? ($max_cpu_for_lpar_numeric * $forecast_multiplier_val) : undef;
    $debug_info{'EffectiveMaxCPUCap'} = defined($effective_max_cpu_cap_val) ? sprintf("%.4f", $effective_max_cpu_cap_val) : $na_str;
    if (defined $effective_max_cpu_cap_val && $calculated_demand > $effective_max_cpu_cap_val) {
        $runq_modified_rec = $effective_max_cpu_cap_val;
        $debug_info{'CappedByMaxCPU'} = "True";
    } else {
        $debug_info{'CappedByMaxCPU'} = (defined $effective_max_cpu_cap_val) ? "False" : "N/A (No LPAR MaxCPU for cap check or MaxCPU not exceeded)";
    }
    if ($runq_modified_rec < 0) { $runq_modified_rec = 0; }
    $debug_info{'FinalAdjustedPhysC'} = sprintf("%.4f", $runq_modified_rec);

    return ($runq_modified_rec, \%debug_info);
}

# --- generate_sizing_hint (Unified Global Pressure Detection with Logging Rationale) ---
# Generates a sizing tier hint, pattern, and overall pressure indication for a VM.
# Also returns a detailed rationale string for its pressure assessment, AND
# specific boolean flags for P-99W1's RunQ pressure conditions.
sub generate_sizing_hint
{
    # This function is now refactored to read all its inputs from the assimilation map.
    my ($vm_map_ref, $log_fh, $adaptive_saturation_thresh) = @_;

    # --- Unpack all required values from the assimilation map ---
    my $vm_name                  = $vm_map_ref->{Configuration}{vm_name}; # We will add vm_name to the map
    my $config_ref               = $vm_map_ref->{Configuration};
    my $results_ref              = $vm_map_ref->{CoreResults}{ProfileValues};
    my $runq_metrics_ref         = $vm_map_ref->{RunQMetrics};
    my $max_cpu_for_vm_numeric   = $config_ref->{max_cpu} // 0;
    my $smt_used_for_vm_numeric  = $config_ref->{smt} // $default_smt_arg;
    my $LOG_FH                   = $log_fh;

    my $na_str_hint = "N/A";
    my @global_pressure_rationale_lines;

    # ... (initial part of rationale logging: Inputs like MaxCPU, SMT etc. - as in previous version) ...
    push @global_pressure_rationale_lines, sprintf("  Input LPAR MaxCPU            : %.2f cores", $max_cpu_for_vm_numeric);
    push @global_pressure_rationale_lines, sprintf("  Input SMT for VM             : %d", $smt_used_for_vm_numeric);
    # Determine if small entitlement handler would be active
    my $vm_entitlement = $config_ref->{'entitlement'} // 0;
    my $is_small_ent_handler = ($vm_entitlement > 0 && $vm_entitlement < 1.0 &&
                                 $max_cpu_for_vm_numeric > $vm_entitlement) ? "Yes" : "No";
    my $burst_used = ($vm_entitlement < 1.0 && $is_small_ent_handler eq "Yes") ?
                     calculate_graduated_burst($vm_entitlement) : 0.25;

    push @global_pressure_rationale_lines, sprintf("  Small Entitlement Handler    : %s", $is_small_ent_handler);
    push @global_pressure_rationale_lines, sprintf("  Burst Allowance Factor       : %.0f%% (Ent=%.2f)",
                                                   $burst_used * 100, $vm_entitlement);

    # --- VIO Server Check ---
    my $is_vio_server = 0;
    if (defined $config_ref->{systemtype} && $config_ref->{systemtype} =~ /VIO Server/i) {
       $is_vio_server = 1;
    }

    # --- Profile Value Parsing (Pattern/Peakiness & P-99W1 PhysC) ---
    my $o3_val_str = $results_ref->{'O3-95W15'} // "0";
    my $o3_val_num = ($o3_val_str ne $na_str_hint && $o3_val_str =~ /^-?[0-9.]+\z/) ? ($o3_val_str + 0) : 0;
    my $b3_val_str = $results_ref->{'B3-95W15'} // "0";
    my $b3_val_num = ($b3_val_str ne $na_str_hint && $b3_val_str =~ /^-?[0-9.]+\z/) ? ($b3_val_str + 0) : 0;
    my $g3_val_str = $results_ref->{'G3-95W15'} // "0";
    my $g3_val_num = ($g3_val_str ne $na_str_hint && $g3_val_str =~ /^-?[0-9.]+\z/) ? ($g3_val_str + 0) : 0;
    my $p99w1_physc_val_str = looks_like_number($results_ref->{$MANDATORY_PEAK_PROFILE_FOR_HINT}) ? sprintf("%.3f", $results_ref->{$MANDATORY_PEAK_PROFILE_FOR_HINT}) : $na_str_hint;
    my $p99w1_physc_val_num = ($p99w1_physc_val_str ne $na_str_hint && $p99w1_physc_val_str =~ /^-?[0-9.]+\z/)
    ? ($p99w1_physc_val_str + 0)
    : 0;
    push @global_pressure_rationale_lines, sprintf("  Input %s PhysC Value     : %s (from nfit output for %s)", $MANDATORY_PEAK_PROFILE_FOR_HINT, $p99w1_physc_val_str, $MANDATORY_PEAK_PROFILE_FOR_HINT);

    # (Pattern and Peakiness logic - unchanged)
    my $suggested_pattern = "G";
    if ($b3_val_num > 0.01 && $o3_val_num > ($b3_val_num * $PATTERN_RATIO_THRESHOLD)) { $suggested_pattern = "O"; }
    elsif ($o3_val_num > 0.01 && $b3_val_num > ($o3_val_num * $PATTERN_RATIO_THRESHOLD)) { $suggested_pattern = "B"; }
    my $peakiness_ratio = ($g3_val_num > 0.001) ? ($p99w1_physc_val_num / $g3_val_num) : 0;
    my $shape_descriptor = "Steady";
    if ($peakiness_ratio >= $HIGH_PEAK_RATIO_THRESHOLD) { $shape_descriptor = "Very Peaky"; }
    elsif ($peakiness_ratio >= $LOW_PEAK_RATIO_THRESHOLD) { $shape_descriptor = "Moderately Peaky"; }


    # --- Unified Pressure Detection ---
    my $pressure_detected_maxcpu_limit = 0;
    # Specific P-99W1 RunQ pressure flags to be returned
    my $p99w1_has_absolute_runq_pressure = 0;
    my $p99w1_has_normalized_runq_pressure = 0;
    my @pressure_points;

    # Fetch P-99W1's specific RunQ metrics
    my $p99w1_abs_runq_p90_val_str = looks_like_number($runq_metrics_ref->{'AbsRunQ_P90'}) ? sprintf("%.3f", $runq_metrics_ref->{'AbsRunQ_P90'}) : $na_str_hint;
    my $p99w1_norm_runq_p90_val_str = looks_like_number($runq_metrics_ref->{'NormRunQ_P90'}) ? sprintf("%.3f", $runq_metrics_ref->{'NormRunQ_P90'}) : $na_str_hint;

    push @global_pressure_rationale_lines, sprintf("  Input %s AbsRunQ P90     : %s threads (from nfit output for %s)", $MANDATORY_PEAK_PROFILE_FOR_HINT, $p99w1_abs_runq_p90_val_str, $MANDATORY_PEAK_PROFILE_FOR_HINT);
    push @global_pressure_rationale_lines, sprintf("  Input %s NormRunQ P90    : %s (from nfit output for %s)", $MANDATORY_PEAK_PROFILE_FOR_HINT, $p99w1_norm_runq_p90_val_str, $MANDATORY_PEAK_PROFILE_FOR_HINT);
    push @global_pressure_rationale_lines, "";


    # 1. MaxCPU Limit Pressure
    # ... (logic as before, sets $pressure_detected_maxcpu_limit, logs to @global_pressure_rationale_lines) ...
    # ... (ensure push @pressure_points, "MaxCPU"; happens if $pressure_detected_maxcpu_limit = 1;) ...
    push @global_pressure_rationale_lines, sprintf("  1. MaxCPU Limit Pressure Check (%s PhysC vs LPAR MaxCPU):", $MANDATORY_PEAK_PROFILE_FOR_HINT);
    # ... (detailed logging lines for MaxCPU check)
    my $maxcpu_limit_calc_threshold = ($max_cpu_for_vm_numeric > 0) ? ($max_cpu_for_vm_numeric * $LIMIT_THRESHOLD_PERC) : 0;
    my $maxcpu_condition_met_str = "FALSE";
    if ($max_cpu_for_vm_numeric > 0 && $p99w1_physc_val_num >= $maxcpu_limit_calc_threshold)
    {
        $pressure_detected_maxcpu_limit = 1;
        $maxcpu_condition_met_str = "TRUE";
        push @pressure_points, "MaxCPU";
    }
    push @global_pressure_rationale_lines, sprintf("     - %s PhysC Value      : %.2f", $MANDATORY_PEAK_PROFILE_FOR_HINT, $p99w1_physc_val_num);
    push @global_pressure_rationale_lines, sprintf("     - LPAR MaxCPU             : %.2f", $max_cpu_for_vm_numeric);
    push @global_pressure_rationale_lines, sprintf("     - Threshold (>= %.0f%%)      : %.2f cores", $LIMIT_THRESHOLD_PERC * 100, $maxcpu_limit_calc_threshold);
    push @global_pressure_rationale_lines, sprintf("     - Condition Met           : (%.2f >= %.2f) -> %s", $p99w1_physc_val_num, $maxcpu_limit_calc_threshold, $maxcpu_condition_met_str);
    push @global_pressure_rationale_lines, sprintf("     - MaxCPU Pressure Flag    : %s", $maxcpu_condition_met_str);
    push @global_pressure_rationale_lines, "";


    # 2. Absolute RunQ Pressure (using P-99W1's AbsRunQ_P90)
    # ... (logic as before, sets $p99w1_has_absolute_runq_pressure, logs to @global_pressure_rationale_lines) ...
    # ... (ensure push @pressure_points, ... happens if $p99w1_has_absolute_runq_pressure = 1;) ...
    my $p99w1_abs_runq_p90_num = ($p99w1_abs_runq_p90_val_str ne $na_str_hint && $p99w1_abs_runq_p90_val_str =~ /^-?[0-9.]+$/) ? ($p99w1_abs_runq_p90_val_str + 0) : undef;
    my $calculated_abs_runq_pressure_ratio = 0;
    my $lpar_max_lcpu_capacity = ($max_cpu_for_vm_numeric > 0 && $smt_used_for_vm_numeric > 0) ? ($max_cpu_for_vm_numeric * $smt_used_for_vm_numeric) : 0;
    push @global_pressure_rationale_lines, sprintf("  2. Absolute RunQ Pressure Check (%s AbsRunQ P90 vs LPAR Capacity):", $MANDATORY_PEAK_PROFILE_FOR_HINT);
    # ... (detailed logging lines for AbsRunQ check)
    if (defined $p99w1_abs_runq_p90_num && $lpar_max_lcpu_capacity > 0)
    {
        $calculated_abs_runq_pressure_ratio = $p99w1_abs_runq_p90_num / $lpar_max_lcpu_capacity;
    }
    my $absrunq_cond_met_str = "FALSE";
    if ($calculated_abs_runq_pressure_ratio > $adaptive_saturation_thresh)
    {
        $p99w1_has_absolute_runq_pressure = 1; # Set the specific flag
        $absrunq_cond_met_str = "TRUE";
        push @pressure_points, sprintf("RunQAbs_%s(P90=%.2f)", $MANDATORY_PEAK_PROFILE_FOR_HINT, $calculated_abs_runq_pressure_ratio);
    }
    push @global_pressure_rationale_lines, sprintf("     - %s AbsRunQ P90      : %s threads", $MANDATORY_PEAK_PROFILE_FOR_HINT, $p99w1_abs_runq_p90_val_str);
    push @global_pressure_rationale_lines, sprintf("     - LPAR Max LCPU Capacity  : (%.2f MaxCPU * %d SMT) = %.2f threads", $max_cpu_for_vm_numeric, $smt_used_for_vm_numeric, $lpar_max_lcpu_capacity);
    push @global_pressure_rationale_lines, sprintf("     - Calculated Ratio        : %.4f", $calculated_abs_runq_pressure_ratio);
    push @global_pressure_rationale_lines, sprintf("     - Threshold               : > %.2f", $adaptive_saturation_thresh);
    push @global_pressure_rationale_lines, sprintf("     - Condition Met           : (%.4f > %.2f) -> %s", $calculated_abs_runq_pressure_ratio, $adaptive_saturation_thresh, $absrunq_cond_met_str);
    push @global_pressure_rationale_lines, sprintf("     - %s Specific Absolute RunQ Pressure Flag: %s", $MANDATORY_PEAK_PROFILE_FOR_HINT, $absrunq_cond_met_str);
    push @global_pressure_rationale_lines, "";


    # 3. Normalised Workload Pressure (using P-99W1's NormRunQ_P90)
    # ... (logic as before, sets $p99w1_has_normalized_runq_pressure, logs to @global_pressure_rationale_lines) ...
    # ... (ensure push @pressure_points, ... happens if $p99w1_has_normalized_runq_pressure = 1;) ...
    my $p99w1_norm_runq_p90_num = ($p99w1_norm_runq_p90_val_str ne $na_str_hint && $p99w1_norm_runq_p90_val_str =~ /^-?[0-9.]+$/) ? ($p99w1_norm_runq_p90_val_str + 0) : undef;
    my $min_abs_runq_for_norm_check = $smt_used_for_vm_numeric > 0 ? $smt_used_for_vm_numeric : 1.0;
    push @global_pressure_rationale_lines, sprintf("  3. Normalised Workload Pressure Check (%s NormRunQ P90):", $MANDATORY_PEAK_PROFILE_FOR_HINT);
    # ... (detailed logging lines for NormRunQ check)
    my $normrunq_cond1_met_str = (defined $p99w1_norm_runq_p90_num && $p99w1_norm_runq_p90_num > $WORKLOAD_PRESSURE_NORM_P90_TRIGGER_THRESHOLD) ? "TRUE" : "FALSE";
    my $normrunq_cond2_met_str = (defined $p99w1_abs_runq_p90_num && $p99w1_abs_runq_p90_num >= $min_abs_runq_for_norm_check) ? "TRUE" : "FALSE";
    my $normrunq_overall_cond_met_str = "FALSE";
    if ($normrunq_cond1_met_str eq "TRUE" && $normrunq_cond2_met_str eq "TRUE")
    {
        $p99w1_has_normalized_runq_pressure = 1; # Set the specific flag
        $normrunq_overall_cond_met_str = "TRUE";
        push @pressure_points, sprintf("RunQNorm_%s(P90=%.2f)", $MANDATORY_PEAK_PROFILE_FOR_HINT, defined $p99w1_norm_runq_p90_num ? $p99w1_norm_runq_p90_num : 0);
    }
    push @global_pressure_rationale_lines, sprintf("     - %s NormRunQ P90     : %s", $MANDATORY_PEAK_PROFILE_FOR_HINT, $p99w1_norm_runq_p90_val_str);
    push @global_pressure_rationale_lines, sprintf("     - Threshold               : > %.2f", $WORKLOAD_PRESSURE_NORM_P90_TRIGGER_THRESHOLD);
    push @global_pressure_rationale_lines, sprintf("     - Condition Met (Norm)    : (%s > %.2f) -> %s", $p99w1_norm_runq_p90_val_str, $WORKLOAD_PRESSURE_NORM_P90_TRIGGER_THRESHOLD, $normrunq_cond1_met_str);
    push @global_pressure_rationale_lines, sprintf("     - %s AbsRunQ P90      : %s (for magnitude check)", $MANDATORY_PEAK_PROFILE_FOR_HINT, $p99w1_abs_runq_p90_val_str);
    push @global_pressure_rationale_lines, sprintf("     - Min AbsRunQ Threshold   : %.2f (typically SMT)", $min_abs_runq_for_norm_check);
    push @global_pressure_rationale_lines, sprintf("     - Condition Met (Mag)     : (%s >= %.2f) -> %s", $p99w1_abs_runq_p90_val_str, $min_abs_runq_for_norm_check, $normrunq_cond2_met_str);
    push @global_pressure_rationale_lines, sprintf("     - %s Specific Normalised Workload Pressure Flag: %s", $MANDATORY_PEAK_PROFILE_FOR_HINT, $normrunq_overall_cond_met_str);
    push @global_pressure_rationale_lines, "";


    # --- Combine pressure flags & Pool Context ---
    my $overall_pressure_detected_for_csv = $pressure_detected_maxcpu_limit ||
    $p99w1_has_absolute_runq_pressure || # Use the specific P-99W1 flags here for overall CSV flag
    $p99w1_has_normalized_runq_pressure;
    # Pool context logic, using @pressure_points
    push @global_pressure_rationale_lines, "  4. Pool Context:";

    # Get Pool ID from nfit's dynamic data, and Pool Name from the optional config file
    my $pool_id_from_nmon = $config_ref->{pool_id} // 0;
    my $pool_name_from_config = $vm_config_data{$vm_name}{pool_name} // undef; # Still need the original config file for this

    # A non-default pool is one where the ID is not 0.
    my $is_non_default_pool = (looks_like_number($pool_id_from_nmon) && $pool_id_from_nmon != 0);

    # Pool ID is now directly from the JSON output of nfit
    push @global_pressure_rationale_lines, sprintf("     - Pool ID                 : %s", $pool_id_from_nmon);
    push @global_pressure_rationale_lines, sprintf("     - Pool Name (from config) : %s", $pool_name_from_config // "N/A");
    push @global_pressure_rationale_lines, sprintf("     - Is Non-Default Pool     : %s", $is_non_default_pool ? "TRUE" : "FALSE");

    if ($is_non_default_pool && $overall_pressure_detected_for_csv) {
        # If we have a pool name, use it. Otherwise, fall back to the ID.
        my $pool_identifier = defined($pool_name_from_config) && $pool_name_from_config ne '' ? $pool_name_from_config : "ID:$pool_id_from_nmon";
        push @pressure_points, "Pool($pool_identifier)";
    }
    push @global_pressure_rationale_lines, "";

    my $overall_pressure_detected_for_csv_str = $overall_pressure_detected_for_csv ? "TRUE" : "FALSE";
    my $pressure_detail_str = @pressure_points ? join(", ", @pressure_points) : "None";
    push @global_pressure_rationale_lines, sprintf("  5. Overall Global Hint Pressure Flag (for CSV): %s", $overall_pressure_detected_for_csv_str);
    push @global_pressure_rationale_lines, sprintf("  6. Final PressureDetail string for CSV     : \"%s\"", $pressure_detail_str);

    my $global_pressure_rationale_text = "Section G: Global Sizing Hint Pressure Assessment (source: generate_sizing_hint)\n" .
    join("\n", @global_pressure_rationale_lines);


    # --- Tiering logic (remains unchanged) ---
    my $initial_tier_range_str = "3/4";
    # --- Tiering logic (with added safety checks for numeric values) ---
    my $p99w1_val_for_tier = $results_ref->{'P-99W1'};
    my $g3_val_for_tier    = $results_ref->{'G3-95W15'};

    if (looks_like_number($p99w1_val_for_tier) && looks_like_number($g3_val_for_tier) && $g3_val_for_tier > 0) {
        if ($p99w1_val_for_tier > $g3_val_for_tier * $HIGH_PEAK_RATIO_THRESHOLD) {
            $shape_descriptor = "Very Peaky";
        }
        elsif ($p99w1_val_for_tier > $g3_val_for_tier * $LOW_PEAK_RATIO_THRESHOLD) {
            $shape_descriptor = "Moderately Peaky";
        }
    }

    if ($shape_descriptor eq "Very Peaky") { $initial_tier_range_str = "1/2"; }
    elsif ($shape_descriptor eq "Moderately Peaky") { $initial_tier_range_str = "2/3"; }
    my $adjusted_tier_str = $initial_tier_range_str;
    if ($overall_pressure_detected_for_csv)
    {
        if ($initial_tier_range_str eq "3/4") { $adjusted_tier_str = "3"; }
        elsif ($initial_tier_range_str eq "2/3") { $adjusted_tier_str = "2"; }
        elsif ($initial_tier_range_str eq "1/2") { $adjusted_tier_str = "1"; }
    }
    my $pattern_tier_string = $suggested_pattern . $adjusted_tier_str;

    if ($is_vio_server)
    {
        # For VIOs, we return the *actual* calculated pressure flags and details,
        # but override the final tier/pattern hint for safety and clarity.
        return (
            "P",    # Override Hint to "P" for Peak/Manual
            $shape_descriptor, # Override Pattern
            $overall_pressure_detected_for_csv, # Return ACTUAL calculated pressure
            $pressure_detail_str,               # Return ACTUAL pressure details
            $global_pressure_rationale_text,
            $p99w1_has_absolute_runq_pressure,
            $p99w1_has_normalized_runq_pressure
        );
    }
    else
    {
        # For non-VIOs, return the standard, calculated hint and pattern.
        return (
            $pattern_tier_string,
            $shape_descriptor,
            $overall_pressure_detected_for_csv,
            $pressure_detail_str,
            $global_pressure_rationale_text,
            $p99w1_has_absolute_runq_pressure,
            $p99w1_has_normalized_runq_pressure
        );
    }
}
# end of generate_sizing_hint

# --- parse_percentile_list_for_header ---
# This sub is used by the OLD global RunQ metric collection logic (which is now superseded
# by per-profile RunQ metrics). It might still be called if that logic path is hit,
# or could be refactored/removed if that path is fully deprecated.
# For now, keeping it as it might be used by initial population of $results_table{$vm_name}{$rq_metric_name}.
# It prepares percentile numbers for use as metric name suffixes.
sub parse_percentile_list_for_header
{
    my ($perc_str, $clean_zeros) = @_;
    $clean_zeros = 1 if !defined $clean_zeros; # Default to cleaning "X.00" to "X"
    my @percentiles_cleaned;
    if (defined $perc_str && $perc_str ne '')
    {
        my @raw_percentiles = split /,\s*/, $perc_str;
        foreach my $p (@raw_percentiles)
        {
            if ($p =~ /^[0-9]+(?:\.[0-9]+)?$/ && $p >= 0 && $p <= 100) # Validate numeric and range
            {
                my $p_label = $p;
                if ($clean_zeros)
                {
                    $p_label = sprintf("%.2f", $p); # Format to two decimal places
                    $p_label =~ s/\.?0+$//;         # Remove trailing ".00" or ".0"
                    $p_label = "0" if $p_label eq "" && ($p eq "0" || $p eq "0.00"); # Handle "0.00" -> "0"
                }
                push @percentiles_cleaned, $p_label;
            }
            else # Invalid percentile value
            {
                die "[ERROR] Invalid percentile value '$p' found in list '$perc_str'. Must be numeric between 0 and 100.\n";
            }
        }
    }
    return \@percentiles_cleaned; # Return reference to array of cleaned percentile labels
}

# --- ensure_percentiles_requested ---
# Checks if a list of required percentiles are present in a given percentile string.
# Potentially used for validating if nfit was asked to calculate necessary percentiles
# for the old global RunQ metric collection. May be less relevant with per-profile logic.
sub ensure_percentiles_requested
{
    my ($perc_list_str, @required_percs) = @_; # perc_list_str is comma-separated, required_percs are numbers
    return 1 unless defined $perc_list_str && $perc_list_str ne ''; # If no list provided, assume not applicable or handled elsewhere

    # Parse the provided list string into a map for easy lookup
    my $parsed_percs_ref = parse_percentile_list_for_header($perc_list_str, 0); # Get raw numbers, no zero cleaning for comparison
    my %present_map = map { $_ => 1 } @{$parsed_percs_ref};

    foreach my $req_p_num (@required_percs) # Iterate through numerically required percentiles
    {
        # Check if the numeric value (or its string representation) exists in the parsed list
        my $req_p_str = "$req_p_num"; # Simple string conversion
        my $req_p_str_formatted = sprintf("%.2f", $req_p_num); # e.g. 90.00
        my $req_p_str_cleaned = $req_p_str_formatted;
        $req_p_str_cleaned =~ s/\.?0+$//;
        $req_p_str_cleaned = "0" if $req_p_str_cleaned eq "" && abs($req_p_num -0) < 0.001;


        unless (exists $present_map{$req_p_str} ||
            exists $present_map{$req_p_str_formatted} ||
            exists $present_map{$req_p_str_cleaned} )
        {
            # Check common string representations due to potential formatting differences
            my $found = 0;
            foreach my $key (keys %present_map) {
                if (abs($key - $req_p_num) < 0.001) { # Floating point comparison
                    $found = 1;
                    last;
                }
            }
            return 0 unless $found; # Required percentile not found
        }
    }
    return 1; # All required percentiles found
}

# --- get_nfit_output_dp_from_flags ---
# Determines the number of decimal places nfit is expected to use for a profile's output,
# based on the rounding flags (-r or -u) passed to nfit for that profile.
# This helps nfit-profile format its *own* adjusted values consistently.
sub get_nfit_output_dp_from_flags
{
    my ($nfit_flags_str_for_this_run) = @_; # Combined global and profile-specific flags for nfit

    # Regex to find -r[=increment] or -u[=increment]
    # It captures the increment value if provided.
    if ($nfit_flags_str_for_this_run =~ /-r(?:=(\d*\.\d+))?|-u(?:=(\d*\.\d+))?/)
    {
        my $increment_val_str = $1 // $2; # $1 for -r=val, $2 for -u=val

        # If -r or -u is present but no increment value, nfit uses its default increment.
        if (!(defined $increment_val_str && $increment_val_str ne ""))
        {
            # nfit's default increment is $DEFAULT_ROUND_INCREMENT (from nfit, assumed here to be same as nfit-profile's)
            # For robustness, it's better if nfit-profile knows nfit's default or this is coordinated.
            # Using nfit-profile's default as a proxy.
            return get_decimal_places($DEFAULT_ROUND_INCREMENT);
        }
        else # Increment value was specified
        {
            return get_decimal_places($increment_val_str);
        }
    }
    # If no -r or -u flag, nfit typically outputs with more precision (e.g., 4 decimal places by default internally).
    # nfit version 2.28.0.4 defaults to 4 DP if no rounding.
    return 4;
}

# --- get_decimal_places ---
# Calculates the number of decimal places in a given number string.
sub get_decimal_places
{
    my ($number_str) = @_;
    # Handle scientific notation by converting to fixed point string first
    $number_str = sprintf("%.15f", $number_str) if ($number_str =~ /e/i);

    if ($number_str =~ /\.(\d+)$/) # If there's a decimal part
    {
        return length($1); # Length of the digits after decimal point
    }
    else # No decimal part
    {
        return 0;
    }
}

# Helper to format a percentile number into a clean string for metric keys.
sub clean_perc_label
{
    my ($p) = @_;
    my $label = sprintf("%.2f", $p);
    $label =~ s/\.?0+$//;
    $label = "0" if $label eq "" && abs($p-0)<0.001;
    return $label;
}

# --- parse_nfit_json_output ---
# Parses the multi-line JSON output from nfit. Each line is a distinct JSON object.
# Returns a hash where keys are VM names and values are arrays of the parsed JSON objects (as Perl hashes).
sub parse_nfit_json_output
{
    my ($raw_output) = @_;
    my %parsed_data;
    my $json_decoder = JSON->new->utf8;
    my @lines = split /\n/, $raw_output;

    foreach my $line (@lines)
    {
        next if $line =~ /^\s*$/; # Skip empty lines

        my $decoded_hash = eval { $json_decoder->decode($line) };
        if ($@ || !ref($decoded_hash) eq 'HASH') {
            warn " [WARN] Could not decode JSON line from nfit: $line. Error: $@";
            next;
        }

        my $vm_name = $decoded_hash->{vmName};
        if ($vm_name) {
            push @{$parsed_data{$vm_name}}, $decoded_hash;
        }

    }

    return \%parsed_data;
}

# ==============================================================================
# Subroutine to determine the start and end date of a seasonal event period.
# It reads the event's configuration and calculates the absolute date range
# for the analysis.
# It accepts a base_date_obj to correctly calculate periods for
# historical months during an --update-history run.
# ==============================================================================
sub determine_event_period {
    my ($event_config, $base_date_obj, $context) = @_;

    my $model_type = $event_config->{model} // '';

    # Context:
    # - forecast: compute the next (or last-completed for recency_decay) period relative to base date
    # - history : compute the period that belongs to the historical month represented by base date
    #
    # Back-compat behaviour:
    # - If caller passes a base_date but no context, assume HISTORY (current behaviour).
    # - If caller passes no base_date, assume FORECAST anchored to today.
    $context //= (defined $base_date_obj) ? 'history' : 'forecast';

    # Use the provided base date for historical calculations, or default to today for forecasts.
    my $base_date = (defined $base_date_obj)
        ? $base_date_obj->truncate(to => 'day')
        : gmtime()->truncate(to => 'day');

    # --- Path for events defined by fixed 'dates' ---
    if (defined $event_config->{dates}) {
        my @date_ranges = split /\s*,\s*/, $event_config->{dates};
        my ($next_event_start, $next_event_end);

        foreach my $range (@date_ranges) {
            if ($range =~ /(\d{4}-\d{2}-\d{2}):(\d{4}-\d{2}-\d{2})/) {
                my $start_obj = Time::Piece->strptime($1, '%Y-%m-%d')->truncate(to => 'day');
                my $end_obj   = Time::Piece->strptime($2, '%Y-%m-%d')->truncate(to => 'day');

                if ($context eq 'forecast') {
                    # FORECAST CONTEXT: Find the *next* event period relative to the current date.
                    if ($end_obj >= $base_date) {
                        if (!defined $next_event_start || $start_obj < $next_event_start) {
                            $next_event_start = $start_obj;
                            $next_event_end   = $end_obj;
                        }
                    }
                } else {
                    # HISTORY CONTEXT: Check if this specific fixed-date event falls
                    # within the historical month being processed. The base_date is the
                    # first day of that historical month.
                    my $month_end = Time::Piece->new($base_date->epoch)->add_months(1) - ONE_DAY;
                    if ($start_obj >= $base_date && $end_obj <= $month_end) {
                        return ($start_obj, $end_obj);
                    }
                }
            }
        }
        # In a forecast context, we return the closest future event found.
        return ($next_event_start, $next_event_end) if (defined $next_event_start && $context eq 'forecast');
    }
    # --- Path for events defined by a recurring 'period' ---
    elsif (defined $event_config->{period} && lc($event_config->{period}) eq 'monthly') {
        my $day_of_period = $event_config->{day_of_period} // -1;
        my $duration_days = $event_config->{duration_days} // 7;

        # FORECAST CONTEXT for 'recency_decay' model: Find the *last completed* peak.
        if ($model_type eq 'recency_decay' && $context eq 'forecast') {
            my ($current_month_start, $current_month_end) = _get_recurring_monthly_period($base_date, $day_of_period, $duration_days);
            if ($current_month_end > $base_date) {
                my $last_month_base = add_months($base_date, -1);
                return _get_recurring_monthly_period($last_month_base, $day_of_period, $duration_days);
            } else {
                return ($current_month_start, $current_month_end);
            }
        }
        # FORECAST CONTEXT for other models: Find the *next upcoming* peak.
        elsif ($context eq 'forecast') {
            my ($current_month_start, $current_month_end) = _get_recurring_monthly_period($base_date, $day_of_period, $duration_days);
            if ($current_month_end >= $base_date) {
                return ($current_month_start, $current_month_end);
            } else {
                my $next_month_base = add_months($base_date, 1);
                return _get_recurring_monthly_period($next_month_base, $day_of_period, $duration_days);
            }
        }
        # HISTORY CONTEXT for any recurring model: Calculate the period for the *specific historical month* provided.
        else {
            return _get_recurring_monthly_period($base_date, $day_of_period, $duration_days);
        }
    }

    # Fallback: return nothing if no valid period is found.
    return;
}

# ==============================================================================
# Subroutine to parse a simple INI-style configuration file.
# This replaces the need for the external Config::Tiny module.
#
# Takes:
# 1. The path to the configuration file.
#
# Returns:
# - A hash reference representing the parsed INI data.
# - Dies on file open error.
# ==============================================================================
sub parse_seasonality_config {
    my ($filepath) = @_;
    my %config_data;
    my $current_section = '';

    open my $fh, '<:encoding(utf8)', $filepath
        or die "[ERROR] Cannot open seasonality config file '$filepath': $!";

    while (my $line = <$fh>) {
        chomp $line;
        $line =~ s/^\s+|\s+$//g; # Trim whitespace
        $line =~ s/\s*[#;].*//;   # Remove comments

        next if $line eq ''; # Skip empty or comment-only lines

        if ($line =~ /^\s*\[\s*([^\]]+?)\s*\]\s*$/) {
            # This is a section header
            $current_section = $1;
        } elsif ($current_section ne '' && $line =~ /^\s*([^=]+?)\s*=\s*(.+)$/) {
            # This is a key-value pair within a section
            my $key = $1;
            my $value = $2;
            $key =~ s/^\s+|\s+$//g;
            $value =~ s/^\s+|\s+$//g;
            $config_data{$current_section}{$key} = $value;
        }
    }
    close $fh;

    # --- Deprecation Guard & Normalisation ---
    # 'allow_compounding' has been removed in favour of the more explicit
    # 'interaction_policy = exclusive|combined'. We do NOT support both keys,
    # as mixed semantics are error-prone and undermine auditability.
    foreach my $section (keys %config_data) {
        # 1. Deprecation Check (Hard Fail)
        if (exists $config_data{$section}{allow_compounding}) {
            die "[ERROR] Config deprecated: 'allow_compounding' is no longer supported (found in section '$section').\n" .
                "        Use 'interaction_policy = exclusive|combined'.\n";
        }

        # 2. Alias Resolution (concurrent_interaction -> interaction_policy)
        # If the user used the alias, map it to the primary key.
        if ($config_data{$section}{concurrent_interaction}) {
            $config_data{$section}{interaction_policy} //= $config_data{$section}{concurrent_interaction};
        }

        # 3. Centralised Defaulting
        # Ensure the key always exists so downstream code doesn't need '// exclusive'
        $config_data{$section}{interaction_policy} //= 'exclusive';

        # 4. Input Sanitisation
        # Handle case variations (Exclusive vs exclusive)
        $config_data{$section}{interaction_policy} = lc($config_data{$section}{interaction_policy});
        unless ($config_data{$section}{interaction_policy} =~ /^(exclusive|combined)$/) {
            warn "[WARN] Invalid interaction_policy in [$section]: '$config_data{$section}{interaction_policy}' (defaulting to 'exclusive')\n";
            $config_data{$section}{interaction_policy} = 'exclusive';
        }
    }

    # ----------------------------------------------------------------------
    # Resolve inheritance EARLY (centralised): Global defaults + per-event overrides
    #
    # Goal: downstream code should consume a fully-resolved event config
    # hash, so helpers do not need to “know” about inheritance rules.
    #
    # Supported early-resolution behaviours:
    #  - Global fallback for common keys (any key set in [Global])
    #  - vms/exclude_vms inheritance, with support for "vms = ALL"
    #  - pre-parsed VM scope filter stored in event config as _vm_scope_filter
    # ----------------------------------------------------------------------
    my %resolved;

    # Preserve Global section as-is (raw), but also treat it as defaults.
    $resolved{Global} = { %{ $config_data{Global} // {} } };

    # Inheritable keys are simply any keys present in [Global]; per-event overrides win.
    # We additionally support explicit vm-scope semantics (vms/exclude_vms), including
    # "vms = ALL" to override a Global include restriction.
    foreach my $section (sort keys %config_data) {
        next if $section =~ /^(?:Global|Adaptive)$/i;

        my $event_raw  = $config_data{$section} // {};
        my $global_raw = $config_data{Global}   // {};

        # Start with Global defaults, then overlay event overrides.
        my %merged = (%$global_raw, %$event_raw);

        # Ensure event name is always available to downstream code.
        $merged{_eventName} = $section;

        # --- VM Scope Resolution (Phase 3.3) ---
        # Resolve inheritance and reserved keyword handling here.
        my $vms_str = exists $event_raw->{vms}
            ? $event_raw->{vms}
            : ($global_raw->{vms} // '');

        my $exclude_vms_str = exists $event_raw->{exclude_vms}
            ? $event_raw->{exclude_vms}
            : ($global_raw->{exclude_vms} // '');

        $vms_str //= '';
        $exclude_vms_str //= '';

        # "vms = ALL" explicitly means "include all VMs" and overrides any Global include list.
        if ($vms_str =~ /^\s*all\s*$/i) {
            $vms_str = '';
        }

        my $vm_scope_filter = _build_vm_scope_filter_from_strings(
            $vms_str,
            $exclude_vms_str,
            exists $event_raw->{vms}        ? 'event'  : 'global',
            exists $event_raw->{exclude_vms}? 'event'  : 'global',
        );

        # Store resolved strings for audit/logging and the parsed filter for fast use.
        $merged{vms}            = $vms_str;
        $merged{exclude_vms}    = $exclude_vms_str;
        $merged{_vm_scope_filter} = $vm_scope_filter;  # may be undef

        $resolved{$section} = \%merged;
    }

    return \%resolved;

}

# ------------------------------------------------------------------------------
# Helper: build a VM scope filter structure from resolved strings.
# Centralised so parse_seasonality_config can resolve inheritance early.
# Returns undef if no filters apply.
# ------------------------------------------------------------------------------
sub _build_vm_scope_filter_from_strings {
    my ($vms_str, $exclude_vms_str, $include_source, $exclude_source) = @_;

    $vms_str //= '';
    $exclude_vms_str //= '';

    return undef unless ($vms_str =~ /\S/ || $exclude_vms_str =~ /\S/);

    my %result = (
        include => undef,
        exclude => undef,
        include_source => $include_source,
        exclude_source => $exclude_source,
    );

    # Parse 'vms' (include list)
    if ($vms_str =~ /\S/) {
        my %include_lookup;
        foreach my $vm_raw (split /,/, $vms_str) {
            my $vm = $vm_raw;
            $vm =~ s/^\s+//;
            $vm =~ s/\s+$//;
            next unless length($vm);
            $include_lookup{$vm} = 1;
        }
        $result{include} = \%include_lookup if %include_lookup;
    }

    # Parse 'exclude_vms' (exclude list)
    if ($exclude_vms_str =~ /\S/) {
        my %exclude_lookup;
        foreach my $vm_raw (split /,/, $exclude_vms_str) {
            my $vm = $vm_raw;
            $vm =~ s/^\s+//;
            $vm =~ s/\s+$//;
            next unless length($vm);
            $exclude_lookup{$vm} = 1;
        }
        $result{exclude} = \%exclude_lookup if %exclude_lookup;
    }

    return undef unless ($result{include} || $result{exclude});
    return \%result;
}

# ==============================================================================
# SUBROUTINE: calculate_multiplicative_forecast (NEW ROBUST DESIGN)
# PURPOSE:    Calculates a forecast using the multiplicative seasonal model.
#             This version implements a robust, multi-stage forecast. It
#             intelligently determines the most appropriate recent baseline and
#             applies a recency-weighted seasonal multiplier derived from
#             historical, saturation-corrected peak data. It also includes
#             logic for volatility, residual peak forecasting, and concurrent event interaction.
# RETURNS:
#   - A hash reference containing the final forecasted results.
#   - A hash reference containing historical data for verbose reporting.
# ==============================================================================
sub calculate_multiplicative_forecast {
    my ($system_cache_dir, $system_identifier, $event_name, $event_config, $full_seasonality_config, $initial_results_table_href, $exclusions_href, $asof_start_obj, $asof_end_obj) = @_;

    # This subroutine implements a robust, multi-stage forecast for multiplicative seasonal events.
    # It intelligently determines the most appropriate recent baseline and applies a recency-weighted
    # seasonal multiplier derived from historical, saturation-corrected peak data.

    print STDERR "    ⧉ Executing Multiplicative Seasonal Forecast for event $event_name on system $system_identifier\n";

    # --- Configuration Constants ---
    # The number of days of stable, non-peak activity to use for the baseline.
    my $baseline_days_config = $event_config->{baseline_period_days} // 16;
    # A baseline must have at least this many clean days to be considered statistically valid.
    my $MIN_BASELINE_DAYS = 7;
    # Placeholder for future enhancement: A trend adjustment factor could be applied here.
    my $trend_adjustment_factor = 1.0;

    # --- Step 1: Get Key Dates and Read Unified History ---
    my $data_cache_file = File::Spec->catfile($system_cache_dir, '.nfit.cache.data');
    my ($cache_start_date_obj, $cache_end_date_obj) = _get_cache_date_range($data_cache_file);

    my $analysis_start_obj = (defined $asof_start_obj && ref($asof_start_obj) eq 'Time::Piece')
        ? $asof_start_obj->truncate(to => 'day')
        : $cache_start_date_obj->truncate(to => 'day');

    my $analysis_end_obj = (defined $asof_end_obj && ref($asof_end_obj) eq 'Time::Piece')
        ? $asof_end_obj->truncate(to => 'day')
        : $cache_end_date_obj->truncate(to => 'day');

    unless ($analysis_start_obj && $analysis_end_obj) {
        warn "  [WARN] Could not determine analysis date range. Forecasting cannot proceed.\n";
        return ({}, {});
    }

    my $unified_history = read_unified_history($system_cache_dir);
    my @event_history_initial;
    my ($most_recent_peak_snapshot, $most_recent_peak_end_obj);

    # Find all historical snapshots for this specific event from the unified history.
    foreach my $month_key (sort { $b cmp $a } keys %$unified_history) {
        my $month_data = $unified_history->{$month_key};
        if (exists $month_data->{SeasonalEventSnapshots}{$event_name}) {
            my $snapshot = $month_data->{SeasonalEventSnapshots}{$event_name};
            $snapshot->{_month_key} = $month_key;
            $snapshot->{eventName} //= $event_name;  # Ensure eventName is set for history lookup

            my $include_snapshot = 0;

            if ($snapshot->{periodEndDate}) {
                my $snap_end_obj;
                eval {
                    $snap_end_obj = Time::Piece->strptime($snapshot->{periodEndDate}, '%Y-%m-%d')->truncate(to => 'day');
                };
                if (!$@ && $snap_end_obj && $snap_end_obj <= $analysis_end_obj) {
                    $include_snapshot = 1;
                }
            } else {
                # Fallback: compare month key to analysis_end month (conservative)
                my $analysis_month_key = $analysis_end_obj->strftime('%Y-%m');
                $include_snapshot = 1 if ($month_key le $analysis_month_key);
            }

            push @event_history_initial, $snapshot if $include_snapshot;
        }
    }

    # Populate most_recent_peak from the first (newest) snapshot in the specified analysis time-frame if available
    if (@event_history_initial) {
        $most_recent_peak_snapshot = $event_history_initial[0];
        if ($most_recent_peak_snapshot->{periodEndDate}) {
            eval {
                $most_recent_peak_end_obj = Time::Piece->strptime(
                    $most_recent_peak_snapshot->{periodEndDate}, '%Y-%m-%d'
                );
            };
            if ($@) {
                warn "  [WARN] Could not parse most recent peak period end-date '$most_recent_peak_snapshot->{periodEndDate}': $@\n";
                $most_recent_peak_end_obj = undef;
            }
        }
    }

    # --- Step 2: Two-Tiered Baseline Strategy ---
    my %current_baseline_results;
    my $baseline_source_log = "N/A";
    my @event_history_final = @event_history_initial; # Start with all available history.

    # --- Path A (Ideal): Attempt to calculate a "Post-Peak" baseline ---
    # The ideal baseline is the N days of data ending on the last day of the cache.
    my $post_peak_baseline_start_obj = $analysis_end_obj->truncate(to => 'day') - (($baseline_days_config - 1) * 86400);

    # Check if this ideal baseline period is contaminated by the most recent historical peak.
    my $is_contaminated = 0;
    if ($most_recent_peak_end_obj) {
        $is_contaminated = ($post_peak_baseline_start_obj <= $most_recent_peak_end_obj);
    }
    # Also check if the clean period is long enough to be statistically valid.
    my $clean_days_available = $is_contaminated ?
    ($analysis_end_obj->epoch - $most_recent_peak_end_obj->epoch) / 86400 - 1 :
    $baseline_days_config;

    if (!$is_contaminated && $clean_days_available >= $MIN_BASELINE_DAYS) {
        $baseline_source_log = "Post-Peak (calculated from " . $post_peak_baseline_start_obj->date . " to " . $analysis_end_obj->date . ")";
        print STDERR "  Φ Multiplicative Seasonal Forecast [Baseline strategy]: selected post-peak window (" . $post_peak_baseline_start_obj->date . " → " . $analysis_end_obj->date . ")\n";

        my $start_str = $post_peak_baseline_start_obj->strftime('%Y-%m-%d');
        my $end_str   = $analysis_end_obj->strftime('%Y-%m-%d');

        my $profile_count = scalar(@profiles);
        my $profile_num = 0;
        foreach my $profile (@profiles) {
            $profile_num++;
            my $profile_name = $profile->{name};
            print STDERR "\r  • Analysing post-peak baseline $profile_num/$profile_count: $profile_name";
            # This is a contextual baseline, so is_generic is 0.
            my ($nfit_cmd, $manifest_obj) = _build_nfit_baseline_command($profile->{flags}, $start_str, $end_str, $system_cache_dir, 0, 0, undef, $profile->{name}, $exclusions_href);
            my $nfit_output = '';
            my $stderr_arg = ">&=" . fileno(STDERR);
            my $pid_nfit = open3(undef, my $stdout_nfit, $stderr_arg, $nfit_cmd);
            while (my $line = <$stdout_nfit>) { $nfit_output .= $line; }
            waitpid($pid_nfit, 0);
            next if $?; # Skip if nfit command failed

            my $parsed = parse_nfit_json_output($nfit_output);
            my $p_key  = "P" . clean_perc_label(($profile->{flags} =~ /-p\s+([0-9.]+)/) ? $1 : $DEFAULT_PERCENTILE);

            foreach my $vm (keys %$parsed) {
                my @vals = map {
                    my $physc = $_->{metrics}{physc} // {};
                    my $blk   = (ref($physc) eq 'HASH') ? ($physc->{$profile_name} // $physc) : {};
                    (ref($blk) eq 'HASH') ? $blk->{$p_key} : undef;
                } @{$parsed->{$vm}};
                my @valid_vals = grep { defined $_ && looks_like_number($_) } @vals;
                if (@valid_vals) {
                    $current_baseline_results{$vm}{$profile->{name}} = sum0(@valid_vals) / scalar(@valid_vals);
                }
            }
        }
        print STDERR "\r  • Analysis of post-peak baseline is complete.\t\t\t\t\n";
    }
    # --- Path B (Fallback): Re-use the "Pre-Peak" baseline from the most recent snapshot ---
    # periodEndDate: end of the Peak
    #
    elsif ($most_recent_peak_snapshot) {
        my $b_start = $most_recent_peak_snapshot->{baselinePeriod}{startDate};
        my $b_end   = $most_recent_peak_snapshot->{baselinePeriod}{endDate};

        if ($b_start && $b_end) {
             $baseline_source_log = "Pre-Peak ($b_start - $b_end)";
        } else {
             # Legacy Fallback
             $baseline_source_log = "Pre-Peak (ending " . $most_recent_peak_snapshot->{periodEndDate} . ")";
        }

        if ($is_contaminated) {
            print STDERR "  Φ Multiplicative Seasonal Forecast [Baseline strategy]: post-peak window disqualified (contaminated); selected pre-peak baseline\n";
        } elsif ($clean_days_available < $MIN_BASELINE_DAYS) {
            print STDERR "  Φ Multiplicative Seasonal Forecast [Baseline strategy]: post-peak window disqualified (too short); selected pre-peak baseline\n";
        } else {
            print STDERR "  Φ Multiplicative Seasonal Forecast [Baseline strategy]: post-peak window disqualified; selected pre-peak baseline\n";
        }

        %current_baseline_results = %{$most_recent_peak_snapshot->{results}{HistoricBaseline} || {}};
        # If we re-use this baseline, we must exclude its corresponding peak from the multiplier calculation.
        shift @event_history_final;
    }

    # --- Path C (Cold Start Recovery): Calculate baseline from pre-event window ---
    # This path activates when:
    #   1. Path A produced no results (post-peak too short/contaminated), AND
    #   2. Path B produced no results (snapshot has no valid baseline data)
    # For weekly events, this is the expected path since post-peak windows are too short.
    #
    unless (%current_baseline_results) {
        if ($analysis_end_obj) {
            print STDERR "  Φ Multiplicative Seasonal Forecast [Baseline strategy]: Cold Start (baseline_period_days)\n";

            if ($analysis_start_obj) {
                # Calculate baseline window relative to anchor date, respecting baseline_period_days
                # Baseline ends the day before the event (exclude event day)
                my $baseline_end_obj = $analysis_end_obj - ONE_DAY;

                # Baseline starts baseline_period_days before the end
                my $baseline_start_obj = $baseline_end_obj - (($baseline_days_config - 1) * ONE_DAY);

                # Clamp to cache start if baseline would extend before available data
                if ($baseline_start_obj->epoch < $analysis_start_obj->epoch) {
                    $baseline_start_obj = $analysis_start_obj;
                }

                my $start_str = $baseline_start_obj->strftime('%Y-%m-%d');
                my $end_str   = $baseline_end_obj->strftime('%Y-%m-%d');
                $baseline_source_log = "Pre-Event Baseline ($start_str to $end_str) [Cold Start, ${baseline_days_config}d config]";

                print STDERR "    ⧗ Cold-start baseline window: $start_str → $end_str\n";

                foreach my $profile (@profiles) {
                    my $profile_name = $profile->{name};
                    # Use is_generic_baseline=1 to strip time filters that might exclude data
                    my ($nfit_cmd, $manifest_obj) = _build_nfit_baseline_command(
                        $profile->{flags}, $start_str, $end_str, $system_cache_dir,
                        0,    # enable_clipping_detection
                        1,    # HistoricBaseline: contextual baseline (keeps time filters, matching the logic for peak; ensure "apples to apples" comparison)
                        undef, # allow_growth_prediction
                        $profile_name,
                        $exclusions_href
                    );

                    my $nfit_output = '';
                    my $stderr_arg = ">&=" . fileno(STDERR);
                    my $pid_nfit = open3(undef, my $stdout_nfit, $stderr_arg, $nfit_cmd);
                    while (my $line = <$stdout_nfit>) { $nfit_output .= $line; }
                    waitpid($pid_nfit, 0);
                    if ($?) {
                        print STDERR "    [DEBUG] nFit baseline command failed (exit $?) for profile $profile_name\n";
                        print STDERR "    [DEBUG] Window: $start_str → $end_str\n";
                        next;
                    }
                    if (!$nfit_output || $nfit_output !~ /\S/) {
                        print STDERR "    [DEBUG] nFit returned empty output for profile $profile_name\n";
                        next;
                    }

                    my $parsed = parse_nfit_json_output($nfit_output);
                    my $p_key = "P" . clean_perc_label(
                        ($profile->{flags} =~ /-p\s+([0-9.]+)/) ? $1 : $DEFAULT_PERCENTILE
                    );

                    foreach my $vm (keys %$parsed) {
                        my @vals = map {
                            my $physc = $_->{metrics}{physc} // {};
                            my $blk   = (ref($physc) eq 'HASH') ? ($physc->{$profile_name} // $physc) : {};
                            (ref($blk) eq 'HASH') ? $blk->{$p_key} : undef;
                        } @{$parsed->{$vm}};
                        my @valid_vals = grep { defined $_ && looks_like_number($_) } @vals;
                        if (@valid_vals) {
                            $current_baseline_results{$vm}{$profile_name} = sum0(@valid_vals) / scalar(@valid_vals);
                        }
                    }
                }

                if (%current_baseline_results) {
                    print STDERR "  ✓ Multiplicative Seasonal Forecast: Calculated Cold Start baseline for " . scalar(keys %current_baseline_results) . " VM(s)\n";
                }
            }
        }
    }

    unless (%current_baseline_results) {
        warn "  [WARN] Could not determine a valid baseline. Forecasting cannot proceed.\n";
        return ({}, {});
    }

    # --- Step 3: Detect Concurrent Events for Interaction (Overlap) ---
    my $today_for_recency = (defined $asof_end_obj && ref($asof_end_obj) eq 'Time::Piece') ? $asof_end_obj->truncate(to => 'day') : gmtime()->truncate(to => 'day');
    my ($next_event_start, $next_event_end) = determine_event_period($event_config, $today_for_recency, 'forecast');

    my $forecast_date_obj = (defined $next_event_start && ref($next_event_start) eq 'Time::Piece') ? $next_event_start : $today_for_recency;

    # Interaction policy controls whether this event combines with other concurrent events.
    # Values: exclusive (default) | combined
    my $primary_interaction_policy = lc($event_config->{interaction_policy} // 'exclusive');
    unless ($primary_interaction_policy =~ /^(exclusive|combined)$/) {
        warn "  [WARN] Invalid interaction_policy '$primary_interaction_policy' for event '$event_name' (defaulting to 'exclusive')\n";
        $primary_interaction_policy = 'exclusive';
    }

    # The current event being processed is ALWAYS the primary event.
    my @active_events;
    my $current_event_cfg_copy = { %$event_config, _eventName => $event_name };
    push @active_events, $current_event_cfg_copy;

    # Detect other concurrent events (overlap discovery is always performed for observability).
    my @detected_concurrent = detect_active_events($forecast_date_obj, $full_seasonality_config);

    my @overlap_event_names;
    my @ignored_overlap_names;

    foreach my $detected (@detected_concurrent) {
        my $detected_name = ($detected->{_eventName} // '');
        next if ($detected_name eq '' || $detected_name eq $event_name);
        push @overlap_event_names, $detected_name;

        if ($primary_interaction_policy eq 'combined') {
            my $detected_policy = lc($detected->{interaction_policy} // 'exclusive');
            if ($detected_policy eq 'combined') {
                push @active_events, $detected;
            } else {
                push @ignored_overlap_names, $detected_name;
            }
        } else {
            push @ignored_overlap_names, $detected_name;
        }
    }

    # --- Step 4: Calculate Recency-Weighted Multiplier & Final Forecast ---
    my %final_forecasts;
    my %historic_data_for_csv; # For verbose reporting

    if (!@active_events) {
        print STDERR "  Φ Multiplicative Seasonal Forecast [strategy]: no active events; baseline-only forecast selected\n";
        # If no events are active, the forecast is simply the baseline.
        foreach my $vm_name (keys %current_baseline_results) {
            foreach my $profile (@profiles) {
                $final_forecasts{$vm_name}{$profile->{name}} = $current_baseline_results{$vm_name}{$profile->{name}};
            }
        }
        return (\%final_forecasts, \%historic_data_for_csv);
    }

    my $is_interaction_run = (scalar(@active_events) > 1);
    my @dampening_factors;

    my $event_count = scalar(@active_events);
    my $event_list  = join(", ", map { $_->{_eventName} } @active_events);

    print STDERR "  ◆ Active seasonal events: $event_count ($event_list)\n";

    # Observability: report overlap discovery even when interaction is exclusive.
    if (@overlap_event_names) {
        print STDERR "  [INFO] Concurrent Event Overlap detected: " . scalar(@overlap_event_names) . " (" . join(", ", @overlap_event_names) . ")\n";
    }

    # Interaction: combined means we will multiply in other events' multipliers (with optional dampening).
    if ($primary_interaction_policy eq 'combined') {
        if ($is_interaction_run) {
            my @dampening_candidates = grep { defined $_ && looks_like_number($_) } map { $_->{interaction_dampening_factor} } @active_events;
            my $dampening_to_apply = @dampening_candidates ? (sort { $a <=> $b } @dampening_candidates)[0] : undef;

            print STDERR "  [INFO] Concurrent Event Interaction: " . scalar(@active_events) . " events active ($event_list)\n";
            if (defined $dampening_to_apply) {
                print STDERR "         → Multipliers will be combined with dampening factor $dampening_to_apply\n";
            } else {
                print STDERR "         → Multipliers will be combined (no dampening configured)\n";
            }
        } else {
            print STDERR "  [INFO] Concurrent Event Interaction: policy=combined (no concurrent combined events)\n";
            if (@overlap_event_names) {
                print STDERR "         → Overlap present but no combined participants: " . join(", ", @overlap_event_names) . "\n";
            }
        }
    } else {
        print STDERR "  [INFO] Concurrent Event Interaction: policy=exclusive (no multiplier combination)\n";
        if (@ignored_overlap_names) {
            print STDERR "         → Ignoring concurrent event(s): " . join(", ", @ignored_overlap_names) . "\n";
        }
    }

    # Main processing loop: iterate through each VM that has a valid baseline.
    foreach my $vm_name (sort keys %current_baseline_results) {
        my $vm_forecasts = {};
        my %vm_historic_multipliers;
        my %vm_historic_residuals;

        # Gather historical data for all active events for this VM.
        foreach my $active_event_cfg (@active_events) {
            my $active_event_name = $active_event_cfg->{_eventName};

            my @history_for_this_event;
            foreach my $hist_event (@event_history_final) {
                if ($hist_event->{eventName} eq $active_event_name) {
                    push @history_for_this_event, $hist_event;
                }
            }

            # Only proceed to calculate multipliers if history exists for this event.
            if (@history_for_this_event) {
                if (defined $active_event_cfg->{interaction_dampening_factor}) {
                    push @dampening_factors, $active_event_cfg->{interaction_dampening_factor};
                }
                foreach my $profile (@profiles) {
                    my $p_name = $profile->{name};

                    my @multipliers;
                    my @residuals;
                    foreach my $hist_event (@history_for_this_event) {
                        my $hist_results = $hist_event->{results};
                        my $hist_peak = $hist_results->{HistoricPeak}{$vm_name}{$p_name};
                        if (defined $hist_results->{ClippingInfo}{$vm_name}{$p_name}{unclippedPeakEstimate} && looks_like_number($hist_results->{ClippingInfo}{$vm_name}{$p_name}{unclippedPeakEstimate})) {
                            $hist_peak = $hist_results->{ClippingInfo}{$vm_name}{$p_name}{unclippedPeakEstimate};
                        }
                        my $hist_base = $hist_results->{HistoricBaseline}{$vm_name}{$p_name};
                        my $hist_residual = $hist_results->{PeakResidual}{$vm_name}{$p_name};

                        if (defined $hist_peak && defined $hist_base && $hist_base > 0.001) {
                            push @multipliers, { value => $hist_peak / $hist_base, date => Time::Piece->strptime($hist_event->{periodEndDate}, '%Y-%m-%d') };
                        }
                        if (defined $hist_residual && looks_like_number($hist_residual)) {
                            push @residuals, { value => $hist_residual, date => Time::Piece->strptime($hist_event->{periodEndDate}, '%Y-%m-%d') };
                        }
                    }
                    next unless @multipliers;

                    my $event_multiplier = calculate_recency_weighted_average(\@multipliers, $today_for_recency, 365);
                    $vm_historic_multipliers{$p_name}{$active_event_name} = { 'multiplier' => $event_multiplier, 'history' => \@multipliers };
                    $vm_historic_residuals{$p_name}{$active_event_name} = \@residuals;

                }
            }
        }

        # Now, synthesize the final forecast for each profile.
        foreach my $profile (@profiles) {
            my $p_name = $profile->{name};

            # EXEMPTION: Skip measurement profiles
            # P-99W1 is an empirical measurement (99.75th percentile of smoothed data),
            # not a sizing recommendation. Applying multipliers would be statistically
            # invalid and could produce physical impossibilities.
            # Note: "Peak" is not a profile - it's PeakValue in CoreResults, populated
            # separately from the raw peak tracker.
            if ($p_name eq $MANDATORY_PEAK_PROFILE_FOR_HINT) {
                next;
            }

            my $current_baseline_val = $current_baseline_results{$vm_name}{$p_name};

            next unless (defined $current_baseline_val && looks_like_number($current_baseline_val));

            # Bootstrap run: use multiplier = 1.0 if no history exists (automatic for new events)
            # No multipliers found for this profile - could be:
            # 1. True bootstrap (no snapshots at all) - _bootstrap_active is set
            # 2. Snapshots exist but have no valid peak/baseline data (prior bootstrap run)
            # In either case, proceed with baseline-only forecast (multiplier = 1.0)
            my $is_bootstrap_run = 0;
            unless (exists $vm_historic_multipliers{$p_name}) {
                # No multipliers found - proceed with baseline-only forecast
                $is_bootstrap_run = 1;
            }

            my $final_multiplier = 1.0;
            unless ($is_bootstrap_run) {
                foreach my $event_data (values %{$vm_historic_multipliers{$p_name}}) {
                    $final_multiplier *= $event_data->{'multiplier'};
                }
            }

            if ($is_interaction_run && @dampening_factors) {
                my $dampening_to_apply = (sort { $a <=> $b } @dampening_factors)[0];
                $final_multiplier = 1 + (($final_multiplier - 1) * $dampening_to_apply);
            }

            my $volatility_buffer = 1.0;
            my $primary_event_cfg = (grep { $_->{_eventName} eq $event_name } @active_events)[0] || $active_events[0];
            if (($primary_event_cfg->{volatility_adjustment} // 0) == 1 && exists $vm_historic_multipliers{$p_name}{$primary_event_cfg->{_eventName}}) {
                my $primary_event_history_ref = $vm_historic_multipliers{$p_name}{$primary_event_cfg->{_eventName}}{'history'};
                my $confidence = $primary_event_cfg->{seasonal_confidence_level} // '0.95';
                $volatility_buffer = calculate_volatility_buffer($primary_event_history_ref, $confidence);
            }

            my $forecasted_residual = 0;
            my @all_residuals_for_profile;
            if (exists $vm_historic_residuals{$p_name}) {
                foreach my $event_name (keys %{$vm_historic_residuals{$p_name}}) {
                    push @all_residuals_for_profile, @{$vm_historic_residuals{$p_name}{$event_name}};
                }
            }
            if (@all_residuals_for_profile) {
                $forecasted_residual = calculate_recency_weighted_average(\@all_residuals_for_profile, $today_for_recency, 365) // 0;
            }

            my $trend_adjusted_baseline = $current_baseline_val * $trend_adjustment_factor;
            my $primary_forecast = $trend_adjusted_baseline * $final_multiplier * $volatility_buffer;
            my $combined_forecast = $primary_forecast + $forecasted_residual;
            my $amplification = $event_config->{peak_amplification_factor} // 1.0;
            my $final_recommendation = $combined_forecast * $amplification;

            $vm_forecasts->{$p_name} = $final_recommendation;

            # Create a comma-separated list of active event names
            my $active_events_str = join(", ", map { $_->{_eventName} } @active_events);

            # Extract observed peak from initial results (before forecast overwrites it)
            my $observed_peak_val = undef;
            if ($initial_results_table_href && exists $initial_results_table_href->{$vm_name}{CoreResults}{ProfileValues}{$p_name}) {
                $observed_peak_val = $initial_results_table_href->{$vm_name}{CoreResults}{ProfileValues}{$p_name};
            }

            $seasonal_debug_info{$vm_name}{$p_name} = {
                historical_multipliers => $vm_historic_multipliers{$p_name}{$event_name}{'history'},
                baseline             => $current_baseline_val,
                baseline_source      => $baseline_source_log,
                trend_factor         => $trend_adjustment_factor,
                multiplier           => $final_multiplier,
                volatility           => $volatility_buffer,
                forecasted_residual  => $forecasted_residual,
                amplification_factor => $amplification,
                forecast             => $final_recommendation,
                observed_peak        => $observed_peak_val,  # Store for history
                OutlierWarning       => _get_warning_for_vm_forecast($vm_name, \%outlier_warnings),
                active_events        => $active_events_str,
                is_bootstrap_run     => $is_bootstrap_run,
            };
        }
        $final_forecasts{$vm_name} = $vm_forecasts;
    }

    # Populate data for the verbose historic_snapshot.csv file.
    if ($most_recent_peak_snapshot) {
        foreach my $vm_name (keys %{$most_recent_peak_snapshot->{results}{HistoricPeak}}) {
            $historic_data_for_csv{$vm_name}{'HistoricPeak'} = $most_recent_peak_snapshot->{results}{HistoricPeak}{$vm_name} || {};
            $historic_data_for_csv{$vm_name}{'HistoricBaseline'} = $most_recent_peak_snapshot->{results}{HistoricBaseline}{$vm_name} || {};
        }
    }

    # Return both the forecasts and the historical data for verbose reporting.
    return (\%final_forecasts, \%historic_data_for_csv);
}

# ==============================================================================
# Helper subroutine to write the multi-file CSV output for seasonal forecasts.
# ==============================================================================
# This version safely enhances the seasonal forecast output:
# - It adds a new, rich format ONLY for the 'final_forecast.csv'.
# - The logic for 'current_baseline' and 'historic_snapshot' is UNCHANGED,
#   ensuring no regressions.
# - It is fully commented and robust.
# ==============================================================================
sub write_seasonal_csv_output {
    my ($type, $system_id, $event_name, $timestamp, $data_href) = @_;

    # Prevent creation of empty files
    if (!defined $data_href || !%{$data_href} || !scalar(keys %{$data_href})) {
        print STDERR "  [INFO] No data available for seasonal output type '$type'; file not generated\n";
        return;
    }

    # Sanitise identifiers for use in filenames.
    my $s_id_safe = $system_id;
    my $e_name_safe = $event_name;
    $s_id_safe =~ s/[^a-zA-Z0-9_.-]//g;
    $e_name_safe =~ s/[^a-zA-Z0-9_.-]//g;

    my $filename = File::Spec->catfile($output_dir, "nfit-profile.$s_id_safe.$e_name_safe.$type.$timestamp.csv");
    # Store filename for final notification
    push @generated_files, $filename;

    print STDERR "    ↳  $filename\n";

    open my $fh, '>', $filename or do {
        warn " [WARN] Could not open output file '$filename': $!";
        return;
    };

    # --- Check if this is the rich forecast report ---
    if ($type eq 'final_forecast') {
        # --- PATH A: Generate the new, rich final_forecast.csv ---

        # Build the enhanced header with all standard columns plus the new one.
        my @header = (
            "VM", "TIER", "Hint", "Pattern", "Pressure", "PressureDetail", "SMT",
            "Serial", "SystemType", "Pool Name", "Pool ID", $PEAK_PROFILE_NAME
        );
        push @header, (map { $_->{name} } @csv_visible_profiles);
        # Add the new SeasonalMultiplier column before the entitlement columns.
        push @header, ("SeasonalMultiplier", "Current - ENT");
        print $fh join(",", map { quote_csv($_) } @header) . "\n";

        foreach my $vm_name (sort keys %$data_href) {
            my $vm_data = $data_href->{$vm_name};
            my $report_data = $vm_data->{_report_data} || {};
            my $cfg_csv = $vm_config_data{$vm_name};

            # Gather standard fields for the row
            my $smt_out = $cfg_csv->{smt} // $default_smt_arg;
            my ($serial_out, $systype_out, $poolname_out, $poolid_out, $ent_out) = ('', '', '', '', '');
            if (defined $cfg_csv) {
                $serial_out = $cfg_csv->{serial} // ''; $systype_out = $cfg_csv->{systemtype} // '';
                $poolname_out = $cfg_csv->{pool_name} // ''; $poolid_out = $cfg_csv->{pool_id} // '';
                $ent_out = $cfg_csv->{entitlement} // '';
            }

            my @row = (
                $vm_name, "", $report_data->{hint} // "", $report_data->{pattern} // "",
                $report_data->{pressure} // "", $report_data->{pressure_detail} // "", $smt_out,
                $serial_out, $systype_out, $poolname_out, $poolid_out, "" # Peak is not applicable for a forecast
            );

            my $seasonal_multiplier_for_row = "N/A";

            # --- Targeted Lookup for Seasonal Multiplier ---
            # 1. Determine Pattern/Tier (Respecting User Override)
            # We can use the global %vm_tier_overrides since it's populated at startup
            my $user_tier_override = $vm_tier_overrides{$vm_name} // "";
            my $hint_pattern_str = ($user_tier_override ne "") ? $user_tier_override : ($report_data->{pattern} // "G");
            my ($hint_pattern) = ($hint_pattern_str =~ /^([A-Z])/);
            $hint_pattern //= 'G';

            # 2. Map to standard profile
            my %pattern_to_profile_map = ('O'=>'O3-95W15', 'B'=>'B3-95W15', 'G'=>'G3-95W15', 'P'=>'G3-95W15');
            my $target_profile_name = $pattern_to_profile_map{$hint_pattern} // 'G3-95W15';

            # 3. Fetch multiplier from that specific profile
            if (exists $report_data->{seasonal_debug}{$target_profile_name}) {
                $seasonal_multiplier_for_row = sprintf("%.2f", $report_data->{seasonal_debug}{$target_profile_name}{multiplier});
            } elsif (exists $report_data->{seasonal_debug}{'G3-95W15'}) {
                # Fallback
                 $seasonal_multiplier_for_row = sprintf("%.2f", $report_data->{seasonal_debug}{'G3-95W15'}{multiplier});
            }

            foreach my $profile (@csv_visible_profiles) {
                my $p_name = $profile->{name};
                my $value = $vm_data->{$p_name} // '';
                push @row, (looks_like_number($value) ? sprintf("%.4f", $value) : $value);
            }

            # Add the new column value and the final entitlement value
            push @row, $seasonal_multiplier_for_row;
            push @row, (looks_like_number($ent_out) ? sprintf("%.2f", $ent_out) : $ent_out);

            print $fh join(",", map { quote_csv($_) } @row) . "\n";
        }

    } else {
        # --- PATH B: Original logic for other file types (UNCHANGED) ---

        # --- Build header ---
        my @header;
        if ($type eq 'historic_snapshot') {
            # Specialised header for the detailed historic report.
            @header = ("VM", "MetricType");
        } else {
            # Standard header for 'current_baseline' and 'final_forecast'
            @header = (
                "VM", "TIER", "Hint", "Pattern", "Pressure", "PressureDetail", "SMT",
                "Serial", "SystemType", "Pool Name", "Pool ID", $PEAK_PROFILE_NAME
            );
        }
        push @header, map { $_->{name} } @csv_visible_profiles;

        # --- Write data rows ---
        foreach my $vm_name (sort keys %$data_href) {
            if ($type eq 'historic_snapshot') {
                # --- Special handling for the historic snapshot file ---
                my $hist_data = $data_href->{$vm_name} || {};
                my $peak_data = $hist_data->{HistoricPeak} || {};
                my $base_data = $hist_data->{HistoricBaseline} || {};
                my @peak_row = ($vm_name, "HistoricPeak");
                foreach my $profile (@csv_visible_profiles) {
                    my $value = $peak_data->{$profile->{name}} // '';
                    push @peak_row, (looks_like_number($value) ? sprintf("%.4f", $value) : $value);
                }
                print $fh join(",", map { quote_csv($_) } @peak_row) . "\n";
                my @base_row = ($vm_name, "HistoricBaseline");
                foreach my $profile (@csv_visible_profiles) {
                    my $value = $base_data->{$profile->{name}} // '';
                    push @base_row, (looks_like_number($value) ? sprintf("%.4f", $value) : $value);
                }
                print $fh join(",", map { quote_csv($_) } @base_row) . "\n";
            } else {
                # --- Standard handling for 'current_baseline' file ---
                my @row = ($vm_name);
                my $vm_data = $data_href->{$vm_name};
                foreach my $profile (@csv_visible_profiles) {
                    my $value = $vm_data->{$profile->{name}} // '';
                    push @row, (looks_like_number($value) ? sprintf("%.4f", $value) : $value);
                }
            }
        }

        # Add the new SeasonalMultiplier column before the entitlement columns.
        push @header, ("SeasonalMultiplier", "Current - ENT");
        if ($add_excel_formulas) {
            push @header, ("NFIT - ENT", "NETT", "NETT%");
        }
        print $fh join(",", map { quote_csv($_) } @header) . "\n";

        my $excel_row_num_counter = 1;
        foreach my $vm_name (sort keys %$data_href) {
            my $vm_data = $data_href->{$vm_name};
            my $report_data = $vm_data->{_report_data} || {};
            my $cfg_csv = $vm_config_data{$vm_name};

            # Gather standard fields for the row
            my $smt_out = $cfg_csv->{smt} // $default_smt_arg;
            my ($serial_out, $systype_out, $poolname_out, $poolid_out, $ent_out) = ('', '', '', '', '');
            if (defined $cfg_csv) {
                $serial_out = $cfg_csv->{serial} // ''; $systype_out = $cfg_csv->{systemtype} // '';
                $poolname_out = $cfg_csv->{pool_name} // ''; $poolid_out = $cfg_csv->{pool_id} // '';
                $ent_out = $cfg_csv->{entitlement} // '';
            }

            $excel_row_num_counter++;
            my @row = (
                $vm_name, "", $report_data->{hint} // "", $report_data->{pattern} // "",
                $report_data->{pressure} // "", $report_data->{pressure_detail} // "", $smt_out,
                $serial_out, $systype_out, $poolname_out, $poolid_out, "" # Peak is not applicable for a forecast
            );

            my $seasonal_multiplier_for_row = "N/A";
            foreach my $profile (@csv_visible_profiles) {
                my $vm_data = $data_href->{$vm_name};
                my $p_name = $profile->{name};
                my $value = $vm_data->{$p_name} // '';
                push @row, (looks_like_number($value) ? sprintf("%.4f", $value) : $value);

                # Use the multiplier from the first available profile for the summary column
                if ($seasonal_multiplier_for_row eq 'N/A' && exists $report_data->{seasonal_debug}{$p_name}) {
                    $seasonal_multiplier_for_row = sprintf("%.2f", $report_data->{seasonal_debug}{$p_name}{multiplier});
                }
            }
        }
    }
    close $fh;
}

# ==============================================================================
# Subroutine to calculate a recency-weighted average.
# This is a key statistical function used by the multiplicative model.
# ==============================================================================
sub calculate_recency_weighted_average {
    my ($data_points_aref, $reference_date_obj, $half_life_days) = @_;

    my $sum_weighted_values = 0;
    my $sum_weights = 0;
    my $lambda = log(2) / $half_life_days;

    foreach my $dp (@$data_points_aref) {
        my $value = $dp->{value};
        my $date_obj = $dp->{date};

        my $days_diff = ($reference_date_obj->epoch - $date_obj->epoch) / ONE_DAY;
        $days_diff = 0 if $days_diff < 0;

        my $weight = exp(-$lambda * $days_diff);
        $sum_weighted_values += $value * $weight;
        $sum_weights += $weight;
    }

    if ($sum_weights > 1e-9) {
        return $sum_weighted_values / $sum_weights;
    } else {
        # Fallback: if all weights are zero (e.g., very old data), return a simple average.
        my @values = map { $_->{value} } @$data_points_aref;
        return @values ? (sum0(@values) / scalar(@values)) : undef;
    }
}

# ==============================================================================
# Subroutine to calculate a statistical volatility buffer.
# This increases the forecast based on the standard deviation of historical
# seasonal multipliers to account for year-over-year variance.
# ==============================================================================
sub calculate_volatility_buffer {
    my ($multipliers_aref, $confidence_level) = @_;

    my @values = map { $_->{value} } @$multipliers_aref;

    # Standard deviation requires at least 2 data points.
    return 1.0 if scalar(@values) < 2;

    # --- Calculate Mean and Standard Deviation ---
    my $sum = sum0(@values);
    my $mean = $sum / scalar(@values);
    return 1.0 if $mean == 0; # Avoid division by zero if mean is zero.

    my $sum_sq_diff = 0;
    foreach my $val (@values) {
        $sum_sq_diff += ($val - $mean)**2;
    }
    my $std_dev = sqrt($sum_sq_diff / (scalar(@values) - 1));

    # --- Z-score for common confidence levels ---
    # This lookup table provides the one-sided Z-score for a given confidence level.
    my %z_scores = (
        '0.90' => 1.645,
        '0.95' => 1.960,
        '0.98' => 2.326,
        '0.99' => 2.576,
    );
    my $z_score = $z_scores{$confidence_level} // 1.960; # Default to 95%

    # The buffer is 1 + (a fraction of the coefficient of variation).
    # This adds a percentage uplift proportional to the historical volatility.
    my $volatility_buffer = 1.0 + ($z_score * ($std_dev / $mean));

    # Sanity check: don't let the buffer be less than 1 (non-reducing).
    return $volatility_buffer > 1.0 ? $volatility_buffer : 1.0;
}

# ==============================================================================
# Subroutine to detect all active seasonal events for a given date.
# Returns:
# - An array of event configuration hashes, sorted by priority (desc).
# Notes:
# - CORRECTED to handle both fixed-date and recurring-period definitions.
# ==============================================================================
sub detect_active_events {
    my ($analysis_date_obj, $seasonality_config_href) = @_;

    my @active_events;
    return @active_events unless (defined $analysis_date_obj && ref($analysis_date_obj) eq 'Time::Piece');

    # *** FIX: Use UTC for consistent comparisons ***
    my $analysis_date_utc = gmtime($analysis_date_obj->epoch)->truncate(to => 'day');

    foreach my $event_name (keys %{$seasonality_config_href}) {
        # SAFETY: Skip config metadata sections.
        # We explicitly skip 'Global' and 'Adaptive' because they are
        # Attribute Containers (parents), not Schedulable Events (children).
        # These are not events and have no dates.
        next if $event_name =~ /^(?:Global|Adaptive)$/i;

        my $event_config = $seasonality_config_href->{$event_name};

        # Determine the period for this event relative to the analysis date.
        my ($start_obj, $end_obj) = determine_event_period($event_config, $analysis_date_utc);

        # --- Discovery Horizon Logic ---
        # Hierarchy: Event Config -> Global Config -> Default (30)
        my $lookback_days = $event_config->{seasonal_lookback_days}
                            // $seasonality_config_href->{Global}{seasonal_lookback_days}
                            // 30;
        my $lookahead_days = $event_config->{seasonal_lookahead_days}
                             // $seasonality_config_href->{Global}{seasonal_lookahead_days}
                             // 30;

        my $is_active = 0;

        if (defined $event_config->{dates}) {
            my @date_ranges = split /\s*,\s*/, $event_config->{dates};
            foreach my $range (@date_ranges) {
                $range =~ s/^\s+|\s+$//g;  # Trim whitespace

                if ($range =~ /^(\d{4}-\d{2}-\d{2}):(\d{4}-\d{2}-\d{2})$/) {
                    # Date range format: YYYY-MM-DD:YYYY-MM-DD
                    my $start_obj = Time::Piece->strptime($1, '%Y-%m-%d')->truncate(to => 'day');
                    my $end_obj   = Time::Piece->strptime($2, '%Y-%m-%d')->truncate(to => 'day');

                    # Apply Horizon:
                    # Active if: Analysis Date >= (Start - LookAhead) AND Analysis Date <= (End + LookBack)
                    my $horizon_start = $start_obj - ($lookahead_days * ONE_DAY);
                    my $horizon_end   = $end_obj + ($lookback_days * ONE_DAY);

                    if ($analysis_date_utc >= $horizon_start && $analysis_date_utc <= $horizon_end) {
                        $is_active = 1;
                        last;
                    }
                }
                elsif ($range =~ /^(\d{4}-\d{2}-\d{2})$/) {
                    # Single date format: YYYY-MM-DD (treat as single-day event)
                    my $date_obj = Time::Piece->strptime($1, '%Y-%m-%d')->truncate(to => 'day');

                    # Apply Horizon:
                    # Active if: Analysis Date >= (Date - LookAhead) AND Analysis Date <= (Date + LookBack)
                    my $horizon_start = $date_obj - ($lookahead_days * ONE_DAY);
                    my $horizon_end   = $date_obj + ($lookback_days * ONE_DAY);

                    if ($analysis_date_utc >= $horizon_start && $analysis_date_utc <= $horizon_end) {
                        $is_active = 1;
                        last;
                    }
                }
            }
        }
        elsif (defined $event_config->{period} && lc($event_config->{period}) eq 'monthly') {
            # For recurring events, determine_event_period returns the period relative to the analysis date.
            # However, with a horizon, we might be "outside" the standard period but "inside" the horizon.
            # So we need to check if the *nearest* occurrence is within the horizon.

            # Simplified approach: Use the returned period from determine_event_period.
            # If determine_event_period returns a period, it means it found one relative to the date.
            # But determine_event_period might be strict?
            # Let's check determine_event_period implementation.
            # It seems to find the "current or next" period.

            # Actually, the user requirement implies we should check if the *event itself* is within the horizon.
            # If determine_event_period returns ($start, $end), we check if analysis_date is within [Start-LookAhead, End+LookBack].

            if (defined $start_obj && defined $end_obj) {
                my $horizon_start = $start_obj - ($lookahead_days * ONE_DAY);
                my $horizon_end   = $end_obj + ($lookback_days * ONE_DAY);

                if ($analysis_date_utc >= $horizon_start && $analysis_date_utc <= $horizon_end) {
                    $is_active = 1;
                }
            }
        }

        if ($is_active) {
            $event_config->{_eventName} = $event_name;
            push @active_events, $event_config;
        }
    }

    return sort { ($b->{priority} // 0) <=> ($a->{priority} // 0) } @active_events;
}

# ==============================================================================
# Helper function to validate data line format
# ==============================================================================
sub is_valid_data_line {
    my ($line) = @_;
    # Simply check if line starts with your timestamp format
    return $line && $line =~ /^\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2},/;
}

# ==============================================================================
# Efficiently gets the start and end timestamps from a large cache file
# by reading only the first and last data lines.
# ==============================================================================
sub _get_cache_date_range {
    my ($data_cache_file) = @_;
    return (undef, undef) unless (-f $data_cache_file && -s $data_cache_file);

    open my $fh, '<', $data_cache_file or die "Could not open $data_cache_file: $!";
    my ($start_ts_str, $end_ts_str);

    # Get first data line (line 2) - validate it's a proper data line
    my $header = <$fh>;  # Skip header
    while (my $line = <$fh>) {
        chomp $line;
        if ($line && is_valid_data_line($line)) {
            ($start_ts_str) = split ',', $line, 2;
            last;
        }
    }

    # Get last line by reading backwards line by line
    my $file_size = -s $data_cache_file;
    my $pos = $file_size;
    my $line_buffer = '';
    my $last_valid_line = '';

    # Read backwards in small chunks to build lines
    while ($pos > 0 && !$last_valid_line) {
        my $chunk_size = ($pos < 1024) ? $pos : 1024;
        $pos -= $chunk_size;

        seek $fh, $pos, 0;
        my $chunk;
        read $fh, $chunk, $chunk_size;

        # Prepend to our buffer
        $line_buffer = $chunk . $line_buffer;

        # Process complete lines from the end
        my @lines = split /\n/, $line_buffer;

        # If we're not at the start, the first line might be partial
        if ($pos > 0) {
            $line_buffer = shift @lines;  # Keep partial line for next iteration
        } else {
            $line_buffer = '';
        }

        # Check lines from end to start
        for my $line (reverse @lines) {
            if ($line && is_valid_data_line($line)) {
                $last_valid_line = $line;
                last;
            }
        }
    }

    if ($last_valid_line) {
        ($end_ts_str) = split ',', $last_valid_line, 2;
    }

    close $fh;

    # Parse timestamps
    my ($start_obj, $end_obj);
    eval { $start_obj = Time::Piece->strptime($start_ts_str, "%Y-%m-%d %H:%M:%S") if $start_ts_str; };
    if ($@) {
        warn " [WARN] Could not parse start timestamp '$start_ts_str' from cache.";
        return (undef, undef);
    }
    eval { $end_obj = Time::Piece->strptime($end_ts_str, "%Y-%m-%d %H:%M:%S") if $end_ts_str; };
    if ($@) {
        warn " [WARN] Could not parse end timestamp '$end_ts_str' from cache.";
        return (undef, undef);
    }

    return ($start_obj, $end_obj);
}

sub _get_warning_for_vm_forecast {
    my ($vm_name, $warnings_href) = @_;
    # This finds the most recent warning for a given VM, as the hash is keyed by snapshot date.
    foreach my $key (sort { $b cmp $a } keys %$warnings_href) {
        if (exists $warnings_href->{$key}{$vm_name}) {
            return $warnings_href->{$key}{$vm_name};
        }
    }
    return undef;
}

sub raw_init_vm_bucket {
    my ($store_hr, $vm) = @_;
    $store_hr->{$vm} //= {};
    die "raw_init_vm_bucket: VM bucket not HASH for $vm"
        unless ref($store_hr->{$vm}) eq 'HASH';
    return $store_hr->{$vm};
}

sub raw_set_profile_rows {
    my ($store_hr, $vm, $profile, $rows_aref) = @_;
    my $bucket = raw_init_vm_bucket($store_hr, $vm);
    $bucket->{$profile} = (ref($rows_aref) eq 'ARRAY') ? $rows_aref : [];
}

sub raw_get_profile_rows {
    my ($store_hr, $vm, $profile) = @_;
    return [] unless ref($store_hr) eq 'HASH';
    my $b = $store_hr->{$vm};
    return [] unless ref($b) eq 'HASH';
    my $a = $b->{$profile};
    return (ref($a) eq 'ARRAY') ? $a : [];
}

sub raw_get_last_state {
    my ($store_hr, $vm, $profile) = @_;
    my $rows = raw_get_profile_rows($store_hr, $vm, $profile);
    return @$rows ? $rows->[-1] : undef;
}

sub _safe_dig {
    my ($href, @path) = @_;
    my $cur = $href;
    for my $k (@path) {
        return undef unless ref($cur) eq 'HASH' && exists $cur->{$k};
        $cur = $cur->{$k};
    }
    return $cur;
}

# ==============================================================================
# SUBROUTINE: map_growth_confidence
# PURPOSE:    Maps a statistical p-value to a human-readable confidence level
#             for the CSV report and rationale log.
# ARGUMENTS:
#   1. $p_value (numeric): The Mann-Kendall p-value (or undef)
#   2. $method_used (string): The method string ('sen_slope', 'none', etc.)
# RETURNS:
#   - Confidence string: 'high', 'medium', 'low', or 'n/a'
# ==============================================================================
sub map_growth_confidence {
    my ($p_value, $method_used) = @_;

    # Default to 'n/a' (Not Applicable) if no growth was calculated or no p-value
    return 'n/a' if (!defined $method_used ||
                     $method_used eq 'none' ||
                     $method_used eq 'sen_slope_not_significant' ||
                     $method_used eq 'sen_slope_too_small' ||
                     !defined $p_value);

    # A p-value of 0 means the trend was extremely significant (p < 0.0001)
    if ($p_value == 0) {
        return 'high';
    }
    # Map based on standard statistical thresholds
    if ($p_value < 0.01) {
        return 'high';      # Very strong evidence (p < 1%)
    } elsif ($p_value < 0.05) {
        return 'medium';    # Conventional significance (p < 5%)
    } else {
        return 'low';       # Weak or no evidence (p >= 5%)
    }
}

# ==============================================================================
# SUBROUTINE: _write_standard_csv_report (Refactored for Assimilation Map)
# PURPOSE:    Encapsulates the entire CSV generation process. It now reads all
#             its data from the final, fully populated assimilation map.
# ==============================================================================
sub _write_standard_csv_report {
    my ($assimilation_map_ref, $report_type, $system_id, $file_timestamp, $is_multiplicative_run_flag, $is_recency_decay_run_flag, $is_predictive_peak_run_flag, $adaptive_runq_saturation_thresh) = @_;

    # Ensure we have data to process before creating a file.
    # The @vm_order array is still used to control the iteration order.
    return unless (@vm_order && ref($assimilation_map_ref) eq 'HASH');

    my $system_id_for_filename = $system_id || 'standard';
    $system_id_for_filename =~ s/[^a-zA-Z0-9_.-]//g;
    my $report_type_for_filename = $report_type;
    $report_type_for_filename =~ s/[^a-zA-Z0-9_.-]//g;

    my $output_filename = File::Spec->catfile($output_dir, "nfit-profile.$system_id_for_filename.$report_type_for_filename.$file_timestamp.csv");
    push @generated_files, $output_filename;

    open my $out_fh, '>', $output_filename or die "FATAL: Cannot open output file '$output_filename': $!";

    my @csv_visible_profiles = grep { $_->{csv_output} } @profiles;

    my @header_for_this_report = @output_header_cols_csv;
    my $formula_col_offset = 0;

    my ($ent_idx) = grep { $header_for_this_report[$_] eq 'Current - ENT' } 0..$#header_for_this_report;
    $ent_idx //= scalar(@header_for_this_report);
    if ($is_multiplicative_run_flag) {
        my @new_cols = ("SeasonalEvents", "T3_SeasonalMultiplier", "Baseline_P99", "BaselineSource", "ForecastModel");

        # Remove all standard Growth columns (Theil-Sen) for this model (as this will perform a seasonal forecast instead)
        @header_for_this_report = grep {
            !/^(ProjectionDays|GrowthMethod|GrowthConfidence|GrowthTrend|GrowthSignificance)$/
        } @header_for_this_report;

        # Inject seasonal columns
        ($ent_idx) = grep { $header_for_this_report[$_] eq 'Current - ENT' } 0..$#header_for_this_report;
        $ent_idx //= scalar(@header_for_this_report); # Fallback
        splice @header_for_this_report, $ent_idx, 0, @new_cols;
        $formula_col_offset = scalar(@new_cols);
    } elsif ($is_recency_decay_run_flag || $nfit_enable_windowed_decay || $nfit_decay_over_states) {
        # Add GrowthAdj (from G3) and GrowthAdj_Source
        my @new_cols = ("GrowthAdj", "GrowthAdj_Min", "GrowthAdj_Max", "GrowthAdj_Source", "ProjectionDays", "GrowthMethod", "GrowthConfidence", "GrowthTrend", "GrowthSignificance");
        splice @header_for_this_report, $ent_idx, 0, @new_cols;
        $formula_col_offset = scalar(@new_cols);
    } elsif ($is_predictive_peak_run_flag) {
        # Remove growth/runq columns for this model
        @header_for_this_report = grep {
            !/^(ProjectionDays|GrowthMethod|GrowthConfidence|GrowthTrend|GrowthSignificance|GrowthAdj.*)$/
        } @header_for_this_report;

        # Find ent_idx again after grep
        ($ent_idx) = grep { $header_for_this_report[$_] eq 'Current - ENT' } 0..$#header_for_this_report;
        $ent_idx //= scalar(@header_for_this_report); # Fallback

        my @new_cols = ("ActiveEvents", "P99_PredictedDelta", "Baseline_P99", "BaselineSource", "ForecastModel");

        splice @header_for_this_report, $ent_idx, 0, @new_cols;
        $formula_col_offset = scalar(@new_cols);
    }

    print {$out_fh} join(",", map { quote_csv($_) } @header_for_this_report) . "\n";

    my $excel_row_num_counter = 1;

    foreach my $vm_name (@vm_order) {
        $excel_row_num_counter++;
        my @data_row_csv;

        # Get the complete, final data for this VM from the assimilation map.
        my $vm_map_ref = $assimilation_map_ref->{$vm_name};
        my $cfg = $vm_map_ref->{Configuration};
        my $core = $vm_map_ref->{CoreResults};
        my $hinting = $vm_map_ref->{Hinting};
        my $modifiers = $vm_map_ref->{CSVModifiers};

        # --- START: Hint-Aware Rationale Selection for CSV ---
        # Call generate_sizing_hint here, *once* per VM, to get the
        # authoritative hint. This is now safe, as the assimilation map is fully built.
        # We use a safe check for $vm_name in %vm_config_data.
        my ($hint_type_tier, $hint_pattern_shape, $pressure_bool, $pressure_detail_str);

        if (exists $vm_config_data{$vm_name}) {
            ($hint_type_tier, $hint_pattern_shape, $pressure_bool, $pressure_detail_str) =
                generate_sizing_hint($vm_map_ref, undef, $adaptive_runq_saturation_thresh);
        } else {
            # VM is not in config file; fall back to safe defaults
            ($hint_type_tier, $hint_pattern_shape, $pressure_bool, $pressure_detail_str) = ('G3', 'G', 0, 'ConfigMissing');
        }

        ## Deterministic fallback chain for profile selection for growth and for the RunQ modifier columns

        # Map the hint pattern (O, B, G) to the correct T3 planning profile
        my $hint_pattern = ($hinting->{AutoTier} =~ /^([A-Z])/) ? $1 : 'G';
        my %pattern_to_profile_map = ('O'=>'O3-95W15', 'B'=>'B3-95W15', 'G'=>'G3-95W15', 'P'=>'G3-95W15');

        my $source_profile_name = $pattern_to_profile_map{$hint_pattern} // 'G3-95W15';

        # --- Populate CSV row directly from the map ---
        push @data_row_csv, $vm_name, ($tier_override_for_csv{$vm_name} // "");
        # Use the hints previously generated
        push @data_row_csv, $hint_type_tier, $hint_pattern_shape;
        push @data_row_csv, ($pressure_bool ? "True" : "False"), $pressure_detail_str // "";
        push @data_row_csv, $cfg->{smt} // "";
        push @data_row_csv, $cfg->{serial_number} // "", $vm_config_data{$vm_name}{systemtype} // ""; # systemtype from old config is richer
        push @data_row_csv, $vm_config_data{$vm_name}{pool_name} // "", $cfg->{pool_id} // "";

        # This now correctly fetches the single, authoritative values for the VM (selecting the same profile name as $source_profile_name)
        push @data_row_csv, (defined $modifiers->{RunQ_Tactical} ? sprintf("%+.3f", $modifiers->{RunQ_Tactical}) : "0.000");
        push @data_row_csv, (defined $modifiers->{RunQ_Strategic} ? sprintf("%+.3f", $modifiers->{RunQ_Strategic}) : "0.000");
        push @data_row_csv, (defined $modifiers->{RunQ_Potential} && looks_like_number($modifiers->{RunQ_Potential}) ? sprintf("%+.3f", $modifiers->{RunQ_Potential}) : ($modifiers->{RunQ_Potential} // "0.000"));
        push @data_row_csv, $modifiers->{RunQ_Source} // "";

        # Push the Peak value, ensuring 3-digit precision
        my $peak_val = $core->{PeakValue};
        push @data_row_csv, (defined $peak_val && looks_like_number($peak_val)) ? sprintf("%.3f", $peak_val) : ($peak_val // "");

        # Push all profile values with enforced 3-digit precision
        foreach my $profile (@csv_visible_profiles) {
            my $val = $core->{ProfileValues}{$profile->{name}};
            my $formatted_val = (defined $val && looks_like_number($val)) ? sprintf("%.3f", $val) : ($val // "");
            push @data_row_csv, $formatted_val;
        }

        # Model-specific columns
        # Important Notes:
        # - A seasonal forecast should not ignore current structural deficits.
        # - Seasonal Forecasts Recommendations = (Baseline x SeasonalMultiplier) + RunQ_Tactical
        if ($is_multiplicative_run_flag) {
            # --- Targeted Lookup for Seasonal Multiplier ---
            # 1. Determine the VM's Pattern/Tier (Respecting User Override)
            my $user_tier_override = $tier_override_for_csv{$vm_name} // "";
            my $pattern_source = ($user_tier_override ne "") ? $user_tier_override : ($hinting->{AutoTier} // "G");
            my ($hint_pattern) = ($pattern_source =~ /^([A-Z])/);
            $hint_pattern //= 'G';

            # 2. Map to standard profile
            my %pattern_to_profile_map = ('O'=>'O3-95W15', 'B'=>'B3-95W15', 'G'=>'G3-95W15', 'P'=>'G3-95W15');
            my $multiplier_source_profile = $pattern_to_profile_map{$hint_pattern} // 'G3-95W15';

            # 3. Fetch multiplier from that specific profile
            my $s_data = $seasonal_debug_info{$vm_name}{$multiplier_source_profile} || {};

            # Fallback if specific profile missing (though unlikely if logic holds)
            if (!keys %$s_data) {
                 $s_data = $seasonal_debug_info{$vm_name}{'G3-95W15'} || {};
            }

            # ActiveEvents (from the specific profile, or generally for the VM if stored globally, but here per-profile)
            push @data_row_csv, ($s_data->{active_events} // "None");

            # SeasonalMultiplier
            push @data_row_csv, (defined $s_data->{multiplier} ? sprintf("%.2f", $s_data->{multiplier}) : "N/A");

            # Baseline_PhysC
            push @data_row_csv, (defined $s_data->{baseline} ? sprintf("%.3f", $s_data->{baseline}) : "N/A");

            # BaselineSource
            push @data_row_csv, ($s_data->{baseline_source} // "N/A");

            # ForecastModel (New Column)
            push @data_row_csv, "Seasonal Multiplicative";

        } elsif ($is_recency_decay_run_flag || $nfit_enable_windowed_decay || $nfit_decay_over_states) {

            # 1. Determine the source profile for growth rationale.
            #    Priority: User TIER > AutoTier > Fallback to 'G'.
            my $user_tier_override_csv = $tier_override_for_csv{$vm_name} // "";
            my $auto_tier_csv = $hinting->{AutoTier} // "G";

            my $pattern_source_csv = ($user_tier_override_csv ne "") ? $user_tier_override_csv : $auto_tier_csv;
            my ($pattern_csv) = ($pattern_source_csv =~ /^([A-Z])/);
            $pattern_csv //= 'G'; # Default to 'G' if regex fails

            # 1. Map the pattern to the corresponding 95th percentile (T3) profile.
            my %pattern_to_profile_map_csv = ('O' => 'O3-95W15', 'B' => 'B3-95W15', 'G' => 'G3-95W15', 'P' => 'G3-95W15');
            my $GROWTH_RATIONALE_SOURCE_PROFILE = $pattern_to_profile_map_csv{$pattern_csv} // 'G3-95W15';

            # Growth Modifier Column Logic with Fallback
            # 2. Attempt to get growth rationale from primary source profile
            my $gr = _safe_dig($vm_map_ref, 'GrowthRationaleByProfile', $GROWTH_RATIONALE_SOURCE_PROFILE) || {};

            # 2a. Check if the primary profile had a computational failure
            my $is_computational_failure = 0;
            if (exists $gr->{skip_reason} && defined $gr->{skip_reason}) {
                # Check for insufficient data
                $is_computational_failure = 1 if ($gr->{skip_reason} =~ /Insufficient daily data points/i);
            } elsif (exists $gr->{hamed_rao_adjustment_factor} &&
                     defined $gr->{hamed_rao_adjustment_factor} &&
                     $gr->{hamed_rao_adjustment_factor} <= 0) {
                # Check for Hamed-Rao corruption
                $is_computational_failure = 1;
            } elsif (exists $gr->{method_used} &&
                     $gr->{method_used} ne 'none' &&
                     !defined $gr->{sen_slope}) {
                # Calculation attempted but critical metrics missing
                $is_computational_failure = 1;
            }

            # 2b. If computational failure and not already using G3-95W15, fallback to G3-95W15
            my $fallback_applied = 0;
            if ($is_computational_failure && $GROWTH_RATIONALE_SOURCE_PROFILE ne 'G3-95W15') {
                if ($verbose) {
                    warn sprintf(
                        "[INFO] Growth calculation failed for %s on VM %s (reason: %s). " .
                        "Falling back to G3-95W15.\n",
                        $GROWTH_RATIONALE_SOURCE_PROFILE,
                        $vm_name,
                        $gr->{skip_reason} // 'computational error'
                    );
                }

                # Attempt fallback to G3-95W15
                my $fallback_gr = _safe_dig($vm_map_ref, 'GrowthRationaleByProfile', 'G3-95W15') || {};

                # Only use fallback if it succeeded
                my $fallback_failed = 0;
                if (exists $fallback_gr->{skip_reason} && $fallback_gr->{skip_reason} =~ /Insufficient daily data points/i) {
                    $fallback_failed = 1;
                } elsif (exists $fallback_gr->{hamed_rao_adjustment_factor} &&
                         defined $fallback_gr->{hamed_rao_adjustment_factor} &&
                         $fallback_gr->{hamed_rao_adjustment_factor} <= 0) {
                    $fallback_failed = 1;
                }

                if (!$fallback_failed) {
                    # Fallback succeeded
                    $gr = $fallback_gr;
                    $GROWTH_RATIONALE_SOURCE_PROFILE = 'G3-95W15';
                    $fallback_applied = 1;
                } else {
                    # Both failed - keep original failure for transparency
                    if ($verbose) {
                        warn sprintf(
                            "WARNING: Fallback to G3-95W15 also failed for VM %s. " .
                            "No growth adjustment will be applied.\n",
                            $vm_name
                        );
                    }
                }
            }

            # 3. Get the specific GrowthAdj for this source profile (for the 'GrowthAdj' column)
            my $source_profile_growth_adj = _safe_dig($vm_map_ref, 'Growth', 'adjustments', $GROWTH_RATIONALE_SOURCE_PROFILE) // 0;

            # 4. Get the VM-wide Min/Max adjustments (from all profiles)
            my $growth_min = $vm_map_ref->{Growth}{min_adj} // 0;
            my $growth_max = $vm_map_ref->{Growth}{max_adj} // 0;

            # 5. Format and push the new columns
            my $source_adj_str = sprintf("%.3f", $source_profile_growth_adj);
            my $min_str = sprintf("%.3f", $growth_min);
            my $max_str = sprintf("%.3f", $growth_max);

            push @data_row_csv, $source_adj_str, $min_str, $max_str, $GROWTH_RATIONALE_SOURCE_PROFILE;

            # 6. Populate all other rationale fields from the source profile's rationale
            #    (This logic is now sourced from $gr, not $source_profile_name)
            my $proj_days = $gr->{projection_days};
            if (defined $proj_days && looks_like_number($proj_days)) {
                push @data_row_csv, int($proj_days);  # Integer format
            } else {
                push @data_row_csv, 'n/a';  # No growth calculation performed
            }

            # GrowthMethod
            my $method = $gr->{method_used} // 'none';
            push @data_row_csv, $method;

            # GrowthConfidence
            my $p_value = $gr->{sen_p_value};
            push @data_row_csv, map_growth_confidence($p_value, $method);

            # GrowthTrend
            # Ensure 'trend' is 'none' if method wasn't a significant trend
            my $trend = $gr->{sen_trend} // 'none';
            if ($method eq 'sen_slope_not_significant' || $method eq 'sen_slope_too_small' || $method eq 'none') {
                $trend = 'none';
            }
            push @data_row_csv, $trend;

            # GrowthSignificance (the p-value itself)
            if (defined $p_value && $method ne 'none') {
                # Format p-value to 4 decimal places, handling p=0
                push @data_row_csv, ($p_value == 0) ? "0.0000" : sprintf("%.4f", $p_value);
            } else {
                push @data_row_csv, 'n/a';
            }
            # --- END: Growth Trend Columns ---

        } elsif ($is_predictive_peak_run_flag) {
            my $s_data = $seasonal_debug_info{$vm_name}{'P-99W1'} || {};

            # 1. ActiveEvents
            push @data_row_csv, ($s_data->{active_events} // $apply_seasonality_event);

            # 2. PredictedDelta (Net Growth)
            # Formula: PredictedPeak - TrueBaseline
            my $delta_val = "N/A";
            if (defined $s_data->{PredictedPeak} && defined $s_data->{TrueBaseline} && looks_like_number($s_data->{PredictedPeak}) && looks_like_number($s_data->{TrueBaseline})) {
                $delta_val = sprintf("%+.3f", $s_data->{PredictedPeak} - $s_data->{TrueBaseline});
            }
            push @data_row_csv, $delta_val;

            # 3. Baseline_P99 (Mapped from TrueBaseline)
            push @data_row_csv, (defined $s_data->{TrueBaseline} ? sprintf("%.3f", $s_data->{TrueBaseline}) : "N/A");

            # 4. BaselineSource
            push @data_row_csv, ($s_data->{baseline_source} // "Generic Non-Peak");

            # 5. ForecastModel (Includes source info)
            my $source_raw = $s_data->{FinalSource} // 'Unknown';
            my $source_display = $source_raw;

            # Restore the user-friendly mapping for the report
            if ($source_raw eq 'PeakPrediction') {
                $source_display = 'PredictedPeak';
            }
            elsif ($source_raw eq 'TrueBaseline') {
                $source_display = 'BaselineIsHigher';
            }

            push @data_row_csv, "Predictive ($source_display)";
        }

        # Entitlement and formula columns
        my $ent_out = $cfg->{entitlement};
        my $current_ent_display = (looks_like_number($ent_out)) ? sprintf("%.2f", $ent_out) : ($ent_out // "");
        push @data_row_csv, $current_ent_display; # Always add Current - ENT
        if ($add_excel_formulas) {
            my $nfit_ent_formula_str = generate_nfit_ent_formula($excel_row_num_counter, scalar(@csv_visible_profiles), $formula_col_offset);
            my $col_nfit_ent_letter = get_excel_col_name(scalar(@output_header_cols_csv) - 2 + $formula_col_offset);
            my $col_curr_ent_letter = get_excel_col_name(scalar(@output_header_cols_csv) - 3 + $formula_col_offset);
            my $nett_user_formula_str = sprintf("=(%s%d-%s%d)", $col_nfit_ent_letter, $excel_row_num_counter, $col_curr_ent_letter, $excel_row_num_counter);
            my $nett_perc_user_formula_str = sprintf("=IFERROR((%s%d-%s%d)/%s%d,\"\")", $col_nfit_ent_letter, $excel_row_num_counter, $col_curr_ent_letter, $excel_row_num_counter, $col_curr_ent_letter, $excel_row_num_counter);
            push @data_row_csv, $nfit_ent_formula_str, $nett_user_formula_str, $nett_perc_user_formula_str;
        }

         print {$out_fh} join(",", map { quote_csv($_) } @data_row_csv) . "\n";
    }

    if ($add_excel_formulas) {
        my %serials_map;
        foreach my $vm_name (@vm_order) {
            my $serial = $assimilation_map_ref->{$vm_name}{Configuration}{serial_number};
            $serials_map{$serial} = 1 if (defined $serial && $serial ne '');
        }
        my @sorted_serials = sort keys %serials_map;
        print_csv_footer($out_fh, $excel_row_num_counter, $physc_data_file, scalar(@csv_visible_profiles), \@sorted_serials, $formula_col_offset);
    }

    close $out_fh;
}

# ==============================================================================
# Subroutine to format a duration in seconds into a human-readable string.
# ==============================================================================
sub format_duration {
    my ($seconds) = @_;
    if ($seconds >= 3600) {
        my $hours = int($seconds / 3600);
        my $minutes = int(($seconds % 3600) / 60);
        return sprintf("%dh %dm", $hours, $minutes);
    }
    return sprintf("%.2fs", $seconds) if $seconds < 60;
    my $minutes = int($seconds / 60);
    my $remaining_seconds = $seconds % 60;
    return sprintf("%dm %.2fs", $minutes, $remaining_seconds);
}

# Helper function to add months to a Time::Piece object
sub add_months {
    my ($time_obj, $months) = @_;

    my $year = $time_obj->year;
    my $month = $time_obj->mon + $months;
    my $day = $time_obj->mday;

    # Handle year rollover
    while ($month > 12) {
        $month -= 12;
        $year++;
    }
    while ($month < 1) {
        $month += 12;
        $year--;
    }

    # Handle day overflow (e.g., Jan 31 + 1 month should be Feb 28/29)
    my $days_in_month = days_in_month($year, $month);
    if ($day > $days_in_month) {
        $day = $days_in_month;
    }

    return Time::Piece->strptime(sprintf('%04d-%02d-%02d', $year, $month, $day), '%Y-%m-%d');
}

# Helper function to get days in a month
sub days_in_month {
    my ($year, $month) = @_;
    my @days = (31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31);

    # Check for leap year
    if ($month == 2 && (($year % 4 == 0 && $year % 100 != 0) || $year % 400 == 0)) {
        return 29;
    }

    return $days[$month - 1];
}

# ==============================================================================
# Calculates the start and end dates for a recurring monthly event based on a
# given reference date. It correctly handles month-end boundaries and invalid
# day-of-month configurations.
# ==============================================================================
sub _get_recurring_monthly_period {
    my ($base_date_obj, $day_of_period, $duration_days) = @_;

    my $end_date_obj;
    if ($day_of_period == -1) {
        # Last day of month: get first day of next month, then subtract one day
        my $next_month = add_months($base_date_obj, 1);
        my $first_of_next = Time::Piece->strptime(sprintf('%04d-%02d-01', $next_month->year, $next_month->mon), '%Y-%m-%d');
        $end_date_obj = $first_of_next - ONE_DAY;
    } else {
        # Specific day of month, clamped to be valid for that month
        my $year  = $base_date_obj->year;
        my $month = $base_date_obj->mon;
        my $day   = $day_of_period;

        my $days_in_target_month = days_in_month($year, $month);
        if ($day > $days_in_target_month) {
            # Clamp to the last day of the month if config is invalid (e.g., day 31 in Feb)
            $day = $days_in_target_month;
        }
        $end_date_obj = Time::Piece->strptime(sprintf('%04d-%02d-%02d', $year, $month, $day), '%Y-%m-%d');
    }

    # This calculation remains the same
    my $start_date_obj = $end_date_obj - (ONE_DAY * ($duration_days - 1));
    return ($start_date_obj, $end_date_obj);
}

# ==============================================================================
# Subroutine to log the rationale for a multiplicative seasonal forecast.
# It provides a clear, aligned, and explanatory summary of how the forecast
# was derived for each profile, matching the format of the standard log.
# ==============================================================================
# ==============================================================================
# SUBROUTINE: _log_seasonal_config_context
# PURPOSE:    Logs the configuration context for a seasonal event.
# ==============================================================================
sub _log_seasonal_config_context {
    my ($fh, $event_config, $context_href) = @_;
    return unless $fh && $event_config;
    my $label_width = 35;
    my $event_name = $event_config->{_eventName} // "Unknown Event";

    print {$fh} "######################################################################\n";
    print {$fh} "# Seasonal Event Configuration: $event_name\n";
    print {$fh} "######################################################################\n";

    # Event Description
    my $desc = $event_config->{description} // "N/A";
    $desc =~ s/^"|"$//g;
    printf {$fh} "  %-${label_width}s : %s\n", "Event Description", $desc;

    # Event Definition
    my $def = $event_config->{dates} // ($event_config->{period} ? $event_config->{period} . " (Day " . ($event_config->{day_of_period} // "?") . ")" : "N/A");
    printf {$fh} "  %-${label_width}s : %s\n", "Event Definition", $def;

    # --- NEW FIELDS ---
    if ($context_href) {
        # Analysis Window
        if ($context_href->{start} && $context_href->{end}) {
            my $days = int(($context_href->{end}->epoch - $context_href->{start}->epoch) / 86400) + 1;
            printf {$fh} "  %-${label_width}s : %s to %s (%d days)\n",
                "Analysis Window", $context_href->{start}->ymd, $context_href->{end}->ymd, $days;
        }

        # Sampling Interval
        if ($context_href->{interval}) {
            printf {$fh} "  %-${label_width}s : %s seconds\n", "Sampling Interval", $context_href->{interval};
        }

        # Projection Horizon (Context specific)
        my $horizon = "Next Occurrence"; # Default for multiplicative/predictive
        if (($event_config->{model}//'') eq 'recency_decay') {
            $horizon = "Standard Growth Projection";
        }
        printf {$fh} "  %-${label_width}s : %s\n", "Projection Horizon", $horizon;
    }
    # ------------------

    # Baseline Lookback
    my $lookback = $event_config->{baseline_period_days} // "N/A";
    printf {$fh} "  %-${label_width}s : %s days\n", "Baseline Lookback", $lookback;

    # Concurrent Event Interaction
    my $policy = lc($event_config->{interaction_policy} // 'exclusive');
    unless ($policy =~ /^(exclusive|combined)$/) {
        $policy = 'exclusive';
    }

    my $interaction_str = "policy=$policy";
    if ($policy eq 'combined') {
        my $dampening = $event_config->{interaction_dampening_factor};
        if (defined $dampening && looks_like_number($dampening)) {
            $interaction_str .= " (dampening: $dampening)";
        }
    }
    printf {$fh} "  %-${label_width}s : %s\n", "Concurrent Event Interaction", $interaction_str;

    # Rule Version
    my $version = $event_config->{last_modified} // "N/A";
    printf {$fh} "  %-${label_width}s : %s\n", "Rule Version", $version;

    # Data Exclusions
    my $exclusions = $event_config->{exclude_dates} // "None";
    printf {$fh} "  %-${label_width}s : %s\n", "Data Exclusions", $exclusions;

    print {$fh} "======================================================================\n";
}

# Logs Rationale for Seasonal Forecasting models
sub log_multiplicative_seasonal_rationale
{
    my ($fh, $event_config, $context) = @_;

    # Ensure the script does not die if the log file handle is not valid.
    return unless $fh;

    # Log Configuration Context
    if (defined $event_config) {
        _log_seasonal_config_context($fh, $event_config, $context);
    }

    # Iterate through each VM that has results, maintaining a consistent order.
    foreach my $vm_name (sort @vm_order)
    {
        # Check if there is seasonal debug information available for this VM.
        next unless exists $seasonal_debug_info{$vm_name};

        # Print a clear, top-level header for each VM in the log.
        print {$fh} "\n######################################################################\n";
        print {$fh} "# Rationale for VM: $vm_name\n";
        print {$fh} "# CPU Forecasting Model: Multiplicative Seasonal Forecast (Event: $apply_seasonality_event)\n";
        print {$fh} "######################################################################\n\n";

        # Iterate through each profile to log its specific forecast calculation.
        foreach my $profile (@profiles)
        {
            my $p_name = $profile->{name};

            # --- Special Handling for P-99W1 ---
            if (defined $MANDATORY_PEAK_PROFILE_FOR_HINT && $p_name eq $MANDATORY_PEAK_PROFILE_FOR_HINT) {
                my $label_width = 38;
                print {$fh} "======================================================================\n";
                printf {$fh} "%-${label_width}s : %s\n", "Profile Processed", parse_profile_name_for_log($p_name);
                print {$fh} "----------------------------------------------------------------------\n";
                print {$fh} "  CPU Forecasting Model: Multiplicative Seasonal Forecast ($apply_seasonality_event)\n\n";
                print {$fh} "  - The P-99W1 profile is a special case used for peak analysis.\n";
                print {$fh} "  - It represents the smoothed (1-min SMA) 99.75th percentile of the\n";
                print {$fh} "    unfiltered historical data.\n";
                print {$fh} "  - NO growth, RunQ, or forecasting modifiers are applied to this value.\n\n";

                my $p99_val = $seasonal_debug_info{$vm_name}{$p_name}{forecast} // 0;
                printf {$fh} "  Final Unmodified Value              : %.4f cores\n", ($p99_val // 'N/A');
                print {$fh} "======================================================================\n\n";
                next;
            }

            next unless exists $seasonal_debug_info{$vm_name}{$p_name};

            my $s_data = $seasonal_debug_info{$vm_name}{$p_name};
            my $profile_desc = parse_profile_name_for_log($p_name);

            # Use a fixed width for labels to ensure consistent alignment of colons.
            my $label_width = 36;

            print {$fh} "======================================================================\n";
            printf {$fh} "%-${label_width}s   : %s\n", "Profile Processed", $profile_desc;
            print {$fh} "----------------------------------------------------------------------\n";
            print {$fh} "  CPU Forecasting Model: Multiplicative Seasonal Forecast ($apply_seasonality_event)\n\n";
            printf {$fh} "  %-${label_width}s : %.4f cores\n", "Current Baseline Value", $s_data->{baseline};
            printf {$fh} "  %-${label_width}s : %.4f\n", "Historical Multiplier", $s_data->{multiplier};

            # Add detailed breakdown of how the multiplier was derived
            if (exists $s_data->{historical_multipliers} && ref($s_data->{historical_multipliers}) eq 'ARRAY' && @{$s_data->{historical_multipliers}}) {
                printf {$fh} "  %-${label_width}s : %s\n", "  Multiplier Methodology", "Recency-weighted average of past events:";
                foreach my $hist_entry (@{$s_data->{historical_multipliers}}) {
                    my $hist_date = ref($hist_entry->{date}) ? $hist_entry->{date}->date : $hist_entry->{date};
                    printf {$fh} "  %-${label_width}s   %s: %.4f\n", "", $hist_date, $hist_entry->{value};
                }
            }

            printf {$fh} "  %-${label_width}s : %.4f\n", "Volatility Buffer", $s_data->{volatility};
            printf {$fh} "  %-${label_width}s : %.4f cores\n", "Forecasted Peak Residual", $s_data->{forecasted_residual};
            printf {$fh} "  %-${label_width}s : %.2f\n", "Peak Amplification Factor", $s_data->{amplification_factor};
            printf {$fh} "  %-${label_width}s : %.4f cores\n", "Final Forecasted Value", $s_data->{forecast};
            print {$fh} "    - Calculation : ((Baseline * Multiplier * Buffer) + Residual) * Amplification\n";
            # --- Print outlier warning if it exists ---
            if (defined $s_data->{OutlierWarning} && $s_data->{OutlierWarning} ne '') {
                print {$fh} "\n  --- Workload Volatility Alert ---\n";
                print {$fh} "  " . $s_data->{OutlierWarning} . "\n";
            }
            print {$fh} "======================================================================\n\n";
        }
    }
}

# ==============================================================================
# Subroutine to determine the correct seasonal analysis path.
# It checks if a multiplicative model has enough historical data to run. If not,
# it switches to the defined fallback event. This makes the tool resilient.
# It returns the name of the event that should ultimately be executed.
# Returns undef on cold-start to signal graceful skip.
# ==============================================================================
sub determine_seasonal_analysis_path {
    my ($event_config, $system_cache_dir, $event_name) = @_;

    # This function is only relevant for models that have historical prerequisites.
    my $model_to_run = $event_config->{model} // '';
    return $event_name unless ($model_to_run eq 'multiplicative_seasonal' || $model_to_run eq 'predictive_peak');

    # Use // to allow explicit 0 for cold-start bootstrap runs
    my $min_history_required = $event_config->{min_historical_years} // 1;

    # Read the unified history to count available snapshots for this event.
    my $unified_history = read_unified_history($system_cache_dir);
    my $total_history_months = scalar(keys %$unified_history);

    my $history_count = 0;
    foreach my $month_data (values %$unified_history) {
        if (exists $month_data->{SeasonalEventSnapshots}{$event_name}) {
            $history_count++;
        }
    }

    if ($history_count >= $min_history_required) {
        print STDERR "  [INFO] Historical snapshots for event '$event_name': $history_count (>= min. $min_history_required)\n";
        return $event_name;
    } else {
        my $fallback_event_name = $event_config->{fallback_event} // '';

        print STDERR "  [WARN] Insufficient history for event '$event_name' (found $history_count, min. $min_history_required)\n";

        if ($fallback_event_name ne '' && exists $seasonality_config->{$fallback_event_name}) {
            print STDERR "  Φ Seasonal Analysis [Event selection]: fallback event '$fallback_event_name' selected\n";
            return $fallback_event_name;
        } else {
            # Automatic bootstrap: history-dependent models proceed with baseline-only forecast
            # This is not an error condition - it's the expected first-run behaviour for new events
            print STDERR "  [INFO] Bootstrap run: proceeding with baseline-only forecast (multiplier = 1.0)\n";
            print STDERR "         Subsequent runs will use accumulated historical multipliers.\n";

            # Signal bootstrap state via internal flag (checked by calculate_multiplicative_forecast)
            $event_config->{_bootstrap_active} = 1;

            return $event_name;
        }
    }
}

# ==============================================================================
# SUBROUTINE: _build_nfit_baseline_command
# PURPOSE:    Constructs the specific nfit command for calculating a baseline
#             for a seasonal model. It ensures a consistent command is built
#             and that any conflicting decay or growth flags from the profile
#             are removed. A baseline must be a pure measurement.
# ARGS:
#   1. $profile_flags_in (string): The raw flags from the profile config.
#   2. $baseline_start_str (string): The start date for the analysis.
#   3. $baseline_end_str (string): The end date for the analysis.
#   4. $system_cache_dir (string): Path to the target cache directory.
#   5. $enable_clipping_detection (boolean, optional): If true, adds the
#      --enable-clipping-detection flag to the command.
#   6. $is_generic_baseline    : if true, all decay and time filters are stripped.
#   7. $allow_growth_prediction: if true, growth predictions will be enabled.
#   8. $profile_name_for_label : profile name to record in the results cache (if specified)
#   9. $exclusions_href (hash ref, optional): Hash of VM exclusions.
# RETURNS:
#   - The fully constructed nfit command string.
# ==============================================================================
sub _build_nfit_baseline_command {
    my ($profile_flags_in, $baseline_start_str, $baseline_end_str, $system_cache_dir, $enable_clipping_detection, $is_generic_baseline, $allow_growth_prediction, $profile_name_for_label, $exclusions_href) = @_;

    my $profile_flags = $profile_flags_in; # Work on a copy.

    # --- NEW: Sanitise incoming flags to remove extraneous quotes from config files.
    # This is the primary fix for the "Unknown option" error.
    $profile_flags =~ s/^\s*"?|"?\s*$//g;

    # A baseline is a historical measurement, not a forecast. Growth prediction
    # should almost always be stripped, EXCEPT for the multiplicative model's
    # "CurrentBaseline", which needs to reflect the true current trend.
    unless ($allow_growth_prediction) {
        $profile_flags =~ s/--enable-growth-prediction\s*//g;
        $profile_flags =~ s/--growth-projection-days\s+\d+\s*//g;
        $profile_flags =~ s/--max-growth-inflation-percent\s+\d+\s*//g;
    }

    # The --enable-windowed-decay Model is a trending analysis:
    # Applying a trending model on top of a period that is supposed to be a static baseline measurement would be logically incorrect.
    $profile_flags =~ s/--enable-windowed-decay\s*//g;

    if ($is_generic_baseline) {
        $profile_flags =~ s/--(?:online|batch|no-weekends)\b\s*//g;
        $profile_flags =~ s/--decay\s+[\w-]+\s*//g;
        $profile_flags =~ s/--runq-decay\s+[\w-]+\s*//g;
        $profile_flags =~ s/--avg-method\s+\w+\s*//g;
    }
    $profile_flags =~ s/--decay-over-states\s*//g;

    # --- REFACTOR: Use Manifest for Exclusion Support ---
    # To support complex exclusions (dates/VMs), we must use a manifest.
    # We construct a temporary profile and manifest, inject exclusions, and return the command.

    my $temp_profile = {
        name  => $profile_name_for_label // 'BaselineProfile',
        flags => $profile_flags
    };

    my $runq_behavior = $runq_perc_behavior_mode // 'none';
    my $manifest = build_transform_manifest([$temp_profile], $nfit_runq_avg_method_str, $runq_behavior);

    # Sanitise for history (baselines are historical)
    my $sanitised_manifest = _sanitise_manifest_for_history($manifest);

    # Inject exclusions
    if (defined $exclusions_href) {
        _inject_exclusions_into_manifest($sanitised_manifest, $exclusions_href);
    }

    # Write to temp file
    # We return the File::Temp object to the caller so it persists until they are done.
    my $fh_manifest = File::Temp->new(
        TEMPLATE => 'nfit_baseline_manifest_XXXXXX',
        SUFFIX   => '.json',
        UNLINK   => 1,
        TMPDIR   => 1
    );
    print $fh_manifest JSON->new->pretty->canonical->encode($sanitised_manifest);
    close $fh_manifest; # Flush but keep object alive

    my $manifest_filename = $fh_manifest->filename;

    # Construct the command using the manifest
    my $base_flags = "-q -k --nmondir \"$system_cache_dir\" $rounding_flags_for_nfit";
    $base_flags .= " --startdate $baseline_start_str" if defined $baseline_start_str;
    $base_flags .= " --enddate $baseline_end_str" if defined $baseline_end_str;

    # Note: vm_filter_arg is NOT needed if we rely on the manifest/exclusions,
    # but nfit still respects -vm as a global filter.
    my $vm_filter_arg = defined($target_vm_name) ? " -vm \"$target_vm_name\"" : "";
    my $smt_flag = "--smt $default_smt_arg";

    my $command = "$nfit_script_path --manifest \"$manifest_filename\" $base_flags $vm_filter_arg $smt_flag";

   # Add clipping detection if requested
    if ($enable_clipping_detection) {
        $command .= " --enable-clipping-detection";
    }

    # Append the profile label if provided. This is for metadata and does not affect the L2 cache key.
    if (defined $profile_name_for_label && $profile_name_for_label ne '') {
        $command .= " --profile-label '$profile_name_for_label'";
    }

    return ($command, $fh_manifest);
}

# ==============================================================================
# Main orchestrator for the 'predictive_peak' model.
# This version is enhanced to use the new two-part residual forecasting method.
# ==============================================================================
sub calculate_predictive_peak_forecast {
    my ($system_cache_dir, $system_identifier, $event_name, $event_config, $full_seasonality_config, $adaptive_runq_saturation_thresh, $exclusions_href, $asof_start_obj, $asof_end_obj) = @_;

    print STDERR "\n  ⧉ Executing Predictive Peak Forecast for event $event_name on system $system_identifier\n";

    # --- Step 1: Get Historical Peak and Residual Data ---
    # Calls the enhanced helper to get a hash containing both data series.
    my $historical_data_href = _get_historical_peak_data($system_cache_dir, $event_name, $event_config);

    # --- Step 2: Calculate the Predicted Peak and Residual for each profile ---
    # Calls the enhanced prediction engine to get forecasts for both series.
    print STDERR "  ⧉ Predictive Peak: projecting next peak intensity and residuals using Theil–Sen estimator (robust regression)\n";
    my ($predicted_components_href, $debug_info_for_vms) =
        _calculate_peak_prediction($historical_data_href, $event_config);

    # --- Step 3: Calculate the Non-Peak Baseline (excluding all historical peaks) ---
    # This logic remains unchanged.
    # --- Step 3: Calculate the Non-Peak Baseline (excluding all historical peaks) ---
    # This logic remains unchanged.
    my $true_baseline_results_href = _get_true_baseline_results($system_cache_dir, $event_name, $event_config, $full_seasonality_config, $exclusions_href, $asof_start_obj, $asof_end_obj);

    # --- Generate Active Events String ---
    # Even though predictive usually targets one event, we check for concurrent overlap for consistency.
    my ($next_start, $next_end) = determine_event_period($event_config);
    my $forecast_date_obj = $next_start // gmtime();
    my @active_events = detect_active_events($forecast_date_obj, $full_seasonality_config);
    my $active_events_str = join(", ", map { $_->{_eventName} } @active_events);
    # Fallback if detection fails (e.g. fixed dates in past)
    $active_events_str ||= $event_name;

    # --- Step 4: Synthesise Final Results ---
    print STDERR "  ⧉ Completing final forecast synthesis (peak, residuals, baseline)\n";

    my %final_results;
    foreach my $vm_name (keys %{$true_baseline_results_href}) {
        foreach my $profile (@profiles) {
            my $p_name = $profile->{name};
            my $baseline_val = $true_baseline_results_href->{$vm_name}{$p_name};

            # Get the two predicted components from the prediction engine.
            my $predicted_peak_val = $predicted_components_href->{$vm_name}{$p_name}{peak};
            my $predicted_residual_val = $predicted_components_href->{$vm_name}{$p_name}{residual};

            # Combine the signal and volatility forecasts. Undefined values are treated as zero.
            my $combined_prediction;
            if (defined $predicted_peak_val) {
                $combined_prediction = ($predicted_peak_val // 0) + ($predicted_residual_val // 0);
            }

            # The final recommendation is the higher of the baseline or the combined prediction.
            my ($final_value, $source) = (0, 'N/A');
            if (defined $baseline_val && defined $combined_prediction) {
                if ($baseline_val > $combined_prediction) {
                    $final_value = $baseline_val;
                    $source = 'TrueBaseline';
                } else {
                    $final_value = $combined_prediction;
                    $source = 'PeakPrediction';
                }
            } elsif (defined $combined_prediction) {
                $final_value = $combined_prediction;
                $source = 'PeakPrediction';
            } elsif (defined $baseline_val) {
                $final_value = $baseline_val;
                # This case indicates that prediction failed (e.g., insufficient history).
                $source = 'BaselineOnly (NoPrediction)';
            }

            # Apply the optional amplification factor
            my $amplification = $event_config->{peak_amplification_factor} // 1.0;
            my $final_recommendation = $final_value * $amplification;

            $final_results{$vm_name}{$p_name} = $final_recommendation;

            # Store all components in the debug hash for comprehensive logging.
            $seasonal_debug_info{$vm_name}{$p_name} = {
                TrueBaseline       => $baseline_val,
                PredictedPeak      => $predicted_peak_val,
                PredictedResidual  => $predicted_residual_val,
                CombinedPrediction => $combined_prediction,
                FinalSource        => $source,
                AmplificationFactor  => $amplification,
                FinalForecast        => $final_recommendation,
                PredictionDebug    => $debug_info_for_vms->{$vm_name}{$p_name},
                OutlierWarning     => _get_warning_for_vm_forecast($vm_name, \%outlier_warnings),
                active_events      => $active_events_str,
                baseline_source    => "Generic Non-Peak (Filtered)", # Predictive always uses filtered generic
                forecast_model     => "Predictive Linear Regression"
            };
        }
    }
    return \%final_results;
}

# ==============================================================================
# Helper to run nfit and get a "True/Non-Peak Baseline" by excluding all historical peak periods.
#
# ==============================================================================
sub _get_true_baseline_results {
    my ($system_cache_dir, $event_name, $event_config, $full_seasonality_config, $exclusions_href, $asof_start_obj, $asof_end_obj) = @_;

    my $data_cache_file = File::Spec->catfile($system_cache_dir, '.nfit.cache.data');
    return {} unless -f $data_cache_file;

    # --- Stage 1: Build a hash of dates to exclude ---
    my @all_peak_periods = find_all_historical_periods($full_seasonality_config, $asof_start_obj, $asof_end_obj);

    my %days_to_exclude_hash;
    my $total_days_to_process = 0;

    # Pre-calculate total days for an accurate progress bar and validation
    foreach my $period (@all_peak_periods) {
        next unless (defined $period && ref($period) eq 'ARRAY' && @$period == 2);
        my ($start, $end) = @{$period};
        next if ($start > $end);
        $total_days_to_process += (int(($end->epoch - $start->epoch) / 86400) + 1);
    }

    if ($total_days_to_process > 0) {
        print STDERR "  [+] Processing $total_days_to_process days for historical peak exclusion\n";

        # Scope all counters locally to prevent bugs across function calls
        my $days_processed = 0;
        my $last_reported_perc = -1;
        my $period_num = 0;

        foreach my $period (@all_peak_periods) {
            $period_num++;
            next unless (defined $period && ref($period) eq 'ARRAY' && @$period == 2);
            my ($start, $end) = @{$period};
            next if ($start > $end);

            my $current = Time::Piece->new($start->epoch);
            while ($current <= $end) {

                $days_to_exclude_hash{$current->strftime('%Y-%m-%d')} = 1;
                $current += ONE_DAY;

                $days_processed++;

                my $current_perc = int(($days_processed / $total_days_to_process) * 100);
                if ($current_perc > $last_reported_perc) {
                    printf STDERR "\r      Expanding peak-day window: %d%% (%d/%d)",
                        $current_perc, $days_processed, $total_days_to_process;
                    $last_reported_perc = $current_perc;
                }
            }
        }
        print STDERR "\n";
    }

    # --- Stage 2: Create the filtered data using the efficient hash lookup ---
    my ($filtered_fh, $filtered_filename) = tempfile(UNLINK => 1);

    my $unique_days_to_exclude = scalar(keys %days_to_exclude_hash);
    if ($unique_days_to_exclude > 0) {
        print STDERR "  [-] Filtering $unique_days_to_exclude day(s) from baseline data\n";

        open(my $cache_fh, '<', $data_cache_file) or die "Cannot open $data_cache_file: $!";
        my $header = <$cache_fh>;
        print $filtered_fh $header;

        my $line_count = 0;
        while (my $line = <$cache_fh>) {
            $line_count++;
            print STDERR "." if $line_count % 500000 == 0;

            my $line_date = substr($line, 0, 10);
            next if exists $days_to_exclude_hash{$line_date};

            print $filtered_fh $line;
        }
        close $cache_fh;
    } else {
        print STDERR "  [INFO] No peak periods to filter; using full analysis window for baseline\n";
        system("cp '$data_cache_file' '$filtered_filename'");
    }

    close $filtered_fh;

    # --- Stage 3: Execute Single-Pass Analysis on the Filtered Data ---
    print STDERR "  ⧉ Synthesising non-peak baseline from filtered data\n";

    # 1. Build and Sanitise Manifest
    #    We use the global @profiles array and RunQ settings
    my $runq_behavior = $runq_perc_behavior_mode // 'none';
    my $tactical_manifest = build_transform_manifest(\@profiles, $nfit_runq_avg_method_str, $runq_behavior);
    my $historical_manifest = _sanitise_manifest_for_history($tactical_manifest);

    # --- INJECT EXCLUSIONS IF PRESENT ---
    if (defined $exclusions_href) {
        _inject_exclusions_into_manifest($historical_manifest, $exclusions_href);
    }

    # 2. Write Manifest to Temp File
    my ($fh_manifest, $manifest_filename) = tempfile(
        'nfit_baseline_manifest_XXXXXX',
        SUFFIX => '.json',
        UNLINK => 1,
        TMPDIR => 1
    );
    print $fh_manifest JSON->new->pretty->canonical->encode($historical_manifest);
    close $fh_manifest;

    # 3. Construct nfit Command
    #    CRITICAL: We point --physc-data and --runq-data to the FILTERED file we just created.
    #    The cache file format (Timestamp, VM, PhysC, RunQ) is compatible with these flags.
    my $nfit_cmd = "$nfit_script_path --manifest $manifest_filename "
                 . " --nmondir \"$system_cache_dir\" "
                 . "--smt $default_smt_arg "
                 . "--runq-avg-method $nfit_runq_avg_method_str "
                 . "--show-progress";

    # 4. Execute
    my $nfit_output = '';
    my $stderr_arg = ">&=" . fileno(STDERR);
    my $pid_nfit = open3(undef, my $stdout_nfit, $stderr_arg, $nfit_cmd);

    while(my $line = <$stdout_nfit>) {
        $nfit_output .= $line;
    }
    waitpid($pid_nfit, 0);

    # 5. Parse and Assimilate Results
    my %baseline_results;
    if ($? == 0) {
        my $parsed = parse_nfit_json_output($nfit_output);

        foreach my $vm (keys %$parsed) {
            # In historical mode (no decay), nfit might return multiple states.
            # We want the aggregated average across the filtered period.
            my @states_for_vm = @{$parsed->{$vm}};

            foreach my $profile (@profiles) {
                my $p_name = $profile->{name};
                # Extract the correct P-metric key (e.g., P95)
                my ($p_val_num) = $profile->{flags} =~ /(?:-p|--percentile)\s+([0-9.]+)/;
                my $p_key = "P" . clean_perc_label($p_val_num // $DEFAULT_PERCENTILE);

                my @valid_vals;
                foreach my $state (@states_for_vm) {
                    my $val = _safe_dig($state, 'metrics', 'physc', $p_name, $p_key);
                    push @valid_vals, $val if (defined $val && looks_like_number($val));
                }

                if (@valid_vals) {
                    # Simple average of the states in the filtered file
                    $baseline_results{$vm}{$p_name} = sum0(@valid_vals) / scalar(@valid_vals);
                }
            }
        }
    } else {
        warn "WARNING: nfit failed during baseline calculation. Exit code: " . ($? >> 8) . "\n";
    }

    # Cleanup temp file if we created one
    unlink $filtered_filename if -f $filtered_filename;

    return \%baseline_results;

}

# ==============================================================================
# SUBROUTINE: _get_historical_peak_data
# PURPOSE:    Reads the snapshot cache for a given event and returns time series
#             data for historical peaks and residuals. This version is enhanced
#             to use the 'unclippedPeakEstimate' from the ClippingInfo block
#             if it exists, ensuring the returned data is corrected for
#             historical saturation.
# ARGS:
#   1. $system_cache_dir (string): Path to the target cache directory.
#   2. $event_name (string): The name of the event to retrieve data for.
#   3. $event_config (hash ref): The configuration for the event.
# RETURNS:
#   - A hash reference containing two keys, 'peaks' and 'residuals', each
#     pointing to a hash of historical data series, structured by profile and VM.
# ==============================================================================
sub _get_historical_peak_data {
    my ($system_cache_dir, $event_name, $event_config) = @_;

    print STDERR "  ⧉ Predictive Peak: assembling saturation-aware historical peak and residual data for event $event_name\n";

    my $unified_history = read_unified_history($system_cache_dir);
    my @event_history;
    # Extract all historical snapshots for the specified event.
    foreach my $month_key (sort keys %$unified_history) {
        my $month_data = $unified_history->{$month_key};
        if (exists $month_data->{SeasonalEventSnapshots}{$event_name}) {
            my $snapshot = $month_data->{SeasonalEventSnapshots}{$event_name};
            $snapshot->{_month_key} = $month_key; # Add date context
            $snapshot->{eventName} //= $event_name;  # Ensure eventName is set for history lookup
            push @event_history, $snapshot;
        }
    }

    my $max_peaks = $event_config->{max_historical_peaks} // 12;
    if (@event_history > $max_peaks) {
        @event_history = @event_history[-$max_peaks..-1]; # Keep only the most recent N peaks.
    }

    my %peaks_by_profile;
    my %residuals_by_profile; # New hash for residual data.

    foreach my $event (@event_history) {
        my $date = $event->{_month_key} . "-01"; # Use the first of the month as the date for the time series.
        my $hist_results = $event->{results};
        my $peak_results = $event->{results}{'PeakValue'} || {};
        my $residual_results = $event->{results}{'PeakResidual'} || {};

        # Process PeakValue data.
        foreach my $vm (keys %$peak_results) {
            foreach my $profile (keys %{$peak_results->{$vm}}) {
                my $value = $peak_results->{$vm}{$profile};
                if (exists $hist_results->{ClippingInfo}{$vm}{$profile}{unclippedPeakEstimate}) {
                    $value = $hist_results->{ClippingInfo}{$vm}{$profile}{unclippedPeakEstimate};
                }
                if (defined $value && looks_like_number($value)) {
                    push @{$peaks_by_profile{$profile}{$vm}}, { value => $value, date => $date };
                }
            }
        }

        # Process PeakResidual data.
        foreach my $vm (keys %$residual_results) {
            foreach my $profile (keys %{$residual_results->{$vm}}) {
                my $value = $residual_results->{$vm}{$profile};
                if (defined $value && looks_like_number($value)) {
                    push @{$residuals_by_profile{$profile}{$vm}}, { value => $value, date => $date };
                }
            }
        }
    }

    # Return a hash containing both data series.
    return {
        peaks => \%peaks_by_profile,
        residuals => \%residuals_by_profile
    };
}

# ==============================================================================
# Core statistical engine for the predictive_peak model.
# This function orchestrates a dual forecast for both the peak signal and the residual.
# This function uses Theil-Sen Estimator (Robust Regression) instead of OLS.
# ==============================================================================
sub _calculate_peak_prediction {
    my ($historical_data_href, $event_config) = @_;

    my $peak_series_per_profile = $historical_data_href->{peaks} || {};
    my $residual_series_per_profile = $historical_data_href->{residuals} || {};

    my %predictions;
    my %debug_info;

    my $min_peaks = $event_config->{min_historical_peaks} // 3;

    # --- Robust Prediction Logic (Theil-Sen) ---
    my $_predict_next_value = sub {
        my ($series_aref) = @_;

        return (undef, "Insufficient history")
            unless (defined $series_aref && ref($series_aref) eq 'ARRAY' && scalar(@$series_aref) >= 2);

        # Extract values. Note: We do NOT strictly need to trim outliers here
        # because Theil-Sen is naturally robust to them (breakdown point ~29%).
        my @values = map { $_->{value} } @$series_aref;

        # 1. Build points [x, y]
        my @points;
        for my $i (0..$#values) {
            push @points, [$i, $values[$i]];
        }

        # 2. Calculate Theil-Sen Slope
        my $sen_result = calculate_sens_slope(\@points);
        unless ($sen_result) {
            return (undef, "Theil-Sen calculation failed (insufficient distinct points)");
        }
        my $slope = $sen_result->{slope};

        # 3. Calculate Robust Intercept
        # Formula: Median(y_i - slope * x_i)
        my @intercepts;
        for my $i (0..$#values) {
            push @intercepts, $values[$i] - ($slope * $i);
        }
        my $intercept = _calculate_median(\@intercepts);

        # 4. Project one step into the future
        my $future_x = scalar(@values); # Next index
        my $projected_value = ($slope * $future_x) + $intercept;

        my $debug_str = sprintf("Theil-Sen (Slope: %.4f, Intercept: %.4f, N: %d)", $slope, $intercept, scalar(@values));

        return ($projected_value, $debug_str);
    };

    foreach my $profile_name (keys %{$peak_series_per_profile}) {
        foreach my $vm_name (keys %{$peak_series_per_profile->{$profile_name}}) {

            my $peak_series_aref = $peak_series_per_profile->{$profile_name}{$vm_name} || [];

            if (scalar(@$peak_series_aref) < $min_peaks) {
                $debug_info{$vm_name}{$profile_name} = {
                    peak_status => "Skipped: Insufficient history (" . scalar(@$peak_series_aref) . "/$min_peaks)",
                    residual_status => "Skipped: Dependent on peak forecast"
                };
                next;
            }

            # Predict Peak
            my ($predicted_peak, $peak_debug) = $_predict_next_value->($peak_series_aref);

            # Predict Residual
            my $residual_series_aref = $residual_series_per_profile->{$profile_name}{$vm_name} || [];
            my ($predicted_residual, $residual_debug) = $_predict_next_value->($residual_series_aref);

            $predictions{$vm_name}{$profile_name} = {
                peak     => $predicted_peak,
                residual => $predicted_residual,
            };

            $debug_info{$vm_name}{$profile_name} = {
                peak_status     => $peak_debug,
                residual_status => $residual_debug,
            };
        }
    }

    return (\%predictions, \%debug_info);
}

# ==============================================================================
# Helper to calculate standard deviation.
# ==============================================================================
sub _calculate_std_dev {
    my ($data_aref) = @_;
    my $n = scalar(@$data_aref);
    return 0 if $n < 2;
    my $mean = sum0(@$data_aref) / $n;
    my $sum_sq_diff = sum0(map { ($_ - $mean)**2 } @$data_aref);
    return sqrt($sum_sq_diff / ($n - 1));
}

# ==============================================================================
# Finds all historical peak periods defined in the seasonality config that
# fall within the date range of the available cache data.
# This version is optimised for performance and robustness by pre-compiling
# regexes, reducing object creation, and preventing runaway loops.
# ==============================================================================
sub find_all_historical_periods {
    my ($full_seasonality_config, $asof_start, $asof_end) = @_;

    # Determine the actual scoped time span of the data in the cache.
    return [] unless $asof_start && $asof_end;

    my @periods;
    my $now = gmtime(); # Cache current time to avoid repeated calls.

    # Pre-compile the regex once, outside the loops.
    my $date_range_regex = qr/(\d{4}-\d{2}-\d{2}):(\d{4}-\d{2}-\d{2})/;

    foreach my $e_name (keys %$full_seasonality_config) {
        my $e_config = $full_seasonality_config->{$e_name};

        # --- Path for events defined with fixed 'dates' ---
        if (defined $e_config->{dates}) {
            my @date_ranges = split /\s*,\s*/, $e_config->{dates};

            foreach my $range (@date_ranges) {
                next unless $range =~ $date_range_regex;

                # Only create Time::Piece objects after a successful regex match.
                my $start = Time::Piece->strptime($1, '%Y-%m-%d');
                my $end = Time::Piece->strptime($2, '%Y-%m-%d');
                push @periods, [$start, $end];
            }

        # --- Path for events defined with a recurring 'period' ---
        } elsif (defined $e_config->{period} && $e_config->{period} eq 'monthly') {
            # Pre-extract config values to avoid repeated hash lookups inside the loop.
            my $day_of_period = $e_config->{day_of_period} // -1;
            my $duration_days = $e_config->{duration_days} // 7;

            my $current_iterator = Time::Piece->new($asof_start->epoch)->truncate(to => 'month');

            # Add runaway protection to prevent infinite loops on very large date ranges.
            my $max_iterations = 1000; # Sensible limit for ~83 years of monthly data.
            my $iteration_count = 0;

            while ($current_iterator <= $asof_end && $iteration_count < $max_iterations) {
                my ($start, $end) = _get_recurring_monthly_period($current_iterator, $day_of_period, $duration_days);

                # Add the period if it's valid, historical, and within the scoped cache's time span.
                if ($start && $end && $end < $now && $end <= $asof_end) {
                    push @periods, [$start, $end];
                }

                $current_iterator = $current_iterator->add_months(1);
                $iteration_count++;
            }

            # Warn the user if the protection limit was reached.
            warn "Monthly iteration limit reached for event '$e_name'" if $iteration_count >= $max_iterations;
        }
    }

    return @periods;
}

# ==============================================================================
# Subroutine to log the rationale for a predictive_peak seasonal forecast.
# ==============================================================================
sub log_predictive_peak_rationale
{
    my ($fh, $event_config, $context) = @_;

    return unless $fh;

    # Log Configuration Context
    if (defined $event_config) {
        _log_seasonal_config_context($fh, $event_config, $context);
    }

    foreach my $vm_name (sort @vm_order) {
        next unless exists $seasonal_debug_info{$vm_name};

        print {$fh} "\n######################################################################\n";
        print {$fh} "# Rationale for VM: $vm_name\n";
        print {$fh} "# CPU Forecasting Model: Predictive Peak Forecast (Event: $apply_seasonality_event)\n";
        print {$fh} "######################################################################\n\n";

        foreach my $profile (@profiles) {
            my $p_name = $profile->{name};

            # --- Special Handling for P-99W1 ---
            if (defined $MANDATORY_PEAK_PROFILE_FOR_HINT && $p_name eq $MANDATORY_PEAK_PROFILE_FOR_HINT) {
                my $label_width = 43;
                print {$fh} "======================================================================\n";
                printf {$fh} "%-${label_width}s : %s\n", "Profile Processed", parse_profile_name_for_log($p_name);
                print {$fh} "----------------------------------------------------------------------\n";
                print {$fh} "  CPU Forecasting Model: Predictive Peak Forecast ($apply_seasonality_event)\n\n";
                print {$fh} "  - The P-99W1 profile is a special case used for peak analysis.\n";
                print {$fh} "  - It represents the smoothed (1-min SMA) 99.75th percentile of the\n";
                print {$fh} "    unfiltered historical data.\n";
                print {$fh} "  - NO growth, RunQ, or forecasting modifiers are applied to this value.\n\n";

                # Note: Key is 'FinalForecast' for predictive model
                my $p99_val = $seasonal_debug_info{$vm_name}{$p_name}{FinalForecast} // 0;
                printf {$fh} "  Final Unmodified Value              : %.4f cores\n", ($p99_val // 'N/A');
                print {$fh} "======================================================================\n\n";
                next;
            }

            next unless exists $seasonal_debug_info{$vm_name}{$p_name};

            my $s_data = $seasonal_debug_info{$vm_name}{$p_name};
            my $profile_desc = parse_profile_name_for_log($p_name);
            my $label_width = 38;

            print {$fh} "======================================================================\n";
            printf {$fh} "%-${label_width}s      : %s\n", "Profile Processed", $profile_desc;
            print {$fh} "----------------------------------------------------------------------\n";

            my $predicted_peak_val = $s_data->{PredictedPeak};

            if (!defined $predicted_peak_val) {
                # This block handles cases where prediction was skipped (e.g., insufficient history).
                my $debug_info = $s_data->{PredictionDebug}{peak_status} // "Insufficient history";
                printf {$fh} "  %-${label_width}s : SKIPPED (%s)\n", "Peak Trend Prediction", $debug_info;
                printf {$fh} "  %-${label_width}s : %.4f cores\n", "1. Non-Peak Baseline", ($s_data->{TrueBaseline} // 0);
                print {$fh} "  --------------------------------------------------------------------\n";
                printf {$fh} "  %-${label_width}s : %.4f cores (Source: Baseline Only)\n", "Final Recommendation", ($s_data->{FinalForecast} // 0);

            } else {
                # This block logs a successful, multi-step forecast.
                # Extract trend details from the debug string if available
                my $trend_details = $s_data->{PredictionDebug}{peak_status} // "Details unavailable";

                printf {$fh} "  1. %-${label_width}s : %.4f cores\n", "Non-Peak Baseline (Floor)", ($s_data->{TrueBaseline} // 0);
                printf {$fh} "  2. %-${label_width}s : %.4f cores\n", "Predicted Peak", $predicted_peak_val;
                printf {$fh} "       - Trend Details                      : %s\n", $trend_details;
                printf {$fh} "  3. %-${label_width}s : %.4f cores\n", "Predicted Peak Residual", ($s_data->{PredictedResidual} // 0);
                printf {$fh} "  4. %-${label_width}s : %.2f\n", "Peak Amplification Factor", ($s_data->{AmplificationFactor} // 1.0);
                print {$fh} "  --------------------------------------------------------------------\n";
                printf {$fh} "  Calculation : MAX( Baseline, (Predicted Peak + Predicted Residual) * Amplification )\n";
                print {$fh} "  --------------------------------------------------------------------\n";
                printf {$fh} "  %-${label_width}s : %.4f cores (Source: %s)\n", "Final Forecasted Value",
                    ($s_data->{FinalForecast} // 0),
                    ($s_data->{FinalSource} // 'N/A');
            }

            # --- Print outlier warning if it exists ---
            if (defined $s_data->{OutlierWarning} && $s_data->{OutlierWarning} ne '') {
                print {$fh} "\n  --- Workload Volatility Alert ---\n";
                print {$fh} "  " . $s_data->{OutlierWarning} . "\n";
            }

            print {$fh} "======================================================================\n\n";
        }
    }
}

# ==============================================================================
# Calculate linear regression (slope, intercept, R-squared) manually
# ==============================================================================
sub calculate_manual_linear_regression
{
    my ($points_aref) = @_;
    my $n = scalar @{$points_aref};

    return undef if $n < 2;

    my ($sum_x, $sum_y, $sum_xy, $sum_x_squared, $sum_y_squared) = (0, 0, 0, 0, 0);

    foreach my $point (@{$points_aref})
    {
        my ($x_val, $y_val) = @{$point};
        $sum_x += $x_val;
        $sum_y += $y_val;
        $sum_xy += $x_val * $y_val;
        $sum_x_squared += $x_val**2;
        $sum_y_squared += $y_val**2;
    }

    my $denominator_slope = ($n * $sum_x_squared) - ($sum_x**2);

    if (abs($denominator_slope) > $FLOAT_EPSILON)
    {
        my $slope_calc = (($n * $sum_xy) - ($sum_x * $sum_y)) / $denominator_slope;
        my $intercept_calc = ($sum_y - ($slope_calc * $sum_x)) / $n;
        return {
            slope     => $slope_calc,
            intercept => $intercept_calc,
            n_points  => $n,
        };
    }

    # Cannot reliably determine a linear trend.
    return undef;
}

# ==============================================================================
# SUBROUTINE: read_unified_history
## PURPOSE:   Reads history from either legacy file or partitioned directory.
#             Stitches partitioned files into a single "Virtual Monolith" hash
#             to ensure zero regression for downstream logic.
# ARGUMENTS:
#   1. $system_cache_dir (string): The path to a specific system's cache directory.
# RETURNS:
#   - A hash reference of the decoded JSON data.
#   - Returns an empty hash reference if the file does not exist, is empty,
#     or is corrupt, ensuring a safe default.
sub read_unified_history {
    my ($system_cache_dir) = @_;

    my $legacy_file   = File::Spec->catfile($system_cache_dir, $UNIFIED_HISTORY_FILE);
    my $partition_dir = File::Spec->catfile($system_cache_dir, '.nfit.history');

    my $history_data = {};

    # PATH A: Partitioned History (Preferred)
    if (-d $partition_dir) {
        opendir(my $dh, $partition_dir) or die "FATAL: Cannot open history dir: $!";
        my @files = grep { /^nfit\.hist\.\d{4}-\d{2}\.json$/ } readdir($dh);
        closedir($dh);

        my $json_decoder = JSON->new->allow_nonref;

        foreach my $file (@files) {
            my $path = File::Spec->catfile($partition_dir, $file);
            my $json_text = do {
                open my $fh, '<:encoding(utf8)', $path or next; # Skip unreadable
                local $/; <$fh>;
            };
            next unless $json_text;

            my $chunk = eval { $json_decoder->decode($json_text) };
            if ($@) {
                warn " [WARN] Corrupt history partition $file: $@";
                next;
            }

            # Merge chunk into main hash (Structure: { "YYYY-MM": { ... } })
            my ($key) = keys %$chunk;
            if ($key) {
                $history_data->{$key} = $chunk->{$key};
            }
        }
        return $history_data;
    }

    # PATH B: Legacy Monolith (Fallback)
    if (-f $legacy_file && -s $legacy_file) {
        eval {
            my $json = JSON->new->allow_nonref;
            local $/;
            open my $fh, '<:encoding(utf8)', $legacy_file or die "[ERROR] $!";
            my $json_text = <$fh>;
            close $fh;
            $history_data = $json->decode($json_text);
        };
        if ($@) {
            warn " [WARN] Could not decode legacy history cache. Treating as empty. Error: $@";
            return {};
        }
        return $history_data;
    }

    # PATH C: New System (Empty)
    return {};
}

# ==============================================================================
# SUBROUTINE: write_unified_history
# PURPOSE:    Writes a new monthly entry to the unified history cache file.
#             This function handles file locking to prevent data corruption from
#             concurrent processes. It performs a safe read-modify-write of
#             the entire data structure.
# ARGUMENTS:
#   1. $system_cache_dir (string): The path to a system's cache directory.
#   2. $month_key (string): The key for the new entry (e.g., "2025-07").
#   3. $new_data_for_month_href (hash ref): The data for the new monthly entry.
# RETURNS:
#   - 1 on success, 0 on failure.
# ==============================================================================
sub write_unified_history {
    my ($system_cache_dir, $month_key, $new_data_for_month_href) = @_;

    my $history_file = File::Spec->catfile($system_cache_dir, $UNIFIED_HISTORY_FILE);

    # Acquire global lock
    my ($lock_fh, $lock_path);
    eval {
        ($lock_fh, $lock_path) = _acquire_history_lock($system_cache_dir);
    };
    if ($@) {
        warn " [WARN] Skipping history write due to lock failure: $@";
        return 0;
    }
    # ------------------------------------------------

    my $success = 0;
    eval {
        # Read the existing history file first.
        my $history_data = read_unified_history($system_cache_dir);

        # Add or overwrite the data for the specified month.
        $history_data->{$month_key} = $new_data_for_month_href;

        # Write the entire updated data structure back to the file.
        my $json = JSON->new->pretty->canonical;
        my $json_text = $json->encode($history_data);

        open my $fh, '>:encoding(utf8)', $history_file or die "Could not open '$history_file' for writing: $!";
        print $fh $json_text;
        close $fh;
        $success = 1;
    };
    if ($@) {
        warn " [WARN] An error occurred while writing to the unified history cache '$history_file': $@";
        $success = 0;
    }

    # Release the lock.
    close $lock_fh;

    # Note: We do NOT unlink the global lock file in the new architecture
    # to prevent race conditions on file creation. It remains as a sentinel.

    return $success;
}

# ==============================================================================
# SUBROUTINE: _build_residual_manifest
# PURPOSE:    Creates a manifest for the residual peak profile. The residual
#             profile is a special, ultra-sensitive profile (typically P99.9
#             with W1 window) used to detect volatile workload spikes.
#
# ARGUMENTS:
#   1. $event_config (hash ref): The event configuration containing the
#      residual_peak_profile flags string.
#   2. $exclusions_href (hash ref, optional): Hash of VM exclusions.
#
# RETURNS:
#   - Hash reference to the manifest, or undef if no residual profile defined.
# ==============================================================================
sub _build_residual_manifest {
    my ($event_config, $exclusions_href) = @_;

    # Check if a residual profile is configured
    my $residual_flags_str = $event_config->{residual_peak_profile};
    return undef unless (defined $residual_flags_str && $residual_flags_str ne '');

    # Create a temporary profile object from the flags string
    # This follows the same structure as profiles loaded from config
    my $temp_profile = {
        name  => 'ResidualPeakProfile',  # Fixed name for identification
        flags => $residual_flags_str
    };

    # 2. Build the manifest
    # Run Queue Residual is irrelevant for the standard residual headroom calculation
    my $manifest = build_transform_manifest([$temp_profile], $nfit_runq_avg_method_str, 'none');

    # --- INJECT EXCLUSIONS IF PRESENT ---
    if (defined $exclusions_href) {
        _inject_exclusions_into_manifest($manifest, $exclusions_href);
    }

    # 3. Create a temporary manifest file;
    return $manifest;
}

# ==============================================================================
# SUBROUTINE: _execute_history_pass
# PURPOSE:    Executes a single nfit pass for history priming using a sanitised
#             manifest. This wrapper reduces code duplication and ensures
#             consistent command construction across all history passes.
#
# ARGUMENTS:
#   1. $system_cache_dir (string): Path to the system cache directory
#   2. $manifest_href (hash ref): The sanitised historical manifest
#   3. $start_date (Time::Piece): Start date for analysis
#   4. $end_date (Time::Piece): End date for analysis
#   5. $enable_clipping (boolean): Whether to enable clipping detection
#   6. $pass_name (string): Descriptive name for logging (e.g., "Baseline")
#
# RETURNS:
#   - Hash reference to parsed results (VM -> [states]), or empty hash on failure
# ==============================================================================
sub _execute_history_pass {
    my ($system_cache_dir, $manifest_href, $start_date, $end_date, $enable_clipping, $pass_name) = @_;

    # Validate inputs
    unless (ref($manifest_href) eq 'HASH') {
        warn "WARNING: _execute_history_pass received invalid manifest for $pass_name pass\n";
        return {};
    }

    # Write manifest to temporary file
    use File::Temp qw(tempfile);
    my ($fh_manifest, $manifest_filename) = tempfile(
        "nfit_seasonal_${pass_name}_XXXXXX",
        SUFFIX => '.json',
        UNLINK => 1,
        TMPDIR => 1
    );

    print $fh_manifest JSON->new->pretty->canonical->encode($manifest_href);
    close $fh_manifest;

    # Build the nfit command
    my $nfit_cmd = "$nfit_script_path --manifest $manifest_filename "
                 . "--nmondir \"$system_cache_dir\" "
                 . "--startdate " . $start_date->ymd . " "
                 . "--enddate " . $end_date->ymd . " "
                 . "--smt $default_smt_arg "
                 . "--runq-avg-method $nfit_runq_avg_method_str";

    # Add clipping detection if requested (typically only for peak analysis)
    $nfit_cmd .= " --enable-clipping-detection" if $enable_clipping;

    # Execute the command
    print STDERR "  • Executing $pass_name analysis: " . $start_date->ymd . " to " . $end_date->ymd . "\n";

    my $nfit_output = '';
    my $stderr_arg = ">&=" . fileno(STDERR);
    my $pid = open3(undef, my $stdout_fh, $stderr_arg, $nfit_cmd);

    while (my $line = <$stdout_fh>) {
        $nfit_output .= $line;
    }

    waitpid($pid, 0);
    my $exit_code = $? >> 8;

    # Check for failure
    if ($exit_code != 0) {
        warn "WARNING: nfit failed during $pass_name pass (exit code: $exit_code)\n";
        warn "Command: $nfit_cmd\n";
        return {};
    }

    # Parse and return results
    my $parsed_results = parse_nfit_json_output($nfit_output);

    return $parsed_results;
}

# ==============================================================================
# SUBROUTINE: _assemble_seasonal_snapshot
# PURPOSE:    Assembles the final SeasonalEventSnapshot structure from the
#             results of three nfit passes (baseline, peak, residual). This
#             function contains the battle-tested logic for extracting values,
#             calculating residuals, and building ClippingInfo blocks.
#
# ARGUMENTS:
#   1. $peak_results_href (hash ref): Parsed peak period results
#   2. $baseline_results_href (hash ref): Parsed baseline period results
#   3. $residual_results_href (hash ref): Parsed residual peak results (may be empty)
#   4. $event_config (hash ref): Event configuration
#   5. $event_name (string): Event name for metadata
#   6. $peak_end (Time::Piece): Peak end date for metadata
#   7. $profiles_aref (array ref): Reference to the profiles array
#   8. $baseline_start (Time::Piece): Baseline analysis period start
#   9. $baseline_end (Time::Piece): Baseline analysis period end
#
# RETURNS:
#   - Hash reference to the complete snapshot structure, or undef on failure
# ==============================================================================
sub _assemble_seasonal_snapshot {
    my ($peak_results_href, $baseline_results_href, $residual_results_href,
        $event_config, $event_name, $peak_end, $profiles_aref, $baseline_start, $baseline_end) = @_;

    # Validate that profiles array was passed correctly
    unless (ref($profiles_aref) eq 'ARRAY' && @$profiles_aref) {
        die "FATAL: _assemble_seasonal_snapshot requires a valid profiles array reference\n";
    }

    # Determine the correct key names based on model type
    my $model_type = $event_config->{model} // '';
    my ($peak_key, $baseline_key) = ('HistoricPeak', 'HistoricBaseline');
    if ($model_type eq 'predictive_peak' or $model_type eq 'adaptive_peak') {
        ($peak_key, $baseline_key) = ('PeakValue', 'BaselineValue');
    }

    # Initialise result structures
    my %peak_period_results;
    my %baseline_period_results;
    my %clipping_info_results;
    my %residual_results_final;

    # ======================================================================
    # STEP 1: Extract baseline values for all VMs and profiles
    # ======================================================================
    # CRITICAL FIX: When nfit runs without decay (historical mode), it may
    # return multiple states per VM. Profiles with time filters (e.g. -online)
    # may appear in earlier states, whilst general profiles appear in later
    # states. We must search through ALL states to find each profile's value.
    # ======================================================================
    foreach my $vm_name (keys %$baseline_results_href) {
        my $states_aref = $baseline_results_href->{$vm_name};
        next unless ref($states_aref) eq 'ARRAY' && @$states_aref;

        # Extract values for each profile
        foreach my $profile (@$profiles_aref) {
            my $profile_name = $profile->{name};
            my $p_key = "P" . clean_perc_label(($profile->{flags} =~ /-p\s+([0-9.]+)/) ? $1 : $DEFAULT_PERCENTILE);

            # Search through all states (newest to oldest) to find this profile's value
            # We search backwards because the most recent state is usually most complete
            my $metric_val = undef;
            for (my $i = $#{$states_aref}; $i >= 0; $i--) {
                my $state = $states_aref->[$i];
                $metric_val = _safe_dig($state, 'metrics', 'physc', $profile_name, $p_key);
                last if defined $metric_val;  # Found it, stop searching
            }

            if (defined $metric_val) {
                $baseline_period_results{$vm_name}{$profile_name} = $metric_val;
            }
        }
    }

    # ======================================================================
    # STEP 2: Extract sanitised peak values for residual calculation
    # ======================================================================
    my %sanitised_peak_values;  # Used for residual calculation

    if (defined $event_config->{residual_peak_profile} &&
        $event_config->{residual_peak_profile} ne '' &&
        ref($residual_results_href) eq 'HASH' &&
        scalar keys %$residual_results_href) {

        foreach my $vm_name (keys %$residual_results_href) {
            my $states_aref = $residual_results_href->{$vm_name};
            next unless ref($states_aref) eq 'ARRAY' && @$states_aref;

            # Extract the residual profile's percentile
            my $residual_flags = $event_config->{residual_peak_profile};
            my $p_key = "P" . clean_perc_label(($residual_flags =~ /-p\s+([0-9.]+)/) ? $1 : $DEFAULT_PERCENTILE);

            # Search through all states (newest to oldest) to find the residual value
            my $metric_val = undef;
            for (my $i = $#{$states_aref}; $i >= 0; $i--) {
                my $state = $states_aref->[$i];
                $metric_val = _safe_dig($state, 'metrics', 'physc', 'ResidualPeakProfile', $p_key);
                last if defined $metric_val;  # Found it, stop searching
            }

            if (defined $metric_val) {
                $sanitised_peak_values{$vm_name} = $metric_val;
            }
        }
    }

    # ======================================================================
    # STEP 3: Extract peak values and clipping info for all profiles
    # ======================================================================
    # Same multi-state search logic as baseline extraction above.
    # ======================================================================
    foreach my $vm_name (keys %$peak_results_href) {
        my $states_aref = $peak_results_href->{$vm_name};
        next unless ref($states_aref) eq 'ARRAY' && @$states_aref;

        # Extract values for each profile
        foreach my $profile (@$profiles_aref) {
            my $profile_name = $profile->{name};
            my $p_key = "P" . clean_perc_label(($profile->{flags} =~ /-p\s+([0-9.]+)/) ? $1 : $DEFAULT_PERCENTILE);

            # Search through all states (newest to oldest) to find this profile's value
            my $standard_peak_value = undef;
            my $clipping_metrics = undef;
            my $source_state = undef;

            for (my $i = $#{$states_aref}; $i >= 0; $i--) {
                my $state = $states_aref->[$i];
                my $val = _safe_dig($state, 'metrics', 'physc', $profile_name, $p_key);
                if (defined $val) {
                    $standard_peak_value = $val;
                    $source_state = $state;
                    last;  # Found it, stop searching
                }
            }

            if (defined $standard_peak_value) {
                $peak_period_results{$vm_name}{$profile_name} = $standard_peak_value;
            }

            # Extract clipping information from the same state where we found the value
            if (defined $source_state) {
                $clipping_metrics = _safe_dig($source_state, 'metrics', 'physc', $profile_name, 'ClippingInfo');
            }

            if ($clipping_metrics && ref($clipping_metrics) eq 'HASH' && exists $clipping_metrics->{isClipped}) {
                my $unmet_demand_est = $clipping_metrics->{unmetDemandEstimate} // 0;
                my $max_cpu = _safe_dig($source_state, 'metadata', 'max_cpu') // 0;

                # Assemble ClippingInfo with the exact structure expected by consumers
                $clipping_info_results{$vm_name}{$profile_name} = {
                    isClipped               => $clipping_metrics->{isClipped},
                    clippingConfidence      => $clipping_metrics->{clippingConfidence} // 'unknown',
                    capacityLimit           => $max_cpu,
                    unmetDemandEstimate     => $unmet_demand_est,
                    unclippedPeakEstimate   => ($standard_peak_value // 0) + $unmet_demand_est,
                    platformSpecificMarkers => {
                        aix_runq_saturation => _safe_dig($clipping_metrics, 'platformMarkers', 'aix_runq_saturation') // 0
                    }
                };
            }

            # Calculate peak residual if we have sanitised peak data
            if (defined $standard_peak_value && exists $sanitised_peak_values{$vm_name}) {
                my $residual = $sanitised_peak_values{$vm_name} - $standard_peak_value;
                $residual_results_final{$vm_name}{$profile_name} = ($residual > 0) ? $residual : 0;
            }
        }
    }

    # ======================================================================
    # STEP 3b: Extract Metadata Peak (Absolute Maximum) for each VM
    # ======================================================================
    # The "Peak" column is a metadata field (-k), not a profile P-metric.
    # It represents the absolute raw maximum PhysC observed during the peak period.
    # We must explicitly harvest it since it's not captured by profile iteration.
    # ======================================================================
    my %metadata_peak_results;
    foreach my $vm_name (keys %$peak_results_href) {
        my $states_aref = $peak_results_href->{$vm_name};
        next unless ref($states_aref) eq 'ARRAY' && @$states_aref;

        my @peak_values;
        foreach my $state (@$states_aref) {
            # Extract the raw Peak metric from the P-99W1 profile (used for hinting)
            my $peak_val = _safe_dig($state, 'metrics', 'physc', $MANDATORY_PEAK_PROFILE_FOR_HINT, 'Peak');
            push @peak_values, $peak_val if (defined $peak_val && looks_like_number($peak_val));
        }

        if (@peak_values) {
            $metadata_peak_results{$vm_name} = max(@peak_values);
        }
    }

    # ======================================================================
    # STEP 4: Validate and return
    # ======================================================================
    unless (scalar keys %peak_period_results && scalar keys %baseline_period_results) {
        warn "      - WARNING: Could not generate valid peak/baseline data for event '$event_name'. Skipping snapshot.\n";
        return undef;
    }

    # Assemble the final snapshot structure
    return {
        eventName     => $event_name,
        periodEndDate => $peak_end->ymd,
        baselinePeriod => {
            startDate => $baseline_start ? $baseline_start->ymd : undef,
            endDate   => $baseline_end ? $baseline_end->ymd : undef,
        },
        periodEndDate => $peak_end->ymd,
        generatedOn   => localtime()->datetime(),
        results       => {
            $peak_key        => \%peak_period_results,
            $baseline_key    => \%baseline_period_results,
            'ClippingInfo'   => \%clipping_info_results,
            'PeakResidual'   => \%residual_results_final,
            'MetadataPeak'   => \%metadata_peak_results,  # Absolute raw peak per VM
        }
    };

}

# ==============================================================================
# SUBROUTINE: _store_model_forecast_to_history
# PURPOSE:    Stores the forecast results from a seasonal model run into the
#             unified history under ModelForecasts for the appropriate event.
#             This enables nfit-forecast to treat these as candidate forecasts.
#
# ARGUMENTS:
#   1. $system_cache_dir      - Path to the system's cache directory
#   2. $event_name            - Name of the seasonal event (e.g., 'month-end')
#   3. $model_type            - Model identifier (e.g., 'predictive_peak')
#   4. $forecast_results_href - Hash ref of {vm => {profile => value}}
#   5. $event_config          - Event configuration hash (optional)
#
# SCHEMA:
#   SeasonalEventSnapshots.{event}.ModelForecasts.{model} = {
#     _meta: { model_version, generated_on, analysis_window, fingerprints },
#     VM01: { forecast: {P-99W1: val}, baseline: {P-99W1: val} },
#     VM02: { ... }
#   }
# ==============================================================================
sub _store_model_forecast_to_history {
    my ($system_cache_dir, $event_name, $model_type, $forecast_results_href, $event_config, $horizon_meta, $analysis_context, $explicit_anchor_bucket, $fingerprints_href) = @_;

    return unless ($system_cache_dir && $event_name && $model_type && $forecast_results_href);
    return unless (scalar keys %$forecast_results_href > 0);

    print STDERR "  • Writing $model_type forecast to history (event '$event_name')\n";

    # Resolve effective analysis window (from analysis_context; fallback to cache span)
    my $data_cache_file = File::Spec->catfile($system_cache_dir, '.nfit.cache.data');
    my ($cache_start, $cache_end) = _get_cache_date_range($data_cache_file);

    my $analysis_start_obj = (defined $analysis_context->{start} && ref($analysis_context->{start}) eq 'Time::Piece')
        ? $analysis_context->{start}->truncate(to => 'day')
        : $cache_start->truncate(to => 'day');

    my $analysis_end_obj = (defined $analysis_context->{end} && ref($analysis_context->{end}) eq 'Time::Piece')
        ? $analysis_context->{end}->truncate(to => 'day')
        : $cache_end->truncate(to => 'day');

    unless ($analysis_start_obj && $analysis_end_obj) {
        print STDERR "  [WARN] Could not determine analysis date range. Forecasting cannot proceed\n";
        return;
    }

    # --- PHASE 1: DETERMINISTIC ANCHOR BUCKET RESOLUTION ---
    # If an explicit anchor bucket is provided by the orchestrator, use it.
    # This ensures deterministic history storage based on the resolved anchor,
    # not on which partitions happen to exist.

    my $target_month_key;

    if (defined $explicit_anchor_bucket && $explicit_anchor_bucket =~ /^\d{4}-\d{2}$/) {
        # Use the orchestrator-provided anchor bucket directly
        $target_month_key = $explicit_anchor_bucket;
        print STDERR "    → Target month (anchor bucket): $target_month_key\n";
    }
    elsif (defined $analysis_context && defined $analysis_context->{anchor_bucket}
           && $analysis_context->{anchor_bucket} =~ /^\d{4}-\d{2}$/) {
        # Fallback to anchor_bucket from analysis_context if available
        $target_month_key = $analysis_context->{anchor_bucket};
        print STDERR "    → Target month (from context): $target_month_key\n";
    }
    else {
        # Legacy fallback: determine from the analysis end date
        $target_month_key = $analysis_end_obj->strftime('%Y-%m');
        print STDERR "    → Target month (inferred): $target_month_key\n";
    }

    my $analysis_days = int(($analysis_end_obj->epoch - $analysis_start_obj->epoch) / ONE_DAY) + 1;
    my $analysis_start_date = $analysis_start_obj->strftime('%Y-%m-%d');
    my $analysis_end_date = $analysis_end_obj->strftime('%Y-%m-%d');

    # Read current history
    print STDERR "  • Loading unified partitioned history\n";
    my $history_data = read_unified_history($system_cache_dir);

    # --- PHASE 1: Relaxed partition existence check ---
    # We no longer require MonthlyWorkloadAnalysis to exist for the target month.
    # The seasonal snapshot can be stored even if the monthly analysis hasn't run yet.

    unless (exists $history_data->{$target_month_key}) {
        print STDERR "    [INFO] Creating new history partition for $target_month_key\n";
        $history_data->{$target_month_key} = {};
    }

    # Optional: inform if MonthlyWorkloadAnalysis is missing (for information only)
    unless (exists $history_data->{$target_month_key}{MonthlyWorkloadAnalysis}) {
        print STDERR "    [INFO] MonthlyWorkloadAnalysis not present for $target_month_key (OK for seasonal-only runs)\n";
    }

    if ($analysis_start_obj && $analysis_end_obj) {
        $analysis_days = int(($analysis_end_obj->epoch - $analysis_start_obj->epoch) / ONE_DAY) + 1;
    }

    # Ensure the structural hierarchy exists
    $history_data->{$target_month_key} //= {};
    $history_data->{$target_month_key}{SeasonalEventSnapshots} //= {};
    $history_data->{$target_month_key}{SeasonalEventSnapshots}{$event_name} //= {
        eventName => $event_name,
    };

    # Ensure periodEndDate is populated if missing (crucial for time-series plotting)
    $history_data->{$target_month_key}{SeasonalEventSnapshots}{$event_name}{periodEndDate} //= $analysis_end_date;

    # Initialise ModelForecasts container if absent
    my $event_snapshot = $history_data->{$target_month_key}{SeasonalEventSnapshots}{$event_name};
    $event_snapshot->{ModelForecasts} //= {};

    my %model_data;

    # Metadata block (underscore prefix distinguishes from VM keys)
    my $meta_block = {
        model_version   => 1,
        generated_on    => localtime()->datetime(),
        analysis_window => {
            days    => $analysis_days,
            start   => $analysis_start_date,
            end     => $analysis_end_date,
        },
    };

    # Inject the detailed AnalysisContext if provided
    if ($analysis_context) {

        # Recalculate window days from the context objects for precision
        my $window_days_calc = 0;
        if ($analysis_context->{start} && $analysis_context->{end}) {
            $window_days_calc = int(($analysis_context->{end}->epoch - $analysis_context->{start}->epoch) / 86400) + 1;
        }

        $meta_block->{AnalysisContext} = {
            # Data Boundaries (The "Ground Truth" for Backfill)
            cache_start            => $cache_start->datetime,
            cache_end              => $cache_end->datetime,

            # Effective analysis window (what this run used)
            analysis_start => $analysis_start_obj->datetime,
            analysis_end => $analysis_end_obj->datetime,

            # Statistical Context
            sampling_interval_seconds => ($analysis_context->{interval} // 0) + 0, # Ensure numeric
            analysis_window_days      => $window_days_calc,
            adaptive_horizon_days     => ($horizon_meta->{days} // 0) + 0,

            # Execution Context (as requested)
            analysis_start_epoch      => $analysis_context->{analysis_start},

            # Phase 3.1: bucket provenance (audit only)
            anchor_bucket             => $analysis_context->{anchor_bucket},
            anchor_source             => $analysis_context->{anchor_source},
            bucket_occurrence_count   => $analysis_context->{bucket_occurrence_count},
            bucket_occurrence_end_dates => $analysis_context->{bucket_occurrence_end_dates},
        };
    }

    # Inject Horizon Metadata if provided
    if ($horizon_meta) {
        $meta_block->{forecast_target_date}  = $horizon_meta->{target_date};
        $meta_block->{forecast_horizon_days} = $horizon_meta->{days};
    }

    # Phase 4: Inject fingerprints for idempotency (if provided)
    if ($fingerprints_href && ref($fingerprints_href) eq 'HASH') {
        $meta_block->{fingerprints} = {
            combined       => $fingerprints_href->{combined},
            event_def      => $fingerprints_href->{event_def},
            exec_ctx       => $fingerprints_href->{exec_ctx},
            chain          => $fingerprints_href->{chain},
            engine_version => $fingerprints_href->{engine_version},
        };
    }

    $model_data{'_meta'} = $meta_block;

    # Build per-VM forecast and baseline data
    my $vm_count = 0;
    foreach my $vm_name (sort keys %$forecast_results_href) {
        my $vm_results = $forecast_results_href->{$vm_name};
        next unless (ref($vm_results) eq 'HASH' && scalar keys %$vm_results > 0);

        my %forecast_by_profile;
        my %baseline_by_profile;
        my %components_by_profile;

        foreach my $profile_key (keys %$vm_results) {
            my $value = $vm_results->{$profile_key};

            # Store forecast value
            if (defined $value && looks_like_number($value)) {
                $forecast_by_profile{$profile_key} = sprintf("%.4f", $value) + 0;
            }

            # Extract baseline and components from seasonal_debug_info if available
            if (exists $seasonal_debug_info{$vm_name}{$profile_key}) {
                my $debug = $seasonal_debug_info{$vm_name}{$profile_key};

                # Model-specific baseline and component extraction
                my $baseline_val;
                if ($model_type eq 'predictive_peak') {
                    $baseline_val = $debug->{TrueBaseline};
                    # Capture prediction components for audit
                    if (defined $debug->{PredictedPeak} || defined $debug->{PredictedResidual}) {
                        $components_by_profile{$profile_key} = {
                            predicted_peak      => $debug->{PredictedPeak},
                            predicted_residual  => $debug->{PredictedResidual},
                            combined_prediction => $debug->{CombinedPrediction},
                            final_source        => $debug->{FinalSource},
                        };
                    }
                }
                elsif ($model_type eq 'multiplicative_seasonal') {
                    $baseline_val = $debug->{baseline};
                    # Capture the seasonal multiplier for audit
                    if (defined $debug->{multiplier}) {
                        $components_by_profile{$profile_key} = {
                            seasonal_multiplier  => $debug->{multiplier},
                            trend_factor         => $debug->{trend_factor},
                            volatility_buffer    => $debug->{volatility},
                            forecasted_residual  => $debug->{forecasted_residual},
                            amplification_factor => $debug->{amplification_factor},
                        };
                    }
                    # Phase 3.2: synthesis distribution metadata (if aggregated)
                    if (exists $debug->{synthesis} && ref($debug->{synthesis}) eq 'HASH') {
                        $components_by_profile{$profile_key} //= {};
                        $components_by_profile{$profile_key}{synthesis} = $debug->{synthesis};
                    }
                }

                if (defined $baseline_val && looks_like_number($baseline_val)) {
                    $baseline_by_profile{$profile_key} = sprintf("%.4f", $baseline_val) + 0;
                }
            }
        }

        # Only store VMs that have actual forecast data
        if (scalar keys %forecast_by_profile > 0) {
            $model_data{$vm_name} = {
                forecast => \%forecast_by_profile,
            };
            $model_data{$vm_name}{baseline} = \%baseline_by_profile if %baseline_by_profile;
            $model_data{$vm_name}{components} = \%components_by_profile if %components_by_profile;
            $vm_count++;
        }
    }

    # Only write if we have actual data
    if ($vm_count > 0) {
        $event_snapshot->{ModelForecasts}{$model_type} = \%model_data;

        # Phase 3.2: Also populate results.HistoricPeak and results.HistoricBaseline
        # This enables multiplier calculation for subsequent occurrences within the same run
        if ($model_type eq 'multiplicative_seasonal') {
            $event_snapshot->{results} //= {};
            $event_snapshot->{results}{HistoricPeak} //= {};
            $event_snapshot->{results}{HistoricBaseline} //= {};

            foreach my $vm_name (keys %model_data) {
                foreach my $profile_key (keys %{ $model_data{$vm_name}{forecast} // {} }) {
                    # HistoricPeak: use observed_peak from debug_info, fallback to forecast
                    my $peak_val = undef;
                    if (exists $seasonal_debug_info{$vm_name}{$profile_key}{observed_peak}) {
                        $peak_val = $seasonal_debug_info{$vm_name}{$profile_key}{observed_peak};
                    }
                    $peak_val //= $model_data{$vm_name}{forecast}{$profile_key};

                    if (defined $peak_val && looks_like_number($peak_val)) {
                        $event_snapshot->{results}{HistoricPeak}{$vm_name}{$profile_key} = $peak_val + 0;
                    }

                    # HistoricBaseline: from model_data baseline
                    my $base_val = $model_data{$vm_name}{baseline}{$profile_key};
                    if (defined $base_val && looks_like_number($base_val)) {
                        $event_snapshot->{results}{HistoricBaseline}{$vm_name}{$profile_key} = $base_val + 0;
                    }
                }
            }
        }

        # Update metadata timestamp
        $history_data->{$target_month_key}{Metadata} //= {};
        $history_data->{$target_month_key}{Metadata}{LastUpdated} = localtime()->datetime();

        # Write back to history
        _write_full_history($system_cache_dir, $history_data);

        print STDERR "  ✓ Stored $model_type forecasts for $vm_count VM(s) in $target_month_key history\n";
    }
    else {
        print STDERR "  [WARN] No valid forecast data to store for $model_type in history\n";
    }
}

# ==============================================================================
# SUBROUTINE: _generate_seasonal_snapshot_for_period
# PURPOSE:    Performs single-pass analysis for a seasonal event's peak and
#             baseline periods using the manifest-driven Single Pass Engine
#             architecture. This function executes nfit three times (baseline,
#             peak, residual) instead of 3 * N times (where N = number of profiles).
#
#             This refactored version achieves the same I/O reduction as the
#             monthly analysis (typically 16x for a 16-profile configuration).
# ==============================================================================
sub _generate_seasonal_snapshot_for_period {
    my ($system_cache_dir, $event_config, $event_name, $peak_start, $peak_end, $baseline_start, $baseline_end, $exclusions_href) = @_;

    print STDERR "  ⧉ Executing Single Pass Engine for seasonal event $event_name (baseline, peak, residual)\n";

    # ======================================================================
    # STEP 1: Build and sanitise the main manifest (all profiles)
    # ======================================================================
    my $runq_behavior = $runq_perc_behavior_mode // 'none';
    my $tactical_manifest = build_transform_manifest(\@profiles, $nfit_runq_avg_method_str, $runq_behavior);
    my $main_historical_manifest = _sanitise_manifest_for_history($tactical_manifest);

    # --- INJECT EXCLUSIONS IF PRESENT ---
    if (defined $exclusions_href) {
        _inject_exclusions_into_manifest($main_historical_manifest, $exclusions_href);
    }

    # ======================================================================
    # STEP 2: Build and sanitise the residual manifest (if configured)
    # ======================================================================
    my $residual_historical_manifest = undef;

    if (defined $event_config->{residual_peak_profile} && $event_config->{residual_peak_profile} ne '') {
        my $residual_tactical_manifest = _build_residual_manifest($event_config);
        if (defined $residual_tactical_manifest) {
            $residual_historical_manifest = _sanitise_manifest_for_history($residual_tactical_manifest);

            # Inject exclusions into residual manifest too
            if (defined $exclusions_href) {
                _inject_exclusions_into_manifest($residual_historical_manifest, $exclusions_href);
            }
        }
    }

    # ======================================================================
    # STEP 3: Execute three nfit passes
    # ======================================================================

    # Pass 1: Baseline period (no clipping detection needed)
    my $baseline_results = _execute_history_pass(
        $system_cache_dir,
        $main_historical_manifest,
        $baseline_start,
        $baseline_end,
        0,  # enable_clipping = false
        "Baseline"
    );

    # Pass 2: Standard peak period (with clipping detection)
    my $peak_results = _execute_history_pass(
        $system_cache_dir,
        $main_historical_manifest,
        $peak_start,
        $peak_end,
        1,  # enable_clipping = true
        "Peak"
    );

    # Pass 3: Residual peak period (only if configured)
    my $residual_results = {};
    if (defined $residual_historical_manifest) {
        $residual_results = _execute_history_pass(
            $system_cache_dir,
            $residual_historical_manifest,
            $peak_start,
            $peak_end,
            0,  # enable_clipping = false
            "Residual"
        );
    }

    # ======================================================================
    # STEP 4: Assemble the final snapshot structure
    # ======================================================================
    my $snapshot_result = _assemble_seasonal_snapshot(
        $peak_results,
        $baseline_results,
        $residual_results,
        $event_config,
        $event_name,
        $peak_end,
        \@profiles,  # Pass profiles array explicitly to ensure correct scoping
        $baseline_start,
        $baseline_end,
    );

    return $snapshot_result;
}

# ==============================================================================
# SUBROUTINE: update_monthly_history (PRODUCTION FIX - Unified In-Memory)
# PURPOSE:    Orchestrates a robust, multi-phase strategy to create and update
#             the unified monthly history cache using a single in-memory hash
#             to prevent data corruption from conflicting file writes.
# ==============================================================================
sub update_monthly_history {
    my ($system_cache_dir, $system_identifier, $seasonality_config, $min_days_for_history, $adaptive_runq_saturation_thresh, $force_flag) = @_;

    print STDERR "▶ Updating Unified History for system: $system_identifier\n";

    # In-Flight auto-migration to per-month history
    # We acquire the lock here to perform the check and potential migration atomically.
    {
        my ($lock_fh, $lock_path);
        eval { ($lock_fh, $lock_path) = _acquire_history_lock($system_cache_dir); };
        if (!$@) {
            if (-f File::Spec->catfile($system_cache_dir, '.nfit.history.json') &&
                !-d File::Spec->catfile($system_cache_dir, '.nfit.history')) {

                _migrate_history_to_partitioned($system_cache_dir);
            }
            close $lock_fh;
        }
    }
    # --------------------------------------------

    my $data_cache_file = File::Spec->catfile($system_cache_dir, '.nfit.cache.data');
    my ($cache_start_obj, $cache_end_obj) = _get_cache_date_range($data_cache_file);

    unless ($cache_start_obj && $cache_end_obj) {
        warn "  [WARN] Unable to determine date range of data cache (.nfit.cache.data). Cannot update history.\n";
        return;
    }
    print STDERR "  ⧗ Data cache time range: " . $cache_start_obj->date . " - " . $cache_end_obj->date . "\n";

    # --- Step 1: Load existing history ONCE ---
    my $history_data = read_unified_history($system_cache_dir);
    my @months_to_process;

    # --- Step 2: Planning Stage - Discover unprocessed months ---
    print STDERR "  ↳  Scanning data cache for unprocessed or incomplete months\n";
    my $iterator_month = Time::Piece->new($cache_start_obj->epoch)->truncate(to => 'month');
    while ($iterator_month <= $cache_end_obj) {
        my $month_key = $iterator_month->strftime('%Y-%m');
        my $month_start_obj = Time::Piece->new($iterator_month->epoch);
        my $month_end_obj   = Time::Piece->new($month_start_obj->epoch)->add_months(1) - ONE_DAY;
        my $effective_start_obj = ($cache_start_obj > $month_start_obj) ? $cache_start_obj : $month_start_obj;
        my $effective_end_obj   = ($cache_end_obj   < $month_end_obj)   ? $cache_end_obj   : $month_end_obj;
        my $days_in_period = ($effective_start_obj <= $effective_end_obj) ? (int(($effective_end_obj->epoch - $effective_start_obj->epoch) / 86400) + 1) : 0;

        my $should_process = 0;

        # Logic: Process if new, if data is more complete, OR if forced.
        if (exists $history_data->{$month_key}) {
            my $stored_days = $history_data->{$month_key}{Metadata}{ProcessedDays} // 0;

            # --- FIX: Check force_flag here ---
            if ($force_flag) {
                print STDERR "  ↳  Forcing re-processing of $month_key (user request)\n";
                $should_process = 1;
            }
            elsif ($days_in_period > $stored_days) {
                 print STDERR "  ↳  Discovered additional data for $month_key ($days_in_period > $stored_days days)\n";
                 $should_process = 1;
            }
        } elsif ($days_in_period >= $min_days_for_history) {
            # New month detected
            $should_process = 1;
        }

        if ($should_process) {
            push @months_to_process, { key => $month_key, start_obj => $effective_start_obj, end_obj => $effective_end_obj, days => $days_in_period };
        }
        $iterator_month = $iterator_month->add_months(1);
    }
    print STDERR "  ✓ Data Cache examination complete\n";

    # Pre-execution validation for the first month to be processed
    if (@months_to_process) {
        my $first_month_key = $months_to_process[0]{key};
        _validate_pre_update_conditions($system_cache_dir, $first_month_key, $force_flag);

        # Create checkpoint backup before any modifications
        print STDERR "  • Creating pre-tactical checkpoint backup\n";
        _create_checkpoint_backup($system_cache_dir, 'pre-tactical', 1);
    }

    # --- Step 3: Execution Stage ---
    if (@months_to_process) {
        print STDERR "  ⧗ Identified " . scalar(@months_to_process) . " month(s) to process: " . join(", ", map { $_->{key} } @months_to_process) . "\n";

        my @all_historical_peaks = find_all_historical_periods($seasonality_config, $cache_start_obj, $cache_end_obj);

        # This single function call modifies the in-memory $history_data hash.
        _generate_monthly_and_seasonal_history(
            $system_cache_dir, \@months_to_process, $seasonality_config,
            \@all_historical_peaks, $history_data, $adaptive_runq_saturation_thresh
        );
    } else {
        print STDERR "  ✓ No new months to process. History is up to date\n";
    }


    # Track if this was a first run (no pre-existing history file)
    my $history_file = File::Spec->catfile($system_cache_dir, '.nfit.history.json');
    my $was_first_run = !(-f $history_file);

    # --- Phase 3: Enrich with Growth Rationale from L2 Cache ---
    my $was_enriched = _enrich_history_with_growth_rationale($system_cache_dir, $history_data);

    # --- Step 4: Final Write ---
    # The history file is written only ONCE at the end if any changes were made.
    if (@months_to_process || $was_enriched) {
        print "  ↳  Finalising updates to history cache.\n";

        _write_full_history($system_cache_dir, $history_data);
        print "  ✓ History cache successfully updated.\n";

        # If this was the first run, create a post-write backup to protect the newly created file
        if ($was_first_run && -f $history_file) {
            print "\n--- Creating Post-Initial-Write Backup ---\n";
            _create_checkpoint_backup($system_cache_dir, 'post-initial-write', 1);
        }
    } else {
        print "  ✓ History cache is already up to date.\n";
    }
}

# ==============================================================================
# SUBROUTINE: _generate_monthly_and_seasonal_history (PRODUCTION FIX - COMBINED)
# PURPOSE:    Processes months to generate both generic analysis and snapshots,
#             modifying a single in-memory hash to prevent data corruption.
# ==============================================================================
sub _generate_monthly_and_seasonal_history {
    my ($system_cache_dir, $months_to_process_aref, $seasonality_config, $all_historical_peaks_aref, $history_data_href, $adaptive_runq_saturation_thresh) = @_;

    foreach my $month_job (@$months_to_process_aref) {
        my $month_key = $month_job->{key};
        my $start_str = $month_job->{start_obj}->strftime('%Y-%m-%d');
        my $end_str   = $month_job->{end_obj}->strftime('%Y-%m-%d');

        # --- PHASE 1: Generate MonthlyWorkloadAnalysis ---
        print "\n--- Processing Month: $month_key (Period: $start_str to $end_str) ---\n";
        my $workload_analysis = _generate_monthly_workload_analysis($system_cache_dir, $start_str, $end_str, $adaptive_runq_saturation_thresh);

        # --- Populate the in-memory hash ---
        $history_data_href->{$month_key} = {
            MonthlyWorkloadAnalysis => $workload_analysis,
            Metadata => {
                ProcessedStartDate => $start_str,
                ProcessedEndDate   => $end_str,
                ProcessedDays      => $month_job->{days},
                LastUpdated        => localtime()->datetime(),
            }
        };

        # --- PHASE 2: Generate SeasonalEventSnapshots for this month ---
        print "  - Discovering seasonal events for month: $month_key...\n";
        my %seasonal_snapshots_for_month;
        my %calculation_cache;

        foreach my $event_name (sort keys %$seasonality_config) {
            next if $event_name eq 'Global';
            my $event_config = $seasonality_config->{$event_name};
            next if (($event_config->{model} // '') eq 'recency_decay');

            my ($peak_start, $peak_end) = determine_event_period($event_config, $month_job->{start_obj});

            # Parse Exclusions
            my $exclusions = undef;
            my $exclusion_fingerprint = 'NONE';
            if (defined $event_config->{exclude_dates}) {
                # We need known_vms for wildcard expansion.
                # Optimisation: You might want to fetch this ONCE outside the loop if performance matters,
                # but fetching it here is safe.
                my $known_vms = _get_known_vms_from_cache($system_cache_dir);
                $exclusions = _parse_exclusion_dates($event_config->{exclude_dates}, $known_vms);
                $exclusion_fingerprint = _compute_exclusion_fingerprint($exclusions);
            }

            if ($peak_start && $peak_end && $peak_start <= $month_job->{end_obj} && $peak_end >= $month_job->{start_obj}) {
                print "    - Found active event: '$event_name'\n";
                my $baseline_days = $event_config->{baseline_period_days} // 16;
                my $potential_baseline_start = $peak_start - ($baseline_days * ONE_DAY);
                my $latest_preceding_peak_end;
                foreach my $p (@{$all_historical_peaks_aref}) {
                    if ($p->[1] < $peak_start && (!defined $latest_preceding_peak_end || $p->[1] > $latest_preceding_peak_end)) {
                        $latest_preceding_peak_end = $p->[1];
                    }
                }
                my $baseline_start_obj = $potential_baseline_start;
                if (defined $latest_preceding_peak_end && $potential_baseline_start <= $latest_preceding_peak_end) {
                    $baseline_start_obj = $latest_preceding_peak_end + ONE_DAY;
                    my $available_days = ($peak_start->epoch - $baseline_start_obj->epoch) / ONE_DAY;
                    print "      - WARNING: Baseline for this event truncated to " . int($available_days) . " day(s) to avoid overlap.\n";
                }
                my $baseline_end_obj = $peak_start - ONE_SECOND;
                my $model_type_for_key = $event_config->{model} // 'unknown';

                # Update Cache Key with Fingerprint
                my $cache_key = $peak_start->date . ":" . $peak_end->date . ":" . $baseline_start_obj->date .  ":" . $model_type_for_key . ":" . $exclusion_fingerprint;

                my $snapshot_results;
                if (exists $calculation_cache{$cache_key}) {
                    print "      - INFO: Reusing cached calculations for identical time period and model type.\n";
                    $snapshot_results = $calculation_cache{$cache_key};
                } else {
                    $snapshot_results = _generate_seasonal_snapshot_for_period(
                        $system_cache_dir,
                        $event_config,
                        $event_name,
                        $peak_start, $peak_end,
                        $baseline_start_obj, $baseline_end_obj,
                        $exclusions
                    );
                    $calculation_cache{$cache_key} = $snapshot_results;
                }
                if ($snapshot_results) {
                    $snapshot_results->{eventName} = $event_name;
                    $seasonal_snapshots_for_month{$event_name} = $snapshot_results;
                }
            }
        }

        if (scalar keys %seasonal_snapshots_for_month > 0) {
            $history_data_href->{$month_key}{SeasonalEventSnapshots} = \%seasonal_snapshots_for_month;
            $history_data_href->{$month_key}{Metadata}{LastUpdated} = localtime()->datetime();
        }
    }
}

# ==============================================================================
# SUBROUTINE: _validate_history_structure
# PURPOSE:    Performs structural validation on the history data structure before
#             writing to disk. This safety check prevents corruption by detecting
#             missing or empty blocks that would break downstream consumers.
#
# ARGUMENTS:
#   1. $history_href (hash ref): The complete history data structure
#
# RETURNS:
#   - 1 on success (structure is valid)
#   - Dies with detailed error message on validation failure
#
# VALIDATION CHECKS:
#   1. Required blocks exist (MonthlyWorkloadAnalysis, Metadata)
#   2. Blocks are not empty
#   3. Required fields are present in nested structures
#   4. Basic data type validation (hashes where expected, etc.)
# ==============================================================================
sub _validate_history_structure {
    my ($history_href) = @_;

    # Validate input type
    unless (ref($history_href) eq 'HASH') {
        die "FATAL: History structure validation failed - not a hash reference\n";
    }

    # Check that we have at least one month of data
    my @months = keys %$history_href;
    unless (@months) {
        die "FATAL: History structure validation failed - no monthly data present\n";
    }

    # Validate each month's structure
    foreach my $month_key (sort @months) {
        my $month_data = $history_href->{$month_key};

        # Check that month data is a hash
        unless (ref($month_data) eq 'HASH') {
            die "FATAL: History structure validation failed for $month_key - month data is not a hash\n";
        }

        # Check for required top-level blocks
        unless (exists $month_data->{MonthlyWorkloadAnalysis}) {
            die "FATAL: History structure validation failed for $month_key - missing MonthlyWorkloadAnalysis block\n";
        }

        unless (exists $month_data->{Metadata}) {
            die "FATAL: History structure validation failed for $month_key - missing Metadata block\n";
        }

        # Validate MonthlyWorkloadAnalysis is not empty
        my $workload_analysis = $month_data->{MonthlyWorkloadAnalysis};
        unless (ref($workload_analysis) eq 'HASH') {
            die "FATAL: History structure validation failed for $month_key - MonthlyWorkloadAnalysis is not a hash\n";
        }

        unless (scalar keys %$workload_analysis) {
            die "FATAL: History structure validation failed for $month_key - MonthlyWorkloadAnalysis is empty\n";
        }

        # Validate that each VM in MonthlyWorkloadAnalysis has required fields
        foreach my $vm_name (keys %$workload_analysis) {
            my $vm_data = $workload_analysis->{$vm_name};

            unless (ref($vm_data) eq 'HASH') {
                die "FATAL: History structure validation failed for $month_key VM $vm_name - VM data is not a hash\n";
            }

            # Check for required fields
            my @required_fields = qw(ProfileValues Hint Pattern Pressure);
            foreach my $field (@required_fields) {
                unless (exists $vm_data->{$field}) {
                    die "FATAL: History structure validation failed for $month_key VM $vm_name - missing required field: $field\n";
                }
            }

            # Validate ProfileValues is a non-empty hash
            unless (ref($vm_data->{ProfileValues}) eq 'HASH' && scalar keys %{$vm_data->{ProfileValues}}) {
                die "FATAL: History structure validation failed for $month_key VM $vm_name - ProfileValues is empty or invalid\n";
            }
        }

        # Validate Metadata structure
        my $metadata = $month_data->{Metadata};
        unless (ref($metadata) eq 'HASH') {
            die "FATAL: History structure validation failed for $month_key - Metadata is not a hash\n";
        }

        # Check for essential metadata fields
        unless (exists $metadata->{ProcessedDays}) {
            die "FATAL: History structure validation failed for $month_key - Metadata missing ProcessedDays\n";
        }

        # Optional: Validate SeasonalEventSnapshots structure if present
        if (exists $month_data->{SeasonalEventSnapshots}) {
            my $seasonal = $month_data->{SeasonalEventSnapshots};
            unless (ref($seasonal) eq 'HASH') {
                die "FATAL: History structure validation failed for $month_key - SeasonalEventSnapshots is not a hash\n";
            }

            # Validate each event's structure
            foreach my $event_name (keys %$seasonal) {
                my $event_data = $seasonal->{$event_name};

                # Allow entry if it has 'results' (Snapshot) OR 'ModelForecasts'/'AdaptiveForecast' (Pure Forecast)
                # Required since recency_decay models (and nfit-forecast backfills) create entries that contain only ModelForecasts (predictions), not results (ingredients).
                my $has_results = (exists $event_data->{results} && ref($event_data->{results}) eq 'HASH');
                my $has_forecast = (exists $event_data->{ModelForecasts} || exists $event_data->{AdaptiveForecast});

                unless (ref($event_data) eq 'HASH' && ($has_results || $has_forecast)) {
                    die "FATAL: History structure validation failed for $month_key event $event_name - missing both 'results' (Snapshot) and 'ModelForecasts' (Forecast) blocks\n";
                }
            }

        }
    }

    # All validation checks passed
    return 1;
}

# ==============================================================================
# SUBROUTINE: _write_full_history (Partition Aware)
# PURPOSE:    Writes the in-memory history hash to the partitioned directory structure.
#             Creates the directory if missing. Enforces Global Lock.
#             Includes structural validation before writing to prevent corruption.
#             Refactored to use the standardised _acquire_history_lock for safety.
# ARGUMENTS:
#   1. $system_cache_dir (string): The path to a system's cache directory.
#   2. $history_data_href (hash ref): The complete history data structure.
# RETURNS:
#   - None
# ==============================================================================
sub _write_full_history {
    my ($system_cache_dir, $history_data_href) = @_;

    # 1. Validation
    # ======================================================================
    # CRITICAL SAFETY CHECK: Validate structure before writing
    # ======================================================================
    # This validation prevents corrupted or incomplete data from being
    # persisted to disk. It's better to fail fast with a clear error
    # than to write broken data that silently breaks downstream consumers.
    # ======================================================================
    eval { _validate_history_structure($history_data_href); };
    if ($@) {
        die "FATAL: Aborting history write due to validation failure:\n$@";
    }

    _add_lightweight_metadata($history_data_href, 'nfit-profile');

    # 2. Global Lock Acquisition
    my ($lock_fh, $lock_path);
    eval { ($lock_fh, $lock_path) = _acquire_history_lock($system_cache_dir); };
    if ($@) { die "FATAL: Could not acquire global history lock: $@"; }

    # 3. Directory Setup
    my $partition_dir = File::Spec->catfile($system_cache_dir, '.nfit.history');
    unless (-d $partition_dir) {
        make_path($partition_dir) or die "FATAL: Cannot create history directory: $!";
    }

    # 4. Write Partitions
    my $json_encoder = JSON->new->pretty->canonical;
    my $write_errors = 0;

    # Sort to give deterministic write order (helps logs/debugging)
    foreach my $month_key (sort keys %$history_data_href) {

        # Skip metadata/non-month keys if any creep in
        next unless $month_key =~ /^\d{4}-\d{2}$/;

        my $filename = "nfit.hist.${month_key}.json";
        my $filepath = File::Spec->catfile($partition_dir, $filename);

        # Wrap in month key for consistency
        my $payload = { $month_key => $history_data_href->{$month_key} };

        my $json_text;
        my $tmp_path = "${filepath}.tmp.$$";
        my $bak_path = "${filepath}.bak";

        eval {
            # Encode FIRST (prevents truncation if encode dies)
            $json_text = $json_encoder->encode($payload);

            # Write to temp file in same directory
            open my $fh, '>:encoding(utf8)', $tmp_path
                or die "Open temp failed ($tmp_path): $!";

            print {$fh} $json_text
                or die "Write temp failed ($tmp_path): $!";

            close $fh
                or die "Close temp failed ($tmp_path): $!";

            # Optional: keep last-good backup (best-effort)
            if (-f $filepath) {
                # If a prior .bak exists, rotate it away to avoid rename failure
                unlink $bak_path if -f $bak_path;
                rename($filepath, $bak_path)
                    or die "Backup rename failed ($filepath -> $bak_path): $!";
            }

            # Atomic replace
            rename($tmp_path, $filepath)
                or die "Atomic rename failed ($tmp_path -> $filepath): $!";

            1;
        } or do {
            my $err = $@ || 'unknown error';

            # Cleanup temp file if it exists
            unlink $tmp_path if -f $tmp_path;

            # If we created/rotated a backup but failed before final rename,
            # try to restore the backup best-effort (avoid leaving file missing)
            if (!-f $filepath && -f $bak_path) {
                rename($bak_path, $filepath);  # best-effort; do not die here
            }

            warn "Error writing partition $filename: $err";
            $write_errors++;
        };
    }

    # 5. Legacy Cleanup (only if ALL partitions succeeded)
    # If we successfully wrote partitions, we ensure no stale monolith exists
    # to confuse readers, though _migrate should have handled this.
    my $legacy_file = File::Spec->catfile($system_cache_dir, $UNIFIED_HISTORY_FILE);
    if (-f $legacy_file && !$write_errors) {
        rename($legacy_file, $legacy_file . ".migrated");
    }

    close $lock_fh;

    if ($write_errors) {
        die "FATAL: Errors occurred writing history partitions. Existing history was preserved. Check logs.";
    }

    return 1;
}

# ==============================================================================
# SUBROUTINE: _create_checkpoint_backup (Directory Aware & Compressed)
# PURPOSE:    Creates an atomic, compressed snapshot of the history.
#             Handles both Legacy (File) and Modern (Directory) structures.
#             Uses Archive::Tar for cross-platform (AIX/Linux) compatibility.
# ARGUMENTS:
#   1. $system_cache_dir: Path to the staging cache
#   2. $checkpoint_type:  Label for the backup (e.g., 'pre-tactical')
#   3. $include_l2_cache: Boolean, whether to include L2 results
# RETURNS:
#   Path to the created backup file, or undef on failure.
# ==============================================================================
sub _create_checkpoint_backup {
    my ($system_cache_dir, $checkpoint_type, $include_l2_cache) = @_;

    # Derive serial number and paths
    my $serial = basename($system_cache_dir);
    my $root_dir = dirname(dirname($system_cache_dir));
    my $backup_root = File::Spec->catfile($root_dir, 'backups', $serial);

    make_path($backup_root) unless -d $backup_root;

    my $timestamp = localtime->strftime('%Y%m%d_%H%M%S');

    # Identify the source (Directory takes precedence over File)
    my $history_dir_source  = File::Spec->catfile($system_cache_dir, '.nfit.history');
    my $history_file_source = File::Spec->catfile($system_cache_dir, '.nfit.history.json');

    my $source_path;
    my $is_directory = 0;

    if (-d $history_dir_source) {
        $source_path = $history_dir_source;
        $is_directory = 1;
    } elsif (-f $history_file_source) {
        $source_path = $history_file_source;
        $is_directory = 0;
    } else {
        print "  - No existing history found to backup (First Run)\n";
        return undef;
    }

    # Generate unique backup filename (.tar.gz for dirs, .json.gz for files)
    my $extension = $is_directory ? "tar.gz" : "json.gz";
    my $backup_target = _get_next_available_filename(
        $backup_root,
        ".nfit.history.${timestamp}",
        $extension
    );

    # Perform the Backup
    print "  • Creating compressed checkpoint (${checkpoint_type})\n";

    eval {
        if ($is_directory) {
            # Create a compressed tarball of the directory contents
            my $tar = Archive::Tar->new;

            # We cd into the directory to keep paths relative inside the tar
            my $cwd = Cwd::getcwd();
            chdir($system_cache_dir);

            # Add the folder (e.g., .nfit.history)
            # This recursively adds contents
            $tar->add_files(basename($source_path));

            # Write compressed file
            $tar->write($backup_target, COMPRESS_GZIP);

            chdir($cwd); # Restore PWD
        } else {
            # Gzip the single legacy file
            # We use IO::Zlib or just simple system gzip if we want to be lazy,
            # but let's stick to Perl for safety.
            # Actually, for a single file, copying then gzipping is safest
            # to avoid race conditions reading the source.

            require File::Copy;
            # Copy to .tmp first
            my $tmp_target = "$backup_target.tmp";
            File::Copy::copy($source_path, $tmp_target) or die "Copy failed: $!";

            # Compress in place
            system("gzip -f \"$tmp_target\""); # Standard on AIX/Linux
            rename("$tmp_target.gz", $backup_target);
        }
        print "  ✓ Created: " . basename($backup_target) . "\n";
    };

    if ($@) {
        warn "  [WARN] Could not create backup at '$backup_target': $@";
        return undef;
    }

    # Backup L2 Cache (Standard Copy - these are transient/rebuildable)
    if ($include_l2_cache) {
        foreach my $cache_file ('.nfit.cache.results', '.nfit.cache.manifest') {
            my $source = File::Spec->catfile($system_cache_dir, $cache_file);
            next unless -f $source;

            # We don't compress L2 cache backups to keep this step fast,
            # as these files are smaller than history.
            my $ext = ($cache_file =~ /\.(\w+)$/) ? $1 : '';
            my $cache_backup = _get_next_available_filename($backup_root, "${cache_file}.${timestamp}", $ext);

            require File::Copy;
            File::Copy::copy($source, $cache_backup) or warn "Could not backup $cache_file: $!";
        }
    }

    _prune_old_backups($backup_root, 24);
    return $backup_target;
}

# ==============================================================================
# SUBROUTINE: _get_next_available_filename
# PURPOSE:    Generates a unique filename using rolling numbering (001, 002, etc.)
#             to prevent collisions, similar to CSV output collision prevention.
# ARGUMENTS:
#   1. $dir (string): Directory where file will be created
#   2. $base_name (string): Base filename without extension
#   3. $extension (string): File extension (without leading dot)
# RETURNS:
#   - Full path to available filename
# ==============================================================================
sub _get_next_available_filename {
    my ($dir, $base_name, $extension) = @_;

    my $counter = 1;
    my $candidate;

    do {
        my $suffix = sprintf(".%03d", $counter);
        if ($extension) {
            $candidate = File::Spec->catfile($dir, "${base_name}${suffix}.${extension}");
        } else {
            $candidate = File::Spec->catfile($dir, "${base_name}${suffix}");
        }
        $counter++;
    } while (-e $candidate && $counter < 1000); # Safety limit

    die "Could not generate unique filename after 999 attempts" if $counter >= 1000;

    return $candidate;
}

# ==============================================================================
# SUBROUTINE: _prune_old_backups
# PURPOSE:    Removes old backup files to prevent unbounded disk usage.
#             Keeps the N most recent backup files based on modification time.
# ARGUMENTS:
#   1. $backup_dir (string): Directory containing backups
#   2. $keep_count (integer): Number of recent backups to retain
# RETURNS:
#   - None
# ==============================================================================
sub _prune_old_backups {
    my ($backup_dir, $keep_count) = @_;

    opendir(my $dh, $backup_dir) or return;
    my @backups = grep { /^\.nfit\.history\.\d{8}_\d{6}\.\d{3}\.json$/ }
                  readdir($dh);
    closedir($dh);

    return if @backups <= $keep_count;

    # Sort by modification time (oldest first)
    my @sorted = map { $_->[0] }
                 sort { $a->[1] <=> $b->[1] }
                 map {
                     my $path = File::Spec->catfile($backup_dir, $_);
                     [$_, (stat($path))[9]]
                 } @backups;

    # Remove oldest files beyond keep_count
    my $remove_count = @sorted - $keep_count;
    for (my $i = 0; $i < $remove_count; $i++) {
        my $old_file = File::Spec->catfile($backup_dir, $sorted[$i]);
        unlink $old_file or warn "Could not remove old backup $old_file: $!";
        print "  - Pruned old backup: " . basename($old_file) . "\n";
    }
}

# ==============================================================================
# SUBROUTINE: _validate_pre_update_conditions
# PURPOSE:    Enforces strict pre-conditions before allowing --update-history to
#             proceed. Ensures both decay models have been executed for complete
#             results, and prevents accidental corruption of enriched months.
# ARGUMENTS:
#   1. $system_cache_dir (string): Path to the system's staging cache
#   2. $month_key (string): The month key being updated (e.g., "2025-09")
#   3. $force_flag (boolean): Whether --force flag was specified
# RETURNS:
#   - Dies with error message if validation fails (unless --force specified)
# ==============================================================================
sub _validate_pre_update_conditions {
    my ($system_cache_dir, $month_key, $force_flag) = @_;

    print "\n--- Pre-Update Validation ---\n";

    # Check 1: Verify both decay models have been executed
    my $l2_cache_file = File::Spec->catfile($system_cache_dir, '.nfit.cache.results');

    if (-f $l2_cache_file) {
        my $json = JSON->new->relaxed;
        my $l2_data;

        eval {
            open my $fh, '<:encoding(utf8)', $l2_cache_file
                or die "Could not open L2 cache: $!";
            my $json_text = do { local $/; <$fh> };
            close $fh;
            $l2_data = $json->decode($json_text);
        };

        if ($@) {
            warn "  [WARN] Unable to read results cache (.nfit.cache.results) for validation: $@\n";
        } else {
            my $has_decay_over_states = 0;
            my $has_windowed_decay = 0;

            # Scan L2 cache for analysisType values
            foreach my $l2_key (keys %$l2_data) {
                my $l2_results_aref = $l2_data->{$l2_key};
                next unless (ref($l2_results_aref) eq 'ARRAY' && @$l2_results_aref);

                my $first_result = $l2_results_aref->[0];
                my $analysis_type = $first_result->{analysisType} // '';

                $has_decay_over_states = 1 if $analysis_type eq 'hybrid_decay_aggregated';
                $has_windowed_decay = 1 if $analysis_type eq 'windowed_decay_aggregated';

                # Early exit if both found
                last if ($has_decay_over_states && $has_windowed_decay);
            }

            unless ($has_decay_over_states && $has_windowed_decay) {
                if ($force_flag) {
                    warn "⚠  Results for one or both decay models are absent (maximum enrichment potential will be lost). Proceeding due to --force flag.\n";
                    if (!$has_decay_over_states) {
                        warn "  ↳  Results for the Hybrid State-Time Decay model (--decay-over-states) are absent.\n";
                    }
                    if (!$has_windowed_decay) {
                        warn "  ↳  Results for the Time-Based Windowed Decay model (--enable-windowed-decay) are absent.\n";
                    }
                } else {
                    die "[ERROR] Incomplete tactical analysis detected.\n" .
                        "Both --decay-over-states AND --enable-windowed-decay must be " .
                        "executed before --update-history.\n" .
                        "Current state: decay-over-states=$has_decay_over_states, " .
                        "windowed-decay=$has_windowed_decay\n" .
                        "Use --force to override this check.\n";
                }
            } else {
                print "  ✓ Results for all decay models are present\n";
            }
        }
    } else {
        if ($force_flag) {
            warn "⚠  Results cache not found. Proceeding due to --force flag.\n";
        } else {
            die "[ERROR] Results cache could not found at $l2_cache_file\n" .
                "Tactical analysis must be completed before attempting to execute `--update-history`.\n" .
                "Use `--force` to override this check.\n";
        }
    }

    # Check 2: Prevent overwriting already-enriched months
    my $history_data = read_unified_history($system_cache_dir);

    if (exists $history_data->{$month_key}) {
        if (exists $history_data->{$month_key}{SeasonalEventSnapshots}) {
            # Check if any snapshot has forecast enrichment
            my $snapshots = $history_data->{$month_key}{SeasonalEventSnapshots};
            foreach my $event_name (keys %$snapshots) {
                if (exists $snapshots->{$event_name}{forecast}) {
                    if ($force_flag) {
                        warn "⚠  Month $month_key already enriched with forecasts. " .
                             "This will be overwritten. Proceeding due to --force flag.\n";
                    } else {
                        die "FATAL: Month $month_key already contains forecast enrichment.\n" .
                            "Re-running tactical analysis would corrupt forecast consistency.\n" .
                            "If intentional, use --force to override.\n" .
                            "Consider restoring from ROOT/backups/<serial>/ if this is an error.\n";
                    }
                    last;
                }
            }
        }
    }

    print "  ✓ Pre-update validation passed\n";
}

# ==============================================================================
# SUBROUTINE: _add_lightweight_metadata
# PURPOSE:    Adds lightweight modification tracking metadata to each month entry
#             in the history cache, providing audit trails without the complexity
#             of cryptographic checksums.
# ARGUMENTS:
#   1. $history_data_href (hash ref): The complete history data structure
#   2. $updated_by (string): Tool name that performed the update
# RETURNS:
#   - None (modifies hash reference in place)
# ==============================================================================
sub _add_lightweight_metadata {
    my ($history_data_href, $updated_by) = @_;

    foreach my $month_key (keys %$history_data_href) {
        $history_data_href->{$month_key}{_metadata} = {
            last_updated => scalar(localtime->strftime('%Y-%m-%dT%H:%M:%S%z')),
            updated_by => $updated_by,
            toolkit_version => $VERSION,
            backup_recommendation => "Restore from ROOT/backups/<serial>/ if corruption suspected"
        };
    }
}

# ==============================================================================
# SUBROUTINE: _enrich_history_with_growth_rationale
# PURPOSE:    Performs a final enrichment pass on the unified history data (which is passed as an argument).
#             It robustly scans the nfit L2 Results Cache for growth-enabled
#             model runs, reads the embedded 'profileLabel' from the data, and
#             injects the 'growthRationale' block into the correct location,
#             keyed by the profile name.
# ==============================================================================
sub _enrich_history_with_growth_rationale {
    my ($system_cache_dir, $history_data) = @_;

    print "\n--- Phase 3: Enriching History with Growth Rationale from L2 Cache ---\n";

    my $l2_cache_file = File::Spec->catfile($system_cache_dir, '.nfit.cache.results');

    unless (-f $l2_cache_file) {
        print "  - INFO: L2 results cache not found. Skipping enrichment.\n";
        return 0; # Return false (was not modified)
    }

    # --- Step 1: Load both the history and the L2 cache ---
    my $l2_cache_data = eval {
        local $/;
        open my $fh, '<:encoding(utf8)', $l2_cache_file or die "Could not open L2 cache '$l2_cache_file': $!";
        my $json_text = <$fh>;
        close $fh;
        JSON->new->allow_nonref->decode($json_text);
    };
    if ($@ || !ref($l2_cache_data) eq 'HASH') {
        warn "  [WARN] Unable to decode results cache ($l2_cache_file). Enrichment cannot proceed.\n";
        return 0; # Return false (was not modified)
    }

    # --- Step 2: Scan L2 cache and build a structured hash of all available growth data ---
    # This hash is the single source of truth for all growth rationale.
    # Structure: {vm_name}{profile_name}{model_type} = rationale_hash
    my %growth_data_found;
    foreach my $l2_key (keys %$l2_cache_data) {
        my $l2_results_aref = $l2_cache_data->{$l2_key};
        next unless (ref($l2_results_aref) eq 'ARRAY' && @$l2_results_aref);

        my $first_result = $l2_results_aref->[0];
        my $analysis_type = $first_result->{analysisType} // '';
        my $model_type;

        if ($analysis_type eq 'hybrid_decay_aggregated') {
            $model_type = 'decay_over_states';
        } elsif ($analysis_type eq 'windowed_decay_aggregated') {
            $model_type = 'windowed_decay';
        } else {
            next; # Not a growth-enabled model result, skip this L2 cache entry.
        }

        foreach my $result (@$l2_results_aref) {
            my $vm = $result->{vmName};
            next unless $vm;

            # Instead of a top-level profileLabel, iterate the metrics.physc
            # block to find all profiles and their nested rationales.
            my $physc_metrics = _safe_dig($result, 'metrics', 'physc');
            next unless (ref($physc_metrics) eq 'HASH');

            foreach my $profile_name (keys %$physc_metrics) {
                my $profile_data = $physc_metrics->{$profile_name};

                # Check for the rationale (the trigger)
                if (ref($profile_data) eq 'HASH' && $profile_data->{growthRationale}) {
                    # --- Store the rationale AND its associated tactical values ---
                    $growth_data_found{$vm}{$profile_name}{$model_type} = {
                        rationale  => $profile_data->{growthRationale},
                        BaseValue  => $profile_data->{BaseValue},
                        FinalValue => $profile_data->{FinalValue}
                    };
                }
           }
        }
    }

    # --- Step 3: Iterate through history and inject the found growth data ---
    my $was_modified = 0;
    foreach my $month_key (keys %$history_data) {
        next unless (ref($history_data->{$month_key}) eq 'HASH' && $history_data->{$month_key}{MonthlyWorkloadAnalysis});
        my $workload_analysis = $history_data->{$month_key}{MonthlyWorkloadAnalysis};

        foreach my $vm_name (keys %$workload_analysis) {
            # Skip VMs for which no growth data was ever found.
            next unless exists $growth_data_found{$vm_name};

            my $vm_analysis = $workload_analysis->{$vm_name};

            # Inject all found growth data for this VM.
            foreach my $profile_name (keys %{$growth_data_found{$vm_name}}) {
                foreach my $model_type (keys %{$growth_data_found{$vm_name}{$profile_name}}) {

                    # --- Unpack the new data wrapper ---
                    my $new_data_wrapper = $growth_data_found{$vm_name}{$profile_name}{$model_type}; # This is the hash {rationale, BaseValue, FinalValue}
                    my $new_rationale    = $new_data_wrapper->{rationale};

                    # Ensure the nested structure exists for Rationale.
                    $vm_analysis->{growthRationale} //= {};
                    $vm_analysis->{growthRationale}{$profile_name} //= {};

                    # --- Create new block for Tactical Values ---
                    # This is the new block to store tactical results
                    $vm_analysis->{TacticalValues} //= {};
                    $vm_analysis->{TacticalValues}{$profile_name} //= {};

                    # --- Check if the Rationale data is new or different ---
                    # We use JSON generation to compare structures. This handles:
                    # 1. Deep nesting (recursive)
                    # 2. Key sorting (canonical)
                    # 3. Numeric/String normalization (mostly)

                    my $json_cmp = JSON->new->canonical->allow_nonref;

                    if (!exists $vm_analysis->{growthRationale}{$profile_name}{$model_type}) {
                        $was_modified = 1;
                    } else {
                        # Compare the serialized canonical strings
                        my $existing_json = $json_cmp->encode($vm_analysis->{growthRationale}{$profile_name}{$model_type});
                        my $new_json      = $json_cmp->encode($new_rationale);

                        if ($existing_json ne $new_json) {
                            $was_modified = 1;
                        }
                    }

                    if ($was_modified) {
                         $vm_analysis->{growthRationale}{$profile_name}{$model_type} = $new_rationale;
                    }

                    # --- Harvest BaseValue and FinalValue ---
                    # This archives the tactical model's input (BaseValue)
                    # and output (FinalValue) for long-term auditing.
                    my $base_val = $new_data_wrapper->{BaseValue};
                    my $final_val = $new_data_wrapper->{FinalValue};

                    if (!exists $vm_analysis->{TacticalValues}{$profile_name}{$model_type} ||
                        $vm_analysis->{TacticalValues}{$profile_name}{$model_type}{BaseValue} ne $base_val ||
                        $vm_analysis->{TacticalValues}{$profile_name}{$model_type}{FinalValue} ne $final_val)
                    {
                        # Add the tactical results for this profile AND model
                        $vm_analysis->{TacticalValues}{$profile_name}{$model_type} = {
                            BaseValue  => $base_val,
                            FinalValue => $final_val
                        };
                        $was_modified = 1;
                    }
                }
            }
        }
    }

    # --- Step 4: Write back to the history file ONLY if it was modified ---
    if ($was_modified) {
        print "  - INFO: Found new/updated growth rationale data in L2 cache. Updating history file...\n";
        _write_full_history($system_cache_dir, $history_data);
    } else {
        print "  - INFO: No new growth rationale data found in L2 cache. History is up to date.\n";
    }

    # Return the modification status to the caller
    return $was_modified;
}

# ==============================================================================
# HELPER SUBROUTINE for the enrichment process.
# PURPOSE:    A robust replication of nfit.pl's generate_canonical_key function.
#             It builds the precise key used to store results in the L2 cache for
#             a specific growth analysis run.
# ==============================================================================
sub _generate_l2_cache_key_for_nfit {
    my ($profile_obj, $model_type, $system_cache_dir) = @_;

    my @key_parts;
    my $flags = $profile_obj->{flags};

    # This simulates the arguments nfit.pl would receive from a typical
    # nfit-profile run that enables growth prediction.

    # --- Flags passed from nfit-profile to nfit ---
    push @key_parts, "--nmondir $system_cache_dir";
    push @key_parts, "--smt $default_smt_arg";
    push @key_parts, "--runq-avg-method $nfit_runq_avg_method_str";
    push @key_parts, "--peak"; # nfit-profile always adds this

    # --- Flags from the profile definition ---
    # Parse them from the string to handle them individually
    if ($flags =~ /--percentile\s+([0-9.]+)/ || $flags =~ /-p\s+([0-9.]+)/) { push @key_parts, "--percentile $1"; }
    if ($flags =~ /--window\s+(\d+)/ || $flags =~ /-w\s+(\d+)/) { push @key_parts, "--window $1"; }
    if ($flags =~ /--avg-method\s+(\w+)/) { push @key_parts, "--avg-method $1"; }
    if ($flags =~ /--decay\s+([\w-]+)/) { push @key_parts, "--decay $1"; }
    if ($flags =~ /--runq-decay\s+([\w-]+)/) { push @key_parts, "--runq-decay $1"; }
    if ($flags =~ /--filter-above-perc\s+([0-9.]+)/) { push @key_parts, "--filter-above-perc $1"; }
    if ($flags =~ /--online/) { push @key_parts, "--online"; }
    if ($flags =~ /--batch/) { push @key_parts, "--batch"; }
    if ($flags =~ /--no-weekends/) { push @key_parts, "--no-weekends"; }

    # --- Flags specific to the growth model ---
    if ($model_type eq 'decay_over_states') {
        push @key_parts, "--decay-over-states";
    } elsif ($model_type eq 'windowed_decay') {
        push @key_parts, "--enable-windowed-decay";
        push @key_parts, "--process-window-unit $nfit_window_unit_str";
        push @key_parts, "--process-window-size $nfit_window_size_val";
    }

    # Growth flags are present in the profile, so they are included automatically
    if ($flags =~ /--enable-growth-prediction/) {
        push @key_parts, "--enable-growth-prediction";
        if ($flags =~ /--growth-projection-days\s+(\d+)/) { push @key_parts, "--growth-projection-days $1"; }
        if ($flags =~ /--max-growth-inflation-percent\s+(\d+)/) { push @key_parts, "--max-growth-inflation-percent $1"; }
    }

    # Note: This replication assumes a standard set of flags passed from nfit-profile.
    # It prioritizes the flags that define the analysis type and profile uniqueness.
    # This is far more robust than the previous simple string concatenation.

    return join(" ", sort @key_parts);
}

# ==============================================================================
# SUBROUTINE: _assemble_monthly_analysis
# PURPOSE:    Assembles the MonthlyWorkloadAnalysis data structure from parsed
#             nfit JSON output. This function contains the battle-tested logic
#             for extracting profile values, calculating sizing hints, building
#             RunQ metrics, and assembling ClippingInfo blocks.
#
# ARGUMENTS:
#   1. $parsed_results_href (hash ref): Parsed JSON from nfit (VM -> [states])
#   2. $adaptive_runq_saturation_thresh (float): Adaptive saturation threshold
#   3. Start time of analysis time-range
#   4. End time of analysis time-range
#
# RETURNS:
#   - Hash reference containing the complete MonthlyWorkloadAnalysis structure
#     keyed by VM name.
#
# EXTRACTED FROM:
#   This function was extracted from _generate_monthly_workload_analysis to
#   enable code reuse and preserve all recent bug fixes to ClippingInfo
#   assembly and RunQ metrics extraction.
# ==============================================================================
sub _assemble_monthly_analysis {
    my ($parsed_results_href, $adaptive_runq_saturation_thresh, $states_db, $month_start_str, $month_end_str) = @_;

    # Validate inputs
    unless (ref($parsed_results_href) eq 'HASH') {
        die "FATAL: _assemble_monthly_analysis requires parsed results hash reference";
    }

    # Parse boundaries for monthly configuration state overlap check
    my $month_start_obj = Time::Piece->strptime($month_start_str, "%Y-%m-%d");
    my $month_end_obj   = Time::Piece->strptime($month_end_str, "%Y-%m-%d") + ONE_DAY - 1;

    my %analysis_results;
    my %collated_results_table;  # Temporary table for hint generation

    # ======================================================================
    # STEP 1: Build collated results table for ALL profiles
    # ======================================================================
    # This step harvests all P-metrics for the MonthlyWorkloadAnalysis.ProfileValues
    # block, fixing the historical data regression.
    # It ALSO surgically extracts the max P-99W1 'Peak' metric, which is
    # a critical, separate input for the hint-generation logic in STEP 2.
    # ======================================================================
    foreach my $vm_name (keys %$parsed_results_href) {
        my $states_aref = $parsed_results_href->{$vm_name};
        next unless ref($states_aref) eq 'ARRAY' && @$states_aref;

        my @p99w1_peak_values_for_hinting; # Array to store P-99W1 'Peak' metrics

        # Iterate all configured profiles to get their specific P-metric
        foreach my $profile (@profiles) {
            my $profile_name = $profile->{name};

            # Find the P-metric for this profile (e.g., "P95")
            my ($p_val_num) = $profile->{flags} =~ /(?:-p|--percentile)\s+([0-9.]+)/;
            my $p_metric_key = "P" . clean_perc_label($p_val_num // $DEFAULT_PERCENTILE);

            my $metric_val = undef;

            # Iterate ALL states (newest to oldest)
            for (my $i = $#{$states_aref}; $i >= 0; $i--) {
                my $state = $states_aref->[$i];

                # A. Find the P-metric value (if we haven't already)
                $metric_val //= _safe_dig($state, 'metrics', 'physc', $profile_name, $p_metric_key);

                # B. If this is the P-99W1 profile, find its 'Peak' metric for hinting
                if ($profile_name eq $MANDATORY_PEAK_PROFILE_FOR_HINT) {
                    my $profile_peak_val = _safe_dig($state, 'metrics', 'physc', $profile_name, 'Peak');
                    push @p99w1_peak_values_for_hinting, $profile_peak_val if (defined $profile_peak_val && looks_like_number($profile_peak_val));
                }
            } # end state loop

            # Store the final P-metric value for the ProfileValues block
            if (defined $metric_val && looks_like_number($metric_val)) {
                $collated_results_table{$vm_name}{$profile_name} = $metric_val + 0;
            }
        } # end profile loop

        # Store the max P-99W1 Profile Peak for hint logic, using the original key
        if (@p99w1_peak_values_for_hinting) {
            $collated_results_table{$vm_name}{$PEAK_PROFILE_NAME} = max(@p99w1_peak_values_for_hinting);
        }
    }

    # ======================================================================
    # STEP 2: Generate hints and extract metrics for each VM
    # ======================================================================
    foreach my $vm_name (keys %collated_results_table) {
        my $vm_config_ref = $vm_config_data{$vm_name};
        my $states_aref = $parsed_results_href->{$vm_name};
        my $last_state_data = $states_aref->[-1] // {};

        # ------------------------------------------------------------------
        # Extract configuration metadata
        # ------------------------------------------------------------------
        my @config_timeline;
        my $last_state_generic = {};
        my $last_state_platform = {};

        # Retrieve authoritative states from cache
        my $auth_states = $states_db->{$vm_name};

        if (defined $auth_states && ref($auth_states) eq 'ARRAY') {
            foreach my $state (@$auth_states) {
                # 1. Check Date Overlap
                my $s_start = $state->{start_epoch};
                my $s_end   = $state->{end_epoch};

                # Logic: State overlaps Month if (StateEnd >= MonthStart) AND (StateStart <= MonthEnd)
                next unless ($s_end >= $month_start_obj->epoch && $s_start <= $month_end_obj->epoch);

                my $md = $state->{metadata} || {};

                # 2. Extract Raw Attributes (Snake_Case from .states file)
                my $ent       = $md->{entitlement} // 0;
                my $vcpu      = $md->{virtual_cpus} // 0;
                my $pool_cpu  = $md->{pool_cpu} // 0;
                my $is_capped = $md->{capped} // 0;
                my $smt       = $md->{smt} // $default_smt_arg;
                my $proc_type = $md->{proc_type} // "Unknown";

                # 3. Calculate MaxUsableCPU (Replicating nfit logic)
                # If capped: Entitlement
                # If uncapped: min(vCPU, Pool)
                my $calc_max;
                if ($is_capped) {
                    $calc_max = $ent;
                } else {
                    # Uncapped logic
                    if ($pool_cpu > 0 && $vcpu > 0) {
                        $calc_max = ($vcpu < $pool_cpu) ? $vcpu : $pool_cpu;
                    } else {
                        $calc_max = ($vcpu > 0) ? $vcpu : $ent; # Fallback to Ent if vCPU missing
                    }
                }
                # Safety clamps
                $calc_max = 0 if $calc_max < 0;

                # 4. Build Generic Shape
                my $generic = {
                    Architecture => 'ppc64',
                    OS_Type      => $md->{os_type} // 'AIX', # Default = AIX
                    MaxUsableCPU => 0 + sprintf("%.2f", $calc_max),
                    RAM_GB       => undef
                };

                # 5. Build Platform Shape
                my $platform = {
                    AIX => {
                        Entitlement   => 0 + $ent,
                        VirtualCPUs   => 0 + $vcpu,
                        Capped        => $is_capped ? JSON::true : JSON::false,
                        Pool_CPU      => $md->{pool_cpu},
                        Pool_ID       => $md->{pool_id},
                        Shared_Weight => $md->{shared_weight} // 128,
                        Proc_Type     => $proc_type,
                        SMT_Level     => 0 + $smt,
                    }
                };

                $last_state_generic = $generic;
                $last_state_platform = $platform;

                push @config_timeline, {
                    Start        => gmtime($s_start)->datetime . "Z",
                    End          => gmtime($s_end)->datetime . "Z",
                    DurationDays => sprintf("%.2f", ($s_end - $s_start) / 86400) + 0,
                    Generic      => $generic,
                    Platform     => $platform
                };
            }
        }

        # Assemble the Configuration Block
        my $configuration_block = {
            Generic => $last_state_generic,
            Configuration_States => \@config_timeline
        };

        # --- Legacy Support for Hint Generation ---
        # We still use the MaxUsableCPU we just calculated for the hint logic
        my $max_cpu_from_state = $last_state_generic->{MaxUsableCPU} // 0;
        my $smt_from_state     = $last_state_platform->{SMT_Level} // $default_smt_arg;

        # ------------------------------------------------------------------
        # Build the VM map structure for sizing hint generation
        # ------------------------------------------------------------------
        my %vm_map = (
            Configuration => {
                vm_name     => $vm_name,
                max_cpu     => $max_cpu_from_state,
                smt         => $smt_from_state,
                entitlement => $vm_config_ref->{entitlement} // 0,
                %{$vm_config_ref || {}}
            },
            CoreResults => {
                ProfileValues => $collated_results_table{$vm_name}
            },
            RunQMetrics => _safe_dig($last_state_data, 'metrics', 'runq') || {}
        );

        # ------------------------------------------------------------------
        # Generate sizing hint using existing, proven logic
        # ------------------------------------------------------------------
        my ($hint, $pattern, $pressure, $pressure_detail, $rationale_text, $has_abs_pressure, $has_norm_pressure) =
            generate_sizing_hint(\%vm_map, undef, $adaptive_runq_saturation_thresh);

        # ------------------------------------------------------------------
        # Extract RunQ metrics from the last state
        # ------------------------------------------------------------------
        my $runq_metrics_block = $last_state_data->{metrics}{runq} || {};

        # ------------------------------------------------------------------
        # Build ClippingInfo for all profiles
        # ------------------------------------------------------------------
        # This logic iterates through all profiles and extracts clipping
        # detection results. The structure here has been carefully debugged
        # and must be preserved exactly.
        # ------------------------------------------------------------------
        my $clipping_info_for_vm = {};

        foreach my $profile_name (keys %{$last_state_data->{metrics}{physc} || {}}) {
            # Skip if this isn't a real profile (e.g., metadata keys)
            next unless ref($last_state_data->{metrics}{physc}{$profile_name}) eq 'HASH';

            # Extract clipping data from this profile's output
            my $clipping_metrics = _safe_dig($last_state_data, 'metrics', 'physc', $profile_name, 'ClippingInfo');

            # Try alternate path if first doesn't work (backwards compatibility)
            $clipping_metrics //= _safe_dig($last_state_data, 'metrics', 'physc', $profile_name, 'Clipping');

            # Only process if we found valid clipping data
            if ($clipping_metrics && ref($clipping_metrics) eq 'HASH' && exists $clipping_metrics->{isClipped}) {
                my $unmet_demand_est = $clipping_metrics->{unmetDemandEstimate} // 0;

                # Get the base value for this profile to calculate unclipped peak estimate
                my $profile_obj = (grep { $_->{name} eq $profile_name } @profiles)[0];
                next unless $profile_obj;  # Safety check

                my $p_key = "P" . clean_perc_label(($profile_obj->{flags} =~ /-p\s+([0-9.]+)/) ? $1 : $DEFAULT_PERCENTILE);
                my $base_peak_for_unclip = _safe_dig($last_state_data, 'metrics', 'physc', $profile_name, $p_key);

                # Get MaxCPU from VM config
                # IMPORTANT: ClippingInfo uses 'maxcpu' (lowercase), not 'max_cpu'
                my $maxcpu = $vm_config_ref->{maxcpu} // 0;

                # Assemble the ClippingInfo structure with exact key names
                # that downstream consumers (nfit-forecast, GUI) expect
                $clipping_info_for_vm->{$profile_name} = {
                    isClipped               => $clipping_metrics->{isClipped},
                    clippingConfidence      => $clipping_metrics->{clippingConfidence} // 'unknown',
                    capacityLimit           => $maxcpu,
                    unmetDemandEstimate     => $unmet_demand_est,
                    unclippedPeakEstimate   => ((defined $base_peak_for_unclip && looks_like_number($base_peak_for_unclip)) ? $base_peak_for_unclip : 0) + $unmet_demand_est,
                    platformSpecificMarkers => {
                        aix_runq_saturation => _safe_dig($clipping_metrics, 'platformMarkers', 'aix_runq_saturation') // 0
                    }
                };
            }

            # --- Harvest DailyProfileSeries Data ---
            # Extract the daily time-series map
            my $daily_series = _safe_dig($last_state_data, 'metrics', 'physc', $profile_name, 'DailySeries');
            if (ref($daily_series) eq 'HASH' && scalar(keys %$daily_series) > 0) {
                # Temporarily store in collated table for final assembly
                $collated_results_table{$vm_name}{_DailySeries}{$profile_name} = $daily_series;
            }
        }

        # ------------------------------------------------------------------
        # Assemble the final data structure for this VM
        # ------------------------------------------------------------------
        # CRITICAL: This structure must NOT include growthRationale or any
        # other predictive metadata. MonthlyWorkloadAnalysis is pure historical.
        # The growthRationale is harvested separately by the enrichment phase.
        # ------------------------------------------------------------------
        $analysis_results{$vm_name} = {
            Hint           => $hint,
            Pattern        => $pattern,
            Pressure       => $pressure,
            PressureDetail => $pressure_detail,
            Configuration  => $configuration_block,
            RunQMetrics    => {
                # Extract the specific RunQ metrics needed by downstream logic
                AbsRunQ_P90  => _safe_dig($runq_metrics_block, 'absolute', $MANDATORY_PEAK_PROFILE_FOR_HINT, 'P90'),
                NormRunQ_P50 => _safe_dig($runq_metrics_block, 'normalized', $MANDATORY_PEAK_PROFILE_FOR_HINT, 'P50'),
                NormRunQ_P90 => _safe_dig($runq_metrics_block, 'normalized', $MANDATORY_PEAK_PROFILE_FOR_HINT, 'P90')
            },
            ProfileValues  => $collated_results_table{$vm_name},
            DailyProfileSeries => $collated_results_table{$vm_name}{_DailySeries} || {},
            ClippingInfo   => $clipping_info_for_vm
        };

        # Cleanup: Remove the temporary _DailySeries key from ProfileValues to keep the schema clean
        delete $analysis_results{$vm_name}{ProfileValues}{_DailySeries};
    }

    return \%analysis_results;
}

# ==============================================================================
# SUBROUTINE: _generate_monthly_workload_analysis
# PURPOSE:    Performs a full single-pass analysis for a given time period
#             (typically one month) using the manifest-driven Single Pass Engine
#             architecture, then assembles the comprehensive MonthlyWorkloadAnalysis
#             data structure.
#
#             This function has been refactored to use the SPE to achieve dramatic
#             I/O reduction (16x for a typical 16-profile configuration).
# ==============================================================================
sub _generate_monthly_workload_analysis {
    my ($system_cache_dir, $start_date_str, $end_date_str, $adaptive_runq_saturation_thresh) = @_;

    print STDERR "\n  [+] Single Pass Engine (monthly analysis)\n";

    # --- Load Configuration States Cache (for this analysis cycle) ---
    my $states_cache_file = File::Spec->catfile($system_cache_dir, '.nfit.cache.states');
    my $states_db = {};
    if (-f $states_cache_file) {
        my $json_text = do {
            open my $fh, '<:encoding(utf8)', $states_cache_file or die "Cannot open states cache: $!";
            local $/; <$fh>;
        };
        $states_db = decode_json($json_text);
    }

    # ======================================================================
    # STEP 1: Build the tactical manifest
    # ======================================================================
    # Use the existing, battle-tested build_transform_manifest function.
    # This function handles all the complexity of profile parameter extraction,
    # transform key generation, and percentile de-duplication.
    # ======================================================================
    my $runq_behavior = $runq_perc_behavior_mode // 'none';
    my $tactical_manifest = build_transform_manifest(\@profiles, $nfit_runq_avg_method_str, $runq_behavior);

    # ======================================================================
    # STEP 2: Create the PURE HISTORICAL manifest
    # ======================================================================
    # The sanitisation function is the primary data purity guardrail.
    # It strips all predictive elements (growth, decay) whilst preserving
    # the profile's fundamental measurement definition.
    # ======================================================================
    my $historical_manifest = _sanitise_manifest_for_history($tactical_manifest);

    # ======================================================================
    # STEP 3: Write manifest to temporary file
    # ======================================================================
    use File::Temp qw(tempfile);
    my ($fh_manifest, $manifest_filename) = tempfile(
        'nfit_history_manifest_XXXXXX',
        SUFFIX => '.json',
        UNLINK => 1,
        TMPDIR => 1
    );

    # Write the manifest using canonical JSON encoding for consistency
    print $fh_manifest JSON->new->pretty->canonical->encode($historical_manifest);
    close $fh_manifest;

    # ======================================================================
    # STEP 4: Build and execute the single nfit command
    # ======================================================================
    my $nfit_cmd = "$nfit_script_path --manifest $manifest_filename "
                 . "--nmondir \"$system_cache_dir\" "
                 . "--startdate $start_date_str "
                 . "--enddate $end_date_str "
                 . "--smt $default_smt_arg "
                 . "--runq-avg-method $nfit_runq_avg_method_str "
                 . "--enable-clipping-detection "  # Historical capacity measurement
                 . "--show-progress";

    # Execute nfit once for all profiles
    print STDERR "  • Invoking nFit engine for date range: $start_date_str to $end_date_str\n";

    my $nfit_output = '';
    my $nfit_error  = '';

    # Capture both STDOUT (JSON) and STDERR (progress/errors)
    my $stderr_arg = ">&=" . fileno(STDERR);
    my $pid = open3(undef, my $stdout_fh, $stderr_arg, $nfit_cmd);

    while (my $line = <$stdout_fh>) {
        $nfit_output .= $line;
    }

    waitpid($pid, 0);
    my $exit_code = $? >> 8;

    # Check for execution failure
    if ($exit_code != 0) {
        die "FATAL: nfit engine failed during monthly analysis.\n"
            . "Exit code: $exit_code\n"
            . "Command: $nfit_cmd\n"
            . "Check STDERR output above for details.\n";
    }

    # ======================================================================
    # STEP 5: Parse the JSON output
    # ======================================================================
    # Use the existing, proven parse_nfit_json_output function.
    # It correctly handles multi-line JSON output where each line is a
    # separate VM result object.
    # ======================================================================
    my $parsed_results = parse_nfit_json_output($nfit_output);

    # Sanity check: ensure we got results
    unless (ref($parsed_results) eq 'HASH' && scalar keys %$parsed_results) {
        warn "WARNING: nfit returned no results for monthly analysis period $start_date_str to $end_date_str\n";
        return {};
    }

    print STDERR "  ✓ Successfully processed " . scalar(keys %$parsed_results) . " VM(s)\n";

    # ======================================================================
    # STEP 6: Assemble the final MonthlyWorkloadAnalysis structure
    # ======================================================================
    # Call the extracted assembly logic which contains all the recent
    # bug fixes for ClippingInfo, RunQ metrics, and profile value extraction.
    # ======================================================================
    my $analysis_results = _assemble_monthly_analysis(
        $parsed_results,
        $adaptive_runq_saturation_thresh,
        $states_db,       # Configuration states
        $start_date_str,  # Boundary for filtering
        $end_date_str     # Boundary for filtering
    );

    return $analysis_results;
}

# ==============================================================================
# SUBROUTINE: detect_sampling_interval
# Robustly detects the NMON sampling interval from the .nfit.cache.data file.
# It reads the start of the cache, isolates timestamps for the first VM found,
# calculates the time difference between consecutive samples, and finds the mode
# of these deltas to determine the most likely interval.
#
# Note: This version introduces jitter bucketing: Before counting the frequency,
#  it checks if a delta is close to a standard interval (60, 300, 600, 900 seconds)
#  within a tolerance ($EPS = 3). If it is, it snaps the value to that standard
#  interval before incrementing the count.
# Returns:
#   - The detected interval in seconds (e.g., 60, 300), or undef on failure.
sub detect_sampling_interval {
    my ($data_cache_file) = @_;

    my $SAMPLES_TO_GATHER       = 50;
    my $MAX_LINES_TO_SCAN       = 5000;
    my $MIN_SAMPLES_FOR_DETECTION = 5;   # required count in a single interval bucket
    my @STD = (60, 300, 600, 900);
    my $EPS = 3;                          # jitter tolerance (seconds)

    return undef unless (-f $data_cache_file && -s $data_cache_file);

    open my $fh, '<:encoding(utf8)', $data_cache_file or do {
        warn " [WARN] Could not open data cache '$data_cache_file' for interval detection: $!";
        return undef;
    };

    <$fh>; # Skip header

    my $target_vm_for_detection;
    my @timestamps;
    my $lines_scanned = 0;

    while (my $line = <$fh>) {
        $lines_scanned++;
        last if $lines_scanned > $MAX_LINES_TO_SCAN;

        my ($ts_str, $vm_name) = ($line =~ /^(\d{4}-\d{2}-\d{2}\s\d{2}:\d{2}:\d{2}),([^,]+)/);
        next unless ($ts_str && $vm_name);

        $target_vm_for_detection //= $vm_name;
        next unless $vm_name eq $target_vm_for_detection;

        eval {
            push @timestamps, Time::Piece->strptime($ts_str, "%Y-%m-%d %H:%M:%S")->epoch;
        };
        last if @timestamps >= $SAMPLES_TO_GATHER;
    }
    close $fh;

    # Need enough timestamps to produce at least MIN_SAMPLES_FOR_DETECTION deltas
    return undef if @timestamps < ($MIN_SAMPLES_FOR_DETECTION + 1);

    @timestamps = sort { $a <=> $b } @timestamps;

    my %delta_counts;
    for (my $i = 1; $i < @timestamps; $i++) {
        my $delta = $timestamps[$i] - $timestamps[$i-1];
        next unless ($delta > 10 && $delta < 1810);

        # Jitter bucketing: snap near-standards to the standard value
        my $bucket = $delta;
        my $best_d = $EPS + 1;
        for my $s (@STD) {
            my $d = abs($delta - $s);
            if ($d < $best_d) { $best_d = $d; $bucket = ($d <= $EPS) ? $s : $bucket; }
        }
        $delta_counts{$bucket}++;
    }

    return undef unless %delta_counts;

    # Find max frequency
    my $max_count = 0;
    foreach my $delta (keys %delta_counts) {
        $max_count = $delta_counts{$delta} if $delta_counts{$delta} > $max_count;
    }

    # Require minimum evidence in the winning bucket
    return undef if $max_count < $MIN_SAMPLES_FOR_DETECTION;

    # Collect all tied winners
    my @tied_deltas = grep { $delta_counts{$_} == $max_count } keys %delta_counts;

    # Deterministic tie-break: closest to a standard, then smallest delta
    if (@tied_deltas > 1) {
        my %distances;
        for my $delta (@tied_deltas) {
            my $min_dist = 1e9;
            foreach my $std (@STD) {
                my $dist = abs($delta - $std);
                $min_dist = $dist if $dist < $min_dist;
            }
            $distances{$delta} = $min_dist;
        }
        @tied_deltas = sort { $distances{$a} <=> $distances{$b} || $a <=> $b } @tied_deltas;
    }

    return $tied_deltas[0];
}

# ==============================================================================
# --- set_adaptive_thresholds ---
# Takes a detected sampling interval, snaps it to a standard value (e.g., 1, 5, 10 min),
# calculates the adaptive thresholds for saturation and efficiency, and logs the rationale.
# This function does NOT modify global state; it returns the calculated values.
#
# Returns:
#   - A list of three values:
#     1. The new saturation threshold (float).
#     2. The new target for efficiency calculation (float).
#     3. The new max downsizing percentage (as a factor, e.g., 0.05 for 5%).
sub set_adaptive_thresholds {
    my ($raw_interval_seconds, $log_fh) = @_;

    # --- Baseline (1-min) thresholds ---
    my $sat_thresh = $RUNQ_PRESSURE_P90_SATURATION_THRESHOLD;
    my $target_norm_runq = $DEFAULT_TARGET_NORM_RUNQ_FOR_EFFICIENCY_CALC;
    my $max_downsize_perc_factor = $MAX_EFFICIENCY_REDUCTION_PERCENTAGE;

    my $snapped_minutes = 1.0;
    my $detection_method_log = "cache-based detection using mode of deltas with epsilon bucketing";
    my $log_message;

    if (!defined $raw_interval_seconds) {
        $log_message = "No reliable interval found; defaulting to 1.0 min; thresholds at 1-min baseline.";
    } else {
        # Snap the detected raw seconds to the nearest standard interval with a tolerance
        my $epsilon = 5; # Allow +/- 5 seconds
        if    (abs($raw_interval_seconds - 60) <= $epsilon)  { $snapped_minutes = 1; }
        elsif (abs($raw_interval_seconds - 300) <= $epsilon) { $snapped_minutes = 5; }
        elsif (abs($raw_interval_seconds - 600) <= $epsilon) { $snapped_minutes = 10; }
        elsif (abs($raw_interval_seconds - 900) <= $epsilon) { $snapped_minutes = 15; }
        else {
             # If it doesn't snap cleanly, default to 1 min but log the anomaly.
            $log_message = sprintf("Unusual interval %ds detected; did not snap to a standard value. Defaulting to 1.0 min.", $raw_interval_seconds);
            $snapped_minutes = 1.0;
        }
    }

    if ($snapped_minutes > 1) {
        # It's a non-default interval, so calculate the adaptive thresholds.
        my $k    = $snapped_minutes;
        my $base = $RUNQ_PRESSURE_P90_SATURATION_THRESHOLD; # Follow the global 1-min baseline
        my $beta = 0.4; # Decay exponent, could be made configurable in the future
        # General formula: 1 + (base-1) * k^(-beta)
        # This removes the hard-coded '0.8' and derives it from the baseline (1.8 - 1.0)
        $sat_thresh = 1 + (($base - 1.0) * ($k**-$beta));
        # Downsizing thresholds: Use the lookup table for conservatism
        my %downsize_targets = ( 5 => 0.75, 10 => 0.72, 15 => 0.70 );
        my %downsize_caps    = ( 5 => 5,    10 => 5,    15 => 5 ); # Cap is 5% for all coarser intervals

        $target_norm_runq = $downsize_targets{$k} // $target_norm_runq;
        # NOTE: The cap is stored as a percentage and converted to a factor here.
        my $cap_perc = $downsize_caps{$k};
        if (defined $cap_perc) {
            $max_downsize_perc_factor = $cap_perc / 100.0;
        }

        $log_message = sprintf("Sampling interval detected %.1fs -> snapped to %.1f min.", $raw_interval_seconds, $snapped_minutes) if defined $raw_interval_seconds;
    } elsif (!defined $log_message) {
         $log_message = sprintf("Sampling interval detected %.1fs -> snapped to 1.0 min. Using baseline thresholds.", $raw_interval_seconds) if defined $raw_interval_seconds;
    }


    # --- Rationale Logging ---
    if ($log_fh) {
        print {$log_fh} "----------------------------------------------------------------------\n";
        print {$log_fh} "Adaptive Threshold Calculation\n";
        print {$log_fh} "  - Method                   : $detection_method_log\n";
        print {$log_fh} "  - Status                   : $log_message\n";
        print {$log_fh} "  - Saturation Threshold     : " . sprintf("%.2f", $sat_thresh) . " x LCPU\n";
        print {$log_fh} "  - Downsizing Target        : " . sprintf("%.2f", $target_norm_runq) . " / LCPU\n";
        print {$log_fh} "  - Downsizing Cap           : " . sprintf("%.1f%%", $max_downsize_perc_factor * 100) . "\n";
        print {$log_fh} "----------------------------------------------------------------------\n";
    }

    return ($sat_thresh, $target_norm_runq, $max_downsize_perc_factor);
}

# --- parse_vm_tier_overrides ---
# Parses a simple INI-style file to allow users to override the auto-detected
# tier for specific VMs, giving them fine-grained control.
sub parse_vm_tier_overrides {
    my ($filepath) = @_;
    my %overrides;
    return \%overrides unless (-f $filepath);

    print STDERR "  ↳  VM-specific tier override configuration file: $filepath\n";
    open my $fh, '<:encoding(utf8)', $filepath or do {
        warn " [WARN] Could not open VM tier override file '$filepath': $!. Skipping overrides.";
        return \%overrides;
    };

    my $current_vm = '';
    while (my $line = <$fh>) {
        chomp $line;
        $line =~ s/\s*[#;].*//;
        $line =~ s/^\s+|\s+$//g;
        next if $line eq '';

        if ($line =~ /^\s*\[\s*([^\]]+?)\s*\]\s*$/) {
            $current_vm = $1;
        } elsif ($current_vm ne '' && $line =~ /^\s*tier\s*=\s*(.+)$/i) {
            my $tier = uc($1);
            $tier =~ s/\s+//g;
            $overrides{$current_vm} = $tier;
            $current_vm = ''; # Reset after reading the tier
        }
    }
    close $fh;
    print STDERR "    ✓ Loaded " . scalar(keys %overrides) . " VM-specific tier overrides\n";
    return \%overrides;
}

# ==============================================================================
# SUBROUTINE: build_transform_manifest
# PURPOSE:    Parses all loaded profiles and aggregates their computational
#             requirements into a single "transform manifest". This manifest
#             identifies each unique data transformation needed for PhysC,
#             NormRunQ, and AbsRunQ, ensuring calculation parameters are
#             correctly isolated.
# ==============================================================================

sub build_transform_manifest {
    my ($profiles_ref, $global_runq_avg_method, $runq_behavior) = @_;
    my %manifest;
    $runq_behavior //= 'none'; # Default safety

    # Helper to create a consistent transform key.
    my $_get_transform_key = sub {
        my ($metric, $method, $window, $decay, $filter, $time, $weekends) = @_;
        return join(":", $metric, $method, $window, $decay // '', $filter, $time, $weekends);
    };

    foreach my $profile (@$profiles_ref) {
        my $flags = $profile->{flags};
        my $p_name = $profile->{name};

        # --- Extract all relevant parameters from the profile's flags ---
        # Note: We re-parse these to ensure we respect the exact definition of each profile.
        my ($p_perc)      = $flags =~ /-p\s+([0-9.]+)/;
        my ($w_min)       = $flags =~ /-w\s+(\d+)/;
        my ($avg_method)  = $flags =~ /--avg-method\s+(\w+)/;
        my ($decay)       = $flags =~ /--decay\s+([\w-]+)/;
        my ($runq_decay)  = $flags =~ /--runq-decay\s+([\w-]+)/;
        my ($filter_perc) = $flags =~ /--filter-above-perc\s+([0-9.]+)/;
        my ($rq_norm_str) = $flags =~ /--runq-norm-perc\s+"?([0-9.,\s]+)"?/;
        my ($rq_abs_str)  = $flags =~ /--runq-abs-perc\s+"?([0-9.,\s]+)"?/;
        my $time_filter = 'none';
        if ($flags =~ / -online\b/)     { $time_filter = 'online'; }
        elsif ($flags =~ / -batch\b/)   { $time_filter = 'batch'; }
        my $no_weekends = ($flags =~ / -no-weekends\b/) ? 1 : 0;

        # --- 1. PhysC Transform ---
        my $physc_method = $avg_method // $DEFAULT_AVG_METHOD;
        my $physc_decay = $decay // $DEFAULT_DECAY_LEVEL;
        my $physc_window = $w_min // 15;
        my $physc_filter = $filter_perc // '0';

        my $physc_key = $_get_transform_key->('PhysC', $physc_method, $physc_window, $physc_decay, $physc_filter, $time_filter, $no_weekends);

        # Find or create the transform. CRITICAL: Initialize 'profiles' with a NEW hash ref '{}'.
        $manifest{$physc_key} //= {
            metric => 'PhysC', method => $physc_method, window => $physc_window, decay => $physc_decay,
            filter_perc => $physc_filter, time_filter => $time_filter, no_weekends => $no_weekends,
            profiles => {}
        };

        # Add this profile's directives under the correct transform.
        if (defined $p_perc) {
            my $p_directives = $manifest{$physc_key}{profiles}{$p_name} //= {};
            push @{ $p_directives->{percentiles} }, $p_perc;

            $p_directives->{enable_growth} = 1 if ($flags =~ /--enable-growth-prediction\b/);
            $p_directives->{enable_clipping} = 1 if ($flags =~ /--enable-clipping-detection\b/);
            $p_directives->{calculate_peak} = 1 if ($p_name eq $MANDATORY_PEAK_PROFILE_FOR_HINT || $flags =~ /-k\b|--peak\b/);
        }

        # Auto-Match RunQ Logic (--runq-perc-behavior match)
        # If behaviour is 'match', create a NormRunQ entry mirroring the PhysC profile
        if ($runq_behavior eq 'match' && defined $p_perc) {
            # NormRunQ key: Metric=NormRunQ, Method=Global, Filter=0, Time=Same as PhysC
            # Note: We use '0' for filter_perc as RunQ is rarely filtered by value like PhysC
            my $norm_runq_key = $_get_transform_key->(
                'NormRunQ', $global_runq_avg_method, $w_min, $decay, '0', $time_filter, $no_weekends
            );

            $manifest{$norm_runq_key} //= {
                metric => 'NormRunQ', method => $global_runq_avg_method, window => $w_min, decay => $decay,
                filter_perc => '0', time_filter => $time_filter, no_weekends => $no_weekends,
                profiles => {}
            };

            # Add the same percentile
            my $nr_directives = $manifest{$norm_runq_key}{profiles}{$p_name} //= {};
            push @{ $nr_directives->{percentiles} }, $p_perc;
        }

        # --- 2. RunQ Transforms ---
        my $runq_eff_decay = $runq_decay // $decay // 'medium';
        my $runq_method = ($global_runq_avg_method eq 'none') ? 'none' : ($global_runq_avg_method // $DEFAULT_NFIT_RUNQ_AVG_METHOD);
        my $runq_window = $w_min // 15;

        if (defined $rq_norm_str) {
            my @norm_percs = grep { looks_like_number($_) } split /[,\s]+/, $rq_norm_str;
            push @norm_percs, (25, 50, 75, 90);
            my %seen; @norm_percs = grep { !$seen{$_}++ } @norm_percs;

            if (@norm_percs) {
                my $norm_key = $_get_transform_key->('NormRunQ', $runq_method, $runq_window, $runq_eff_decay, '0', $time_filter, $no_weekends);
                $manifest{$norm_key} //= {
                    metric => 'NormRunQ', method => $runq_method, window => $runq_window, decay => $runq_eff_decay,
                    filter_perc => '0', time_filter => $time_filter, no_weekends => $no_weekends,
                    profiles => {}
                };
                if (@norm_percs) {
                    my $n_directives = $manifest{$norm_key}{profiles}{$p_name} //= {};
                    push @{ $n_directives->{percentiles} }, @norm_percs;
                }
            }
        }

        if (defined $rq_abs_str) {
            my @abs_percs = grep { looks_like_number($_) } split /[,\s]+/, $rq_abs_str;
            push @abs_percs, $p_perc if ($runq_perc_behavior_mode eq 'match' && defined $p_perc);
            my %seen; @abs_percs = grep { !$seen{$_}++ } @abs_percs;

            if (@abs_percs) {
                my $abs_key = $_get_transform_key->('AbsRunQ', $runq_method, $runq_window, $runq_eff_decay, '0', $time_filter, $no_weekends);
                $manifest{$abs_key} //= {
                    metric => 'AbsRunQ', method => $runq_method, window => $runq_window, decay => $runq_eff_decay,
                    filter_perc => '0', time_filter => $time_filter, no_weekends => $no_weekends,
                    profiles => {}
                };
                if (@abs_percs) {
                    my $a_directives = $manifest{$abs_key}{profiles}{$p_name} //= {};
                    push @{ $a_directives->{percentiles} }, @abs_percs;
                }
            }
        }
    }

    # Final pass to de-duplicate percentiles
    foreach my $key (keys %manifest) {
        foreach my $p_name (keys %{ $manifest{$key}{profiles} }) {
            if (exists $manifest{$key}{profiles}{$p_name}{percentiles}) {
                my $percs_ref = \@{ $manifest{$key}{profiles}{$p_name}{percentiles} };
                my %seen; @$percs_ref = grep { !$seen{$_}++ } @$percs_ref;
            }
        }
    }

    # remove profiles that never got valid percentiles:
    foreach my $key (keys %manifest) {
        foreach my $pname (keys %{ $manifest{$key}{profiles} }) {
            my $pd = $manifest{$key}{profiles}{$pname};
            delete $manifest{$key}{profiles}{$pname}
                unless (ref($pd) eq 'HASH' && exists $pd->{percentiles} && @{ $pd->{percentiles} });
        }
    }

    return \%manifest;
}

# ==============================================================================
# SUBROUTINE: _sanitise_manifest_for_history
# PURPOSE:    Transforms a "tactical" manifest (from build_transform_manifest)
#             into a "historical" manifest by surgically removing all predictive
#             and time-weighting directives whilst preserving the profile's
#             fundamental historical definition.
#
# ARGUMENTS:
#   1. $manifest_href (hash ref): The tactical manifest to sanitise.
#
# RETURNS:
#   - A hash reference to the sanitised manifest.
#
# CRITICAL DESIGN PRINCIPLES:
#   This function is the primary data purity guardrail for the history priming
#   process. It implements the rules defined in:
#   "nFit - Maintaining Cache Structure Purity.md"
#
#   What is REMOVED (predictive/time-weighting):
#     - decay keys (inter-period time-weighting)
#     - runq_decay keys (RunQ time-weighting)
#     - enable_growth directives (forward-looking predictions)
#
#   What is PRESERVED (profile definition/historical measurement):
#     - method (sma/ema/wma) - defines how noise is smoothed
#     - window - defines the smoothing window size
#     - time_filter (online/batch) - defines WHAT data to measure
#     - no_weekends - defines operational time boundaries
#     - filter_perc - defines data quality thresholds
#     - calculate_peak - historical maximum measurement
#     - enable_clipping - historical capacity limit detection
#
# RATIONALE:
#   A workload profile like "G3-95W15" is not just "P95". It is explicitly
#   defined as "the P95 of a 15-minute simple moving average, excluding
#   weekends, during online hours". This complete definition must be preserved
#   in the historical record, as it defines the measurement methodology.
#
#   Time-context filters (-online, -no-weekends) are NOT predictive. They
#   define WHAT is being measured (operational hours vs. all hours). Removing
#   them would corrupt the historical record by blending non-operational data
#   into profiles explicitly designed to exclude it.
# ==============================================================================
sub _sanitise_manifest_for_history {
    my ($manifest_href) = @_;

    # Validate input to catch programming errors early
    unless (ref($manifest_href) eq 'HASH') {
        die "FATAL: _sanitise_manifest_for_history requires a hash reference. Received: "
            . (defined $manifest_href ? ref($manifest_href) || 'scalar' : 'undef');
    }

    # Perform a true deep copy to avoid any possibility of modifying the
    # original tactical manifest. This is critical for maintainability.
    use Storable qw(dclone);
    my $sanitised_href = dclone($manifest_href);

    # Iterate through each transform in the manifest
    foreach my $transform_key (keys %$sanitised_href) {
        my $transform = $sanitised_href->{$transform_key};

        # Sanity check: ensure we're working with a proper transform structure
        next unless ref($transform) eq 'HASH';

        # ======================================================================
        # STEP 1: Remove predictive time-weighting (decay)
        # ======================================================================
        # Decay applies recency weighting across time periods, making older
        # data contribute less to the final metric. This is predictive logic
        # (assumes recent behaviour is more relevant for forecasting).
        #
        # For historical records, we want pure statistical aggregation where
        # all time periods in the analysis window are weighted equally.
        #
        # We DELETE the keys entirely rather than setting them to 'none' or 'low'
        # because:
        #   a) Absence of the key causes nfit to perform pure aggregation
        #   b) Even 'low' decay applies some time-weighting
        #   c) Explicit deletion makes the intent unambiguous
        # ======================================================================
        delete $transform->{decay};
        delete $transform->{runq_decay} if exists $transform->{runq_decay};

        # ======================================================================
        # STEP 2: PRESERVE smoothing method and window
        # ======================================================================
        # The method (sma/ema) and window define HOW the profile smooths noise
        # within each time period. This is NOT predictiveÃ¢â‚¬â€it's the measurement
        # methodology.
        #
        # Example: "G3-95W15" explicitly means "P95 of 15-minute SMA"
        #          Changing this would create a different metric entirely.
        #
        # NO CHANGES to: $transform->{method}, $transform->{window}
        # ======================================================================

        # ======================================================================
        # STEP 3: PRESERVE time-context filters
        # ======================================================================
        # Time-context filters define WHAT operational state is being measured:
        #   - time_filter (online/batch): Operational hours vs. batch windows
        #   - no_weekends: Weekday operations vs. full week
        #   - filter_perc: Data quality threshold (exclude idle periods)
        #
        # These are fundamental to the profile's definition and MUST be preserved.
        #
        # A profile defined with "-online -no-weekends" is explicitly measuring
        # "workload during operational business hours". Stripping these filters
        # would contaminate the measurement by including non-operational data.
        #
        # NO CHANGES to: $transform->{time_filter}, $transform->{no_weekends},
        #                $transform->{filter_perc}
        # ======================================================================

        # ======================================================================
        # STEP 4: Sanitise profile-level directives
        # ======================================================================
        next unless exists $transform->{profiles} && ref($transform->{profiles}) eq 'HASH';

        foreach my $profile_name (keys %{ $transform->{profiles} }) {
            my $directives = $transform->{profiles}{$profile_name};
            next unless ref($directives) eq 'HASH';

            # ------------------------------------------------------------------
            # Remove growth prediction (CRITICAL for data purity)
            # ------------------------------------------------------------------
            # Growth prediction is forward-looking and must never appear in
            # historical records. The history cache must contain only "what
            # actually happened", not "what we predict will happen".
            #
            # We use DELETE rather than setting to 0 to make the absence
            # explicit and unambiguous.
            # ------------------------------------------------------------------
            delete $directives->{enable_growth};

            # ------------------------------------------------------------------
            # PRESERVE historical measurement flags
            # ------------------------------------------------------------------
            # These directives capture factual historical measurements:
            #   - calculate_peak: The actual maximum value observed
            #   - enable_clipping: Detection of capacity saturation
            #
            # Both are measurements of "what happened" and belong in the
            # historical record.
            #
            # NO CHANGES to: $directives->{calculate_peak},
            #                $directives->{enable_clipping}
            # ------------------------------------------------------------------
        }
    }

    return $sanitised_href;
}

# ==============================================================================
# SUBROUTINE: run_single_pass_analysis
# PURPOSE:    Orchestrates the single-pass analysis for a given system cache.
#             It builds the manifest, assembles and executes the nfit command
#             with all necessary global flags, and returns the parsed results.
# ==============================================================================
sub run_single_pass_analysis {
    my ($system_cache_dir, $profiles_ref, $args_ref) = @_;

    print STDERR "    [+] Single Pass Engine Analysis\n";

    # 1. Build the manifest using the function already added.
    print STDERR "      • Building transform manifest\n";

    # [FIX] Extract runq_perc_behavior from args (default to 'none' if missing)
    my $runq_behavior = $args_ref->{runq_perc_behavior} // 'none';

    # [FIX] Pass it to the builder function
    my $transform_manifest = build_transform_manifest($profiles_ref, $args_ref->{runq_avg_method}, $runq_behavior);

    # --- INJECT EXCLUSIONS IF PRESENT ---
    if (defined $args_ref->{exclusions}) {
        _inject_exclusions_into_manifest($transform_manifest, $args_ref->{exclusions});
    }

    # *** ADD THIS COMPLETE DIAGNOSTIC BLOCK TO PROVE THE CORRECTNESS OF THE MANIFEST***
#    {
#        warn "\n" . "=" x 70 . "\n";
#        warn "STEP 1: MANIFEST DEBUG IN nfit-profile.pl (After build_transform_manifest)\n";
#        warn "=" x 70 . "\n\n";
#
#        # Test profile to trace
#        my $test_profile = 'B3-95W15';
#
#        # Count PhysC transforms
#        my @physc_transforms = grep {
#            $transform_manifest->{$_}{metric} eq 'PhysC'
#        } keys %$transform_manifest;
#        warn "Total PhysC transforms created: " . scalar(@physc_transforms) . "\n\n";
#
#        # Check which transforms contain our test profile
#        my @transforms_with_test = grep {
#            $transform_manifest->{$_}{metric} eq 'PhysC' &&
#            exists $transform_manifest->{$_}{profiles}{$test_profile}
#        } keys %$transform_manifest;
#
#        warn "Transforms containing '$test_profile': " . scalar(@transforms_with_test) . "\n";
#
#        if (@transforms_with_test) {
#            foreach my $key (sort @transforms_with_test) {
#                my $prof_data = $transform_manifest->{$key}{profiles}{$test_profile};
#                my $has_pct = defined $prof_data->{percentiles};
#                my $pct_val = $has_pct ? "[" . join(",", @{$prof_data->{percentiles}}) . "]" : "UNDEFINED";
#                warn "  - $key\n";
#                warn "    Percentiles: $pct_val\n";
#            }
#        } else {
#            warn "  ERROR: '$test_profile' not found in ANY PhysC transform!\n";
#        }
#
#        warn "\n";
#
#        # Show summary of ALL PhysC transforms and their profiles
#        warn "Complete PhysC Transform Summary:\n";
#        warn "-" x 70 . "\n";
#        foreach my $key (sort @physc_transforms) {
#            my @profiles_in_transform = sort keys %{$transform_manifest->{$key}{profiles}};
#            warn "Transform: $key\n";
#            warn "  Profiles (" . scalar(@profiles_in_transform) . "): " . join(", ", @profiles_in_transform) . "\n";
#
#            # Check if percentiles are defined for each
#            my @with_pct = grep {
#                defined $transform_manifest->{$key}{profiles}{$_}{percentiles}
#            } @profiles_in_transform;
#            my @without_pct = grep {
#                !defined $transform_manifest->{$key}{profiles}{$_}{percentiles}
#            } @profiles_in_transform;
#
#            if (@without_pct) {
#                warn "  WARNING: Profiles without percentiles: " . join(", ", @without_pct) . "\n";
#            }
#        }
#
#        warn "\n" . "=" x 70 . "\n";
#        warn "END STEP 1 MANIFEST DEBUG\n";
#        warn "=" x 70 . "\n\n";
#    }
#    # *** END DIAGNOSTIC BLOCK ***

    # Check if any profile in the manifest requires growth prediction.
    my $any_profile_has_growth = 0;
    foreach my $transform (values %$transform_manifest) {
        foreach my $profile_directives (values %{ $transform->{profiles} }) {
            if ($profile_directives->{enable_growth}) {
                $any_profile_has_growth = 1;
                last;
            }
        }
        last if $any_profile_has_growth;
    }

    # 2. Create a temporary file for the manifest.
    my ($fh_manifest, $manifest_filename) = tempfile(UNLINK => 1);
    print $fh_manifest JSON->new->pretty->encode($transform_manifest);
    close $fh_manifest;

    # --- BEGIN STEP 1 DEBUGGING ---
#    print STDERR "--- DEBUG: TRANSFORM MANIFEST ---\n";
#    my $manifest_json_for_debug = JSON->new->pretty->encode($transform_manifest);
#    print STDERR $manifest_json_for_debug . "\n";
#    print STDERR "Start Date:            " . ($args_ref->{start_date} // 'undef') . "\n";
#    print STDERR "End Date:              " . ($args_ref->{end_date} // 'undef') . "\n";
#    print STDERR "Analysis Reference:    " . ($args_ref->{analysis_reference_date} // 'undef') . "\n";
#    print STDERR "Enable Windowed Decay: " . ($args_ref->{enable_windowed_decay} // '0') . "\n";
#    print STDERR "--- MANIFEST DEBUG END. SCRIPT WILL NOW EXIT. ---\n";
#    exit 0; # Exit early for debugging
    # --- END STEP 1 DEBUGGING ---

    # 3. Assemble the complete nfit command with all preserved global flags.
    my $command = $args_ref->{nfit_path} . " --manifest $manifest_filename";

    # Add required I/O and pass-through global flags.
    $command .= " --nmondir \"$system_cache_dir\"";
    $command .= " " . $args_ref->{rounding_flags} if $args_ref->{rounding_flags};
    $command .= " --smt $args_ref->{default_smt}" if defined $args_ref->{default_smt};
    $command .= " --runq-avg-method $args_ref->{runq_avg_method}" if defined $args_ref->{runq_avg_method};

    # Add optional filtering flags.
    $command .= " --startdate $args_ref->{start_date}" if defined $args_ref->{start_date};
    $command .= " --enddate $args_ref->{end_date}" if defined $args_ref->{end_date};
    $command .= " --vm \"$args_ref->{vm_name}\"" if defined $args_ref->{vm_name};

    # Add flags for specific analysis models, auto-enabling decay for growth
    my $standard_model = 0;
    my $decay_mode_set = 0;
    if ($args_ref->{enable_windowed_decay}) {
        $command .= " --enable-windowed-decay";
        $decay_mode_set = 1;
        $standard_model = 1;
    }
    if ($args_ref->{decay_over_states}) {
        $command .= " --decay-over-states";
        $decay_mode_set = 1;
        $standard_model = 1;
    }

    # If any profile needs growth, a decay model is required. Auto-enable one if not set.
    if ($any_profile_has_growth && !$decay_mode_set) {
        $command .= " --enable-windowed-decay";
        # Optionally print a notice to stderr that a default mode was activated.
        print STDERR "  Φ Auto-enabling windowed-decay for profile-level growth prediction\n";
    }

    # Pass the global growth flag if needed, and other flags like clipping detection
    $command .= " --enable-growth-prediction" if $any_profile_has_growth;
    $command .= " --analysis-reference-date $args_ref->{analysis_reference_date}" if defined $args_ref->{analysis_reference_date};
    $command .= " --enable-clipping-detection" if $args_ref->{enable_clipping_detection}; # Pass this through
    $command .= " -q" unless ($standard_model);  # execute nfit engine silently

    # 4. Execute the nfit engine and capture its output.
    print STDERR "      • Executing nFit Engine";
    printf STDERR " (%s - %s)", $args_ref->{start_date} // '-', $args_ref->{end_date} // '-' if (defined $args_ref->{start_date} or defined $args_ref->{end_date});
    print STDERR "\n";
    my $raw_nfit_output = `$command`;
    my $exit_status = $? >> 8;

    if ($exit_status != 0) {
        die "      [ERROR] nFit Single Pass Engine execution failed for system '$system_cache_dir' with exit code $exit_status.\nCommand: $command\nOutput: $raw_nfit_output";
    }

    # --- BEGIN STEP 2 DEBUGGING ---
    #print STDERR "--- DEBUG: RAW JSON OUTPUT FROM nfit.pl ---\n";
    #print STDERR "nfit.pl Exit Status: " . $exit_status . "\n";
    #print STDERR "Command Executed:\n$command\n\n";
    #print STDERR "Output Received:\n";
    #print STDERR $raw_nfit_output . "\n";
    #print STDERR "--- JSON DEBUG END. SCRIPT WILL NOW EXIT. ---\n";
    #exit 0; # Exit early for debugging
    # --- END STEP 2 DEBUGGING ---

    # 5. Parse and return the final results.
    return parse_nfit_json_output($raw_nfit_output);
}

# Helper for hash references (with safe fallback)
sub _safe_get {
    my ($href, $key, $fallback) = @_;
    return (ref($href) eq 'HASH' && exists $href->{$key}) ? $href->{$key} : $fallback;
}

# ==============================================================================
# SUBROUTINE: build_assimilation_map
# PURPOSE:    Acts as an anti-corruption layer by consuming the raw, nested JSON
#             output from the nfit.pl engine and transforming it into a stable,
#             predictable, and mostly flat Perl hash structure. This map becomes
#             the single source of truth for all subsequent consumer logic within
#             nfit-profile.pl.
# ARGUMENTS:
#   1. $parsed_nfit_results_href (hash ref): The raw, decoded Perl hash from nfit.pl.
#   2. $profiles_aref (array ref): A reference to the global @profiles array.
#   3. $adaptive_runq_saturation_thresh (float): The dynamically calculated threshold for detecting Absolute RunQ saturation.
#        This value is passed directly to 'generate_sizing_hint' to determine the VM's pressure status.
#
# RETURNS:
#   - A hash reference to the fully populated assimilation map.
# ==============================================================================
sub build_assimilation_map {
    my ($parsed_nfit_results_href, $profiles_aref, $adaptive_runq_saturation_thresh) = @_;

    my %assimilation_map;

    # Iterate through each VM returned by the nfit engine.
    foreach my $vm_name (sort keys %{$parsed_nfit_results_href}) {
        my @states_for_vm = @{ $parsed_nfit_results_href->{$vm_name} };
        next unless @states_for_vm;

        # The representative state is used to source common metadata and RunQ metrics.
        my $representative_state = $states_for_vm[-1];
        my $is_aggregated = (ref($representative_state) eq 'HASH' && ($representative_state->{analysisType} // '') =~ /aggregated/i);

        # Initialise the map entry for this VM with a predictable structure.
        my $vm_map = $assimilation_map{$vm_name} = {
            Configuration    => {},
            CoreResults      => { ProfileValues => {}, PeakValue => undef },
            Hinting          => {},
            RunQMetrics      => {},
            Growth           => { min_adj => 0.0, max_adj => 0.0, adjustment => 0.0, rationale => {} },
            RawNfitStates    => \@states_for_vm,
            SeasonalForecast => {},
            CSVModifiers     => {},
            # GrowthRationaleByProfile will store ALL rationales for logging
            GrowthRationaleByProfile => {},
        };

        # --- Block 1: Populate Configuration ---
        my $metadata_block = _safe_dig($representative_state, 'metadata') || {};
        $vm_map->{Configuration} = {
            smt           => _safe_dig($representative_state, 'metadata', 'smt'),
            entitlement   => _safe_dig($representative_state, 'metadata', 'entitlement'),
            max_cpu       => _safe_dig($representative_state, 'metadata', 'max_cpu'),
            virtual_cpus  => _safe_dig($representative_state, 'metadata', 'virtual_cpus'),
            is_capped     => _safe_dig($representative_state, 'metadata', 'capped'),
            pool_id       => _safe_dig($representative_state, 'metadata', 'pool_id'),
            pool_cpu      => _safe_dig($representative_state, 'metadata', 'pool_cpu'),
            serial_number => _safe_dig($representative_state, 'metadata', 'serial_number'),
            proc_type     => _safe_dig($representative_state, 'metadata', 'proc_type'),
            proc_version  => _safe_dig($representative_state, 'metadata', 'proc_version'),
            proc_clock    => _safe_dig($representative_state, 'metadata', 'proc_clock'),
        };

        # --- Block 2: Populate CoreResults (ProfileValues and PeakValue) ---
        if ($is_aggregated) {
            # For aggregated runs, data is already per-profile.
            # Growth is now calculated per profile inside nfit.pl.
            my $physc_metrics = _safe_dig($representative_state, 'metrics', 'physc') || {};
            my @growth_values_for_vm;

            foreach my $profile_name (keys %$physc_metrics) {
                my $profile_data = $physc_metrics->{$profile_name};

                # CRITICAL: Extract the growth-inclusive baseline for RunQ modifier processing.
                # BaseValue from nfit is ALWAYS pre-growth. FinalValue is BaseValue + GrowthAdj.
                my $base_val   = _safe_dig($profile_data, 'BaseValue');
                my $growth_adj = _safe_dig($profile_data, 'GrowthAdj');
                my $final_val  = _safe_dig($profile_data, 'FinalValue'); # Growth-inclusive
                my $rationale  = _safe_dig($profile_data, 'growthRationale');

                # Store the growth-inclusive FinalValue as the starting point for nfit-profile modifiers.
                # This ensures that profile metrics in the CSV include the GrowthAdj.
                # Fallback: If FinalValue is not available, compute it as BaseValue + GrowthAdj.
                my $growth_inclusive_value = $final_val;
                if (!defined $growth_inclusive_value || !looks_like_number($growth_inclusive_value)) {
                    my $base_numeric = (defined $base_val && looks_like_number($base_val)) ? $base_val : 0;
                    my $growth_numeric = (defined $growth_adj && looks_like_number($growth_adj)) ? $growth_adj : 0;
                    $growth_inclusive_value = $base_numeric + $growth_numeric;
                }
                $vm_map->{CoreResults}{ProfileValues}{$profile_name} = $growth_inclusive_value;

                # Store the pre-growth BaseValue separately for audit trail purposes
                $vm_map->{Growth}{base_values}{$profile_name} = $base_val // 0;

                # Store per-profile growth adjustment for later use
                my $growth_adj_for_profile = (defined $growth_adj && looks_like_number($growth_adj)) ? $growth_adj : 0;
                push @growth_values_for_vm, $growth_adj_for_profile;
                $vm_map->{Growth}{adjustments}{$profile_name} = $growth_adj_for_profile;

                # Harvest ALL rationales for audit logging
                if (ref($rationale) eq 'HASH' && scalar keys %$rationale) {
                    $vm_map->{GrowthRationaleByProfile}{$profile_name} = $rationale;
                }
            }

            # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
            # Calculate min/max from non-zero growth values only
            # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
            # RATIONALE: Zero growth means "no trend detected", not "minimum is zero".
            # Capacity planners need the actual range of predicted growth values.
            # - If all profiles have 0 growth → min=0, max=0 (correct: no growth)
            # - If some profiles have growth → show min/max of non-zero values
            # - Negative growth (declining trends) is preserved in the range
            # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

            # Filter to non-zero growth values (preserving negatives)
            my @non_zero_growth = grep { abs($_) > 1e-9 } @growth_values_for_vm;

            if (@non_zero_growth) {
                # Calculate range from actual growth predictions
                $vm_map->{Growth}{max_adj} = max(@non_zero_growth);
                $vm_map->{Growth}{min_adj} = min(@non_zero_growth);
            } else {
                # No growth detected on any profile
                $vm_map->{Growth}{max_adj} = 0.0;
                $vm_map->{Growth}{min_adj} = 0.0;
            }

            # The PeakValue comes from the raw peak tracker, not a specific profile's aggregated value.
            $vm_map->{CoreResults}{PeakValue} = _safe_dig($representative_state, 'metadata', 'peakPhyscFromLatestState');

       } else {
            # Logic for per-state runs remains correct.
            my %profile_sums;
            my %profile_counts;
            foreach my $profile (@$profiles_aref) {
                my ($p_val_num) = $profile->{flags} =~ /(?:-p|--percentile)\s+([0-9.]+)/;
                my $p_metric_key = "P" . clean_perc_label($p_val_num // $DEFAULT_PERCENTILE);
                foreach my $state (@states_for_vm) {
                    my $metric_val = _safe_dig($state, 'metrics', 'physc', $profile->{name}, $p_metric_key);
                    if (defined $metric_val && looks_like_number($metric_val)) {
                        $profile_sums{$profile->{name}} += $metric_val;
                        $profile_counts{$profile->{name}}++;
                    }
                }
            }
            foreach my $profile_name (keys %profile_sums) {
                if ($profile_counts{$profile_name} > 0) {
                    $vm_map->{CoreResults}{ProfileValues}{$profile_name} = $profile_sums{$profile_name} / $profile_counts{$profile_name};
                }
            }
            my @peak_values;
            foreach my $state (@states_for_vm) {
                my $peak_val = _safe_dig($state, 'metrics', 'physc', $MANDATORY_PEAK_PROFILE_FOR_HINT, 'Peak');
                push @peak_values, $peak_val if (defined $peak_val && looks_like_number($peak_val));
            }
            $vm_map->{CoreResults}{PeakValue} = max(@peak_values) if @peak_values;
        }

        # --- Block 3: Populate RunQMetrics (Comprehensive) ---
        # This loop iterates through ALL profiles to find and collate every RunQ
        # metric calculated by the nfit engine into a single, unified block.
        $vm_map->{RunQMetrics}{SourceProfile} = $MANDATORY_PEAK_PROFILE_FOR_HINT; # P-99W1 remains the default source
        foreach my $profile (@$profiles_aref) {
            my $p_name = $profile->{name};
            my $runq_norm_metrics = _safe_dig($representative_state, 'metrics', 'runq', 'normalized', $p_name) || {};
            my $runq_abs_metrics  = _safe_dig($representative_state, 'metrics', 'runq', 'absolute', $p_name) || {};
            foreach my $key (keys %$runq_norm_metrics) {
                $vm_map->{RunQMetrics}{"NormRunQ_$key"} //= $runq_norm_metrics->{$key};
            }
            foreach my $key (keys %$runq_abs_metrics) {
                $vm_map->{RunQMetrics}{"AbsRunQ_$key"} //= $runq_abs_metrics->{$key};
            }
        }

        # Calculate IQRC.
        my ($p25, $p50, $p75) = ($vm_map->{RunQMetrics}{'NormRunQ_P25'}, $vm_map->{RunQMetrics}{'NormRunQ_P50'}, $vm_map->{RunQMetrics}{'NormRunQ_P75'});
        if (defined $p50 && $p50 > $FLOAT_EPSILON && defined $p75 && defined $p25) {
            $vm_map->{RunQMetrics}{IQRC} = ($p75 - $p25) / $p50;
        }

        # --- Block 4: Populate Growth ---
        if ($is_aggregated) {
            # In this new logic, we intentionally leave Growth.adjustment and
            # Growth.rationale EMPTY.
            # They will be populated dynamically in the consumer functions
            # (log_profile_rationale and _write_standard_csv_report)
            # based on the correct profile context.
            $vm_map->{Growth}{adjustment} = 0.0; # Default to 0
            $vm_map->{Growth}{rationale}  = {};  # Default to empty
        }
    }
    return \%assimilation_map;
}

# ==============================================================================
# SUBROUTINE: _acquire_history_lock
# PURPOSE:    Acquires an EXCLUSIVE lock on the history infrastructure.
#             This enables safe read-modify-write operations across the
#             transition from Monolith -> Partitioned architecture.
# RETURNS:    ($fh, $lock_path) - Filehandle and path. Keep $fh open to hold lock.
# ==============================================================================
sub _acquire_history_lock {
    my ($system_cache_dir) = @_;

    my $lock_file = File::Spec->catfile($system_cache_dir, $HISTORY_LOCK_FILENAME);

    open my $lock_fh, '>', $lock_file
        or die "FATAL: Could not create lock file '$lock_file': $!";

    # Blocking lock - wait until acquired
    flock($lock_fh, LOCK_EX)
        or die "FATAL: Could not acquire exclusive lock on '$lock_file': $!";

    # Auto-flush to ensure lock intent is registered if we write pid later
    my $old_fh = select($lock_fh); $| = 1; select($old_fh);

    return ($lock_fh, $lock_file);
}

# ==============================================================================
# SUBROUTINE: _migrate_history_to_partitioned
# PURPOSE:    One-time migration of monolithic history file to partitioned layout.
#             Preserves data integrity by creating the directory, populating it,
#             and renaming the legacy file only upon success.
# ==============================================================================
sub _migrate_history_to_partitioned {
    my ($system_cache_dir) = @_;

    my $legacy_file = File::Spec->catfile($system_cache_dir, '.nfit.history.json');
    my $partition_dir = File::Spec->catfile($system_cache_dir, '.nfit.history');

    return unless -f $legacy_file; # Nothing to migrate

    # This function is called INSIDE a lock, so we don't re-acquire it here.

    print "  - Migrating legacy history to partitioned format.\n";

    # 1. Load Legacy Data
    my $json_text = do {
        open my $fh, '<:encoding(utf8)', $legacy_file or die "FATAL: Cannot read legacy history: $!";
        local $/; <$fh>;
    };
    my $data = JSON->new->decode($json_text);

    # 2. Create Directory
    unless (-d $partition_dir) {
        make_path($partition_dir) or die "FATAL: Cannot create history partition directory: $!";
    }

    # 3. Write Partition Files
    my $json_encoder = JSON->new->pretty->canonical;

    foreach my $month_key (keys %$data) {
        # We wrap the data in the month key to maintain the exact structure
        # { "YYYY-MM": { ... } } inside the file. This simplifies stitching.
        my $month_payload = { $month_key => $data->{$month_key} };

        # Naming Convention: nfit.hist.YYYY-MM.json
        my $filename = "nfit.hist.${month_key}.json";
        my $filepath = File::Spec->catfile($partition_dir, $filename);

        open my $fh, '>:encoding(utf8)', $filepath
            or die "FATAL: Cannot write partition file $filename: $!";
        print $fh $json_encoder->encode($month_payload);
        close $fh;
    }

    # 4. Rename Legacy File (The "Commit" operation)
    my $migrated_name = $legacy_file . ".migrated";
    rename($legacy_file, $migrated_name)
        or die "FATAL: Failed to rename legacy file after migration: $!";

    print "  - Migration complete. Legacy file renamed to .migrated.\n";
}

# ==============================================================================
# SUBROUTINE: calculate_sens_slope
# PURPOSE:    Calculate Theil-Sen slope estimator (median of pairwise slopes)
# ARGUMENTS:
#   1. $series_aref - Array ref of [x, y] pairs (time series points)
# RETURNS:
#   Hash ref: { slope => $value, n_slopes => $count, method => 'exact_pairs' }
#   Returns undef if insufficient data (n < 2)
# ==============================================================================
sub calculate_sens_slope {
    my ($series_aref) = @_;
    my $n = scalar @{$series_aref};

    # Need at least 2 points to calculate a slope
    return undef if $n < 2;

    my @slopes;
    my $FLOAT_EPSILON = 1e-9;

    # Calculate all pairwise slopes: slope_ij = (y_j - y_i) / (x_j - x_i)
    for my $i (0 .. $n-2) {
        for my $j ($i+1 .. $n-1) {
            my ($x1, $y1) = @{$series_aref->[$i]};
            my ($x2, $y2) = @{$series_aref->[$j]};

            my $delta_x = $x2 - $x1;

            # Only calculate slope if x values are distinct
            if (abs($delta_x) > $FLOAT_EPSILON) {
                push @slopes, ($y2 - $y1) / $delta_x;
            }
        }
    }

    # If no valid slopes could be calculated, return undef
    return undef if @slopes == 0;

    # Sen's slope is the median of all pairwise slopes
    my $median_slope = _calculate_median(\@slopes);

    return {
        slope     => $median_slope,
        n_slopes  => scalar(@slopes),
        method    => 'exact_pairs',
    };
}

# ==============================================================================
# SUBROUTINE: _calculate_median
# PURPOSE:    Calculate median of an array of numbers
# ARGUMENTS:
#   1. $values_aref - Array ref of numeric values
# RETURNS:
#   Median value (scalar)
# ==============================================================================
sub _calculate_median {
    my ($values_aref) = @_;

    # Sort values in ascending order
    my @sorted = sort { $a <=> $b } @{$values_aref};
    my $n = scalar @sorted;

    # For odd n, return middle value
    if ($n % 2 == 1) {
        return $sorted[int($n/2)];
    }
    # For even n, return average of two middle values
    else {
        return ($sorted[$n/2 - 1] + $sorted[$n/2]) / 2;
    }
}

# --- usage_wrapper ---
# Generates and returns the usage/help message for the script.
sub usage_wrapper
{
    my $script_name = $0;
    $script_name =~ s{.*/}{}; # Get only script name, remove path
    return <<END_USAGE;
Usage: $script_name (--mgsys <serial> | --nmondir <directory>) <pc_file> [options]

nfit-profile is a workload profiling and forecasting orchestrator for capacity planning.

It executes the nfit profiling engine across configured workload profiles and models to
characterise baseline behaviour, operational demand, and future peak risk.
The resulting outputs are designed for forensic analysis, operational decision-making,
and forward-looking capacity forecasts.

Supported capabilities include:
  - Workload profiling (state-based, forensic)
  - Operational forecasting (hybrid state–time decay, run-queue aware)
  - Seasonal and business-cycle forecasting (recency-anchored decay, multiplicative seasonal, predictive peak)

CSV reports and model-specific rationale logs are written to $LOG_FILE_DIR.

INPUT SELECTION:
  --mgsys <serial>          : Managed system serial to analyse.
                              If --nmondir is not provided, the staged data cache
                               is derived from the default base path ('./stage/').
                              May be combined with --nmondir to select a system
                               from a non-default staged cache hierarchy.

  --nmondir <directory>     : Path to a staged data cache directory.
                                The directory must be one of the following:
                                 - a base directory containing multiple systems (multi-cache mode), or
                                 - a single system cache directory (single-cache mode).

  --default-smt, --smt <N>  : Optional default SMT level for RunQ calculations.
                               (Default: $DEFAULT_SMT_VALUE_PROFILE)

OVERVIEW OF WORKLOAD PROFILING, MODELLING, AND FORECASTING MODES:
  (default)
        Workload Profiling
        State-based, forensic workload characterisation without time decay.
        Suitable for historical analysis and configuration investigations.

  --enable-windowed-decay
        Operational Forecasting (Time-Based Windowed Decay)
        Applies recency weighting over a moving time window to produce
        short-term, operational capacity forecasts.

  --decay-over-states
        Operational Forecasting (Hybrid State–Time Decay)
        Synthesises state-based results first, then applies recency decay.
        Designed for stable environments with infrequent configuration changes.

        Both operational forecasting modes may incorporate:
          - Run-queue pressure signals
          - Growth adjustment using robust trend estimation (Theil–Sen)

  --apply-seasonality <event>
        Seasonal Forecasting (Event-Specific)
        Executes the forecasting model configured for a single specified event.
        Models are configured in `etc/nfit.seasonality.cfg` and may include:
          - Recency-Anchored Decay (recency_decay)
          - Multiplicative Seasonal (multiplicative_seasonal)
          - Predictive Peak (robust regression: Theil–Sen) (predictive_peak)

  --seasonal
        Seasonal Forecasting (Automatic, comprehensive)
        Executes all applicable seasonal forecasting models mentioned above,
        as defined in etc/nfit.seasonality.cfg.

RUN-QUEUE METRIC CONFIGURATION:

  --runq-norm-percentiles <list>            : Percentiles for normalised run-queue analysis
                                              (Default: "$DEFAULT_RUNQ_NORM_PERCS").
                                              The specified list is combined with profile-specific settings and ensures P50,P90.

  --runq-abs-percentiles <list>             : Percentiles for absolute run-queue analysis
                                              (Default: "$DEFAULT_RUNQ_ABS_PERCS").
                                              The specified list is combined with profile-specific settings and ensures P90.

  --runq-perc-behavior <mode>               : Manages how run-queue pressure is translated into additive CPU recommendations.
                                              fixed (default) : Uses the 90th percentile of run-queue pressure for additive CPU calculations.
                                              match           : Matches the run-queue percentile to the workload profile’s PhysC percentile.

SEASONALITY AND BUSINESS-CYCLE FORECASTING:

Seasonal analysis is driven by the configuration in etc/nfit.seasonality.cfg.

Model behaviour depends on the event configuration:
  - Multiplicative seasonal models
      Produce multiple CSV outputs (final forecast, current baseline, and historical snapshot)
  - Recency-decay models (e.g. month-end)
      Anchor the forecast to the last completed peak period to avoid start-of-period dilution
  - Predictive peak models
      Estimate future peak demand using robust statistical projection

  --apply-seasonality <event>               : Execute the configured forecasting model for a specific business event.

  --seasonal                                : Execute all configured seasonal models applicable to the analysis period.

  --analysis-reference-date <YYYY-MM-DD>    : Seasonal Analysis Reference Date (As-of Anchor)
                                              Sets the "as-of" date used to evaluate whether seasonal events are
                                              complete/active and therefore eligible for forecasting.

                                              Default: effective analysis end date (-e), otherwise the last
                                              recorded date in the staged data cache.
                                              Must precede --enddate if both are set.

  --update-history                          : Populate or extend the historical system snapshot cache.
                                              Discovers completed months and seasonal events from the staged data cache
                                              and records workload and forecasting snapshots for advanced analysis and adaptive scoring.

  --min-history-days <N>                    : When updating history, allow partial months with at least N days of data
                                              (Default: 28 days).

  --force                                   : Override validation checks. Use with caution.
  --reset-seasonal-cache                    : Remove cached seasonal snapshots (this feature is not yet implemented).

RECENCY AND DECAY CONTROLS:

  --enable-windowed-decay                   : Enable time-based windowed processing.
  --decay-over-states                       : Enable hybrid state–time decay.
                                              (Mutually exclusive with windowed decay)

  --process-window-unit <days|weeks>        : Unit for the processing window size.
                                              (Default: $DEFAULT_PROCESS_WINDOW_UNIT_FOR_NFIT)
  --process-window-size <N>                 : Size of the processing window in the selected unit.
                                              (Default: $DEFAULT_PROCESS_WINDOW_SIZE_FOR_NFIT)

  --decay-half-life-days <N>                : Half-life for recency weighting.
                                             (Default: $DEFAULT_DECAY_HALF_LIFE_DAYS_FOR_NFIT)

  --runq-avg-method <none|sma|ema>          : Averaging method applied to RunQ data before percentile calculation.
                                              (Default: $DEFAULT_NFIT_RUNQ_AVG_METHOD)

CONFIGURATION FILES:
  -config <vm_cfg_csv>                      : Optional. VM configuration CSV file.
  --profiles-config <path>                  : Optional. Profiles definition file (INI format).
                                              Can contain 'runq_modifier_behavior = additive_only' per profile,
                                              and profile-specific 'nfit_flags' including --runq-norm-perc/--runq-abs-perc.

ANALYSIS FILTERING OPTIONS:
  -s, --startdate <YYYY-MM-DD>              : Global start date for analysis. Optional.
                                              Limits the data considered for analysis and forecasting.
                                              Data earlier than this date will be ignored, even if present
                                              in the current staged data cache.
  -e, --enddate <YYYY-MM-DD>                : Global end date for analysis. Optional.
                                              Limits the data considered for analysis and forecasting.
                                              Data later than this date will be ignored, even if present
                                              in the current staged data cache.

  -vm, --lpar <name>                        : Analyse only the specified VM/LPAR name (passed to nfit). Optional.

ROUNDING OPTIONS:
  -r[=increment]                            : Optional. nfit rounds results to NEAREST increment.
  -u[=increment]                            : Optional. nfit rounds results UP to nearest increment.
                                              (Default increment: $DEFAULT_ROUND_INCREMENT)
OTHER:
  --nfit-path <path>                        : Optional. Path to the 'nfit' script.
  -h, --help                                : Display this help message.
  -v, --version                             : Display script version and nfit version used.

CSV OUTPUT OPTIONS:
  --excel-formulas[=<true|false>]           : Optional. Add Excel-specific formula columns and summary footer.
                                              (Default: false).

OUTPUT CSV COLUMNS (MODEL-DEPENDENT):

All CSV outputs share a common base structure:
  VM, TIER, Hint, Pattern, Pressure, PressureDetail, SMT, Serial, SystemType, Pool Name, Pool ID, Peak, [Profile Columns], Current - ENT
  (Note: Workload profile values are potentially growth-adjusted & RunQ-modified)

Additional columns are model-dependent:
  Operational Forecasting (Windowed / Hybrid Decay):
    RunQ_Tactical, RunQ_Strategic, RunQ_Potential, RunQ_Source
    GrowthAdj, GrowthAdj_Min, GrowthAdj_Max, GrowthAdj_Source, ProjectionDays, GrowthMethod, GrowthConfidence, GrowthTrend, GrowthSignificant

  Seasonal Forecasting Models:
    Recency-Anchored Decay:
      GrowthAdj, GrowthAdj_Min, GrowthAdj_Max, GrowthAdj_Source, ProjectionDays, GrowthMethod, GrowthConfidence, GrowthTrend, GrowthSignificance
    Multiplicative Seasonal:
      ActiveEvents, Baseline_P99, BaselineSource, ForecastModel
    Predictive Peak:
      ActiveEvents, P99_PredictedDelta, Baseline_P99, BaselineSource, ForecastModel

END_USAGE
}

# ==============================================================================
# SUBROUTINE: _parse_exclusion_dates
# PURPOSE:    Parses the exclude_dates configuration into a structured format,
#             expanding wildcards against the known VM list.
# ARGUMENTS:
#   1. $raw_value    - Raw string value from config (may be multiline/accumulated)
#   2. $known_vms_aref - Reference to array of known VM names (for wildcard expansion)
# RETURNS:
#   Hash ref with 'global' and 'vm_specific' arrays
# ==============================================================================
sub _parse_exclusion_dates {
    my ($raw_value, $known_vms_aref) = @_;

    return undef unless defined $raw_value && $raw_value ne '';

    my %exclusions = (
        global      => [],
        vm_specific => {},
    );

    # Build VM lookup for wildcard matching
    my @known_vms = @{$known_vms_aref // []};

    # Split on commas and newlines, trim whitespace
    my @entries = grep { $_ ne '' }
                  map { s/^\s+|\s+$//gr }
                  split /[,\n]+/, $raw_value;

    foreach my $entry (@entries) {
        # Expected format: VM_PATTERN:START_DATE:END_DATE
        # Robust parsing with whitespace tolerance
        my @parts = split /:/, $entry;
        next unless scalar @parts >= 2;

        my $vm_pattern = $parts[0] // '';
        my $start_date = $parts[1] // '';
        my $end_date   = $parts[2] // $start_date;  # Single date if no end

        # Trim whitespace from each part
        s/^\s+|\s+$//g for ($vm_pattern, $start_date, $end_date);

        # Validate date format (YYYY-MM-DD)
        unless ($start_date =~ /^\d{4}-\d{2}-\d{2}$/ && $end_date =~ /^\d{4}-\d{2}-\d{2}$/) {
            warn "WARNING: Invalid date format in exclude_dates: '$entry'. Skipping.\n";
            next;
        }

        # Expand date range into individual dates
        my @date_range = _expand_date_range($start_date, $end_date);

        # Handle VM pattern
        if ($vm_pattern eq '*' || uc($vm_pattern) eq 'ALL') {
            # Global exclusion - applies to all VMs
            push @{$exclusions{global}}, @date_range;
        }
        elsif ($vm_pattern =~ /\*/) {
            # Wildcard pattern (e.g., "DB-*") - expand against known VMs
            my $regex = $vm_pattern;
            $regex =~ s/\*/\.\*/g;  # Convert glob to regex
            $regex = qr/^$regex$/i;

            my @matching_vms = grep { $_ =~ $regex } @known_vms;

            if (scalar @matching_vms == 0) {
                warn "WARNING: Wildcard pattern '$vm_pattern' matched no known VMs. Skipping.\n";
                next;
            }

            foreach my $matched_vm (@matching_vms) {
                push @{$exclusions{vm_specific}{$matched_vm}}, @date_range;
            }
        }
        else {
            # Explicit VM name
            push @{$exclusions{vm_specific}{$vm_pattern}}, @date_range;
        }
    }

    # De-duplicate all arrays
    my %seen;
    @{$exclusions{global}} = grep { !$seen{$_}++ } @{$exclusions{global}};

    foreach my $vm (keys %{$exclusions{vm_specific}}) {
        my %vm_seen;
        @{$exclusions{vm_specific}{$vm}} = grep { !$vm_seen{$_}++ }
                                            @{$exclusions{vm_specific}{$vm}};
    }

    return \%exclusions;
}

# ==============================================================================
# SUBROUTINE: _expand_date_range
# PURPOSE:    Expands a start/end date pair into a list of individual dates.
# ==============================================================================
sub _expand_date_range {
    my ($start_str, $end_str) = @_;

    my @dates;

    eval {
        my $current = Time::Piece->strptime($start_str, '%Y-%m-%d');
        my $end     = Time::Piece->strptime($end_str, '%Y-%m-%d');

        # Sanity check: end must be >= start
        if ($end < $current) {
            warn "WARNING: End date '$end_str' is before start date '$start_str'. Swapping.\n";
            ($current, $end) = ($end, $current);
        }

        # Safety limit: prevent runaway expansion
        my $max_days = 366;  # One year maximum
        my $day_count = 0;

        while ($current <= $end && $day_count < $max_days) {
            push @dates, $current->strftime('%Y-%m-%d');
            $current += ONE_DAY;
            $day_count++;
        }

        if ($day_count >= $max_days) {
            warn "WARNING: Date range '$start_str:$end_str' exceeds $max_days days. Truncated.\n";
        }
    };

    if ($@) {
        warn "WARNING: Failed to parse date range '$start_str:$end_str': $@\n";
        return ();
    }

    return @dates;
}

# ==============================================================================
# SUBROUTINE: _get_known_vms_from_cache
# PURPOSE:    Retrieves the list of known VM names from the L1 cache or states.
# ARGUMENTS:
#   1. $system_cache_dir - Path to the system's cache directory
# RETURNS:
#   Array reference of VM names
# ==============================================================================
sub _get_known_vms_from_cache {
    my ($system_cache_dir) = @_;

    my @known_vms;

    # Try states cache first (faster, structured)
    my $states_file = File::Spec->catfile($system_cache_dir, '.nfit.cache.states');
    if (-f $states_file) {
        eval {
            open my $fh, '<:encoding(utf8)', $states_file or die $!;
            local $/;
            my $states = decode_json(<$fh>);
            close $fh;
            @known_vms = keys %$states;
        };
        return \@known_vms if @known_vms;
    }

    # Fallback: scan data cache header or first N lines
    my $data_file = File::Spec->catfile($system_cache_dir, '.nfit.cache.data');
    if (-f $data_file) {
        eval {
            open my $fh, '<', $data_file or die $!;
            <$fh>;  # Skip header
            my %seen_vms;
            my $line_count = 0;
            while (my $line = <$fh>) {
                last if $line_count++ > 10000;  # Sample first 10K lines
                my (undef, $vm) = split ',', $line, 3;
                $seen_vms{$vm} = 1 if defined $vm;
            }
            close $fh;
            @known_vms = keys %seen_vms;
        };
    }

    return \@known_vms;
}

# ==============================================================================
# SUBROUTINE: _compute_exclusion_fingerprint
# PURPOSE:    Computes a deterministic fingerprint for the exclusion set.
# ==============================================================================
sub _compute_exclusion_fingerprint {
    my ($exclusions_href) = @_;
    return 'NONE' unless (defined $exclusions_href &&
                          (scalar @{$exclusions_href->{global} // []} > 0 ||
                           scalar keys %{$exclusions_href->{vm_specific} // {}} > 0));

    # Canonical representation for deterministic hashing
    my @parts;
    push @parts, "G:" . join(",", sort @{$exclusions_href->{global} // []});

    foreach my $vm (sort keys %{$exclusions_href->{vm_specific} // {}}) {
        push @parts, "$vm:" . join(",", sort @{$exclusions_href->{vm_specific}{$vm}});
    }

    require Digest::MD5;
    return Digest::MD5::md5_hex(join("|", @parts));
}

# ==============================================================================
# SUBROUTINE: _compute_event_definition_fingerprint (Phase 4: Idempotency)
# PURPOSE:    Computes a deterministic fingerprint for a normalised event
#             definition. Changes to any configuration element that affects
#             forecast computation will produce a different fingerprint.
# ARGUMENTS:
#   $event_cfg (hashref) - The resolved event configuration
# RETURNS:
#   String - MD5 hex digest of the canonical event representation
# ==============================================================================
sub _compute_event_definition_fingerprint {
    my ($event_cfg) = @_;
    return 'NONE' unless (defined $event_cfg && ref($event_cfg) eq 'HASH');

    require Digest::MD5;

    # Canonical keys that affect computation (alphabetically sorted for stability)
    # Includes user-modifiable fields that should trigger reprocessing:
    #   - dates, exclude_dates, vms, exclude_vms, last_modified
    my @fingerprint_keys = qw(
        aggregation_method
        baseline_period_days
        confidence_level
        dates
        day_of_period
        description
        duration_days
        exclude_dates
        exclude_vms
        fallback_event
        interaction_dampening_factor
        interaction_policy
        last_modified
        min_history_days
        min_historical_peaks
        min_historical_years
        model
        outlier_detection
        peak_amplification_factor
        period
        priority
        regime_detection
        residual_peak_profile
        seasonal_confidence_level
        seasonal_lookback_days
        source_event
        vms
        volatility_adjustment
    );

    my @parts;
    foreach my $key (sort @fingerprint_keys) {
        my $val = $event_cfg->{$key};
        if (defined $val) {
            # Normalise whitespace and case for string values
            if (!ref($val)) {
                $val =~ s/^\s+|\s+$//g;  # Trim
                $val =~ s/\s+/ /g;       # Collapse internal whitespace
            }
            push @parts, "$key=$val";
        }
    }

    # Include VM scope filter fingerprint if present
    if (exists $event_cfg->{_vm_scope_filter} && ref($event_cfg->{_vm_scope_filter}) eq 'HASH') {
        my $scope = $event_cfg->{_vm_scope_filter};
        if ($scope->{include} && ref($scope->{include}) eq 'HASH') {
            push @parts, "vms_include=" . join(",", sort keys %{$scope->{include}});
        }
        if ($scope->{exclude} && ref($scope->{exclude}) eq 'HASH') {
            push @parts, "vms_exclude=" . join(",", sort keys %{$scope->{exclude}});
        }
    }

    my $canonical = join("|", @parts);
    return Digest::MD5::md5_hex($canonical);
}

# ==============================================================================
# SUBROUTINE: _compute_execution_context_fingerprint (Phase 4: Idempotency)
# PURPOSE:    Computes a deterministic fingerprint for the execution context
#             of a bucket run. This captures runtime parameters that affect
#             the result.
# ARGUMENTS:
#   $exec_ctx (hashref) - The bucket execution context
#   $model_type (string) - Model identifier
# RETURNS:
#   String - MD5 hex digest of the canonical execution context
# ==============================================================================
sub _compute_execution_context_fingerprint {
    my ($exec_ctx, $model_type) = @_;
    return 'NONE' unless (defined $exec_ctx && ref($exec_ctx) eq 'HASH');

    require Digest::MD5;

    my @parts;

    # Core temporal boundaries
    if ($exec_ctx->{effective_start_date} && ref($exec_ctx->{effective_start_date}) eq 'Time::Piece') {
        push @parts, "start=" . $exec_ctx->{effective_start_date}->ymd;
    }
    if ($exec_ctx->{effective_end_date} && ref($exec_ctx->{effective_end_date}) eq 'Time::Piece') {
        push @parts, "end=" . $exec_ctx->{effective_end_date}->ymd;
    }

    # Anchor information
    push @parts, "anchor_bucket=" . ($exec_ctx->{anchor_bucket} // 'unknown');
    push @parts, "model=" . ($model_type // 'unknown');

    # Cache bounds (ground truth)
    if ($exec_ctx->{cache_start_date} && ref($exec_ctx->{cache_start_date}) eq 'Time::Piece') {
        push @parts, "cache_start=" . $exec_ctx->{cache_start_date}->ymd;
    }
    if ($exec_ctx->{cache_end_date} && ref($exec_ctx->{cache_end_date}) eq 'Time::Piece') {
        push @parts, "cache_end=" . $exec_ctx->{cache_end_date}->ymd;
    }

    # Engine version (ensures stale results are invalidated on logic changes)
    push @parts, "engine_version=$SEASONAL_ENGINE_VERSION";

    my $canonical = join("|", sort @parts);
    return Digest::MD5::md5_hex($canonical);
}

# ==============================================================================
# SUBROUTINE: _compute_bucket_fingerprint (Phase 4: Idempotency)
# PURPOSE:    Computes the combined fingerprint for a bucket. This is the
#             primary fingerprint used for skip/redo decisions.
# ARGUMENTS:
#   $event_cfg (hashref)  - The resolved event configuration
#   $exec_ctx (hashref)   - The bucket execution context
#   $model_type (string)  - Model identifier
#   $prev_bucket_fp (string|undef) - Fingerprint of the prior bucket (chain-of-custody)
# RETURNS:
#   Hashref containing:
#     combined     - The combined bucket fingerprint
#     event_def    - Event definition fingerprint
#     exec_ctx     - Execution context fingerprint
#     chain        - Chain-of-custody fingerprint (may be 'NONE' for first bucket)
#     engine_version - Current engine version
# ==============================================================================
sub _compute_bucket_fingerprint {
    # Added $dependency_fp to the end of the signature
    my ($event_cfg, $exec_ctx, $model_type, $prev_bucket_fp, $dependency_fp) = @_;

    require Digest::MD5;

    my $event_def_fp = _compute_event_definition_fingerprint($event_cfg);
    my $exec_ctx_fp  = _compute_execution_context_fingerprint($exec_ctx, $model_type);

    # Chain-of-custody: hash of previous bucket's combined fingerprint
    my $chain_fp = 'NONE';
    if (defined $prev_bucket_fp && $prev_bucket_fp ne '' && $prev_bucket_fp ne 'NONE') {
        $chain_fp = Digest::MD5::md5_hex("chain:$prev_bucket_fp");
    }

    # Upstream Dependencies (Phase 4 addition)
    # Defaults to 'NONE' to ensure the hash is stable even if dependencies are empty
    my $dep_fp_str = (defined $dependency_fp && $dependency_fp ne '') ? $dependency_fp : 'NONE';

    # Canonical Assembly
    # We use a hash structure and JSON encoding to ensure deterministic key sorting
    # and clean extensibility without manually managing pipe delimiters.
    my $fingerprint_components = {
        event_def      => $event_def_fp,
        exec_ctx       => $exec_ctx_fp,
        chain          => $chain_fp,
        dependencies   => $dep_fp_str,
        # Ensure engine version is captured if available in scope, else default
        engine_version => $SEASONAL_ENGINE_VERSION // '1.0',
    };

    # Encode with options to ensure stability:
    # canonical (sort keys), utf8, allow_nonref
    my $json_struct = JSON->new->utf8->canonical->allow_nonref->encode($fingerprint_components);

    # Combined fingerprint
    my $combined = Digest::MD5::md5_hex($json_struct);

    return {
        combined       => $combined,
        event_def      => $event_def_fp,
        exec_ctx       => $exec_ctx_fp,
        chain          => $chain_fp,
        dependencies   => $dep_fp_str,
        engine_version => $fingerprint_components->{engine_version},
    };
}

# ==============================================================================
# SUBROUTINE: _load_stored_bucket_fingerprint (Phase 4: Idempotency)
# PURPOSE:    Loads the stored fingerprint metadata for a bucket from history.
#             Uses memoisation to avoid repeated disk reads within the same
#             nfit-profile invocation.
# ARGUMENTS:
#   $system_cache_dir (string) - Path to system cache directory
#   $event_name (string)       - Event name
#   $model_type (string)       - Model identifier
#   $bucket_key (string)       - Bucket key (YYYY-MM)
# RETURNS:
#   Hashref with fingerprint metadata, or undef if not found
# ==============================================================================
{
    # Memoisation cache - scoped to avoid polluting global namespace
    my %_fingerprint_history_cache;

    sub _load_stored_bucket_fingerprint {
        my ($system_cache_dir, $event_name, $model_type, $bucket_key) = @_;


        # ------------------------------------------------------------------
        # Phase 4 DEBUG: lookup probe (only when -v / -vv)
        # Shows exactly what month key we search, and what we find (or don't).
        # ------------------------------------------------------------------
        if (($verbose // 0) >= 1) {
            my $bk_ref = ref($bucket_key) || '';
            my $bucket_preview = $bk_ref ? "$bucket_key" : (defined $bucket_key ? $bucket_key : '(undef)');
            print STDERR "      ↳ [DEBUG] Phase 4 load-stored: event=$event_name model=$model_type bucket_key=$bucket_preview"
                . " bucket_ref=" . ($bk_ref || '(scalar)')
                . "\n";
        }

        # Memoise: load history once per system_cache_dir per invocation
        my $cache_key = $system_cache_dir;
        unless (exists $_fingerprint_history_cache{$cache_key}) {
            $_fingerprint_history_cache{$cache_key} = read_unified_history($system_cache_dir);
        }
        if (($verbose // 0) >= 2) {
            my @months = sort keys %{$_fingerprint_history_cache{$cache_key} // {}};
            print STDERR "      ↳ [DEBUG] Phase 4 load-stored: unified history months=" . scalar(@months)
                . " first=" . ($months[0] // '(none)') . " last=" . ($months[-1] // '(none)') . "\n";
        }

        my $history_data = $_fingerprint_history_cache{$cache_key};
        return undef unless (defined $history_data && ref($history_data) eq 'HASH');

        # Navigate to the stored _meta block
        my $meta = $history_data->{$bucket_key}
                              ->{SeasonalEventSnapshots}
                              ->{$event_name}
                              ->{ModelForecasts}
                              ->{$model_type}
                              ->{'_meta'};

        return undef unless (defined $meta && ref($meta) eq 'HASH');

        # Return fingerprint block if present
        return $meta->{fingerprints} if (exists $meta->{fingerprints});

        return undef;
    }

    # Helper to clear memoisation cache (useful if history is modified mid-run)
    sub _clear_fingerprint_history_cache {
        %_fingerprint_history_cache = ();
    }
}

# ==============================================================================
# SUBROUTINE: _should_reprocess_bucket (Phase 4: Idempotency)
# PURPOSE:    Determines whether a bucket should be reprocessed or skipped.
#             Implements the deterministic skip/redo gate.
# ARGUMENTS:
#   $system_cache_dir (string)  - Path to system cache directory
#   $event_name (string)        - Event name
#   $event_cfg (hashref)        - Event configuration
#   $model_type (string)        - Model identifier
#   $bucket_key (string)        - Bucket key (YYYY-MM)
#   $exec_ctx (hashref)         - Bucket execution context
#   $prev_bucket_fp (string|undef) - Previous bucket's combined fingerprint
#   $force_flag (boolean)       - Whether --force was specified
# RETURNS:
#   Hashref with:
#     reprocess        - Boolean: true if bucket should be reprocessed
#     reason           - String: explanation of decision
#     reason_category  - String: 'skip'|'config_changed'|'rules_changed'|
#                                'chain_changed'|'missing_prior'|'force'
#     current_fp       - Current computed fingerprint hashref
#     stored_fp        - Stored fingerprint hashref (or undef)
# ==============================================================================
sub _should_reprocess_bucket {
    my ($system_cache_dir, $event_name, $event_cfg, $model_type, $bucket_key, $exec_ctx, $prev_bucket_fp, $dependency_fp, $force_flag) = @_;


    # ------------------------------------------------------------------
    # Phase 4 DEBUG: argument sanity probe (only when -v / -vv)
    # This is designed to catch the exact issue you're seeing:
    # bucket_key accidentally arriving as a HASH ref -> "HASH(0x...)"
    # ------------------------------------------------------------------
    if (($verbose // 0) >= 1) {
        my $bk_ref  = ref($bucket_key) || '';
        my $ec_ref  = ref($exec_ctx)   || '';
        my $ev_ref  = ref($event_cfg)  || '';
        my $sc_ref  = ref($seasonality_config) || '';

        my $bucket_preview = $bk_ref ? "$bucket_key" : (defined $bucket_key ? $bucket_key : '(undef)');
        print STDERR "      ↳ [DEBUG] Phase 4 gate args: event=$event_name model=$model_type bucket_key=$bucket_preview"
            . " bucket_ref=" . ($bk_ref || '(scalar)')
            . " exec_ctx_ref=" . ($ec_ref || '(undef)')
            . " event_cfg_ref=" . ($ev_ref || '(undef)')
            . " seasonality_cfg_ref=" . ($sc_ref || '(undef)')
            . "\n";

        if ($bk_ref) {
            print STDERR "      ↳ [DEBUG] Phase 4 gate WARN: bucket_key is a reference; prior fingerprint lookup will never match months like '2025-04'\n";
            if (($verbose // 0) >= 2) {
                require Data::Dumper;
                local $Data::Dumper::Terse  = 1;
                local $Data::Dumper::Indent = 0;
                print STDERR "      ↳ [DEBUG] Phase 4 gate bucket_key dump: " . Data::Dumper::Dumper($bucket_key) . "\n";
            }
        }
    }

    # Compute current fingerprint (always, to ensure provenance is persisted even under --force)
    my $current_fp = _compute_bucket_fingerprint($event_cfg, $exec_ctx, $model_type, $prev_bucket_fp, $dependency_fp);

    # Always reprocess if forced (but still return fingerprint for storage)
    if ($force_flag) {
        return {
            reprocess        => 1,
            reason           => 'User requested --force',
            reason_category  => 'force',
            current_fp       => $current_fp,
            stored_fp        => undef,
        };
    }

    # Load stored fingerprint
    my $stored_fp = _load_stored_bucket_fingerprint($system_cache_dir, $event_name, $model_type, $bucket_key);

    # No prior fingerprint - must process (bootstrap)
    unless (defined $stored_fp) {
        return {
            reprocess        => 1,
            reason           => "No prior fingerprint (bootstrap or first run for bucket $bucket_key)",
            reason_category  => 'missing_prior',
            current_fp       => $current_fp,
            stored_fp        => undef,
        };
    }

    # Compare fingerprints
    my @diff_reasons;

    # Check engine version first (trumps other checks)
    if (($stored_fp->{engine_version} // '') ne ($current_fp->{engine_version} // '')) {
        push @diff_reasons, sprintf("engine version changed (%s -> %s)",
            $stored_fp->{engine_version} // 'unknown',
            $current_fp->{engine_version} // 'unknown');
    }

    # Check Upstream Dependencies
    if (($stored_fp->{dependencies} // 'NONE') ne ($current_fp->{dependencies} // 'NONE')) {
        push @diff_reasons, "upstream history inputs changed (backfill or regeneration)";
    }

    # Check event definition fingerprint (config changes)
    if (($stored_fp->{event_def} // '') ne ($current_fp->{event_def} // '')) {
        push @diff_reasons, "event configuration changed (dates, vms, exclude_dates, etc.)";
    }

    # Check execution context fingerprint (window/anchor changes)
    if (($stored_fp->{exec_ctx} // '') ne ($current_fp->{exec_ctx} // '')) {
        push @diff_reasons, "execution context changed (analysis window or cache bounds)";
    }

    # Check chain-of-custody fingerprint (upstream dependency changed)
    if (($stored_fp->{chain} // '') ne ($current_fp->{chain} // '')) {
        push @diff_reasons, "upstream dependency changed (prior bucket was reprocessed)";
    }

    # If any differences found, reprocess
    if (@diff_reasons) {
        my $reason_category = 'config_changed';
        if (grep { /engine version/ } @diff_reasons) {
            $reason_category = 'rules_changed';
        } elsif (grep { /upstream dependency/ } @diff_reasons) {
            $reason_category = 'chain_changed';
        }

        return {
            reprocess        => 1,
            reason           => join("; ", @diff_reasons),
            reason_category  => $reason_category,
            current_fp       => $current_fp,
            stored_fp        => $stored_fp,
        };
    }

    # All fingerprints match - skip
    return {
        reprocess        => 0,
        reason           => "Fingerprints match (combined: " . substr($current_fp->{combined}, 0, 12) . "...)",
        reason_category  => 'skip',
        current_fp       => $current_fp,
        stored_fp        => $stored_fp,
    };
}

# ==============================================================================
# SUBROUTINE: _resolve_upstream_dependency_fingerprint
# PURPOSE:    Resolves the fingerprints of the historical inputs required for this anchor date.
# Performs lightweight JSON reads of the history files to avoid loading full datasets.
sub _resolve_upstream_dependency_fingerprint {
    my ($cache_path, $event_cfg, $anchor_date_obj) = @_;

    return 'NONE' unless ($cache_path && -d $cache_path);
    require Digest::MD5;

    my @dependency_parts;
    my $target_month_idx = $anchor_date_obj->mon;
    my $model_type = $event_cfg->{model} // 'multiplicative_seasonal';
    my $lookback_days = $event_cfg->{seasonal_lookback_days} // 730;

    # 1. Identify which past months are dependencies
    #    For multiplicative models, we only care about the same calendar month in prior years.
    my $is_month_matched = ($model_type eq 'multiplicative_seasonal');

    # Scan back 5 years (or lookback limit) to find relevant history files
    my $start_year = $anchor_date_obj->year - 5;
    my $end_year   = $anchor_date_obj->year;

    for my $y ($start_year .. $end_year) {
        for my $m (1..12) {
            # Skip future/current
            my $candidate_str = sprintf("%04d-%02d", $y, $m);
            my $candidate_date = Time::Piece->strptime($candidate_str, "%Y-%m");
            next if $candidate_date >= $anchor_date_obj->truncate(to => 'month');

            # Apply Model Logic (Same Month)
            if ($is_month_matched) {
                next unless $m == $target_month_idx;
            }

            # 2. Check if file exists and read its meta fingerprint
            #    Assumes standard file naming: "$cache_path/$candidate_str.json"
            #    (Adjust filename pattern if your system uses a prefix)
            my $file = File::Spec->catfile($cache_path, "$candidate_str.json");

            if (-f $file) {
                my $fp = 'missing_meta';
                # Lightweight Read: We only need the top-level structure or specific event key
                eval {
                    local $/;
                    open(my $fh, '<', $file) or die $!;
                    my $json_text = <$fh>;
                    close $fh;
                    my $data = decode_json($json_text);
                    my $evt = $event_cfg->{_eventName};

                    # Grab the stored fingerprint if it exists
                    # Navigate to: SeasonalEventSnapshots -> {event} -> ModelForecasts -> {model} -> _meta -> fingerprints
                    my $model_meta = $data->{SeasonalEventSnapshots}{$evt}{ModelForecasts}{$model_type}{'_meta'};
                    if ($model_meta && ref($model_meta) eq 'HASH' && $model_meta->{fingerprints}) {
                        # Use the combined fingerprint as the dependency marker
                        $fp = $model_meta->{fingerprints}{combined} // 'missing_combined';
                    }
                };
                push @dependency_parts, "$candidate_str=$fp";
            }
        }
    }

    return 'NONE' unless @dependency_parts;
    return Digest::MD5::md5_hex(join("|", @dependency_parts));
}

# ==============================================================================
# SUBROUTINE: _inject_exclusions_into_manifest
# PURPOSE:    Adds the _exclusions and _exclusion_fingerprint blocks to a
#             manifest for transmission to the nfit engine.
# ==============================================================================
sub _inject_exclusions_into_manifest {
    my ($manifest_href, $exclusions_href) = @_;

    return $manifest_href unless defined $exclusions_href;

    # Validate: no wildcards should be present at this stage
    foreach my $vm (keys %{$exclusions_href->{vm_specific} // {}}) {
        if ($vm =~ /\*/) {
            die "FATAL: Unexpanded wildcard '$vm' in exclusions. " .
                "Wildcards must be expanded before manifest injection.\n";
        }
    }

    # Only inject if there are actual exclusions
    my $has_exclusions = (scalar @{$exclusions_href->{global} // []} > 0) ||
                         (scalar keys %{$exclusions_href->{vm_specific} // {}} > 0);

    if ($has_exclusions) {
        $manifest_href->{_exclusions} = $exclusions_href;
        $manifest_href->{_exclusion_fingerprint} = _compute_exclusion_fingerprint($exclusions_href);
    }

    return $manifest_href;
}

# ==============================================================================
# SUBROUTINE: _group_events_by_fingerprint
# PURPOSE:    Groups events that share identical computational requirements
#             (boundaries + exclusions) to enable nfit pass consolidation.
# ==============================================================================
sub _group_events_by_fingerprint {
    my ($events_aref, $seasonality_config) = @_;

    my %groups;

    foreach my $event_cfg (@$events_aref) {
        my $event_name = $event_cfg->{_eventName};

        # Determine boundaries
        my ($peak_start, $peak_end) = determine_event_period($event_cfg);
        my $baseline_days = $event_cfg->{baseline_period_days} // 16;
        my $baseline_start = $peak_start ? ($peak_start - ($baseline_days * ONE_DAY)) : undef;

        # Parse exclusions (Note: we need known_vms here for full expansion, but for grouping
        # we might just use the raw config if we assume consistent expansion later.
        # However, to be safe, we should probably expand.
        # For now, let's use a simplified fingerprint based on the raw config string if available,
        # or just skip optimisation for now to keep it simple and safe as per instructions.)

        # Actually, let's skip the complex grouping optimisation for this first iteration
        # to ensure safety and correctness, as the user emphasized "production ready" and "safe".
        # We can implement a simple 1-to-1 mapping for now.

        my $fingerprint = $event_name; # Simple unique fingerprint
        push @{$groups{$fingerprint}}, $event_cfg;
    }

    return %groups;
}

# ==============================================================================
# SUBROUTINE: _translate_skip_reason
# PURPOSE:    Converts internal reason categories to user-friendly messages
# ==============================================================================
sub _translate_skip_reason {
    my ($category) = @_;
    return 'unchanged since last analysis'      if $category eq 'skip';
    return 'no changes detected'                if !$category;
    return $category;  # fallback
}

# ==============================================================================
# SUBROUTINE: _translate_change_reason
# PURPOSE:    Converts internal change reasons to user-friendly messages
# ==============================================================================
sub _translate_change_reason {
    my ($category, $detail) = @_;

    return 'configuration changed'              if $category eq 'config_changed' && $detail =~ /event configuration/;
    return 'analysis window changed'            if $category eq 'config_changed' && $detail =~ /execution context/;
    return 'upstream history updated'           if $category eq 'config_changed' && $detail =~ /upstream history/;
    return 'engine version updated'             if $category eq 'rules_changed';
    return 'prior bucket was reprocessed'       if $category eq 'chain_changed';
    return 'changes detected';  # fallback
}

# ==============================================================================
# SUBROUTINE: _execute_automatic_seasonal_discovery
#
# PURPOSE:
#   Orchestrates the execution of seasonal forecasting events as-of a specific
#   analysis reference date.
#
#   This subroutine is the single authoritative entry point for all seasonal
#   model execution in nfit-profile, and is used by both:
#     - `--seasonal`                : (execute all active seasonal events)
#     - `--apply-seasonality <E>`   : (execute a single explicit event, if active)
#
#   PHASE 1 ENHANCEMENT:
#   This function now performs CENTRALISED ANCHOR RESOLUTION before dispatching
#   to model executors. This ensures:
#     - Deterministic anchor assignment across all model types
#     - Correct history partition (anchor bucket) for storage
#     - Consistent time-blinded analysis windows
#
# ARGUMENTS:
#   $seasonality_config   (hashref)
#   $current_cache_path   (string)
#   $profiles_aref        (arrayref)
#   $args_href            (hashref) - includes cache bounds, effective dates, etc.
#   $analysis_anchor_date (Time::Piece | undef)
#   $event_filter         (string | undef)
#
# RETURNS:
#   None.
#
# ==============================================================================
sub _execute_automatic_seasonal_discovery {
    my ($seasonality_config, $current_cache_path, $profiles_aref, $args_href, $analysis_anchor_date, $event_filter) = @_;

    _phase('Seasonal Discovery');

    # 1. Resolve the canonical reference date ONCE
    # This date is used for BOTH eligibility checking AND anchor resolution,
    # ensuring they cannot diverge.
    #
    # Priority:
    #   1. CLI override (--analysis-reference-date / -e)
    #   2. Passed-in anchor date (from caller context)
    #   3. Effective end date from cache/CLI
    #   4. System time (last resort)

    # 1. Determine if CLI override is explicitly present
    my $cli_override_present = (defined $nfit_analysis_reference_date_str
                                && length $nfit_analysis_reference_date_str);
    my $cli_override_date = undef;

    if ($cli_override_present) {
        $cli_override_date = _parse_anchor_date_string($nfit_analysis_reference_date_str);
        unless ($cli_override_date) {
            warn "  [WARN] Could not parse --analysis-reference-date '$nfit_analysis_reference_date_str'\n";
            $cli_override_present = 0;
        }
    }

    # 2. Resolve canonical reference date for ELIGIBILITY checking
    # Priority: CLI override > passed-in anchor > effective end > cache end > now
    my $canonical_reference_date = $cli_override_date
                                // $analysis_anchor_date
                                // $args_href->{effective_end_date}
                                // $args_href->{cache_end_date}
                                // gmtime();

    # 3. Detect active events using the canonical date
    my @active_events = detect_active_events($canonical_reference_date, $seasonality_config);

    # Filter (in case --apply-seasonality was specified)
    if (defined $event_filter && length $event_filter) {
        @active_events = grep { ($_->{_eventName} // '') eq $event_filter } @active_events;

        if (scalar @active_events == 0) {
            print STDERR "  [INFO] Seasonal event $event_filter is not active for reference date (" . $canonical_reference_date->ymd . ")\n";
            return;
        }
    }

    if (scalar @active_events == 0) {
        print STDERR "  [INFO] No active seasonal events found for reference date (" . $canonical_reference_date->ymd . ")\n";
        return;
    }

    print STDERR "  [INFO] Detected " . scalar(@active_events) . " active seasonal event(s): " . join(", ", map { $_->{_eventName} } @active_events) . "\n";


    # ----------------------------------------------------------------------
    # Phase 4 Idempotency Gate: DEBUG PROBE (no behavioural change)
    # Purpose: quickly confirm whether Phase 4 subs exist/are compiled and
    # whether the orchestrator is invoking them.
    # ----------------------------------------------------------------------
    if (($verbose // 0) >= 1) {
        my @need = qw(
            _should_reprocess_bucket
            _compute_bucket_fingerprint
            _load_stored_bucket_fingerprint
        );
        my @present = grep { defined &{$_} } @need;
        my @missing = grep { !defined &{$_} } @need;

        print STDERR "  [DEBUG] Phase 4 probe: idempotency subs present: "
            . (@present ? join(', ', @present) : '(none)')
            . "\n";
        if (@missing) {
            print STDERR "  [DEBUG] Phase 4 probe: idempotency subs missing: "
                . join(', ', @missing)
                . "\n";
            print STDERR "  [DEBUG] Phase 4 probe: gate cannot execute; run will always behave as pre-Phase-4\n";
        }
    }

    # 2. Iterate and execute with CENTRALISED ANCHOR RESOLUTION
    foreach my $event_cfg (@active_events) {
        my $event_name = $event_cfg->{_eventName};
        print STDERR "\n  ⧉ Executing seasonal forecast for event $event_name\n";

        # Phase 3: Anchor buckets
        #
        # If the user explicitly provided --analysis-reference-date, we treat it
        # as an intentional anchor override and execute a single run (no bucket loop).
        #
        # Otherwise, if the event has enumerable occurrences, we bucketise completed
        # occurrences by anchor month (YYYY-MM) and execute either:
        #   - latest bucket only (default)
        #   - all buckets (chronological) when --seasonal-scope all

        my @bucket_keys_to_run;
        my $buckets_href = {};
        my $occurrences_aref = _enumerate_event_occurrences(
            $event_cfg,
            $args_href->{effective_start_date},
            $canonical_reference_date
        );

        if (!$cli_override_present && $occurrences_aref && @$occurrences_aref) {
            $buckets_href = _bucketise_completed_occurrences_by_month($occurrences_aref, $canonical_reference_date);
            my @keys = sort keys %$buckets_href;

            if (!@keys) {
                print STDERR "    [-] Skipping event $event_name (no completed occurrences as-of reference date " . $canonical_reference_date->ymd . ")\n";
                next;
            }

            if ($seasonal_scope eq 'all') {
                @bucket_keys_to_run = @keys; # chronological
            } else {
                @bucket_keys_to_run = ($keys[-1]); # latest only (default)
            }
        } else {
            # Fallback to Phase 2 single-anchor resolution (also covers free-running events)
            @bucket_keys_to_run = ('__single__');
        }

        # Phase 4: Chain-of-custody tracking for idempotency
        # Track the previous bucket's combined fingerprint to establish dependency chain
        my $prev_bucket_combined_fp = undef;

        foreach my $bucket_key (@bucket_keys_to_run) {
            # Reset global state to ensure clean execution per bucket
            _reset_seasonal_state();

            my $anchor_info;

            if ($bucket_key eq '__single__') {
                $anchor_info = _resolve_seasonal_anchor_date(
                    $event_cfg,
                    $args_href->{effective_start_date},
                    $canonical_reference_date,
                    $cli_override_present ? $nfit_analysis_reference_date_str : undef
                );
            } else {
                my $bucket_occ_aref = $buckets_href->{$bucket_key} // [];
                my $bucket_anchor_end = _max_occurrence_end($bucket_occ_aref);

                unless ($bucket_anchor_end) {
                    print STDERR "    [WARN] Could not determine bucket anchor for $event_name ($bucket_key)\n";
                    next;
                }

                # For single-occurrence buckets, calculate the actual event PEAK start
                # (not the calendar month start) for proper window bounding.
                #
                # For month-end events (day_of_period=-1), the occurrence 'start' from
                # _enumerate_monthly_occurrences is the first of the month, but the
                # actual event peak starts (duration_days - 1) days before month end.
                my $actual_peak_start = undef;
                if ($bucket_occ_aref && @$bucket_occ_aref) {
                    my $last_occ = $bucket_occ_aref->[-1];  # Use last occurrence in bucket
                    if ($last_occ && $last_occ->{end}) {
                        my $occ_end = $last_occ->{end}->truncate(to => 'day');
                        my $duration = $event_cfg->{duration_days} // 4;
                        # Peak starts (duration - 1) days before the end
                        # e.g., duration=4: May 31 - 3 = May 28
                        $actual_peak_start = $occ_end - (($duration - 1) * ONE_DAY);
                    }
                }

                # Important: force a scalar context on the truncate() to ensure object context (or else it will return an empty list)
                $anchor_info = {
                    anchor_date    => scalar($bucket_anchor_end->truncate(to => 'day')),
                    anchor_bucket  => $bucket_key,
                    anchor_source  => 'bucket_latest_end',
                    occurrence_start => $actual_peak_start,
                    occurrence_end => scalar($bucket_anchor_end->truncate(to => 'day')),
                    is_complete    => 1,
                    # Phase 3.1: attach the evidence used to form this bucket
                    bucket_occurrences => $bucket_occ_aref,
                };
            }

            # Log anchor resolution
            if ($anchor_info->{anchor_date} && ref($anchor_info->{anchor_date}) eq 'Time::Piece' && $anchor_info->{anchor_date}->epoch > 0) {

                my $anchor_ymd   = $anchor_info->{anchor_date}->ymd;
                my $bucket_month = $anchor_info->{anchor_bucket} // 'unknown';
                my $event_label  = $event_name // 'event';

                # Default: always show the anchoring milestone
                print STDERR "    Φ Anchoring $event_label analysis to reference date $anchor_ymd for history month $bucket_month\n";

                # -v: user-friendly context about what data is being used
                if (($verbose // 0) >= 1) {
                    my $occ_n = 0;
                    if ($anchor_info->{bucket_occurrences} && ref($anchor_info->{bucket_occurrences}) eq 'ARRAY') {
                        $occ_n = scalar @{$anchor_info->{bucket_occurrences}};
                    }
                    # Translate anchor_source to plain English
                    my $src = $anchor_info->{anchor_source} // 'unknown';
                    my $src_desc = $src eq 'bucket_latest_end'   ? 'latest completed occurrence'
                                 : $src eq 'bucket_first_end'    ? 'earliest occurrence'
                                 : $src eq 'bucket_aggregated'   ? 'aggregated occurrences'
                                 : $src;

                    my $occ_label = $occ_n == 1 ? '1 occurrence' : "$occ_n occurrences";
                    print STDERR "      ↳  Analysis basis: $src_desc ($occ_label in bucket)\n";

                    # Show occurrence end dates if multiple
                    if ($occ_n > 1 && $anchor_info->{bucket_occurrences}) {
                        my @end_dates = map { $_->{end} ? $_->{end}->ymd : () } @{$anchor_info->{bucket_occurrences}};
                        print STDERR "      ↳  Occurrence end dates: " . join(', ', @end_dates) . "\n";
                    }
                }

            } else {
                print STDERR "    [WARN] Could not resolve anchor date for event $event_name\n";
                next;
            }

            # Skip incomplete occurrences - do not produce partial/incorrect results
            unless ($anchor_info->{is_complete}) {
                print STDERR "    [-] Skipping event $event_name (occurrence is not complete as-of reference date " . $canonical_reference_date->ymd . ")\n";
                next;
            }

            # Build the execution context with anchor information
            my $exec_ctx = _build_anchor_execution_context($anchor_info, $args_href, $event_cfg);

            # --- PHASE 4: IDEMPOTENCY GATE ---

            # Resolve Upstream Dependencies
            # We explicitly calculate the state of the history required by this bucket.
            # If a history month was backfilled/regenerated, its fingerprint will have changed.
            my $dependency_fp = _resolve_upstream_dependency_fingerprint(
                $current_cache_path,
                $event_cfg,
                $anchor_info->{anchor_date}
            );

            # Check if this bucket needs reprocessing or can be skipped
            my $model_type = $event_cfg->{model} // '';
            my $effective_bucket_key = ($bucket_key eq '__single__')
                ? ($anchor_info->{anchor_bucket} // 'unknown')
                : $bucket_key;

            my $reprocess_decision = _should_reprocess_bucket(
                $current_cache_path,
                $event_name,
                $event_cfg,
                $model_type,
                $effective_bucket_key,
                $exec_ctx,
                $prev_bucket_combined_fp,
                $dependency_fp,
                $force_update  # Use the global --force flag
            );

            if (!$reprocess_decision->{reprocess}) {
                # Default: show skip with user-friendly reason
                # Skip this bucket - fingerprints match
                my $skip_reason = _translate_skip_reason($reprocess_decision->{reason_category});
                print STDERR "    ↳  [SKIP] Bucket $effective_bucket_key: $skip_reason\n";

                # -vv only: show fingerprint details
                if (($verbose // 0) >= 2) {
                    my $fp_short = substr($reprocess_decision->{stored_fp}{combined} // '', 0, 12);
                    print STDERR "      ↳  [DEBUG] Fingerprint: $fp_short...\n";
                }

                # Update chain-of-custody with stored fingerprint for next bucket
                if ($reprocess_decision->{stored_fp} && $reprocess_decision->{stored_fp}{combined}) {
                    $prev_bucket_combined_fp = $reprocess_decision->{stored_fp}{combined};
                }
                next;
            }

            # Log reprocess reason
            my $reason_cat = $reprocess_decision->{reason_category} // 'unknown';
            if ($reason_cat eq 'missing_prior') {
                print STDERR "    ↳  [NEW] First analysis for $effective_bucket_key\n";
            } elsif ($reason_cat eq 'force') {
                print STDERR "    ↳  [FORCE] Reprocessing $effective_bucket_key ($reprocess_decision->{reason})\n";
            } else {
                my $change_desc = _translate_change_reason($reason_cat, $reprocess_decision->{reason});
                print STDERR "    ↳  [UPDATE] Change detected: $effective_bucket_key: $change_desc\n";
            }

            # -vv only: developer diagnostics
            if (($verbose // 0) >= 2) {
                print STDERR "      ↳  [DEBUG] Category: $reason_cat\n";
                print STDERR "      ↳  [DEBUG] Detail: $reprocess_decision->{reason}\n";
                if ($reprocess_decision->{current_fp}) {
                    require Data::Dumper;
                    local $Data::Dumper::Terse = 1;
                    local $Data::Dumper::Indent = 0;
                    print STDERR "      ↳  [DEBUG] Current fingerprint: " . Data::Dumper::Dumper($reprocess_decision->{current_fp}) . "\n";
                }
            }

            # Attach fingerprints to execution context for downstream storage
            $exec_ctx->{_fingerprints} = $reprocess_decision->{current_fp};

            # --- DISPATCH TO MODEL EXECUTOR ---
            if ($model_type eq 'multiplicative_seasonal') {
                # Phase 3.2: if Phase 3.1 bucket provenance exists and we have multiple occurrences,
                # aggregate multiplicative forecasts within the bucket.
                if (exists $anchor_info->{bucket_occurrences}
                    && ref($anchor_info->{bucket_occurrences}) eq 'ARRAY'
                    && scalar(@{ $anchor_info->{bucket_occurrences} }) > 1) {
                    _execute_multiplicative_seasonal_bucket_aggregated(
                        $event_name, $event_cfg, $current_cache_path, $profiles_aref, $exec_ctx, $seasonality_config,
                        $anchor_info->{bucket_occurrences}
                    );
                } else {
                    _execute_multiplicative_seasonal($event_name, $event_cfg, $current_cache_path, $profiles_aref, $exec_ctx, $seasonality_config);
                }
            }
            elsif ($model_type eq 'recency_decay') {
                _execute_recency_decay($event_name, $event_cfg, $current_cache_path, $profiles_aref, $exec_ctx, $seasonality_config);
            }
            elsif ($model_type eq 'predictive_peak') {
                _execute_predictive_peak($event_name, $event_cfg, $current_cache_path, $profiles_aref, $exec_ctx, $seasonality_config);
            }
            else {
                warn "  WARNING: Unknown or unsupported model type '$model_type' for event '$event_name'. Skipping.\n";
            }

            # Phase 4: Update chain-of-custody for next bucket
            if ($exec_ctx->{_fingerprints} && $exec_ctx->{_fingerprints}{combined}) {
                $prev_bucket_combined_fp = $exec_ctx->{_fingerprints}{combined};
            }
        }

    }

    print STDERR "\n✔ Completed Automatic Seasonal Discovery\n";
}

# ==============================================================================
# SUBROUTINE: _reset_seasonal_state
# PURPOSE:    Resets global variables to a clean state between event executions.
# ==============================================================================
sub _reset_seasonal_state {
    # Reset globals used by nfit-profile logic
    $nfit_decay_over_states = 0;
    $nfit_enable_windowed_decay = 0;
    $is_multiplicative_forecast_run = 0;
    $is_predictive_peak_model_run = 0;

    # Clear any other globals that might persist (e.g., from previous runs in the same process)
    # Note: @vm_order and %assimilation_map are usually rebuilt per run, so they should be fine.
}

# ==============================================================================
# SUBROUTINE: _resolve_seasonal_anchor_date
#
# PURPOSE:
#   Determines the appropriate analysis reference date (anchor) for a seasonal
#   event. This is the SINGLE authoritative function for anchor resolution,
#   ensuring deterministic and consistent behaviour across all model types.
#
#   The anchor date is the point in time "as-of" which the analysis is performed.
#   It defines:
#     - The temporal boundary for time-blinded analysis
#     - The decay weight reference point (Time Zero)
#     - The history partition where results are stored
#
# RESOLUTION PRIORITY:
#   1. User-specified CLI argument (--analysis-reference-date / -e)
#   2. Smart default for month-end events (last completed month-end)
#   3. Smart default for date-range events (event end date if completed)
#   4. Fallback to effective end date from cache/CLI
#
# ARGUMENTS:
#   $event_config       (hashref)     - Event configuration from seasonality config
#   $effective_end_date (Time::Piece) - Cache end or CLI-specified end date
#   $cli_reference_date (string|undef)- User-specified reference date (YYYY-MM-DD)
#
# RETURNS:
#   hashref with:
#     anchor_date      => Time::Piece object (the resolved anchor)
#     anchor_bucket    => string (YYYY-MM format for history storage)
#     anchor_source    => string (explanation of how anchor was determined)
#     occurrence_end   => Time::Piece object (event occurrence end, if applicable)
#     is_complete      => boolean (whether the event occurrence is complete)
#
# ==============================================================================
# ==============================================================================
# SUBROUTINE: _enumerate_event_occurrences
#
# PURPOSE:
#   Enumerates candidate occurrences for an event definition within the supplied
#   analysis window. This is a pure calendar operation: it does not decide
#   completion, anchoring, or storage.
#
# ARGUMENTS:
#   $event_config       (hashref)     - Event configuration
#   $analysis_start     (Time::Piece) - Start of analysis window
#   $analysis_end       (Time::Piece) - End of analysis window (also serves as default reference date)
#
# RETURNS:
#   Arrayref of occurrence hashrefs:
#     { start => Time::Piece, end => Time::Piece, kind => 'monthly'|'dates' }
#
# ==============================================================================
sub _enumerate_event_occurrences {
    my ($event_config, $analysis_start, $analysis_end) = @_;

    return [] unless ($event_config && $analysis_start && $analysis_end);

    my $period        = $event_config->{period} // '';
    my $day_of_period = $event_config->{day_of_period} // 0;

    # Month-end events: monthly with day_of_period == -1
    if ($period eq 'monthly' && $day_of_period == -1) {
        return _enumerate_monthly_occurrences($analysis_start, $analysis_end);
    }

    # Explicit date events: dates = ...
    if (defined $event_config->{dates} && length($event_config->{dates})) {
        return _enumerate_explicit_date_occurrences($event_config->{dates}, $analysis_start, $analysis_end);
    }

    # No enumerable occurrences for free-running events
    return [];
}

# ==============================================================================
# SUBROUTINE: _enumerate_monthly_occurrences
#
# PURPOSE:
#   Produces month-end occurrences between the supplied window bounds.
#
# RETURNS:
#   Arrayref of { start => month_start, end => month_end, kind => 'monthly' }
#
# ==============================================================================
sub _enumerate_monthly_occurrences {
    my ($analysis_start, $analysis_end) = @_;

    return [] unless ($analysis_start && $analysis_end);

    my @occ;

    # ------------------------------------------------------------
    # Always include the most recent completed month-end <= analysis_end,
    # even if it falls before analysis_start. This prevents narrow -s windows
    # from accidentally producing "no month-end candidates" mid-month.
    # ------------------------------------------------------------
    my $last_month_end = ($analysis_end->truncate(to => 'month') - 1)->truncate(to => 'day');
    if ($last_month_end->epoch <= $analysis_end->truncate(to => 'day')->epoch) {
        my $last_month_start = $last_month_end->truncate(to => 'month')->truncate(to => 'day');
        push @occ, { start => $last_month_start, end => $last_month_end, kind => 'monthly' };
    }

    my $cursor = $analysis_start->truncate(to => 'month');
    my $end_month = $analysis_end->truncate(to => 'month');

    while ($cursor->epoch <= $end_month->epoch) {
        my $month_start = $cursor->truncate(to => 'day');
        my $month_end   = ($cursor->add_months(1) - 1)->truncate(to => 'day');

        # Only include if the occurrence end is within the analysis window bounds
        if ($month_end->epoch >= $analysis_start->truncate(to => 'day')->epoch &&
            $month_end->epoch <= $analysis_end->truncate(to => 'day')->epoch) {
            push @occ, { start => $month_start, end => $month_end, kind => 'monthly' };
        }

        $cursor = $cursor->add_months(1);
    }

    # De-duplicate in case the "last completed month-end" is also within bounds
    my %seen_end;
    @occ = grep { !$seen_end{ $_->{end}->ymd }++ } @occ;

    return \@occ;
}

################################################################################
# Phase 3 helpers: bucketisation for high-frequency / multi-occurrence events
################################################################################

# ==============================================================================
# SUBROUTINE: _bucketise_completed_occurrences_by_month
#
# PURPOSE:
#   Groups occurrences into anchor buckets (YYYY-MM) based on occurrence end date,
#   keeping only those occurrences completed as-of the supplied reference date.
#
# RETURNS:
#   Hashref: { 'YYYY-MM' => [ {start=>..., end=>..., kind=>...}, ... ], ... }
# ==============================================================================
sub _bucketise_completed_occurrences_by_month {
    my ($occurrences_aref, $reference_date) = @_;

    return {} unless ($occurrences_aref && ref($occurrences_aref) eq 'ARRAY' && @$occurrences_aref);
    return {} unless ($reference_date && ref($reference_date) eq 'Time::Piece');

    my %buckets;
    foreach my $occ (@$occurrences_aref) {
        next unless $occ && $occ->{end} && ref($occ->{end}) eq 'Time::Piece';
        next unless _occurrence_is_completed_asof($occ->{end}, $reference_date);

        my $bucket = $occ->{end}->strftime('%Y-%m');
        push @{$buckets{$bucket}}, $occ;
    }

    return \%buckets;
}

# ==============================================================================
# SUBROUTINE: _max_occurrence_end
#
# PURPOSE:
#   Returns the maximum (latest) occurrence end date from a bucket list.
#
# RETURNS:
#   Time::Piece | undef
# ==============================================================================
sub _max_occurrence_end {
    my ($bucket_occ_aref) = @_;
    return undef unless ($bucket_occ_aref && ref($bucket_occ_aref) eq 'ARRAY' && @$bucket_occ_aref);

    my $max_end = undef;
    foreach my $occ (@$bucket_occ_aref) {
        next unless $occ && $occ->{end} && ref($occ->{end}) eq 'Time::Piece';
        $max_end = $occ->{end} unless defined $max_end;
        $max_end = $occ->{end} if ($occ->{end}->epoch > $max_end->epoch);
    }
    return $max_end;
}

# ==============================================================================
# SUBROUTINE: _enumerate_explicit_date_occurrences
#
# PURPOSE:
#   Parses an explicit 'dates' configuration value into candidate occurrences.
#
# SUPPORTED FORMS:
#   - YYYY-MM-DD
#   - YYYY-MM-DD:YYYY-MM-DD
#   - Comma-separated list of the above
#
# RETURNS:
#   Arrayref of { start => Time::Piece, end => Time::Piece, kind => 'dates' }
#
# ==============================================================================
sub _enumerate_explicit_date_occurrences {
    my ($dates_str, $analysis_start, $analysis_end) = @_;

    return [] unless (defined $dates_str && length($dates_str) && $analysis_start && $analysis_end);

    my @occ;
    my @items = split /\s*,\s*/, $dates_str;

    foreach my $item (@items) {
        next unless defined $item && length $item;

        my ($start_s, $end_s);
        if ($item =~ /:/) {
            ($start_s, $end_s) = split /\s*:\s*/, $item, 2;
        } else {
            $start_s = $item;
            $end_s   = $item;
        }

        my $start_obj = _parse_anchor_date_string($start_s);
        my $end_obj   = _parse_anchor_date_string($end_s);

        next unless ($start_obj && $end_obj);

        $start_obj = $start_obj->truncate(to => 'day');
        $end_obj   = $end_obj->truncate(to => 'day');

        # Normalise inverted ranges defensively
        if ($end_obj->epoch < $start_obj->epoch) {
            my $tmp = $start_obj;
            $start_obj = $end_obj;
            $end_obj = $tmp;
        }

        # Include if the occurrence end is within the analysis window bounds
        if ($end_obj->epoch >= $analysis_start->truncate(to => 'day')->epoch &&
            $end_obj->epoch <= $analysis_end->truncate(to => 'day')->epoch) {
            push @occ, { start => $start_obj, end => $end_obj, kind => 'dates' };
        }
    }

    return \@occ;
}

# ==============================================================================
# SUBROUTINE: _select_latest_completed_occurrence
#
# PURPOSE:
#   Selects the most recent completed occurrence (by end date) as-of the supplied
#   reference date.
#
# RETURNS:
#   Occurrence hashref, or undef if none completed.
#
# ==============================================================================
sub _select_latest_completed_occurrence {
    my ($occurrences_aref, $reference_date) = @_;

    return undef unless ($occurrences_aref && ref($occurrences_aref) eq 'ARRAY' && @$occurrences_aref);
    return undef unless ($reference_date && ref($reference_date) eq 'Time::Piece');

    my @completed = grep {
        $_->{end} && _occurrence_is_completed_asof($_->{end}, $reference_date)
    } @$occurrences_aref;

    return undef unless @completed;

    @completed = sort { $a->{end}->epoch <=> $b->{end}->epoch } @completed;
    return $completed[-1];
}

# ==============================================================================
# SUBROUTINE: _resolve_seasonal_anchor_date
#
# PURPOSE:
#   Determines the appropriate analysis reference date (anchor) for a seasonal
#   event. This is the SINGLE authoritative function for anchor resolution.
#
#   Phase 2: Priority 2/3 are implemented via occurrence enumeration and latest
#   completed occurrence selection (month-end + explicit dates), rather than
#   inline heuristics.
# ==============================================================================
sub _resolve_seasonal_anchor_date {
    my ($event_config, $analysis_start_date, $effective_end_date, $cli_reference_date) = @_;

    my $result = {
        anchor_date    => undef,
        anchor_bucket  => undef,
        anchor_source  => 'unknown',
        occurrence_end => undef,
        is_complete    => 0,
    };

    # Validate inputs
    unless ($event_config && $effective_end_date && ref($effective_end_date) eq 'Time::Piece') {
        warn "  [WARN] _resolve_seasonal_anchor_date: Missing required arguments\n";
        return $result;
    }

    # Default analysis start (needed for occurrence enumeration)
    if (!defined $analysis_start_date || ref($analysis_start_date) ne 'Time::Piece') {
        $analysis_start_date = $effective_end_date->truncate(to => 'day');
    }

    # --- Priority 1: User-specified CLI override ---
    # Explicit CLI override is treated as an intentional anchor and is NOT subject
    # to smart snapping. We still compute occurrence_end for metadata.
    if (defined $cli_reference_date && length $cli_reference_date) {
        my $cli_anchor = _parse_anchor_date_string($cli_reference_date);
        if ($cli_anchor) {
            $cli_anchor = $cli_anchor->truncate(to => 'day');

            $result->{anchor_date}   = $cli_anchor;
            $result->{anchor_bucket} = $cli_anchor->strftime('%Y-%m');
            $result->{anchor_source} = 'cli_override';
            $result->{is_complete}   = 1;

            my ($occ_start, $occ_end) = determine_event_period($event_config, $cli_anchor, 'historical');
            $result->{occurrence_end} = $occ_end if $occ_end;

            return $result;
        }
    }

    # --- Priority 2/3: Event-bound anchoring via occurrence enumeration ---
    # For month-end and explicit 'dates' events, enumerate candidate occurrences
    # within the analysis window and select the latest completed occurrence.
    my $occurrences_aref = _enumerate_event_occurrences($event_config, $analysis_start_date, $effective_end_date);
    if ($occurrences_aref && ref($occurrences_aref) eq 'ARRAY' && @$occurrences_aref) {
        my $latest = _select_latest_completed_occurrence($occurrences_aref, $effective_end_date);

        if ($latest && $latest->{end}) {
            my $anchor = $latest->{end}->truncate(to => 'day');

            $result->{anchor_date}    = $anchor;
            $result->{anchor_bucket}  = $anchor->strftime('%Y-%m');
            $result->{occurrence_start} = $latest->{start} ? $latest->{start}->truncate(to => 'day') : undef;
            $result->{anchor_source}  = 'occurrence_end';
            $result->{occurrence_end} = $anchor;
            $result->{is_complete}    = 1;

            return $result;
        }

        # Occurrences exist, but none completed as-of reference date
        my @sorted = sort { $a->{end}->epoch <=> $b->{end}->epoch } @$occurrences_aref;
        my $most_recent = $sorted[-1];
        $result->{occurrence_end} = $most_recent->{end} if ($most_recent && $most_recent->{end});

        $result->{anchor_date}   = $effective_end_date->truncate(to => 'day');
        $result->{anchor_bucket} = $result->{anchor_date}->strftime('%Y-%m');
        $result->{anchor_source} = 'no_completed_occurrence';
        $result->{is_complete}   = 0;

        return $result;
    }

    # --- Priority 4: Fallback to effective end date for free-running events ---
    $result->{anchor_date}   = $effective_end_date->truncate(to => 'day');
    $result->{anchor_bucket} = $result->{anchor_date}->strftime('%Y-%m');
    $result->{anchor_source} = 'fallback_effective_end';
    $result->{is_complete}   = 1;

    # Best-effort occurrence metadata (may be undef for free-running events)
    my ($occ_start, $occ_end) = determine_event_period($event_config, $effective_end_date, 'historical');
    $result->{occurrence_start} = $occ_start if $occ_start;
    $result->{occurrence_end} = $occ_end if $occ_end;

    return $result;
}

# ==============================================================================
# SUBROUTINE: _occurrence_is_completed_asof
#
# PURPOSE:
#   Determines whether an event occurrence is complete as-of a reference date.
#   An occurrence is complete if its end date is on or before the reference date.
#
# ARGUMENTS:
#   $occurrence_end (Time::Piece) - The end date of the event occurrence
#   $reference_date (Time::Piece) - The date against which to evaluate completion
#
# RETURNS:
#   1 if the occurrence is complete, 0 otherwise
#
# NOTES:
#   - Comparison is done at day granularity (time-of-day is ignored)
#   - An occurrence ending on the reference date IS considered complete
#     (the reference date spans the entire day)
#
# ==============================================================================
sub _occurrence_is_completed_asof {
    my ($occurrence_end, $reference_date) = @_;

    return 0 unless ($occurrence_end && $reference_date);

    # Compare at day granularity
    my $occ_day = $occurrence_end->truncate(to => 'day');
    my $ref_day = $reference_date->truncate(to => 'day');

    # Completed if occurrence end is on or before reference date
    return ($occ_day->epoch <= $ref_day->epoch) ? 1 : 0;
}

# ==============================================================================
# SUBROUTINE: _build_anchor_execution_context
#
# PURPOSE:
#   Builds a complete execution context for a seasonal model run, incorporating
#   the resolved anchor information. This context is passed to model executors.
#
# ARGUMENTS:
#   $anchor_info   (hashref) - Output from _resolve_seasonal_anchor_date
#   $base_args     (hashref) - Base args_href from caller
#   $event_config  (hashref) - Event configuration
#
# RETURNS:
#   hashref with execution context including:
#     - All base args
#     - Resolved anchor date and bucket
#     - Effective analysis window (clamped to anchor)
#     - Anchor metadata for logging/storage
#
# ==============================================================================
sub _build_anchor_execution_context {
    my ($anchor_info, $base_args, $event_config) = @_;

    # Start with a copy of base args
    my %ctx = %$base_args;

    # Inject anchor information
    $ctx{anchor_date}        = $anchor_info->{anchor_date};
    $ctx{anchor_bucket}      = $anchor_info->{anchor_bucket};
    $ctx{anchor_source}      = $anchor_info->{anchor_source};
    $ctx{anchor_is_complete} = $anchor_info->{is_complete};

    # Propagate occurrence boundaries for downstream event-period filtering
    $ctx{occurrence_start}   = $anchor_info->{occurrence_start};
    $ctx{occurrence_end}     = $anchor_info->{occurrence_end};

    # Clamp effective_end_date to anchor if anchor is earlier
    # This ensures time-blinded analysis: we cannot use data beyond the anchor
    if ($anchor_info->{anchor_date} && $ctx{effective_end_date}) {
        if ($anchor_info->{anchor_date}->epoch < $ctx{effective_end_date}->epoch) {
            $ctx{effective_end_date} = $anchor_info->{anchor_date};
        }
    }

    # Clamp effective_start_date based on occurrence start and baseline_period_days
    # This prevents prior event occurrences from contaminating the analysis window
    if ($anchor_info->{occurrence_start} && $event_config) {
        my $baseline_days = $event_config->{baseline_period_days} // 16;

        # Analysis should start at: occurrence_start - baseline_period_days
        # This gives enough room for baseline calculation without including prior events
        my $bounded_start = $anchor_info->{occurrence_start} - ($baseline_days * ONE_DAY);

        # Only tighten the window; never expand beyond original bounds
        if ($ctx{effective_start_date} && $bounded_start->epoch > $ctx{effective_start_date}->epoch) {
            $ctx{effective_start_date} = $bounded_start;
        }
    }

    # Compute the effective analysis window string for the nfit engine
    $ctx{analysis_reference_date_str} = $anchor_info->{anchor_date}
        ? $anchor_info->{anchor_date}->ymd
        : $ctx{effective_end_date}->ymd;

    # ----------------------------------------------------------------------
    # Phase 3.1: bucket provenance (auditability only; does not alter maths)
    # ----------------------------------------------------------------------
    if ($anchor_info->{bucket_occurrences} && ref($anchor_info->{bucket_occurrences}) eq 'ARRAY') {
        $ctx{anchor_bucket_key} = $anchor_info->{anchor_bucket};
        $ctx{bucket_occurrences} = $anchor_info->{bucket_occurrences};
        $ctx{bucket_occurrence_count} = scalar @{$anchor_info->{bucket_occurrences}};
        $ctx{bucket_occurrence_end_dates} = [
            map { ($_->{end} && ref($_->{end}) eq 'Time::Piece') ? $_->{end}->ymd : () }
            @{$anchor_info->{bucket_occurrences}}
        ];
    }

    return \%ctx;
}

# ==============================================================================
# SUBROUTINE: _parse_anchor_date_string
#
# PURPOSE:
#   Safely parses a date string (YYYY-MM-DD format) into a Time::Piece object.
#
# ARGUMENTS:
#   $date_str (string) - Date in YYYY-MM-DD format
#
# RETURNS:
#   Time::Piece object on success, undef on failure
#
# ==============================================================================
sub _parse_anchor_date_string {
    my ($date_str) = @_;

    return undef unless (defined $date_str && $date_str =~ /^\d{4}-\d{2}-\d{2}$/);

    my $parsed;
    eval {
        $parsed = Time::Piece->strptime($date_str, '%Y-%m-%d');
    };

    return $@ ? undef : $parsed;
}

# ==============================================================================
# SUBROUTINE: _execute_multiplicative_seasonal
# PURPOSE:    Executes the Multiplicative Seasonal model for a specific event.
# ==============================================================================
sub _execute_multiplicative_seasonal {
    my ($event_name, $event_config, $current_cache_path, $profiles_aref, $exec_ctx, $seasonality_config, $opts_href) = @_;

    # Phase 3.2: Output control flags (suppressed during intra-bucket aggregation)
    my $store_history  = 1;
    my $emit_csv       = 1;
    my $emit_rationale = 1;

    if ($opts_href && ref($opts_href) eq 'HASH') {
        $store_history  = ($opts_href->{store_history}  // 1) ? 1 : 0;
        $emit_csv       = ($opts_href->{emit_csv}       // 1) ? 1 : 0;
        $emit_rationale = ($opts_href->{emit_rationale} // 1) ? 1 : 0;
    }

    # 1. Determine analysis path (history check) & Load Config
    my $effective_event_name = determine_seasonal_analysis_path(
        $event_config, $current_cache_path, $event_name
    );

    # Handle cold-start graceful skip
    unless (defined $effective_event_name) {
        print STDERR "  [-] Skipping forecast for event $event_name (cold start; execution of `--update-history` is required)\n";
        return;
    }

    my $effective_config = $event_config;
    if ($effective_event_name ne $event_name) {
        $effective_config = $seasonality_config->{$effective_event_name} // $event_config;
        print STDERR "  [INFO] Falling back to event '$effective_event_name' (due to history constraints)\n";
    }

    # 2. Setup Flags
    $is_multiplicative_forecast_run = 1;
    $apply_seasonality_event = $event_name; # Set global for logging context

    # 3. Prepare Exclusions
    my $exclusions = undef;
    if (defined $effective_config->{exclude_dates}) {
        my $known_vms = _get_known_vms_from_cache($current_cache_path);
        $exclusions = _parse_exclusion_dates($effective_config->{exclude_dates}, $known_vms);
    }

    # 4. Run nFit Analysis
    my $parsed_results = run_single_pass_analysis(
        $current_cache_path,
        $profiles_aref,
        {
            %$exec_ctx,
            enable_windowed_decay   => 1,  # Required for growth/baseline stats
            decay_over_states       => 0,  # Ensure we decay over TIME, not state changes
            enable_growth_prediction => 1, # Must be True if windowed_decay is True
            vm_name => undef,
            exclusions => $exclusions,
            start_date => $exec_ctx->{effective_start_date}->ymd,
            end_date   => $exec_ctx->{effective_end_date}->ymd,
        }
    );

    # 5. Assimilate Results
    my $assimilation_map = build_assimilation_map($parsed_results, $profiles_aref, $exec_ctx->{adaptive_saturation_thresh});
    @vm_order = sort keys %{$assimilation_map};

    # 5a. Phase 3.3: Apply VM scope filter if configured (supports 'vms' and 'exclude_vms')
    my $vm_scope_filter = _parse_vm_scope_filter($event_config);
    my $vm_scope_meta = _apply_vm_scope_filter(
        $vm_scope_filter,
        \@vm_order,
        $event_name,
        $assimilation_map  # Use assimilation_map keys as cache VM set
    );

    # Early exit if all VMs filtered out
    if ($vm_scope_meta && scalar(@vm_order) == 0) {
        print STDERR "  [-] Skipping forecast storage for event '$event_name' (no matching VMs after scope filter)\n";
        return {};
    }

    # 5b. Correct PeakValue to reflect EVENT PERIOD only, not baseline period
    # The engine window may include baseline days (for efficiency), but PeakValue
    # must be the maximum observed during the actual event occurrence(s).
    my $event_start_epoch = $exec_ctx->{occurrence_start}
        ? $exec_ctx->{occurrence_start}->epoch
        : $exec_ctx->{effective_end_date}->epoch - (($effective_config->{duration_days} // 1) * 86400);
    my $event_end_epoch = $exec_ctx->{effective_end_date}->epoch;

    foreach my $vm_name (@vm_order) {
        my $states_aref = $parsed_results->{$vm_name};
        next unless ref($states_aref) eq 'ARRAY' && @$states_aref;

        my @event_period_peaks;
        foreach my $state (@$states_aref) {
            # Check if this state falls within the event period
            my $state_end_str = _safe_dig($state, 'metadata', 'end_date');
            next unless $state_end_str;

            my $state_end_obj;
            eval { $state_end_obj = Time::Piece->strptime($state_end_str, '%Y-%m-%d'); };
            next if $@ || !$state_end_obj;

            # Only include states that end within or after the event start
            if ($state_end_obj->epoch >= $event_start_epoch) {
                my $peak_val = _safe_dig($state, 'metrics', 'physc', $MANDATORY_PEAK_PROFILE_FOR_HINT, 'Peak');
                push @event_period_peaks, $peak_val if (defined $peak_val && looks_like_number($peak_val));
            }
        }

        # Override PeakValue with max from event period only
        if (@event_period_peaks) {
            $assimilation_map->{$vm_name}{CoreResults}{PeakValue} = max(@event_period_peaks);
        }
    }

    # 6. Calculate Forecast
    my ($forecast_results, $historic_data) = calculate_multiplicative_forecast(
        $current_cache_path,
        $exec_ctx->{system_identifier},
        $effective_event_name,
        $effective_config,
        $seasonality_config,
        $assimilation_map,
        $exclusions,
        $exec_ctx->{effective_start_date},
        $exec_ctx->{effective_end_date}
    );

    # Merge Forecast Results (preserving event-period measurement values)
    # The assimilation_map was built from the engine run on the event window.
    # PeakValue and P-99W1 in assimilation_map represent the EVENT PERIOD observations.
    # We must preserve these before overwriting ProfileValues with forecast results.

    # Pre-extract P-99W1 metric key for fallback extraction from parsed_results
    my $p99w1_p_metric_key;
    my ($p99w1_profile) = grep { $_->{name} eq $MANDATORY_PEAK_PROFILE_FOR_HINT } @$profiles_aref;
    if ($p99w1_profile) {
        my ($p_val_num) = $p99w1_profile->{flags} =~ /(?:-p|--percentile)\s+([0-9.]+)/;
        $p99w1_p_metric_key = "P" . clean_perc_label($p_val_num // $DEFAULT_PERCENTILE);
    }

    foreach my $vm_name (@vm_order) {
        # Preserve measurement values BEFORE overwriting
        my $preserved_peak_value = $assimilation_map->{$vm_name}{CoreResults}{PeakValue};
        my $preserved_p99w1 = $assimilation_map->{$vm_name}{CoreResults}{ProfileValues}{$MANDATORY_PEAK_PROFILE_FOR_HINT};

        # Fallback: If P-99W1 not in assimilation_map, extract directly from parsed_results
        if (!defined $preserved_p99w1 && $p99w1_p_metric_key) {
            my $states_aref = $parsed_results->{$vm_name};
            if (ref($states_aref) eq 'ARRAY' && @$states_aref) {
                my @p99w1_values;
                foreach my $state (@$states_aref) {
                    # Try aggregated format first (FinalValue)
                    my $val = _safe_dig($state, 'metrics', 'physc', $MANDATORY_PEAK_PROFILE_FOR_HINT, 'FinalValue');
                    # Fallback to P-metric key
                    $val //= _safe_dig($state, 'metrics', 'physc', $MANDATORY_PEAK_PROFILE_FOR_HINT, $p99w1_p_metric_key);
                    push @p99w1_values, $val if (defined $val && looks_like_number($val));
                }
                $preserved_p99w1 = max(@p99w1_values) if @p99w1_values;
            }
        }

        # Merge forecasted profile values (overwrites ProfileValues hash)
        if (exists $forecast_results->{$vm_name}) {
            $assimilation_map->{$vm_name}{CoreResults}{ProfileValues} = $forecast_results->{$vm_name};
        }

        # Restore PeakValue (immutable historical fact - not a forecast)
        if (defined $preserved_peak_value && looks_like_number($preserved_peak_value)) {
            $assimilation_map->{$vm_name}{CoreResults}{PeakValue} = $preserved_peak_value;
        }

        # Restore P-99W1 (measurement profile - exempt from forecasting)
        if (defined $preserved_p99w1 && looks_like_number($preserved_p99w1)) {
            $assimilation_map->{$vm_name}{CoreResults}{ProfileValues}{$MANDATORY_PEAK_PROFILE_FOR_HINT} = $preserved_p99w1;
        }
    }

    # DEBUG: Trace PeakValue source
    if (($verbose // 0) > 1) {
        my $sample_vm = (keys %$assimilation_map)[0];
        if ($sample_vm && ($opts_href->{emit_csv} // 1)) {
            my $peak_val = $assimilation_map->{$sample_vm}{CoreResults}{PeakValue};
            print STDERR "  [DEBUG] CSV-emitting run for bucket: " . ($exec_ctx->{anchor_bucket} // 'N/A') . "\n";
            print STDERR "  [DEBUG] effective_start_date: " . ($exec_ctx->{effective_start_date} ? $exec_ctx->{effective_start_date}->ymd : 'undef') . "\n";
            print STDERR "  [DEBUG] effective_end_date: " . ($exec_ctx->{effective_end_date} ? $exec_ctx->{effective_end_date}->ymd : 'undef') . "\n";
            print STDERR "  [DEBUG] occurrence_start: " . ($exec_ctx->{occurrence_start} ? $exec_ctx->{occurrence_start}->ymd : 'undef') . "\n";
            print STDERR "  [DEBUG] Sample VM '$sample_vm' PeakValue: " . ($peak_val // 'undef') . "\n";
        }
    }

    # --- ENRICHMENT LOOP ---
    foreach my $vm_name (@vm_order) {
        my $vm_map_ref = $assimilation_map->{$vm_name};
        $vm_map_ref->{Configuration}{vm_name} = $vm_name;
        $vm_map_ref->{RunQMetrics} //= $per_profile_runq_metrics{$vm_name} || {};

        # Generate Hints
        my ($hint_type_tier, $hint_pattern_shape, $hint_pressure_bool, $pressure_detail_str, $rationale, $has_abs, $has_norm) =
            generate_sizing_hint(
                $vm_map_ref,
                undef,
                $exec_ctx->{adaptive_saturation_thresh}
            );

        # Store Hints & Globals
        $vm_map_ref->{Hinting}{AutoTier}       = $hint_type_tier;
        $vm_map_ref->{Hinting}{Pattern}        = $hint_pattern_shape;
        $vm_map_ref->{Hinting}{Pressure}       = $hint_pressure_bool;
        $vm_map_ref->{Hinting}{PressureDetail} = $pressure_detail_str // 'N/A';
        $hint_tier_for_csv{$vm_name}           = $hint_type_tier;
        $hint_pattern_for_csv{$vm_name}        = $hint_pattern_shape;

        # Populate FinalTierForVM (Critical for STD logic)
        $vm_map_ref->{Hinting}{FinalTierForVM} = $vm_tier_overrides{$vm_name} // $hint_type_tier;
        $tier_override_for_csv{$vm_name}       = $vm_tier_overrides{$vm_name} // "";

        # Prepare for RunQ Modifiers
        my $user_tier = $vm_tier_overrides{$vm_name} // "";
        my $pattern_source = ($user_tier ne "") ? $user_tier : $hint_type_tier;
        my ($pattern_char) = ($pattern_source =~ /^([A-Z])/);
        $pattern_char //= 'G';
        my %pat_map = ('O'=>'O3-95W15', 'B'=>'B3-95W15', 'G'=>'G3-95W15', 'P'=>'G3-95W15');
        my $runq_source_profile_name = $pat_map{$pattern_char} // 'G3-95W15';

        foreach my $profile (@$profiles_aref) {
            my $p_name = $profile->{name};

            # --- GUARD: Skip RunQ Modifiers for P-99W1 ---
            next if ($p_name eq $MANDATORY_PEAK_PROFILE_FOR_HINT);

            my %pressure_flags = ( abs_pressure => $has_abs, norm_pressure => $has_norm );
            my %adaptive_thresholds = ( saturation => $exec_ctx->{adaptive_saturation_thresh}, target => $exec_ctx->{adaptive_target_norm_runq}, max_reduction => $exec_ctx->{adaptive_max_efficiency_reduction} );

            my ($adjusted_physc, $debug_info_ref) = calculate_runq_modified_physc(
                $vm_name, $vm_map_ref, $profile, \%pressure_flags, \%adaptive_thresholds
            );

            if (looks_like_number($adjusted_physc)) {
                $vm_map_ref->{CoreResults}{ProfileValues}{$p_name} = sprintf("%.4f", $adjusted_physc);
            }

            if ($p_name eq $runq_source_profile_name) {
                $vm_map_ref->{CSVModifiers}{RunQ_Tactical}  = $debug_info_ref->{'FinalAdditive'};
                $vm_map_ref->{CSVModifiers}{RunQ_Strategic} = $debug_info_ref->{'RunQ_Strategic'};
                $vm_map_ref->{CSVModifiers}{RunQ_Potential} = $debug_info_ref->{'RunQ_Potential'};
                $vm_map_ref->{CSVModifiers}{RunQ_Source}    = $runq_source_profile_name;
            }
        }
    }

    # 7. Generate CSV (Rolling Numbering)
    # Phase 3.2: suppressed during aggregation iterations; anchor_bucket embedded for backfill
    if ($emit_csv) {
        my $date_str = localtime->strftime('%Y%m%d');
        my $anchor_bucket = $exec_ctx->{anchor_bucket} // '';

        # Prefix anchor_bucket to unique_suffix for backfill disambiguation
        my $unique_suffix;
        if ($anchor_bucket =~ /^\d{4}-\d{2}$/) {
            $unique_suffix = "$anchor_bucket.$date_str";
        } else {
            $unique_suffix = $date_str;
        }

        # Embed anchor_bucket in filename for backfill disambiguation
        my $base_name = "nfit-profile." . ($exec_ctx->{system_identifier}//'') . "." . $event_name;
        $base_name =~ s/[^a-zA-Z0-9_.-]//g;

        my $counter = 0;
        while (-e File::Spec->catfile($output_dir, "$base_name.$unique_suffix.csv")) {
            $counter++;
            if ($anchor_bucket =~ /^\d{4}-\d{2}$/) {
                $unique_suffix = "$anchor_bucket.$date_str-$counter";
            } else {
                $unique_suffix = "$date_str-$counter";
            }
        }

        _write_standard_csv_report(
            $assimilation_map,
            $event_name,
            $exec_ctx->{system_identifier},
            $unique_suffix,
            1, # is_multiplicative
            0, 0,
            $exec_ctx->{adaptive_saturation_thresh},
            $anchor_bucket  # Pass anchor_bucket for potential column inclusion
        );

        # Verbose Output Files
        if ($exec_ctx->{verbose}) {
            print STDERR "  ◆ Audit Trail\n";

            # Reconstruct baseline data map from debug info
            my %baseline_data_for_verbose;
            foreach my $vm (keys %seasonal_debug_info) {
                foreach my $prof (keys %{$seasonal_debug_info{$vm}}) {
                    $baseline_data_for_verbose{$vm}{$prof} = $seasonal_debug_info{$vm}{$prof}{baseline};
                }
            }

            write_seasonal_csv_output("current_baseline", $exec_ctx->{system_identifier}, $event_name, $unique_suffix, \%baseline_data_for_verbose);
            write_seasonal_csv_output("historic_snapshot", $exec_ctx->{system_identifier}, $event_name, $unique_suffix, $historic_data);
        }
   }

    # Log Rationale (Phase 3.2: suppressed during aggregation iterations)
    if ($emit_rationale && defined $open_log_files{$exec_ctx->{system_identifier}}) {
        my $ctx = {
            start       => $exec_ctx->{effective_start_date},
            end         => $exec_ctx->{effective_end_date},
            cache_start => $exec_ctx->{cache_start_date},
            cache_end   => $exec_ctx->{cache_end_date},
            interval    => $exec_ctx->{sampling_interval}
        };
        log_multiplicative_seasonal_rationale($open_log_files{$exec_ctx->{system_identifier}}, $effective_config, $ctx);
    }

    # Determine Forecast Horizon for Metadata (anchored to effective end date)
    my $asof_end = $exec_ctx->{effective_end_date} // $exec_ctx->{cache_end_date};
    my ($next_start, $next_end) = determine_event_period($effective_config, $asof_end, 'forecast');
    my $horizon_meta = undef;

    if ($next_end) {
        my $ref_time = (defined $asof_end && ref($asof_end) eq 'Time::Piece')
            ? $asof_end
            : Time::Piece->new();

        my $days_diff = int(($next_end->epoch - $ref_time->epoch) / 86400);
        $days_diff = 0 if $days_diff < 0;

        $horizon_meta = {
            target_date => $next_end->ymd,
            days        => $days_diff
        };
    }

    # Build the context object from existing args
    my $analysis_context = {
        analysis_start => time(),
        start       => $exec_ctx->{effective_start_date}, # Time::Piece object
        end         => $exec_ctx->{effective_end_date},   # Time::Piece object
        cache_start => $exec_ctx->{cache_start_date},
        cache_end   => $exec_ctx->{cache_end_date},
        interval    => $exec_ctx->{sampling_interval}, # Integer
        # PHASE 1: Include anchor bucket for deterministic storage
        anchor_bucket   => $exec_ctx->{anchor_bucket},
        anchor_source   => $exec_ctx->{anchor_source},
        # Phase 3.1: bucket provenance
        bucket_occurrence_count     => $exec_ctx->{bucket_occurrence_count},
        bucket_occurrence_end_dates => $exec_ctx->{bucket_occurrence_end_dates},
        # Phase 3.3: VM scope metadata (undef if no filtering applied)
        vm_scope => $vm_scope_meta,
    };

    # Store Forecast
    if ($store_history) {
        _store_model_forecast_to_history(
            $current_cache_path,
            $effective_event_name,
            'multiplicative_seasonal',
            $forecast_results,
            $effective_config,
            $horizon_meta,
            $analysis_context,
            $exec_ctx->{anchor_bucket},
            $exec_ctx->{_fingerprints}  # Phase 4: Idempotency fingerprints
        );
    }
    # For aggregated mode, return the raw forecast hashref to caller
    return $forecast_results;
}

# ==============================================================================
# SUBROUTINE: _execute_recency_decay
# PURPOSE:    Executes the Recency Decay model for a specific event.
# ==============================================================================
sub _execute_recency_decay {
    my ($event_name, $event_config, $current_cache_path, $profiles_aref, $args_href, $seasonality_config) = @_;

    # 1. Setup Logic
    $apply_seasonality_event = $event_name;

    # Recency decay cannot run with state-based decay
    if ($args_href->{decay_over_states}) {
        warn "    Skipping Recency Decay for '$event_name': Cannot run with --decay-over-states.\n";
        return;
    }

    my $asof_start = $args_href->{effective_start_date} // $args_href->{cache_start_date};
    my $asof_end = $args_href->{effective_end_date} // $args_href->{cache_end_date};

    # "Relative to our analysis end, when is the NEXT event?"
    my ($event_start_obj, $event_end_obj) = determine_event_period($event_config, $asof_end, 'forecast');

    unless ($event_start_obj && $event_end_obj) {
        warn "    Skipping Recency Decay for '$event_name': Could not determine event period.\n";
        return;
    }

    # --- PHASE 1: USE ORCHESTRATOR-PROVIDED ANCHOR ---
    # The anchor is resolved by _execute_automatic_seasonal_discovery and passed
    # via $args_href. No special handling needed here — all models receive the
    # same anchored context.

    my $anchor_date   = $args_href->{anchor_date};
    my $anchor_bucket = $args_href->{anchor_bucket} // '';
    my $anchor_source = $args_href->{anchor_source} // 'orchestrator';

    unless ($anchor_date) {
        warn "    [ERROR] No anchor context is available\n";
        return;
    }

    # Set the global for decay weight calculations (Time Zero reference)
    $nfit_analysis_reference_date_str = $anchor_date->ymd;

    # The effective_end_date is already clamped to anchor by the orchestrator
    my $run_end_date_obj = $args_href->{effective_end_date};
    my $run_end_date     = $run_end_date_obj->ymd;

    # 2. Prepare Exclusions
    my $exclusions = undef;
    if (defined $event_config->{exclude_dates}) {
        my $known_vms = _get_known_vms_from_cache($current_cache_path);
        $exclusions = _parse_exclusion_dates($event_config->{exclude_dates}, $known_vms);
    }

    # 3. Run Analysis
    # CRITICAL: The 'analysis_reference_date' defines "Time Zero" for decay weights.
    # It must be the Effective End Date (Current), NOT the Event Date (Future).
    my $parsed_results = run_single_pass_analysis(
        $current_cache_path,
        $profiles_aref,
        {
            %$args_href,
            enable_windowed_decay => 1,
            decay_over_states => 0,
            enable_growth_prediction => 1, # Required for valid decay stats

            analysis_reference_date => $nfit_analysis_reference_date_str,
            exclusions => $exclusions,

            # Use the calculated effective analysis dates (strings) from the main loop
            start_date => $asof_start->ymd,
            end_date => $run_end_date
        }
    );

    # 4. Assimilate
    my $assimilation_map = build_assimilation_map($parsed_results, $profiles_aref, $args_href->{adaptive_saturation_thresh});
    @vm_order = sort keys %{$assimilation_map};

    # 4a. Phase 3.3: Apply VM scope filter if configured (supports 'vms' and 'exclude_vms')
    my $vm_scope_filter = _parse_vm_scope_filter($event_config);
    my $vm_scope_meta = _apply_vm_scope_filter(
        $vm_scope_filter,
        \@vm_order,
        $event_name,
        $assimilation_map  # Use assimilation_map keys as cache VM set
    );

    # Early exit if all VMs filtered out
    if ($vm_scope_meta && scalar(@vm_order) == 0) {
        print STDERR "  [-] Skipping forecast storage for event '$event_name' (no matching VMs after scope filter)\n";
        return {};
    }

    # 5. Log Global Configuration (Once)
    my $log_fh = $open_log_files{$args_href->{system_identifier}};
    if ($log_fh) {
        my $ctx = {
            start       => $asof_start,
            end         => $run_end_date_obj,  # anchored end used for decay/time-zero
            cache_start => $args_href->{cache_start_date},
            cache_end   => $args_href->{cache_end_date},
            interval    => $args_href->{sampling_interval}
        };
        _log_seasonal_config_context($log_fh, $event_config, $ctx);
    }

    # 6. Enrichment, Logging, and Result Extraction
    my %recency_results_for_history;

    foreach my $vm_name (@vm_order) {
        my $vm_map_ref = $assimilation_map->{$vm_name};
        $vm_map_ref->{Configuration}{vm_name} = $vm_name;
        $vm_map_ref->{RunQMetrics} //= $per_profile_runq_metrics{$vm_name} || {};

        # Generate Hints
        my ($hint_type_tier, $hint_pattern_shape, $hint_pressure_bool, $pressure_detail_str, $rationale, $has_abs, $has_norm) =
            generate_sizing_hint(
                $vm_map_ref,
                $log_fh,
                $args_href->{adaptive_saturation_thresh}
            );

        # Store Hints & Globals
        $vm_map_ref->{Hinting}{AutoTier}       = $hint_type_tier;
        $vm_map_ref->{Hinting}{Pattern}        = $hint_pattern_shape;
        $vm_map_ref->{Hinting}{Pressure}       = $hint_pressure_bool;
        $vm_map_ref->{Hinting}{PressureDetail} = $pressure_detail_str // 'N/A';
        $hint_tier_for_csv{$vm_name}           = $hint_type_tier;
        $hint_pattern_for_csv{$vm_name}        = $hint_pattern_shape;
        $tier_override_for_csv{$vm_name}       = $vm_tier_overrides{$vm_name} // "";

        # --- Determine Source Profile for Standardised Modifiers ---
        my $user_tier = $vm_tier_overrides{$vm_name} // "";
        my $pattern_source = ($user_tier ne "") ? $user_tier : $hint_type_tier;
        my ($pattern_char) = ($pattern_source =~ /^([A-Z])/);
        $pattern_char //= 'G';
        my %pat_map = ('O'=>'O3-95W15', 'B'=>'B3-95W15', 'G'=>'G3-95W15', 'P'=>'G3-95W15');
        my $runq_source_profile_name = $pat_map{$pattern_char} // 'G3-95W15';

        # --- Pre-calculate CSVModifiers using the Source Profile ---
        # This must happen BEFORE the profile loop so all profiles can use RunQ_Potential.
        my ($runq_source_profile_obj) = grep { $_->{name} eq $runq_source_profile_name } @$profiles_aref;

        if ($runq_source_profile_obj) {
            my %pressure_flags_src = ( abs_pressure => $has_abs, norm_pressure => $has_norm );
            my %adaptive_thresholds_src = (
                saturation    => $args_href->{adaptive_saturation_thresh},
                target        => $args_href->{adaptive_target_norm_runq},
                max_reduction => $args_href->{adaptive_max_efficiency_reduction}
            );

            my (undef, $debug_info_ref_src) = calculate_runq_modified_physc(
                $vm_name, $vm_map_ref, $runq_source_profile_obj,
                \%pressure_flags_src, \%adaptive_thresholds_src
            );

            # Populate CSVModifiers for use by all profiles
            $vm_map_ref->{CSVModifiers}{RunQ_Tactical}  = $debug_info_ref_src->{'FinalAdditive'};
            $vm_map_ref->{CSVModifiers}{RunQ_Strategic} = $debug_info_ref_src->{'RunQ_Strategic'};
            $vm_map_ref->{CSVModifiers}{RunQ_Potential} = $debug_info_ref_src->{'RunQ_Potential'};
            $vm_map_ref->{CSVModifiers}{RunQ_Source}    = $runq_source_profile_name;
        }

        # --- Set Standardised Growth Metadata for Logging ---
        my $standard_growth_adj = $vm_map_ref->{Growth}{adjustments}{$runq_source_profile_name} // 0;
        $vm_map_ref->{Growth}{StandardAdjustment} = {
            Value  => $standard_growth_adj,
            Source => $runq_source_profile_name
        };

        # --- Apply Standardised Modifiers to All Profiles ---
        my $runq_potential = $vm_map_ref->{CSVModifiers}{RunQ_Potential} // 0;

        foreach my $profile (@$profiles_aref) {
            my $p_name = $profile->{name};

            # --- GUARD: Skip modifiers for P-99W1 (Peak Analysis Profile) ---
            if ($p_name eq $MANDATORY_PEAK_PROFILE_FOR_HINT) {
                if ($log_fh) {
                   my $base_physc = $vm_map_ref->{CoreResults}{ProfileValues}{$p_name};
                   log_peak_profile_rationale($log_fh, $vm_map_ref, $profile, $base_physc);
                }
                next;
            }

            # Get base value (pre-growth) and apply standardised adjustments
            my $base_val = $vm_map_ref->{Growth}{base_values}{$p_name};
            my $base_physc_for_log = $vm_map_ref->{CoreResults}{ProfileValues}{$p_name};

            if (defined $base_val && looks_like_number($base_val)) {
                # Final = Base + StandardGrowth + RunQ_Potential
                my $final_val = $base_val + $standard_growth_adj + $runq_potential;
                $vm_map_ref->{CoreResults}{ProfileValues}{$p_name} = sprintf("%.4f", $final_val);
            }

            my $final_csv_value = $vm_map_ref->{CoreResults}{ProfileValues}{$p_name} // "N/A";

            # Inline Rationale Logging (still call calculate_runq_modified_physc for debug_info)
            if ($log_fh) {
                my %pressure_flags = ( abs_pressure => $has_abs, norm_pressure => $has_norm );
                my %adaptive_thresholds = (
                    saturation    => $args_href->{adaptive_saturation_thresh},
                    target        => $args_href->{adaptive_target_norm_runq},
                    max_reduction => $args_href->{adaptive_max_efficiency_reduction}
                );

                my (undef, $debug_info_ref) = calculate_runq_modified_physc(
                    $vm_name, $vm_map_ref, $profile, \%pressure_flags, \%adaptive_thresholds
                );

                my $raw_states_aref_for_log = $vm_map_ref->{RawNfitStates} || [];
                log_profile_rationale(
                    $log_fh, $vm_map_ref, $profile,
                    $base_physc_for_log, $final_csv_value, $debug_info_ref,
                    $raw_states_aref_for_log,
                    $args_href->{adaptive_saturation_thresh}
                );
            }
        }

        # Extract for History
        if (exists $vm_map_ref->{CoreResults}{ProfileValues}) {
             $recency_results_for_history{$vm_name} = { %{$vm_map_ref->{CoreResults}{ProfileValues}} };
        }
    }

    # 7. Generate CSV (Rolling Numbering)
    # Phase 3.2: anchor_bucket prefixed to unique_suffix for backfill disambiguation
    my $date_str = localtime->strftime('%Y%m%d');

    # Prefix anchor_bucket to unique_suffix for backfill disambiguation
    my $unique_suffix;
    if ($anchor_bucket =~ /^\d{4}-\d{2}$/) {
        $unique_suffix = "$anchor_bucket.$date_str";
    } else {
        $unique_suffix = $date_str;
    }

    my $base_name = "nfit-profile." . ($args_href->{system_identifier}//'') . "." . $event_name;
    $base_name =~ s/[^a-zA-Z0-9_.-]//g;

    my $counter = 0;
    while (-e File::Spec->catfile($output_dir, "$base_name.$unique_suffix.csv")) {
        $counter++;
        if ($anchor_bucket =~ /^\d{4}-\d{2}$/) {
            $unique_suffix = "$anchor_bucket.$date_str-$counter";
        } else {
            $unique_suffix = "$date_str-$counter";
        }
    }

    _write_standard_csv_report(
        $assimilation_map,
        $event_name,
        $args_href->{system_identifier},
        $unique_suffix,
        0,
        1, # is_recency_decay
        0,
        $args_href->{adaptive_saturation_thresh},
        $anchor_bucket  # Pass anchor_bucket for potential column inclusion
    );

    # 3. Horizon Metadata Calculation
    # We use the event_end_obj (e.g., May 31) derived earlier from the Event Config.
    # This ensures the metadata in history matches the "Time Machine" intent, not wall-clock time.

    my $horizon_meta = undef;

    if (defined $event_end_obj && defined $run_end_date_obj) {
        # Calculate days between Analysis Anchor (Apr 30) and Target (May 31)
        my $days_diff = int(($event_end_obj->epoch - $run_end_date_obj->epoch) / 86400);
        $days_diff = 0 if $days_diff < 0;

        $horizon_meta = {
            target_date => $event_end_obj->ymd,
            days        => $days_diff
        };
    }

    # Fallback (Safety only - should rarely be hit if determine_event_period works)
    # Note: We do NOT default to localtime() here to avoid pollution.
    # If we can't determine the target, it's better to have undef meta than wrong meta.

    # Build the context object from existing args
    my $analysis_context = {
        analysis_start  => time(),
        start       => $asof_start,         # Time::Piece object
        end         => $run_end_date_obj,   # Time::Piece object
        cache_start => $args_href->{cache_start_date},
        cache_end   => $args_href->{cache_end_date},
        interval    => $args_href->{sampling_interval}, # Integer
        # PHASE 1: Include anchor bucket for deterministic storage
        anchor_bucket   => $args_href->{anchor_bucket} // $anchor_bucket,
        anchor_source   => $args_href->{anchor_source} // $anchor_source,
        # Phase 3.1: bucket provenance
        bucket_occurrence_count     => $args_href->{bucket_occurrence_count},
        bucket_occurrence_end_dates => $args_href->{bucket_occurrence_end_dates},
        # Phase 3.3: VM scope metadata (undef if no filtering applied)
        vm_scope => $vm_scope_meta,
    };

    # 8. Store Forecast
    _store_model_forecast_to_history(
        $current_cache_path,
        $event_name,
        'recency_decay',
        \%recency_results_for_history,
        $event_config,
        $horizon_meta,
        $analysis_context,
        $args_href->{anchor_bucket},
        $args_href->{_fingerprints}  # Phase 4: Idempotency fingerprints
    );

    # Phase 3.2: allow orchestration to aggregate results
    return \%recency_results_for_history;
}

# ==============================================================================
# SUBROUTINE: _execute_predictive_peak
# PURPOSE:    Executes the Predictive Peak model for a specific event.
# ==============================================================================
sub _execute_predictive_peak {
    my ($event_name, $event_config, $current_cache_path, $profiles_aref, $args_href, $seasonality_config) = @_;

    # 1. Determine analysis path
    my $effective_event_name = determine_seasonal_analysis_path(
        $event_config, $current_cache_path, $event_name
    );

    # Handle cold-start graceful skip
    unless (defined $effective_event_name) {
        print STDERR "  [-] Skipping predictive peak event '$event_name' (cold start - run --update-history first)\n";
        return;
    }

    my $effective_config = $event_config;
    if ($effective_event_name ne $event_name) {
        $effective_config = $seasonality_config->{$effective_event_name} // $event_config;
        print STDERR "  [INFO] Falling back to event '$effective_event_name' (due to history constraints)\n";
    }

    $is_predictive_peak_model_run = 1;
    $apply_seasonality_event = $event_name;

    # 2. Exclusions
    my $exclusions = undef;
    if (defined $effective_config->{exclude_dates}) {
        my $known_vms = _get_known_vms_from_cache($current_cache_path);
        $exclusions = _parse_exclusion_dates($effective_config->{exclude_dates}, $known_vms);
    }

    # 3. Run Analysis
    my $parsed_results = run_single_pass_analysis(
        $current_cache_path,
        $profiles_aref,
        {
            %$args_href,
            # Use the calculated effective analysis dates from the main loop
            start_date              => $args_href->{effective_start_date}->ymd,
            end_date                => $args_href->{effective_end_date}->ymd,
            enable_windowed_decay   => 1,  # Critical for trend detection
            decay_over_states       => 0,
            enable_growth_prediction => 1,
            exclusions => $exclusions,
        }
    );

    # 4. Assimilate
    my $assimilation_map = build_assimilation_map($parsed_results, $profiles_aref, $args_href->{adaptive_saturation_thresh});
    @vm_order = sort keys %{$assimilation_map};

    # 4a. Phase 3.3: Apply VM scope filter if configured (supports 'vms' and 'exclude_vms')
    my $vm_scope_filter = _parse_vm_scope_filter($event_config);
    my $vm_scope_meta = _apply_vm_scope_filter(
        $vm_scope_filter,
        \@vm_order,
        $event_name,
        $assimilation_map  # Use assimilation_map keys as cache VM set
    );

    # Early exit if all VMs filtered out
    if ($vm_scope_meta && scalar(@vm_order) == 0) {
        print STDERR "  [-] Skipping forecast storage for event '$event_name' (no matching VMs after scope filter)\n";
        return;
    }

    # 5. Calculate Forecast
    my $forecast_results = calculate_predictive_peak_forecast(
        $current_cache_path,
        $args_href->{system_identifier},
        $effective_event_name,
        $effective_config,
        $seasonality_config,
        $args_href->{adaptive_saturation_thresh},
        $exclusions,
        $args_href->{effective_start_date},
        $args_href->{effective_end_date}
    );

    # Merge Forecast
    foreach my $vm_name (@vm_order) {
        if (exists $forecast_results->{$vm_name}) {
             $assimilation_map->{$vm_name}{CoreResults}{ProfileValues} = $forecast_results->{$vm_name};
        }
    }

    # 6. Enrichment
    foreach my $vm_name (@vm_order) {
        my $vm_map_ref = $assimilation_map->{$vm_name};
        $vm_map_ref->{Configuration}{vm_name} = $vm_name;
        $vm_map_ref->{RunQMetrics} //= $per_profile_runq_metrics{$vm_name} || {};

        my ($hint_type_tier, $hint_pattern_shape, $hint_pressure_bool, $pressure_detail_str, $rationale, $has_abs, $has_norm) =
            generate_sizing_hint($vm_map_ref, undef, $args_href->{adaptive_saturation_thresh});

        $vm_map_ref->{Hinting}{AutoTier}       = $hint_type_tier;
        $vm_map_ref->{Hinting}{Pattern}        = $hint_pattern_shape;
        $vm_map_ref->{Hinting}{Pressure}       = $hint_pressure_bool;
        $vm_map_ref->{Hinting}{PressureDetail} = $pressure_detail_str // 'N/A';
        $hint_tier_for_csv{$vm_name}           = $hint_type_tier;
        $hint_pattern_for_csv{$vm_name}        = $hint_pattern_shape;
        $tier_override_for_csv{$vm_name}       = $vm_tier_overrides{$vm_name} // "";

        my $user_tier = $vm_tier_overrides{$vm_name} // "";
        my $pattern_source = ($user_tier ne "") ? $user_tier : $hint_type_tier;
        my ($pattern_char) = ($pattern_source =~ /^([A-Z])/);
        $pattern_char //= 'G';
        my %pat_map = ('O'=>'O3-95W15', 'B'=>'B3-95W15', 'G'=>'G3-95W15', 'P'=>'G3-95W15');
        my $runq_source_profile_name = $pat_map{$pattern_char} // 'G3-95W15';

        foreach my $profile (@$profiles_aref) {
            my $p_name = $profile->{name};

            # --- GUARD: Skip RunQ Modifiers for P-99W1 ---
            next if ($p_name eq $MANDATORY_PEAK_PROFILE_FOR_HINT);

            my %pressure_flags = ( abs_pressure => $has_abs, norm_pressure => $has_norm );
            my %adaptive_thresholds = ( saturation => $args_href->{adaptive_saturation_thresh}, target => $args_href->{adaptive_target_norm_runq}, max_reduction => $args_href->{adaptive_max_efficiency_reduction} );

            my ($adjusted_physc, $debug_info_ref) = calculate_runq_modified_physc(
                $vm_name, $vm_map_ref, $profile, \%pressure_flags, \%adaptive_thresholds
            );
            if (looks_like_number($adjusted_physc)) {
                $vm_map_ref->{CoreResults}{ProfileValues}{$p_name} = sprintf("%.4f", $adjusted_physc);
            }

            if ($p_name eq $runq_source_profile_name) {
                $vm_map_ref->{CSVModifiers}{RunQ_Tactical}  = $debug_info_ref->{'FinalAdditive'};
                $vm_map_ref->{CSVModifiers}{RunQ_Strategic} = $debug_info_ref->{'RunQ_Strategic'};
                $vm_map_ref->{CSVModifiers}{RunQ_Potential} = $debug_info_ref->{'RunQ_Potential'};
                $vm_map_ref->{CSVModifiers}{RunQ_Source}    = $runq_source_profile_name;
            }
        }

        # 7. Peak Value logic
        # For multiplicative_seasonal: PeakValue was set by build_assimilation_map from
        # the raw peak tracker (peakPhyscFromLatestState or max of P-99W1 Peak metrics).
        # This is an immutable historical fact and must NOT be modified.
        # The 'TrueBaseline'/'PredictedPeak' fields are specific to predictive_peak model.
        #
        # No action needed here - CoreResults.PeakValue is already correct.
    }

    # 8. Generate CSV (Rolling Numbering)
    # Phase 3.2: anchor_bucket prefixed to unique_suffix for backfill disambiguation
    my $date_str = localtime->strftime('%Y%m%d');
    my $anchor_bucket = $args_href->{anchor_bucket} // '';

    # Prefix anchor_bucket to unique_suffix for backfill disambiguation
    my $unique_suffix;
    if ($anchor_bucket =~ /^\d{4}-\d{2}$/) {
        $unique_suffix = "$anchor_bucket.$date_str";
    } else {
        $unique_suffix = $date_str;
    }

    my $base_name = "nfit-profile." . ($args_href->{system_identifier}//'') . "." . $event_name;
    $base_name =~ s/[^a-zA-Z0-9_.-]//g;

    my $counter = 0;
    while (-e File::Spec->catfile($output_dir, "$base_name.$unique_suffix.csv")) {
        $counter++;
        if ($anchor_bucket =~ /^\d{4}-\d{2}$/) {
            $unique_suffix = "$anchor_bucket.$date_str-$counter";
        } else {
            $unique_suffix = "$date_str-$counter";
        }
    }

    # Log Rationale
    if (defined $open_log_files{$args_href->{system_identifier}}) {
        my $ctx = {
            start    => $args_href->{effective_start_date},
            end      => $args_href->{effective_end_date},
            cache_start    => $args_href->{cache_start_date},
            cache_end      => $args_href->{cache_end_date},
            interval => $args_href->{sampling_interval}
        };
        log_predictive_peak_rationale($open_log_files{$args_href->{system_identifier}}, $effective_config, $ctx);
    }

    _write_standard_csv_report(
        $assimilation_map,
        $event_name,
        $args_href->{system_identifier},
        $unique_suffix,
        0,
        0,
        1, # is_predictive_peak
        $args_href->{adaptive_saturation_thresh},
        $anchor_bucket  # Pass anchor_bucket for potential column inclusion
    );

    # Determine Forecast Horizon for Metadata (anchored to effective end date)
    my ($next_start, $next_end) = determine_event_period($effective_config, $args_href->{effective_end_date}, 'forecast');
    my $horizon_meta = undef;

    if ($next_end) {
        my $ref_time = (defined $args_href->{effective_end_date} && ref($args_href->{effective_end_date}) eq 'Time::Piece') ? $args_href->{effective_end_date} : Time::Piece->new();

        my $days_diff = int(($next_end->epoch - $ref_time->epoch) / 86400);
        $days_diff = 0 if $days_diff < 0;

        $horizon_meta = {
            target_date => $next_end->ymd,
            days        => $days_diff
        };
    }

    # Build the context object from existing args
    my $analysis_context = {
        analysis_start => time(),
        start       => $args_href->{effective_start_date}, # Must be Time::Piece object
        end         => $args_href->{effective_end_date},   # Must be Time::Piece object
        cache_start => $args_href->{cache_start_date},
        cache_end   => $args_href->{cache_end_date},
        interval    => $args_href->{sampling_interval}, # Integer
        # PHASE 1: Include anchor bucket for deterministic storage
        anchor_bucket   => $args_href->{anchor_bucket},
        anchor_source   => $args_href->{anchor_source},
        # Phase 3.1: bucket provenance (optional)
        bucket_occurrence_count     => $args_href->{bucket_occurrence_count},
        bucket_occurrence_end_dates => $args_href->{bucket_occurrence_end_dates},
        # Phase 3.3: VM scope metadata (undef if no filtering applied)
        vm_scope => $vm_scope_meta,
    };

    # 9. Store Forecast
    _store_model_forecast_to_history(
        $current_cache_path,
        $effective_event_name,
        'predictive_peak',
        $forecast_results,
        $effective_config,
        $horizon_meta,
        $analysis_context,
        $args_href->{anchor_bucket},
        $args_href->{_fingerprints}  # Phase 4: Idempotency fingerprints
    );
}

# ==============================================================================
# SUBROUTINE: _resolve_effective_window
# PURPOSE: Determines the actual start and end dates for the analysis.
#          Clamps requested dates (CLI) to the available data (Cache).
# ==============================================================================
sub _resolve_effective_window {
    my ($cache_start, $cache_end, $cli_start, $cli_end) = @_;

    # 1. Default to cache bounds
    my $eff_start = $cache_start;
    my $eff_end   = $cache_end;

    # 2. Apply CLI Start (Clamp to Cache End if necessary, though validation handles that)
    if (defined $cli_start) {
        $eff_start = $cli_start;
        # If user asks for start before cache, clamp to cache start?
        # Ideally, we allow the engine to handle it, but for "effective" logic:
        if ($eff_start < $cache_start) {
            warn " [WARN] Requested --startdate $cli_start is before cache start (using " . $cache_start->ymd . ").\n";
            $eff_start = $cache_start;
        }
    }

    # 3. Apply CLI End
    if (defined $cli_end) {
        $eff_end = $cli_end;
        if ($eff_end > $cache_end) {
            warn " [WARN] Requested --enddate $cli_end is after cache end (using " . $cache_end->ymd . ").\n";
            $eff_end = $cache_end;
        }
    }

    # 4. Safety Check: Start > End
    if ($eff_start > $eff_end) {
        die "[ERROR] Effective start date (" . $eff_start->ymd . ") is after effective end date (" . $eff_end->ymd . "). Check your -s/-e flags against the cache range.\n";
    }

    return ($eff_start, $eff_end);
}
###############################################################################
# Phase 3.2 – Intra-Event Aggregation (Within-Bucket)
#
# Only multiplicative_seasonal is aggregated across multiple occurrences
# within a bucket. Other models remain "run once per bucket end".
#
# Adds:
#   - aggregation_method (median default; supports max/mean/p75)
#   - synthesis metadata (min/max/stddev/sample_size/method)
###############################################################################

sub _resolve_aggregation_method {
    my ($event_cfg, $seasonality_config) = @_;

    # Per-event override
    if (defined $event_cfg->{aggregation_method} && length $event_cfg->{aggregation_method}) {
        return lc $event_cfg->{aggregation_method};
    }

    # Optional global default (if your loader provides it)
    # this code is deprecated (inheritance is now performed early)
    if ($seasonality_config
        && ref($seasonality_config) eq 'HASH'
        && $seasonality_config->{Global}
        && ref($seasonality_config->{Global}) eq 'HASH'
        && defined $seasonality_config->{Global}{aggregation_method}
        && length $seasonality_config->{Global}{aggregation_method}) {
        return lc $seasonality_config->{Global}{aggregation_method};
    }

    return 'median';
}

sub _parse_vm_scope_filter {
    my ($event_config) = @_;

    return undef unless (defined $event_config && ref($event_config) eq 'HASH');

    # VM scope inheritance and resolution are performed in parse_seasonality_config.
    # At this stage we only return the already-resolved, already-parsed filter.
    my $vm_scope_filter = $event_config->{_vm_scope_filter};
    return undef unless ($vm_scope_filter && ref($vm_scope_filter) eq 'HASH');

    return $vm_scope_filter;
}

# ==============================================================================
# SUBROUTINE: _apply_vm_scope_filter
# PURPOSE:    Phase 3.3 - Applies VM scope filtering to @vm_order array.
#             Supports both inclusion ('vms') and exclusion ('exclude_vms').
#             Logs filter application and missing VM warnings.
#             Returns metadata hashref for AnalysisContext, or undef if no filter.
# ARGUMENTS:
#   $vm_scope_filter - hashref from _parse_vm_scope_filter (or undef)
#                      Structure: { include => {...}, exclude => {...} }
#   $vm_order_aref   - reference to @vm_order array (modified in place)
#   $event_name      - string, for logging context
#   $cache_vm_set    - hashref of all VMs present in cache (for missing VM detection)
# RETURNS:
#   hashref with vm_scope metadata, or undef if no filtering applied
# FILTER PRECEDENCE:
#   1. Start with all VMs from @vm_order
#   2. If 'include' is specified, filter DOWN to only those VMs
#   3. If 'exclude' is specified, SUBTRACT those VMs from the result
# ==============================================================================
sub _apply_vm_scope_filter {
    my ($vm_scope_filter, $vm_order_aref, $event_name, $cache_vm_set) = @_;

    # No filter configured - return undef (no metadata needed)
    return undef unless ($vm_scope_filter && ref($vm_scope_filter) eq 'HASH');

    my $include_filter = $vm_scope_filter->{include};
    my $exclude_filter = $vm_scope_filter->{exclude};

    # Both empty - no filtering
    return undef unless ($include_filter || $exclude_filter);

    my $before_count = scalar(@$vm_order_aref);
    my @filter_actions;

    # Step 1: Apply include filter (whitelist)
    if ($include_filter && ref($include_filter) eq 'HASH' && %$include_filter) {
        my $include_count = scalar(keys %$include_filter);

        # Check for configured include VMs not present in the cache
        if ($cache_vm_set && ref($cache_vm_set) eq 'HASH') {
            foreach my $configured_vm (sort keys %$include_filter) {
                unless ($cache_vm_set->{$configured_vm}) {
                    print STDERR "    [SCOPE] Warning: VM '$configured_vm' in 'vms' not found in cache for event '$event_name'\n";
                }
            }
        }

        @$vm_order_aref = grep { $include_filter->{$_} } @$vm_order_aref;
        push @filter_actions, "included $include_count";
    }

    # Step 2: Apply exclude filter (blacklist)
    if ($exclude_filter && ref($exclude_filter) eq 'HASH' && %$exclude_filter) {
        my $exclude_count = scalar(keys %$exclude_filter);
        my $excluded_actual = 0;

        my @filtered;
        foreach my $vm (@$vm_order_aref) {
            if ($exclude_filter->{$vm}) {
                $excluded_actual++;
            } else {
                push @filtered, $vm;
            }
        }
        @$vm_order_aref = @filtered;

        # Log warning for exclude VMs not in the current set (informational only)
        if ($cache_vm_set && ref($cache_vm_set) eq 'HASH') {
            foreach my $configured_vm (sort keys %$exclude_filter) {
                unless ($cache_vm_set->{$configured_vm}) {
                    print STDERR "    [SCOPE] Info: VM '$configured_vm' in 'exclude_vms' not present in cache for event '$event_name'\n";
                }
            }
        }

        push @filter_actions, "excluded $exclude_count ($excluded_actual matched)";
    }

    my $after_count = scalar(@$vm_order_aref);

    # Log filter application summary (include inheritance source if available)
    my $action_summary = join(', ', @filter_actions);

    # Append source info if available
    my @source_notes;
    if ($vm_scope_filter->{include_source}) {
        push @source_notes, "vms from " . $vm_scope_filter->{include_source};
    }
    if ($vm_scope_filter->{exclude_source}) {
        push @source_notes, "exclude_vms from " . $vm_scope_filter->{exclude_source};
    }
    my $source_info = @source_notes ? ' [' . join(', ', @source_notes) . ']' : '';

    if ($after_count == 0) {
        print STDERR "    [-] Warning: No VMs remain after filtering for event '$event_name' ($action_summary)$source_info - no forecasts will be generated\n";
    } else {
        print STDERR "    [-] Filtered to $after_count VM(s) for event '$event_name' ($action_summary, from $before_count)$source_info\n";
    }

    # Build metadata for AnalysisContext
    my $vm_scope_meta = {
        type             => ($include_filter && $exclude_filter) ? 'include_and_exclude'
                         : ($include_filter) ? 'explicit_include'
                         : 'explicit_exclude',
        include_configured => ($include_filter ? scalar(keys %$include_filter) : 0),
        exclude_configured => ($exclude_filter ? scalar(keys %$exclude_filter) : 0),
        matched_count      => $after_count,
        vms                => [ sort @$vm_order_aref ],
        # Store raw config for fingerprinting (downstream consumers can re-hash)
        include_list       => ($include_filter ? [ sort keys %$include_filter ] : undef),
        exclude_list       => ($exclude_filter ? [ sort keys %$exclude_filter ] : undef),
    };

    return $vm_scope_meta;
}

sub _aggregate_numeric {
    my ($vals_aref, $method) = @_;
    return undef unless ($vals_aref && ref($vals_aref) eq 'ARRAY');

    my @v = grep { defined($_) && looks_like_number($_) } @$vals_aref;
    return undef unless @v;

    $method = lc($method // 'median');

    if ($method eq 'max') {
        my $m = $v[0];
        for my $x (@v) { $m = $x if $x > $m; }
        return $m;
    }
    elsif ($method eq 'mean') {
        my $sum = 0;
        $sum += $_ for @v;
        return $sum / @v;
    }
    elsif ($method eq 'p75') {
        @v = sort { $a <=> $b } @v;
        my $idx = int(0.75 * (@v - 1) + 0.5);
        $idx = 0 if $idx < 0;
        $idx = $#v if $idx > $#v;
        return $v[$idx];
    }
    else { # median default
        @v = sort { $a <=> $b } @v;
        my $n = @v;
        return $v[int($n/2)] if ($n % 2 == 1);
        return ($v[$n/2 - 1] + $v[$n/2]) / 2.0;
    }
}

sub _stddev_sample {
    my ($vals_aref) = @_;
    return undef unless ($vals_aref && ref($vals_aref) eq 'ARRAY');

    my @v = grep { defined($_) && looks_like_number($_) } @$vals_aref;
    return undef unless @v >= 2;

    my $sum = 0;
    $sum += $_ for @v;
    my $mean = $sum / @v;

    my $ss = 0;
    for my $x (@v) {
        my $d = $x - $mean;
        $ss += ($d * $d);
    }
    return sqrt($ss / (@v - 1));
}

sub _synthesis_summary {
    my ($vals_aref, $method) = @_;
    my @v = grep { defined($_) && looks_like_number($_) } @{ $vals_aref // [] };
    return undef unless @v;

    my $n = scalar(@v);
    my $min = $v[0];
    my $max = $v[0];
    for my $x (@v) {
        $min = $x if $x < $min;
        $max = $x if $x > $max;
    }

    my $std = _stddev_sample(\@v);

    # Phase 3.2: Confidence quality flagging per design specification
    # Low confidence if: N < 2, or coefficient of variation > 0.5 (high dispersion)
    my $confidence = 'High';
    if ($n < 2) {
        $confidence = 'Low';  # Insufficient sample size for robust aggregation
    }
    elsif (defined $std && $std > 0) {
        # Calculate CV using the midpoint as a denominator proxy
        my $midpoint = ($min + $max) / 2;
        if ($midpoint > 0) {
            my $cv = $std / $midpoint;
            $confidence = 'Low' if ($cv > 0.5);  # High dispersion indicates unstable signal
        }
    }

    return {
        method       => $method,
        sample_size  => $n,
        min_observed => $min + 0,
        max_observed => $max + 0,
        confidence   => $confidence,
        (defined $std ? (std_dev => $std + 0) : ()),
    };
}

sub _execute_multiplicative_seasonal_bucket_aggregated {
    my ($event_name, $event_cfg, $current_cache_path, $profiles_aref, $bucket_exec_ctx, $seasonality_config, $bucket_occ_aref) = @_;

    # Aggregration of event per bucket (month)
    # "Intra-Month Aggregation" example -> if an event happens 4 times in a month (e.g., Weekly Batch)
    my $method = _resolve_aggregation_method($event_cfg, $seasonality_config);

    # Defensive: if only one occurrence, fall back to normal single execution.
    my $occ_n = ($bucket_occ_aref && ref($bucket_occ_aref) eq 'ARRAY') ? scalar(@$bucket_occ_aref) : 0;
    if ($occ_n <= 1) {
        _execute_multiplicative_seasonal($event_name, $event_cfg, $current_cache_path, $profiles_aref, $bucket_exec_ctx, $seasonality_config);
        return;
    }

    print STDERR "    Φ Multiplicative: aggregating $occ_n $event_name event occurrence(s) in bucket " . ($bucket_exec_ctx->{anchor_bucket} // 'unknown') . " (method: $method)\n";

    # Collect per-occurrence forecast values and multipliers
    # Phase 3.2: collect all component fields for complete history storage
    my %forecast_samples;       # vm -> profile -> [forecast, ...]
    my %multiplier_samples;     # vm -> profile -> [multiplier, ...]
    my %baseline_samples;       # vm -> profile -> [baseline, ...]
    my %trend_samples;          # vm -> profile -> [trend_factor, ...]
    my %volatility_samples;     # vm -> profile -> [volatility, ...]
    my %residual_samples;       # vm -> profile -> [forecasted_residual, ...]
    my %amplification_samples;  # vm -> profile -> [amplification_factor, ...]

    # We run the model once per occurrence end-date, but do NOT write history.
    foreach my $occ (@$bucket_occ_aref) {
        next unless ($occ && ref($occ) eq 'HASH');
        next unless ($occ->{end} && ref($occ->{end}) eq 'Time::Piece');

        my $occ_end = $occ->{end}->truncate(to => 'day');
        my $occ_start = ($occ->{start} && ref($occ->{start}) eq 'Time::Piece')
            ? scalar($occ->{start}->truncate(to => 'day'))
            : $occ_end;  # Fallback to end if start missing

        # Build a per-occurrence anchor_info that stays within the same bucket
        my $occ_anchor_info = {
            anchor_date    => $occ_end,
            anchor_bucket  => $bucket_exec_ctx->{anchor_bucket},
            anchor_source  => 'occurrence_end',
            occurrence_start => $occ_start,
            occurrence_end => $occ_end,
            is_complete    => 1,
        };

        my $occ_exec_ctx = _build_anchor_execution_context($occ_anchor_info, $bucket_exec_ctx, $event_cfg);

        # Ensure per-occurrence run is isolated from previous run state.
        _reset_seasonal_state();
        %seasonal_debug_info = ();  # important: prevent leakage across runs

        my $forecast_href = _execute_multiplicative_seasonal(
            $event_name, $event_cfg, $current_cache_path, $profiles_aref, $occ_exec_ctx, $seasonality_config,
            { store_history => 0, emit_csv => 0, emit_rationale => 0 }  # Suppress all intermediate outputs
        );

        next unless ($forecast_href && ref($forecast_href) eq 'HASH' && scalar(keys %$forecast_href) > 0);

        # Capture forecast values
        foreach my $vm (keys %$forecast_href) {
            next unless ref($forecast_href->{$vm}) eq 'HASH';
            foreach my $profile (keys %{ $forecast_href->{$vm} }) {
                my $val = $forecast_href->{$vm}{$profile};
                next unless defined($val) && looks_like_number($val);
                push @{ $forecast_samples{$vm}{$profile} }, ($val + 0);
            }
        }

        # Collect all component fields from seasonal_debug_info
        foreach my $vm (keys %seasonal_debug_info) {
            next unless ref($seasonal_debug_info{$vm}) eq 'HASH';
            foreach my $profile (keys %{ $seasonal_debug_info{$vm} }) {
                my $d = $seasonal_debug_info{$vm}{$profile};
                next unless (ref($d) eq 'HASH');

                # Collect multiplier
                my $m = $d->{multiplier};
                push @{ $multiplier_samples{$vm}{$profile} }, ($m + 0)
                    if (defined $m && looks_like_number($m));

                # Collect baseline
                my $b = $d->{baseline};
                push @{ $baseline_samples{$vm}{$profile} }, ($b + 0)
                    if (defined $b && looks_like_number($b));

                # Collect trend_factor
                my $t = $d->{trend_factor};
                push @{ $trend_samples{$vm}{$profile} }, ($t + 0)
                    if (defined $t && looks_like_number($t));

                # Collect volatility
                my $v = $d->{volatility};
                push @{ $volatility_samples{$vm}{$profile} }, ($v + 0)
                    if (defined $v && looks_like_number($v));

                # Collect forecasted_residual
                my $r = $d->{forecasted_residual};
                push @{ $residual_samples{$vm}{$profile} }, ($r + 0)
                    if (defined $r && looks_like_number($r));

                # Collect amplification_factor
                my $a = $d->{amplification_factor};
                push @{ $amplification_samples{$vm}{$profile} }, ($a + 0)
                    if (defined $a && looks_like_number($a));
            }
        }
    }

    # Aggregate into one forecast per VM/profile
    my %aggregated_forecast;
    foreach my $vm (keys %forecast_samples) {
        foreach my $profile (keys %{ $forecast_samples{$vm} }) {
            my $agg = _aggregate_numeric($forecast_samples{$vm}{$profile}, $method);
            next unless defined $agg;
            $aggregated_forecast{$vm}{$profile} = $agg + 0;
        }
    }

    # Phase 3.3: Apply VM scope filter to aggregated results (supports 'vms' and 'exclude_vms')
    # Filter is applied AFTER aggregation to ensure synthesis stats reflect all occurrences
    my $vm_scope_filter = _parse_vm_scope_filter($event_cfg);
    my $vm_scope_meta = undef;

    if ($vm_scope_filter) {
        my $include_filter = $vm_scope_filter->{include};
        my $exclude_filter = $vm_scope_filter->{exclude};

        if ($include_filter || $exclude_filter) {
            my @all_vms = sort keys %aggregated_forecast;
            my $before_count = scalar(@all_vms);
            my @filter_actions;

            # Step 1: Apply include filter (whitelist)
            my %working_set = %aggregated_forecast;
            if ($include_filter && ref($include_filter) eq 'HASH' && %$include_filter) {
                my $include_count = scalar(keys %$include_filter);

                # Check for configured include VMs not present
                foreach my $configured_vm (sort keys %$include_filter) {
                    unless (exists $aggregated_forecast{$configured_vm}) {
                        print STDERR "    [SCOPE] Warning: VM '$configured_vm' in 'vms' not found in aggregated results for event '$event_name'\n";
                    }
                }

                %working_set = ();
                foreach my $vm (keys %aggregated_forecast) {
                    $working_set{$vm} = $aggregated_forecast{$vm} if $include_filter->{$vm};
                }
                push @filter_actions, "included $include_count";
            }

            # Step 2: Apply exclude filter (blacklist)
            if ($exclude_filter && ref($exclude_filter) eq 'HASH' && %$exclude_filter) {
                my $exclude_count = scalar(keys %$exclude_filter);
                my $excluded_actual = 0;

                my %filtered;
                foreach my $vm (keys %working_set) {
                    if ($exclude_filter->{$vm}) {
                        $excluded_actual++;
                    } else {
                        $filtered{$vm} = $working_set{$vm};
                    }
                }
                %working_set = %filtered;
                push @filter_actions, "excluded $exclude_count ($excluded_actual matched)";
            }

            my $after_count = scalar(keys %working_set);
            my $action_summary = join(', ', @filter_actions);

            if ($after_count == 0) {
                print STDERR "    [SCOPE] Warning: No VMs remain after filtering aggregated results for event '$event_name' ($action_summary)\n";
            } else {
                print STDERR "    [SCOPE] Filtered aggregated results to $after_count VM(s) for event '$event_name' ($action_summary, from $before_count)\n";
            }

            %aggregated_forecast = %working_set;

            # Build metadata for AnalysisContext
            $vm_scope_meta = {
                type               => ($include_filter && $exclude_filter) ? 'include_and_exclude'
                                    : ($include_filter) ? 'explicit_include'
                                    : 'explicit_exclude',
                include_configured => ($include_filter ? scalar(keys %$include_filter) : 0),
                exclude_configured => ($exclude_filter ? scalar(keys %$exclude_filter) : 0),
                matched_count      => $after_count,
                vms                => [ sort keys %aggregated_forecast ],
                include_list       => ($include_filter ? [ sort keys %$include_filter ] : undef),
                exclude_list       => ($exclude_filter ? [ sort keys %$exclude_filter ] : undef),
            };
        }
    }

    # Early exit if all VMs filtered out
    unless (%aggregated_forecast) {
        print STDERR "  [-] Skipping history storage for event '$event_name' bucket " . ($bucket_exec_ctx->{anchor_bucket} // 'unknown') . " (no VMs after scope filter)\n";
        return;
    }

    # Stamp aggregated metadata into %seasonal_debug_info for history storage
    # Phase 3.2: include all component fields required by _store_model_forecast_to_history
    %seasonal_debug_info = (); # rebuild only with synthesis summaries
    foreach my $vm (keys %multiplier_samples) {
        foreach my $profile (keys %{ $multiplier_samples{$vm} }) {
            my $syn = _synthesis_summary($multiplier_samples{$vm}{$profile}, $method);
            next unless $syn;

            # Aggregate each component field
            # Multiplier uses the configured aggregation method; other fields use median for stability
            $seasonal_debug_info{$vm}{$profile}{synthesis} = $syn;
            my $agg_m = _aggregate_numeric($multiplier_samples{$vm}{$profile}, $method);
            my $agg_b = _aggregate_numeric($baseline_samples{$vm}{$profile} // [], 'median');
            my $agg_t = _aggregate_numeric($trend_samples{$vm}{$profile} // [], 'median');
            my $agg_v = _aggregate_numeric($volatility_samples{$vm}{$profile} // [], 'median');
            my $agg_r = _aggregate_numeric($residual_samples{$vm}{$profile} // [], 'median');
            my $agg_a = _aggregate_numeric($amplification_samples{$vm}{$profile} // [], 'median');

             $seasonal_debug_info{$vm}{$profile} = {
                multiplier           => (defined $agg_m ? ($agg_m + 0) : undef),
                baseline             => (defined $agg_b ? ($agg_b + 0) : undef),
                trend_factor         => (defined $agg_t ? ($agg_t + 0) : undef),
                volatility           => (defined $agg_v ? ($agg_v + 0) : undef),
                forecasted_residual  => (defined $agg_r ? ($agg_r + 0) : undef),
                amplification_factor => (defined $agg_a ? ($agg_a + 0) : undef),
                synthesis            => $syn,
            };
        }
    }

    # Build analysis context for storage (bucket-level)
    my $analysis_context = {
        analysis_start  => time(),
        start       => $bucket_exec_ctx->{effective_start_date},
        end         => $bucket_exec_ctx->{effective_end_date},
        cache_start => $bucket_exec_ctx->{cache_start_date},
        cache_end   => $bucket_exec_ctx->{cache_end_date},
        interval    => $bucket_exec_ctx->{sampling_interval},
        anchor_bucket => $bucket_exec_ctx->{anchor_bucket},
        anchor_source => $bucket_exec_ctx->{anchor_source},
        bucket_occurrence_count => $occ_n,
        bucket_occurrence_end_dates => [ map { $_->{end} ? $_->{end}->ymd : () } @$bucket_occ_aref ],
        # Phase 3.3: VM scope metadata (undef if no filtering applied)
        vm_scope => $vm_scope_meta,
    };

    # Horizon meta is optional for multiplicative; keep undef unless you already compute it
    my $horizon_meta = undef;

    _store_model_forecast_to_history(
        $current_cache_path,
        $event_name,
        'multiplicative_seasonal',
        \%aggregated_forecast,
        $event_cfg,
        $horizon_meta,
        $analysis_context,
        $bucket_exec_ctx->{anchor_bucket},
        $bucket_exec_ctx->{_fingerprints}  # Phase 4: Idempotency fingerprints
    );

    # Phase 3.2: Generate CSV output for aggregated bucket
    # Re-run the final occurrence with CSV/rationale enabled
    # Note: This generates a CSV with the last occurrence's data structure but the
    # forecasts stored to history are the properly aggregated values.
    if (%aggregated_forecast && @$bucket_occ_aref) {
        my $last_occ = $bucket_occ_aref->[-1];
        if ($last_occ && $last_occ->{end}) {
            my $last_occ_end = scalar($last_occ->{end}->truncate(to => 'day'));
            my $last_occ_start = ($last_occ->{start} && ref($last_occ->{start}) eq 'Time::Piece')
                ? scalar($last_occ->{start}->truncate(to => 'day'))
                : $last_occ_end;
            my $bucket_month = $bucket_exec_ctx->{anchor_bucket} // 'unknown';

            my $final_anchor_info = {
                anchor_date      => $last_occ_end,
                anchor_bucket    => $bucket_month,
                anchor_source    => 'bucket_aggregated_csv',
                occurrence_start => $last_occ_start,
                occurrence_end   => $last_occ_end,
                is_complete      => 1,
            };

            my $final_exec_ctx = _build_anchor_execution_context($final_anchor_info, $bucket_exec_ctx, $event_cfg);

            _reset_seasonal_state();
            %seasonal_debug_info = ();

            # Run with CSV enabled, history disabled (already stored above)
            _execute_multiplicative_seasonal(
                $event_name,
                $event_cfg,
                $current_cache_path,
                $profiles_aref,
                $final_exec_ctx,
                $seasonality_config,
                { store_history => 0, emit_csv => 1, emit_rationale => 1 }
            );
        }
    }

    return;
}
