#!/usr/bin/env perl
# NAME     : nfit
# AUTHOR   : NiÃ«l Lambrechts (https://github.com/niel-lambrechts)
# PURPOSE  : Analyse NMON PhysC and optionally RunQ data for AIX and Linux on Power
#            VM right-sizing recommendations. Calculates rolling average (SMA or EMA)
#            percentiles, optionally absolute peaks, and specified percentiles of
#            normalised and absolute run-queue statistics.
#            RunQ data can now also be smoothed using SMA or EMA before percentile calculation.
#            If windowed decay is enabled, metrics are calculated per window,
#            weighted by recency, and then aggregated.
#            NEW: Can predict future growth based on windowed data trends.
#            Supports filtering by date (start and end), time, VM, weekends,
#            and percentile threshold, plus rounding options.
# REQUIRES : Perl, Time::Piece, POSIX (for ceil), List::Util (for sum0, max, min), Getopt::Long, File::Temp, version, File::Spec

use strict;
use warnings;
use Getopt::Long qw(GetOptions);
use File::Temp qw(tempfile tempdir);
use List::Util qw(sum0 max min);
use Scalar::Util qw(looks_like_number);
use POSIX qw(ceil);
use Time::Piece;
use Time::Seconds;
use version;
use File::Spec;
use File::Find;
use List::MoreUtils qw(uniq);
use JSON;
use Fcntl qw(:flock);
use File::Basename qw(dirname);
use Cwd 'abs_path';
use Digest::SHA qw(sha256_hex);
use Storable qw(freeze dclone);
my $MANDATORY_PEAK_PROFILE_FOR_HINT = "P-99W1";

# --- Force STDERR to be unbuffered for real-time progress updates ---
# This is critical for the --force-progress flag to work correctly when
# this script is called from another process like nfit-profile.
select STDERR;
$| = 1;
select STDOUT;

# --- Version ---
my $VERSION = '5.25.199.0';

# --- Configuration ---
my $DEFAULT_AVG_METHOD     = 'ema';
my $DEFAULT_DECAY_LEVEL    = 'medium';
my $DEFAULT_WINDOW_MINUTES = 15;
my $DEFAULT_PERCENTILE     = 95;
my $DEFAULT_ROUND_INCREMENT= 0.05;
my $DEFAULT_SMT            = 8;
my $DEFAULT_RUNQ_NORM_PERC = "50,90";
my $DEFAULT_RUNQ_ABS_PERC  = "90";
my $DEFAULT_RUNQ_AVG_METHOD = "ema";

my $FLOAT_EPSILON          = 1e-9; # Small number for float comparisons
my $ACTIVE_PHYSC_THRESHOLD = 0.15; # Original threshold for RunQ normalization
my $SAFETY_MARGIN_FOR_THRESHOLD_CONST = 1.05; # Safety margin for ACTIVE_PHYSC_THRESHOLD calculation
my $PLACEHOLDER_ENTITLEMENT_ADJUSTMENT_FACTOR = 1.0; # Placeholder in ACTIVE_PHYSC_THRESHOLD calc

# Windowed Decay Defaults (if enabled for nfit itself)
my $DEFAULT_PROCESS_WINDOW_UNIT = "weeks";
my $DEFAULT_PROCESS_WINDOW_SIZE = 1;
my $DEFAULT_DECAY_HALF_LIFE_DAYS = 30;

# EMA Alpha values based on decay level
my %EMA_ALPHAS = (
    'low'        => 0.03,
    'medium'     => 0.08,
    'high'       => 0.15,
    'very-high'  => 0.30,
    'extreme'    => 0.40,
);

# --- Growth Prediction Configuration (NEW) ---
my $DEFAULT_GROWTH_PROJECTION_DAYS         = 90;
my $DEFAULT_MAX_GROWTH_INFLATION_PERCENT   = 25;

# --- Cache Configuration ---
my $CACHE_MANIFEST_FILE = ".nfit.cache.manifest";
my $CACHE_STATES_FILE   = ".nfit.cache.states";
my $CACHE_DATA_FILE     = ".nfit.cache.data";
my $CACHE_LOCK_FILE     = ".nfit.cache.lock";

# Internal constants for growth heuristics (not user-configurable initially)
my $GROWTH_MIN_HISTORICAL_PERIODS       = 5;    # Min number of windowed periods to attempt trend
my $GROWTH_MAX_CV_THRESHOLD             = 0.50; # Max Coefficient of Variation (StdDev/Mean); if > this, data too volatile
my $GROWTH_MIN_POSITIVE_SLOPE_THRESHOLD = 0.01; # Min slope (units/period) to consider as actual growth for inflation
my $GROWTH_MAX_PROJECTION_HISTORY_RATIO = 2.0;  # Max ratio of projection duration to history duration used for trend

# --- Clipping Detection Configuration ---
my $CLIPPING_DEFINITE_THRESHOLD = 0.95; # P99.75 > 95% of max_capacity = definite clip
my $CLIPPING_POTENTIAL_THRESHOLD = 0.90; # P99.75 > 90% of max_capacity = potential clip
my $PCR_LOW_CONFIDENCE_THRESHOLD = 0.4;  # Peak Curvature Ratio below this adds confidence
my $PCR_HIGH_CONFIDENCE_THRESHOLD = 0.2; # Peak Curvature Ratio below this adds high confidence

# --- Argument Parsing ---
my $manifest_file;
my $physc_csv_file;
my $physc_csv_dirname;
my $runq_csv_file;
my $vm_config_file;
my $nmon_dir;
my $mgsys_filter;
my $start_date_str;
my $end_date_str;
my $target_vm_name;
my $round_arg;
my $roundup_arg;
my $smt_value = $DEFAULT_SMT;
my $runq_norm_perc_str = $DEFAULT_RUNQ_NORM_PERC;
my $runq_abs_perc_str  = $DEFAULT_RUNQ_ABS_PERC;
my $runq_avg_method_str = $DEFAULT_RUNQ_AVG_METHOD;

# Windowed Decay options
my $enable_windowed_decay = 0;
my $process_window_unit_str = $DEFAULT_PROCESS_WINDOW_UNIT;
my $process_window_size_val = $DEFAULT_PROCESS_WINDOW_SIZE;
my $decay_half_life_days_val = $DEFAULT_DECAY_HALF_LIFE_DAYS;
my $analysis_reference_date_str;
my $decay_over_states_flag = 0;

# Growth Prediction options (NEW)
my $enable_growth_prediction = 0;
my $growth_projection_days = $DEFAULT_GROWTH_PROJECTION_DAYS;
my $max_growth_inflation_percent = $DEFAULT_MAX_GROWTH_INFLATION_PERCENT;

my $show_states_flag = 0;
my $include_states_selector = 'all'; # Default value
my $reset_cache = 0;
my $enable_clipping_detection = 0;
my $profile_label;

my ($help, $show_version, $verbose, $quiet, $show_progress_flag) = (0, 0, 0, 0, 0);

GetOptions(
    'manifest=s'          => \$manifest_file,
    'physc-data|pc=s'     => \$physc_csv_file,
    'runq-data|rq=s'      => \$runq_csv_file,
    'config=s'            => \$vm_config_file,
    'nmondir=s'           => \$nmon_dir,
    'mgsys|system|serial|host=s' => \$mgsys_filter,
    'startdate|s=s'       => \$start_date_str,
    'enddate|ed=s'        => \$end_date_str,
    'round|r:f'           => \$round_arg,
    'roundup|u:f'         => \$roundup_arg,
    'vms|vm|lpar|lpars=s' => \$target_vm_name,
    'smt=i'               => \$smt_value,
    'runq-avg-method=s'   => \$runq_avg_method_str,
    # Windowed Decay Options
    'enable-windowed-decay'     => \$enable_windowed_decay,
    'process-window-unit=s'     => \$process_window_unit_str,
    'process-window-size=i'     => \$process_window_size_val,
    'decay-half-life-days=i'    => \$decay_half_life_days_val,
    'analysis-reference-date=s' => \$analysis_reference_date_str,
    'decay-over-states'         => \$decay_over_states_flag,
    # Growth Prediction Options (NEW)
    'enable-growth-prediction'       => \$enable_growth_prediction,
    'growth-projection-days=i'       => \$growth_projection_days,
    'max-growth-inflation-percent=i' => \$max_growth_inflation_percent,
    'show-states'               => \$show_states_flag,
    'include-states=s'          => \$include_states_selector,
    'reset-cache'               => \$reset_cache,
    # Clipping
    'enable-clipping-detection' => \$enable_clipping_detection,
    # General Options
    'help|h'              => \$help,
    'verbose|v'           => \$verbose,
    'version'             => \$show_version,
    'q|quiet'             => \$quiet,
    'show-progress'       => \$show_progress_flag,
) or die usage();

# --- Validation ---
if ($show_version)
{
    print STDERR "nfit version $VERSION\n";
    exit 0;
}

if ($help)
{
    print STDERR usage();
    exit 0;
}

# This script now requires a cache directory to be specified, either directly
# via --nmondir or implicitly via --mgsys.
if (!$nmon_dir && !$mgsys_filter)
{
    print STDERR "Error: You must specify a data source cache via --nmondir or --mgsys.\n\n";
    print STDERR usage();
    exit 0;
}

# --- Smart Dispatcher Logic ---
# This logic determines the final, absolute path to the cache directory that
# will be used for analysis. It provides flexibility for the user.
my $resolved_cache_dir;
my $DEFAULT_BASE_STAGE_DIR = File::Spec->catfile(dirname(abs_path($0)), 'stage');

if (defined $nmon_dir)
{
    # Case 1: User provides --nmondir. It could be a base path or a specific cache.
    if (defined $mgsys_filter && -d File::Spec->catfile($nmon_dir, $mgsys_filter))
    {
        # The user gave a base path and a serial, and the combined path exists.
        $resolved_cache_dir = File::Spec->catfile($nmon_dir, $mgsys_filter);
    }
    else
    {
        # Assume --nmondir is the full path to the cache.
        $resolved_cache_dir = $nmon_dir;
    }
}
elsif (defined $mgsys_filter)
{
    # Case 2: User provides --mgsys only. Use the new default location.
    $resolved_cache_dir = File::Spec->catfile($DEFAULT_BASE_STAGE_DIR, $mgsys_filter);
}

# --- Final Validation of the Resolved Cache Directory ---
unless (defined $resolved_cache_dir && -d $resolved_cache_dir)
{
    die "Error: Could not find a valid cache directory at the resolved path: '$resolved_cache_dir'\n" .
    "       Please ensure the path is correct and the cache was created by an nfit-stage tool.\n";
}

# Check for the manifest file to confirm it's a valid cache.
my $manifest_file_check = File::Spec->catfile($resolved_cache_dir, $CACHE_MANIFEST_FILE);
unless (-f $manifest_file_check)
{
    die "Error: The directory '$resolved_cache_dir' does not appear to be a valid nfit cache (missing '$CACHE_MANIFEST_FILE').\n";
}

# Re-assign the validated, absolute path back to the nmon_dir variable for use throughout the script.
$nmon_dir = abs_path($resolved_cache_dir);

$runq_avg_method_str = lc($runq_avg_method_str);
if ($runq_avg_method_str ne 'none' && $runq_avg_method_str ne 'sma' && $runq_avg_method_str ne 'ema')
{
    die "Error: --runq-avg-method must be 'none', 'sma', or 'ema'. Got '$runq_avg_method_str'.\n";
}

if ($physc_csv_file)
{
    die "Error: PhysC data file (--physc-data) not found: $physc_csv_file\n" if (! -f $physc_csv_file);
    $physc_csv_dirname = dirname($physc_csv_file);
}

if (defined $runq_csv_file && ! -f $runq_csv_file)
{
    die "Error: RunQ data file (--runq-data) not found: $runq_csv_file\n";
}

if ($smt_value <= 0)
{
    die "Error: --smt value must be a positive integer.\n";
}
if (defined $start_date_str && $start_date_str !~ /^\d{4}-\d{2}-\d{2}$/)
{
    die "Error: Invalid startdate (-s) format '$start_date_str'. Use YYYY-MM-DD.\n";
}
if (defined $end_date_str && $end_date_str !~ /^\d{4}-\d{2}-\d{2}$/)
{
    die "Error: Invalid enddate (-ed) format '$end_date_str'. Use YYYY-MM-DD.\n";
}
if (defined $start_date_str && defined $end_date_str)
{
    my ($s_tp_val, $e_tp_val);
    eval { $s_tp_val = Time::Piece->strptime($start_date_str, "%Y-%m-%d"); };
    if ($@ || (defined $start_date_str && !$s_tp_val) )
    {
        die "Error parsing startdate '$start_date_str': $@\n";
    }
    eval { $e_tp_val = Time::Piece->strptime($end_date_str, "%Y-%m-%d"); };
    if ($@ || (defined $end_date_str && !$e_tp_val) )
    {
        die "Error parsing enddate '$end_date_str': $@\n";
    }
    if ($s_tp_val && $e_tp_val && $e_tp_val < $s_tp_val)
    {
        die "Error: --enddate ($end_date_str) cannot be before --startdate ($start_date_str).\n";
    }
}
if (defined($round_arg) && defined($roundup_arg))
{
    die "Error: -round (-r) and -roundup (-u) options are mutually exclusive.\n";
}
if ($enable_windowed_decay)
{
    if ($process_window_unit_str ne "days" && $process_window_unit_str ne "weeks")
    {
        die "Error: --process-window-unit must be 'days' or 'weeks'. Got '$process_window_unit_str'.\n";
    }
    if ($process_window_size_val < 1)
    {
        die "Error: --process-window-size must be at least 1. Got '$process_window_size_val'.\n";
    }
    if ($decay_half_life_days_val < 1)
    {
        die "Error: --decay-half-life-days must be at least 1. Got '$decay_half_life_days_val'.\n";
    }
    if (defined $analysis_reference_date_str && $analysis_reference_date_str !~ /^\d{4}-\d{2}-\d{2}$/)
    {
        die "Error: Invalid --analysis-reference-date format '$analysis_reference_date_str'. Use YYYY-MM-DD.\n";
    }
}

# Growth Prediction Option Validation (NEW)
if ($enable_growth_prediction)
{
    # Growth prediction now works with EITHER standard windowed decay OR the new hybrid model.
    unless ($enable_windowed_decay || $decay_over_states_flag)
    {
        print STDERR "Warning: --enable-growth-prediction requires either --enable-windowed-decay or --decay-over-states to be active. Growth prediction will be SKIPPED.\n";
        $enable_growth_prediction = 0;
    }

    if ($growth_projection_days < 1)
    {
        die "Error: --growth-projection-days must be at least 1.\n";
    }
    if ($max_growth_inflation_percent < 0 || $max_growth_inflation_percent > 200) # Cap inflation percentage
    {
        die "Error: --max-growth-inflation-percent must be between 0 and 200.\n";
    }
}

my @runq_norm_percentiles_to_calc = parse_percentile_list($runq_norm_perc_str, "runq-norm-perc");
my @runq_abs_percentiles_to_calc  = parse_percentile_list($runq_abs_perc_str,  "runq-abs-perc");

my $rounding_method = 'none';
my $round_increment = undef;
my $output_dp = 4;
if (defined $round_arg)
{
    $rounding_method = 'standard';
    $round_increment = (length $round_arg && $round_arg =~ /^[0-9.]*$/) ? $round_arg : $DEFAULT_ROUND_INCREMENT;
    print STDERR "Applying standard rounding to nearest $round_increment\n";
}
elsif (defined $roundup_arg)
{
    $rounding_method = 'up';
    $round_increment = (length $roundup_arg && $roundup_arg =~ /^[0-9.]*$/) ? $roundup_arg : $DEFAULT_ROUND_INCREMENT;
    print STDERR "Applying ceiling rounding up to nearest $round_increment\n";
}
if ($rounding_method ne 'none')
{
    if (!defined $round_increment || $round_increment <= $FLOAT_EPSILON)
    {
        die "Error: Rounding increment must be positive (got '$round_increment').\n";
    }
    $output_dp = get_decimal_places($round_increment);
}

{
    # Block to keep calculation-specific variables local
    # Restoring original logic as requested, with a note about unreachable code.
    my %smt_adjustment_map = (
        1 => 1.00,
        2 => 0.95,
        4 => 0.90,
        8 => 1.10,
    );

    my $smt_adjustment_factor = $smt_adjustment_map{$smt_value} // 1.00;  # default to 1.00

    my $entitlement_adjustment_factor = $PLACEHOLDER_ENTITLEMENT_ADJUSTMENT_FACTOR;
    my $calculated_threshold = $ACTIVE_PHYSC_THRESHOLD *
    $smt_adjustment_factor *
    $entitlement_adjustment_factor *
    $SAFETY_MARGIN_FOR_THRESHOLD_CONST;
    my $MIN_ACTIVE_PHYSC_THRESHOLD = 0.10;
    $ACTIVE_PHYSC_THRESHOLD = ($calculated_threshold < $MIN_ACTIVE_PHYSC_THRESHOLD) ? $MIN_ACTIVE_PHYSC_THRESHOLD : $calculated_threshold;
}

# --- Main Execution Path ---
print STDERR "\nnfit version $VERSION\n";
print STDERR "-------------------------\n";

# In nfit.pl, after all GetOptions and validation logic

# --- Main Execution Path ---
if ($manifest_file) {
    # New single-pass engine entry point.
    # We now construct a single hash reference containing all necessary global
    # flags to pass to the engine. This keeps the function call clean and
    # ensures all original functionality is preserved.
    run_single_pass_from_manifest({
        # --- File/Cache Arguments ---
        manifest_file           => $manifest_file,
        nmon_dir                => $nmon_dir, # This is the resolved path from the Smart Dispatcher

        # --- Filtering Arguments ---
        target_vm_name          => $target_vm_name,
        start_date_str          => $start_date_str,
        end_date_str            => $end_date_str,
        include_states          => $include_states_selector,

        # --- RunQ and SMT Arguments ---
        smt_value               => $smt_value,
        runq_avg_method_str     => $runq_avg_method_str,

        # --- Analysis Model Flags ---
        enable_windowed_decay   => $enable_windowed_decay,
        decay_over_states       => $decay_over_states_flag,
        enable_growth_prediction=> $enable_growth_prediction,
        enable_clipping_detection=> $enable_clipping_detection,

        # --- Decay Model Tunables ---
        process_window_unit     => $process_window_unit_str,
        process_window_size     => $process_window_size_val,
        decay_half_life_days    => $decay_half_life_days_val,
        analysis_reference_date => $analysis_reference_date_str,

        # --- Growth Model Tunables ---
        growth_projection_days  => $growth_projection_days,
        max_growth_inflation_percent => $max_growth_inflation_percent,
        
        # --- Rounding Arguments ---
        round_arg               => $round_arg,
        roundup_arg             => $roundup_arg,
        
        # --- Utility/Verbosity Flags ---
        verbose                 => $verbose,
        quiet                   => $quiet,
    });
    
    # The new engine handles everything, so we exit cleanly.
    exit 0;
}

# ==============================================================================
# Subroutines
# ==============================================================================

# --- clean_perc_label ---
# Helper to format a percentile number into a clean string for metric keys.
sub clean_perc_label {
    my ($p) = @_;
    my $label = sprintf("%.2f", $p);
    $label =~ s/\.?0+$//;
    $label = "0" if $label eq "" && abs($p-0)<0.001;
    return $label;
}

# --- calculate_rolling_average ---
# Generic sub to calculate a single rolling average point (SMA or EMA).
sub calculate_rolling_average {
    my ($value, $method, $sma_queue_aref, $prev_ema_sref, $window, $alpha) = @_;

    my $avg_to_return;
    if ($method eq 'sma') {
        push @$sma_queue_aref, $value;
        shift @$sma_queue_aref while @$sma_queue_aref > $window;
        if (@$sma_queue_aref == $window) {
            $avg_to_return = calculate_average(@$sma_queue_aref);
        }
    } else { # ema
        if (defined $value) {
            if (!defined $$prev_ema_sref) { $$prev_ema_sref = $value; }
            else { $$prev_ema_sref = ($value * $alpha) + ($$prev_ema_sref * (1 - $alpha)); }
        }
        push @$sma_queue_aref, $value; # Use sma_queue as a simple counter
        shift @$sma_queue_aref while @$sma_queue_aref > $window;
        if (@$sma_queue_aref == $window) {
            $avg_to_return = $$prev_ema_sref;
        }
    }
    return $avg_to_return;
}

# --- print_state_windows_report ---
# Takes the data structure returned by define_configuration_states and
# prints a formatted, compact report to STDOUT.
#
# Args:
#   1. $states_href (hash ref): The state windows data structure.
#
sub print_state_windows_report
{
    my ($states_href) = @_;

    my $header_format = "%-20s %-10s %-20s %-20s %-10s %-45s %s\n";
    my $row_format    = "%-20s %-10s %-20s %-20s %-10.2f %-45s %s\n";

    printf $header_format, "VM_Name", "State_ID", "Start_Time", "End_Time", "Duration", "Config_Summary", "Processor";

    my $separator = ("-" x 20) . " " . ("-" x 10) . " " . ("-" x 20) . " " . ("-" x 20) . " " . ("-" x 10) . " " . ("-" x 45) . " " . ("-" x 30) . "\n";
    print STDOUT $separator;

    foreach my $vm_name (sort keys %$states_href)
    {
        my @vm_states = @{$states_href->{$vm_name}};
        my $total_states = scalar(@vm_states);
        foreach my $state (@vm_states)
        {
            my $md = $state->{metadata};

            # Build the compact configuration summary string
            my $capped_str = ($md->{capped} // 0) ? "Capped" : "Uncapped";
            my $config_summary = sprintf("Ent:%.2f vCPU:%d PoolCPU:%d SMT:%d %s",
                $md->{entitlement} // 0,
                $md->{virtual_cpus} // 0,
                $md->{pool_cpu} // 0,
                $md->{smt} // 0,
                $capped_str
            );

            printf $row_format,
            $state->{vm_name},
            $state->{state_id} . "/" . $total_states,
            $state->{start_time}->strftime('%Y-%m-%d %H:%M'),
            $state->{end_time}->strftime('%Y-%m-%d %H:%M'),
            $state->{duration_days},
            $config_summary,
            $md->{processor_state};
        }
    }
}

# --- parse_state_selector ---
# Parses the user's state selection string (e.g., "1,3,5-7,-1") and returns
# an array of the actual state window objects that match the selection.
#
# Args:
#   1. $selector_str (string): The raw string from the --include-states flag.
#   2. $available_states_aref (array ref): An array of all state window hashes for a VM.
#
# Returns:
#   - An array containing the state window hashes that were selected.
#
sub parse_state_selector
{
    my ($selector_str, $available_states_aref) = @_;
    my @selected_states;
    return @$available_states_aref if (lc($selector_str) eq 'all');
    return () if (scalar(@$available_states_aref) == 0);
    my %selected_ids;
    my $total_states = scalar(@$available_states_aref);
    my @parts = split ',', $selector_str;
    foreach my $part (@parts) {
        $part =~ s/\s+//g;
        if ($part =~ /^(\d+)-(\d+)$/) {
            for my $id ($1 .. $2) { $selected_ids{$id} = 1; }
        } elsif ($part =~ /^-(\d+)$/) {
            my $offset = $1;
            if ($total_states - $offset >= 0) { $selected_ids{ $total_states - ($offset - 1) } = 1; }
        } elsif ($part =~ /^\d+$/) {
            $selected_ids{$part} = 1;
        }
    }
    foreach my $state (@$available_states_aref) {
        push @selected_states, $state if exists $selected_ids{$state->{state_id}};
    }
    return @selected_states;
}

# Calculate basic statistics (mean, stddev, CV) for a list of values
# Input: reference to an array of numeric values
# Output: hashref { mean, stddev, cv, count } or undef if insufficient data
sub calculate_statistics_for_trend
{
    my ($values_aref) = @_;
    my @defined_values = grep { defined $_ && $_ =~ /^-?[0-9.]+$/ } @{$values_aref};
    my $count = scalar @defined_values;

    return undef if $count == 0;

    my $sum = sum0(@defined_values);
    my $mean = $sum / $count;
    my $stddev = 0;
    my $cv = undef;

    if ($count > 1)
    {
        my $sum_sq_diff = 0;
        foreach my $val (@defined_values)
        {
            $sum_sq_diff += ($val - $mean)**2;
        }
        $stddev = sqrt($sum_sq_diff / ($count - 1));
    }

    if (abs($mean) > $FLOAT_EPSILON)
    {
        $cv = $stddev / $mean;
    }
    elsif ($stddev < $FLOAT_EPSILON && abs($mean) < $FLOAT_EPSILON)
    {
        $cv = 0;
    }

    return {
        mean   => $mean,
        stddev => $stddev,
        cv     => $cv,
        count  => $count,
    };
}

# Calculate linear regression (slope, intercept, R-squared) manually
# Input: reference to an array of [x, y] points, where x and y are numeric.
#        x is typically a time index (0, 1, 2,...), y is the metric value.
# Output: hashref { slope, intercept, r_squared, n_points } or undef if
#         insufficient data (n < 2) or if slope cannot be determined (e.g., all x values are identical).
sub calculate_manual_linear_regression
{
    my ($points_aref) = @_;
    my $n = scalar @{$points_aref};

    # Regression requires at least 2 distinct points to define a line.
    return undef if $n < 2;

    my $sum_x = 0;
    my $sum_y = 0;
    my $sum_xy = 0;
    my $sum_x_squared = 0;
    my $sum_y_squared = 0;

    foreach my $point (@{$points_aref})
    {
        my ($x_val, $y_val) = @{$point};

        # Assuming $x_val and $y_val are already numeric based on upstream processing.
        # If strict validation is needed here, it can be added, but the input
        # preparation logic should ensure numeric data.

        $sum_x += $x_val;
        $sum_y += $y_val;
        $sum_xy += $x_val * $y_val;
        $sum_x_squared += $x_val**2;
        $sum_y_squared += $y_val**2;
    }

    my $slope_calc     = undef;
    my $intercept_calc = undef;
    my $r_squared_calc = undef;

    # Denominator for slope calculation: N * sum(x^2) - (sum(x))^2
    # This is also N * SS_xx (where SS_xx is sum of squares for x)
    my $denominator_slope = ($n * $sum_x_squared) - ($sum_x**2);

    # Check if denominator is too close to zero (implies x values are not distinct enough
    # or only one unique x value if n > 1, which makes slope undefined or infinite).
    if (abs($denominator_slope) > $FLOAT_EPSILON)
    {
        $slope_calc = (($n * $sum_xy) - ($sum_x * $sum_y)) / $denominator_slope;
        $intercept_calc = ($sum_y - ($slope_calc * $sum_x)) / $n;

        # Calculate R-squared (Coefficient of Determination)
        # R^2 = (N * sum(xy) - sum(x) * sum(y))^2 / ((N * sum(x^2) - (sum(x))^2) * (N * sum(y^2) - (sum(y))^2))
        my $numerator_r_sq_squared = (($n * $sum_xy) - ($sum_x * $sum_y))**2;
        my $denominator_r_sq_part_yy = ($n * $sum_y_squared) - ($sum_y**2);

        if (abs($denominator_r_sq_part_yy) > $FLOAT_EPSILON) # Avoid division by zero if all y values are the same
        {
            $r_squared_calc = $numerator_r_sq_squared / ($denominator_slope * $denominator_r_sq_part_yy);
            # Clamp R-squared to [0, 1] due to potential floating point inaccuracies
            $r_squared_calc = 0.0 if defined $r_squared_calc && $r_squared_calc < 0;
            $r_squared_calc = 1.0 if defined $r_squared_calc && $r_squared_calc > 1.0;
        }
        elsif (abs($numerator_r_sq_squared) < $FLOAT_EPSILON**2) # All y are same, and slope is ~0
        {
            # If all y values are identical, the line should perfectly predict them if slope is also ~0.
            $r_squared_calc = 1.0;
        }
        else
        {
            # This case implies an issue or perfect vertical correlation if all x were same (but ss_xx_calc > 0 here)
            # If y is constant, variance of y is 0. If model doesn't perfectly predict this constant, R^2 can be odd.
            # Given y is constant, and slope is non-zero, it means the line isn't horizontal.
            # SS_tot would be 0. SS_res would be >0. R^2 = 1 - SS_res/SS_tot is undefined.
            # However, if $denominator_r_sq_part_yy is near zero, it implies SS_tot is near zero.
            # If the numerator $numerator_r_sq_squared is also near zero, it suggests the model fits perfectly (R^2 = 1).
            # If numerator is not zero but denominator_yy is, it's effectively a poor fit for variation that doesn't exist.
            $r_squared_calc = 0.0; # Default to 0 for poor/undefined fit in this edge scenario
        }
    }
    else
    {
        # Denominator for slope is zero or too small.
        # This happens if all x values are (nearly) identical.
        # Cannot reliably determine a linear trend.
        return undef;
    }

    return {
        slope     => $slope_calc,
        intercept => $intercept_calc,
        r_squared => $r_squared_calc,
        n_points  => $n,
    };
}

# --- Existing Subroutines from Original Script ---
sub parse_percentile_list
{
    my ($perc_str, $arg_name) = @_;
    my @percentiles;
    if (defined $perc_str && $perc_str ne '')
    {
        my @raw_percentiles = split /,\s*/, $perc_str;
        foreach my $p (@raw_percentiles)
        {
            if ($p !~ /^[0-9]+(?:\.[0-9]+)?$/ || $p < 0 || $p > 100)
            {
                die "Error: Invalid percentile value '$p' in --$arg_name list. Must be numeric between 0 and 100.\n";
            }
            push @percentiles, $p + 0;
        }
    }
    return @percentiles;
}

sub get_nmon_overall_date_range
{
    my ($nmon_file, $global_start_filter_str, $global_end_filter_str) = @_;
    print STDERR "Scanning NMON file '$nmon_file' for overall effective date range...\n" if ($verbose);
    # Handle both compressed or uncompressed files.
    my $fh;
    if ($nmon_file =~ /\.nmon\.gz$/i) {
        open $fh, "-|", "gzip", "-dc", "--", $nmon_file or do {
            warn "Warning: Could not decompress '$nmon_file': $!. Skipping.";
            next;
        };
    }
    elsif ($nmon_file =~ /\.nmon\.(bz2|bzip2)$/i) {
        open $fh, "-|", "bzcat", "--", $nmon_file or do {
            warn "Warning: Could not bzcat '$nmon_file': $!. Skipping.";
            next;
        };
    }
    elsif ($nmon_file =~ /\.nmon$/i) {
        open $fh, '<:encoding(utf8)', $nmon_file or do {
            warn "Warning: Could not open '$nmon_file': $!. Skipping.";
            next;
        };
    }
    else {
        warn "Warning: Unsupported file type '$nmon_file'. Skipping.";
        next;
    }
    my $min_date_obj;
    my $max_date_obj;
    my $header_skipped = 0;
    my $first_data_line_checked = 0;
    my $start_filter_obj;
    my $end_filter_obj;
    if (defined $global_start_filter_str)
    {
        eval { $start_filter_obj = Time::Piece->strptime($global_start_filter_str, "%Y-%m-%d"); };
        if ($@ || (defined $global_start_filter_str && ! (defined $start_filter_obj && $start_filter_obj->isa('Time::Piece') ) ) )
        {
            die "Error parsing global start date filter '$global_start_filter_str': $@\n";
        }
    }
    if (defined $global_end_filter_str)
    {
        eval { $end_filter_obj = Time::Piece->strptime($global_end_filter_str, "%Y-%m-%d"); };
        if ($@ || (defined $global_end_filter_str && ! (defined $end_filter_obj && $end_filter_obj->isa('Time::Piece') ) ) )
        {
            die "Error parsing global end date filter '$global_end_filter_str': $@\n";
        }
    }
    while (my $line = <$fh>)
    {
        chomp $line;
        $line =~ s/\r$//;
        next if $line =~ /^\s*$/;
        if (!$header_skipped && !$first_data_line_checked)
        {
            if ($line =~ /^(Time,|Date,Time,Hostname,ZZZZ)/i)
            {
                $header_skipped = 1;
                next;
            }
            $first_data_line_checked = 1;
        }
        elsif ($header_skipped && $line =~ /^(Time,|Date,Time,Hostname,ZZZZ)/i)
        {
            next;
        }
        if ($line =~ /^(\d{4}-\d{2}-\d{2}[T\s]\d{2}:\d{2}:\d{2})/)
        {
            my $timestamp_str_local = $1;
            $timestamp_str_local =~ s/T/ /;
            my $current_tp;
            eval { $current_tp = Time::Piece->strptime($timestamp_str_local, "%Y-%m-%d %H:%M:%S"); };
            if ($@ || !$current_tp || !$current_tp->isa('Time::Piece'))
            {
                next;
            }
            my $current_date_day_obj = $current_tp->truncate(to => 'day');
            if (defined $start_filter_obj && $current_date_day_obj < $start_filter_obj) { next; }
            if (defined $end_filter_obj   && $current_date_day_obj > $end_filter_obj)   { next; }
            if (!defined $min_date_obj || !$min_date_obj->isa('Time::Piece') || $current_tp < $min_date_obj)
            {
                $min_date_obj = $current_tp;
            }
            if (!defined $max_date_obj || !$max_date_obj->isa('Time::Piece') || $current_tp > $max_date_obj)
            {
                $max_date_obj = $current_tp;
            }
        }
    }
    close $fh;
    if (defined $min_date_obj && $min_date_obj->isa('Time::Piece') && defined $max_date_obj && $max_date_obj->isa('Time::Piece'))
    {
        print STDERR "Effective NMON data range for windowing: " . $min_date_obj->datetime . " to " . $max_date_obj->datetime . "\n";
        my $ret_start = $min_date_obj->truncate(to => 'day');
        my $ret_end   = $max_date_obj->truncate(to => 'day');
        unless (defined $ret_start && $ret_start->isa('Time::Piece') && defined $ret_end && $ret_end->isa('Time::Piece'))
        {
            print STDERR "Error: Truncated date objects became invalid in get_nmon_overall_date_range.\n";
            return (undef, undef);
        }
        return ($ret_start, $ret_end);
    }
    else
    {
        print STDERR "Warning: Could not determine valid min/max dates from NMON file '$nmon_file' after global filters.\n";
        return (undef, undef);
    }
}

sub generate_processing_time_windows
{
    my ($period_start_obj, $period_end_obj, $unit_str, $size_val) = @_;
    my @windows;
    return () unless (defined $period_start_obj && defined $period_end_obj && $period_start_obj->isa('Time::Piece') && $period_end_obj->isa('Time::Piece') && $period_start_obj <= $period_end_obj);
    my $current_window_start = Time::Piece->new($period_start_obj->epoch);
    while ($current_window_start <= $period_end_obj)
    {
        my $current_window_end;
        if ($unit_str eq "days")
        {
            $current_window_end = Time::Piece->new($current_window_start->epoch) + (ONE_DAY() * ($size_val - 1));
        }
        elsif ($unit_str eq "weeks")
        {
            $current_window_end = Time::Piece->new($current_window_start->epoch) + (ONE_WEEK() * $size_val) - ONE_DAY();
        }
        else
        {
            die "Unsupported window unit: $unit_str\n";
        }
        if ($current_window_end > $period_end_obj)
        {
            $current_window_end = Time::Piece->new($period_end_obj->epoch);
        }
        my $representative_date = Time::Piece->new($current_window_end->epoch);
        push @windows, [Time::Piece->new($current_window_start->epoch), Time::Piece->new($current_window_end->epoch), $representative_date];
        # Move to the start of the next day for the next window
        $current_window_start = $current_window_end->truncate(to => 'day') + ONE_DAY;
    }
    return @windows;
}

sub get_window_key_for_timestamp
{
    my ($timestamp_obj, $windows_aref, $hint_idx) = @_;
    my $timestamp_day_obj = $timestamp_obj->truncate(to => 'day');
    if (defined $hint_idx && $hint_idx >=0 && $hint_idx < @{$windows_aref})
    {
        my ($win_start, $win_end) = @{$windows_aref->[$hint_idx]};
        if ($timestamp_day_obj >= $win_start && $timestamp_day_obj <= $win_end)
        {
            return $windows_aref->[$hint_idx][2]->ymd('') . "_" . $hint_idx;
        }
    }
    for (my $i=0; $i < @{$windows_aref}; $i++)
    {
        my ($win_start, $win_end) = @{$windows_aref->[$i]};
        if ($timestamp_day_obj >= $win_start && $timestamp_day_obj <= $win_end)
        {
            return $windows_aref->[$i][2]->ymd('') . "_" . $i;
        }
    }
    return undef;
}

sub get_weighted_metric_for_vm
{
    my ($vm_data_by_window_href, $vm_name_local, $metric_key, # Use local var
        $processing_windows_aref, $analysis_ref_obj_arg, $decay_hl_days_arg, $sprintf_fmt_optional) = @_;

    my @metric_window_values;
    if (exists $vm_data_by_window_href->{$vm_name_local})
    {
        foreach my $win_key (sort {
                my ($ad, $ai) = ($a =~ /^(\d{8})_(\d+)$/);
                my ($bd, $bi) = ($b =~ /^(\d{8})_(\d+)$/);
                return ($ad cmp $bd) || ($ai <=> $bi);
            } keys %{$vm_data_by_window_href->{$vm_name_local}})
        {
            if (exists $vm_data_by_window_href->{$vm_name_local}{$win_key}{$metric_key} &&
                defined $vm_data_by_window_href->{$vm_name_local}{$win_key}{$metric_key} &&
                $vm_data_by_window_href->{$vm_name_local}{$win_key}{$metric_key} ne "N/A" )
            {
                my ($win_date_str_part, $win_idx_part) = ($win_key =~ /^(\d{8})_(\d+)$/);
                if (defined $win_idx_part && $win_idx_part < @{$processing_windows_aref})
                {
                    my $rep_date_obj = $processing_windows_aref->[$win_idx_part][2];
                    unless (defined $rep_date_obj && $rep_date_obj->isa('Time::Piece'))
                    {
                        print STDERR "Warning: Representative date for window key '$win_key' is invalid for VM '$vm_name_local', metric '$metric_key'. Skipping this window point.\n";
                        next;
                    }
                    push @metric_window_values, {
                        value => $vm_data_by_window_href->{$vm_name_local}{$win_key}{$metric_key},
                        date  => $rep_date_obj
                    };
                }
                else
                {
                    print STDERR "Warning: Could not accurately map window key '$win_key' to a processing window definition for VM '$vm_name_local', metric '$metric_key'. Skipping this window point.\n";
                }
            }
        }
    }

    if (@metric_window_values)
    {
        my $final_val_str;
        if ($metric_key eq 'Peak') {
            # For the Peak, we want the absolute maximum value found across all windows.
            my @values = map { $_->{value} } grep { defined $_->{value} && looks_like_number($_->{value}) } @metric_window_values;
            $final_val_str = @values ? max(@values) : "N/A";
        } else {
            # For all other metrics, perform the recency-weighted average.
            $final_val_str = calculate_recency_weighted_average(
                \@metric_window_values, $analysis_ref_obj_arg, $decay_hl_days_arg
            );
        }
        if (defined $sprintf_fmt_optional && defined $final_val_str && $final_val_str ne "N/A" && $final_val_str =~ /^-?[0-9.]+$/)
        {
            return sprintf($sprintf_fmt_optional, $final_val_str + 0);
        }
        return $final_val_str;
    }
    return "N/A";
}

sub calculate_recency_weighted_average
{
    my ($windowed_data_ref, $analysis_ref_obj_arg, $half_life_days_arg) = @_;
    my $sum_weighted_values = 0;
    my $sum_weights = 0;
    return "N/A" if (!defined $analysis_ref_obj_arg || !$analysis_ref_obj_arg->isa('Time::Piece'));
    return "N/A" if (!defined $half_life_days_arg || $half_life_days_arg <= 0);

    my $lambda = log(2) / $half_life_days_arg;

    foreach my $dp_ref (@{$windowed_data_ref})
    {
        my $value_str = $dp_ref->{value};
        next if (!defined $value_str || $value_str eq "N/A" || $value_str !~ /^-?[0-9.]+$/);
        my $value = $value_str + 0;
        my $date_obj = $dp_ref->{date};
        next if (!defined $date_obj || !$date_obj->isa('Time::Piece'));

        my $date_obj_day = $date_obj->truncate(to => 'day');
        my $analysis_ref_day = $analysis_ref_obj_arg->truncate(to => 'day');

        my $days_diff_seconds = $analysis_ref_day->epoch - $date_obj_day->epoch;
        my $days_diff = $days_diff_seconds / ONE_DAY(); # Use seconds from Time::Seconds object
        $days_diff = 0 if $days_diff < 0;

        my $weight = exp(-$lambda * $days_diff);
        $sum_weighted_values += $value * $weight;
        $sum_weights += $weight;
    }

    if ($sum_weights > $FLOAT_EPSILON)
    {
        return sprintf("%.4f", $sum_weighted_values / $sum_weights);
    }
    else
    {
        return "N/A";
    }
}

sub get_vm_index_by_name
{
    my ($vm_name_to_find, $vm_names_list_ref) = @_;
    for (my $i=0; $i < @{$vm_names_list_ref}; $i++)
    {
        if ($vm_names_list_ref->[$i] eq $vm_name_to_find)
        {
            return $i;
        }
    }
    return undef;
}

sub calculate_average
{
    my (@data) = @_;
    my @numbers = grep { defined $_ && looks_like_number($_) } @data;
    return 0 if !@numbers;
    return sum0(@numbers) / scalar(@numbers);
}

sub calculate_percentile
{
    my ($data_ref, $p) = @_;
    my @data_input = @{$data_ref};
    my @data = grep { defined($_) && $_ =~ /^-?[0-9]+(?:\.[0-9]+)?$/ } @data_input;
    my $n = scalar @data;
    return undef if $n == 0;
    @data = sort { $a <=> $b } @data;
    return $data[0] if $n == 1;

    my $rank_fractional = ($p / 100) * ($n - 1);
    my $k = int($rank_fractional);
    my $d = $rank_fractional - $k;

    if ($p == 0) { return $data[0]; }
    if ($p == 100) { return $data[$n-1]; }

    if ($k >= $n - 1)
    {
        return $data[$n - 1];
    }
    elsif ($k < 0)
    {
        return $data[0];
    }
    else
    {
        my $val_k = $data[$k];
        my $val_k_plus_1 = ($k + 1 < $n) ? $data[$k + 1] : $data[$k];
        return $val_k + $d * ($val_k_plus_1 - $val_k);
    }
}

sub calculate_rolling_average_series
{
    my ($data_series_ref, $method, $window_size, $alpha_val) = @_;
    my @output_series;
    my @current_window_sma;
    my $current_ema;

    if ($method eq 'none')
    {
        return $data_series_ref;
    }

    foreach my $value (@{$data_series_ref})
    {
        my $avg_val_to_add = undef;
        if ($method eq 'sma')
        {
            if (defined $value && $value =~ /^-?[0-9.]+$/)
            {
                push @current_window_sma, ($value + 0);
            }
            else
            {
                push @current_window_sma, undef;
            }
            shift @current_window_sma while scalar @current_window_sma > $window_size;
            if (scalar @current_window_sma == $window_size)
            {
                $avg_val_to_add = calculate_average(@current_window_sma);
            }
        }
        elsif ($method eq 'ema')
        {
            if (defined $value && $value =~ /^-?[0-9.]+$/)
            {
                my $numeric_value = $value + 0;
                if (!defined $current_ema)
                {
                    $current_ema = $numeric_value;
                }
                else
                {
                    $current_ema = ($numeric_value * $alpha_val) + ($current_ema * (1 - $alpha_val));
                }
            }
            push @current_window_sma, $value;
            shift @current_window_sma while scalar @current_window_sma > $window_size;
            if (scalar @current_window_sma == $window_size)
            {
                $avg_val_to_add = $current_ema;
            }
        }
        push @output_series, $avg_val_to_add if defined $avg_val_to_add;
    }
    return \@output_series;
}

sub apply_rounding
{
    my ($value, $increment, $method) = @_;
    return $value unless (defined $value && $value =~ /^-?[0-9]+(?:\.[0-9]+)?$/);
    return $value if $method eq 'none' || !defined $increment || $increment <= $FLOAT_EPSILON;

    my $rounded_value;
    if ($method eq 'standard')
    {
        $rounded_value = int( ($value / $increment) + ( ($value >= 0) ? 0.5 : -0.5) ) * $increment;
    }
    elsif ($method eq 'up')
    {
        $rounded_value = ceil( $value / $increment ) * $increment;
    }
    else
    {
        $rounded_value = $value;
    }
    return $rounded_value;
}

sub get_decimal_places
{
    my ($number_str) = @_;
    $number_str = sprintf("%.15f", $number_str) if ($number_str =~ /e/i);
    if ($number_str =~ /\.(\d+)$/)
    {
        return length($1);
    }
    else
    {
        return 0;
    }
}

# --- define_time_windows_for_state ---
# Subdivides a configuration state window into smaller, regular time-based
# windows for decay and growth analysis.
#
sub define_time_windows_for_state {
    my ($state_obj, $ref_date_str, $unit, $size) = @_;
    my @windows;
    my $start = $state_obj->{start_time};
    my $end   = $state_obj->{end_time};

    my $current_win_start = $start->truncate(to => 'day');

    while ($current_win_start < $end) {
        my $current_win_end;
        if ($unit eq 'days') {
            $current_win_end = $current_win_start + ($size * ONE_DAY()) - 1;
        } else { # weeks
            $current_win_end = $current_win_start + ($size * ONE_WEEK()) - 1;
        }

        $current_win_end = $end if $current_win_end > $end;

        push @windows, {
            start => $current_win_start,
            end   => $current_win_end,
            rep_date => $current_win_end, # Representative date for weighting is the end of the window
        };

        $current_win_start = $current_win_end->truncate(to => 'day') + ONE_DAY();
    }
    return @windows;
}

sub get_window_index_for_timestamp {
    my ($timestamp_obj, $windows_aref) = @_;
    my $ts_epoch = $timestamp_obj->epoch;
    for (my $i=0; $i < @{$windows_aref}; $i++) {
        my ($win_start, $win_end) = @{$windows_aref->[$i]};
        return $i if ($ts_epoch >= $win_start->epoch && $ts_epoch <= $win_end->epoch);
    }
    return undef;
}


sub parse_csv_line {
    my ($line) = @_;
    my @fields;

    while (length($line)) {
        if ($line =~ s/^"((?:[^"]|"")*)"[,]*//) {
            my $field = $1;
            $field =~ s/""/"/g;  # unescape double quotes
            push @fields, $field;
        } elsif ($line =~ s/^([^",]*)[,]*//) {
            push @fields, $1;
        } else {
            # fallback (shouldn't happen with consistent CSV)
            last;
        }
    }

    return @fields;
}

## --- BEGIN L2 RESULTS CACHE SUBROUTINES --- ##

# ==============================================================================
# Subroutine to look up a result from the L2 cache file.
# Returns: An array reference of result hashes, or undef if not found.
# ==============================================================================
sub lookup_cached_result
{
    my ($key, $cache_file) = @_;

    return undef unless (-f $cache_file && -s $cache_file);

    # Read the entire file content as a single string.
    my $json_text = do {
        open my $fh, '<:encoding(utf8)', $cache_file or do {
            warn "Warning: Could not open results cache '$cache_file' for reading: $!";
            return undef;
        };
        local $/;
        my $content = <$fh>;
        close $fh;
        $content;
    };

    # Decode the single JSON object from the file.
    my $json_decoder = JSON->new->allow_nonref;
    my $cache_data = eval { decode_json($json_text) };
    if ($@ || !defined($cache_data) || ref($cache_data) ne 'HASH') {
        print STDERR "Warning: Results cache '$cache_file' is invalid. Resetting.\n";
        unlink $cache_file or warn "Warning: Could not remove corrupt cache file '$cache_file': $!";
        return undef;
    }

    # The value is now an array reference of hashes.
    return $cache_data->{$key};
}

# ==============================================================================
# Subroutine to save a result to the L2 cache file under an exclusive lock.
# ==============================================================================
sub save_result_to_cache
{
    my ($key, $result_objects_aref, $cache_file, $lock_file) = @_;

    # This subroutine now performs a safe read-modify-write of the entire JSON cache file.
    # Acquire an exclusive lock to prevent race conditions when writing.
    open my $lock_fh, '>', $lock_file or do {
        warn "Warning: Could not create lock file '$lock_file' for writing cache: $!. Skipping cache write.";
        return;
    };
    flock($lock_fh, LOCK_EX) or do {
        warn "Warning: Could not acquire exclusive lock on '$lock_file': $!. Skipping cache write.";
        close $lock_fh;
        return;
    };
    # Read the existing cache file into a Perl hash.
    my $cache_data = {};
    if (-f $cache_file && -s $cache_file) {
        my $json_text = do {
            open my $read_fh, '<:encoding(utf8)', $cache_file;
            local $/;
            <$read_fh>;
        };
        my $json_decoder = JSON->new->allow_nonref;
        my $decoded = eval { $json_decoder->decode($json_text) };
        # If the file is valid JSON hash, use it. Otherwise, start fresh.
        $cache_data = $decoded if (ref($decoded) eq 'HASH');
    }

    # Add or update the result for the current key with the array of result objects.
    $cache_data->{$key} = $result_objects_aref;

    # Encode the entire updated hash back to a pretty-printed JSON string.
    my $json_encoder = JSON->new->pretty->canonical;
    my $output_json = $json_encoder->encode($cache_data);

    # Overwrite the cache file with the new content.
    open my $write_fh, '>:encoding(utf8)', $cache_file or do {
        warn "Warning: Could not open results cache '$cache_file' for writing: $!";
        close $lock_fh;
        return;
    };

    print $write_fh $output_json;
    close $write_fh;
    close $lock_fh; # Releases the lock automatically on close.
}

# Subroutine for the Hybrid State-Time Decay Model
sub synthesize_hybrid_timeseries {
    my ($state_results_aref, $metric_key) = @_;
    my @timeseries_points;

    foreach my $state_result (@$state_results_aref) {
        my $start_obj = $state_result->{state_obj}{start_time}->truncate(to => 'day');
        my $end_obj   = $state_result->{state_obj}{end_time}->truncate(to => 'day');
        my $metric_val = $state_result->{metrics}{$metric_key};

        # Skip if this state didn't have a valid metric
        next unless defined $metric_val && looks_like_number($metric_val);

        my $current_day = $start_obj;
        while ($current_day <= $end_obj) {
            # Add a data point for every day the state was active
            push @timeseries_points, {
                value => $metric_val,
                date  => Time::Piece->new($current_day->epoch) # Use a copy
            };
            $current_day += ONE_DAY;
        }
    }
    return @timeseries_points;
}

# ==============================================================================
# SUBROUTINE: calculate_growth_prediction
# PURPOSE:    Performs trend analysis on a time series of metric values and
#             calculates a growth adjustment factor based on a linear
#             regression projection. This function is a vital part of the
#             engine's forecasting capability.
#
# ARGS:
#   1. $timeseries_values_aref (array ref): An array of the numeric metric
#                                           values over time (e.g., from each
#                                           processing window).
#   2. $baseline_val (float): The final aggregated metric value, which serves
#                             as the base for any calculated inflation.
#
# RETURNS:
#   - A hash reference containing the results and a detailed rationale:
#     {
#       growth_adj     => The final calculated growth adjustment value (float),
#       growth_adj_abs => The absolute value of the adjustment (float),
#       rationale      => A hash ref with debug info for logging by consumers
#                         like nfit-profile.pl.
#     }
# ==============================================================================
sub calculate_growth_prediction
{
    my ($timeseries_values_aref, $baseline_val) = @_;

    # --- Pre-declare all rationale variables to ensure correct scope ---
    # Initialise to default values to handle cases where the logic exits early.
    my ($growth_adj_value, $growth_adj_abs_value) = (0.0, 0.0);
    my ($stats, $cv_val, $regression, $slope, $projected_val, $inflation_perc);
    my $num_hist_periods = 0;
    # Use explicit strings for check results to provide clearer rationale.
    my $cv_check_passed = "Skipped";
    my $slope_check_passed = "Skipped";
    my $was_capped = "N/A";

    # Growth calculation is only possible if there is a positive baseline to inflate.
    return {
        growth_adj     => 0.0,
        growth_adj_abs => 0.0,
        rationale      => { num_hist_periods => 0, stats_cv => "N/A", cv_check_passed => "Skipped (No baseline)" }
    } unless (defined $baseline_val && looks_like_number($baseline_val) && $baseline_val > $FLOAT_EPSILON);

    my @timeseries_points;
    my $time_idx = 0;

    # Create [x, y] points for regression, where x is the time index (0, 1, 2...).
    foreach my $metric_val (@$timeseries_values_aref)
    {
        if (defined $metric_val && looks_like_number($metric_val))
        {
            push @timeseries_points, [$time_idx, $metric_val];
            $time_idx++;
        }
    }

    $num_hist_periods = scalar(@timeseries_points);

    # Ensure there are enough data points to establish a meaningful trend.
    if ($num_hist_periods >= $GROWTH_MIN_HISTORICAL_PERIODS)
    {
        my @values_for_stats = map { $_->[1] } @timeseries_points;
        $stats = calculate_statistics_for_trend(\@values_for_stats);

        # Only proceed if statistics could be calculated.
        if (defined $stats && defined $stats->{cv})
        {
            $cv_val = $stats->{cv};
            # Check if the data is not excessively volatile.
            $cv_check_passed = ($cv_val < $GROWTH_MAX_CV_THRESHOLD) ? 1 : 0;

            if ($cv_check_passed)
            {
                $regression = calculate_manual_linear_regression(\@timeseries_points);
                if (defined $regression && defined $regression->{slope})
                {
                    $slope = $regression->{slope};

                   # Check if there is a clear positive growth trend.
                    $slope_check_passed = ($slope > $GROWTH_MIN_POSITIVE_SLOPE_THRESHOLD) ? 1 : 0;

                    if ($slope_check_passed)
                    {
                        # Project the value N days into the future, scaled by the window size.
                        my $projection_periods = ($process_window_unit_str eq 'weeks') ? ($growth_projection_days / 7) * (1 / $process_window_size_val) : ($growth_projection_days / $process_window_size_val);

                        # The Hybrid model provides daily data points, so its projection period is simpler.
                        if (scalar(@$timeseries_values_aref) > $num_hist_periods * 1.5) # Heuristic for daily data
                        {
                            $projection_periods = $growth_projection_days;
                        }

                        my $intercept = $regression->{intercept}; # Retrieve intercept from regression result
                        $projected_val = $slope * ($num_hist_periods - 1 + $projection_periods) + $intercept;

                        if ($projected_val > $baseline_val)
                        {
                            $inflation_perc = (($projected_val - $baseline_val) / $baseline_val) * 100;
                            $was_capped = ($inflation_perc > $max_growth_inflation_percent) ? 1 : 0;

                            # Cap the inflation to the configured maximum percentage.
                            if ($was_capped) {
                                $inflation_perc = $max_growth_inflation_percent;
                            }

                            # Calculate the final adjustment values.
                            $growth_adj_value = ($baseline_val * $inflation_perc / 100);
                            $growth_adj_abs_value = $growth_adj_value;
                        }
                    }
                }
            }
        }
    }

    # --- Assemble the final rationale hash ---
    # All variables used here were pre-declared, ensuring this is always safe to execute.
    my %growth_rationale = (
        num_hist_periods => $num_hist_periods,
        stats_cv         => defined($cv_val) ? sprintf("%.4f", $cv_val) : "N/A",
        cv_check_passed  => $cv_check_passed,
        intercept        => defined($regression->{intercept}) ? sprintf("%.4f", $regression->{intercept}) : "N/A",
        slope            => defined($slope) ? sprintf("%.4f", $slope) : "N/A",
        slope_check_passed => $slope_check_passed,
        projected_val    => defined($projected_val) ? sprintf("%.4f", $projected_val) : "N/A",
        inflation_perc   => defined($inflation_perc) ? sprintf("%.2f", $inflation_perc) : "N/A",
        was_capped       => $was_capped,
    );

    # --- Return the final result as a single hash reference ---
    return {
        growth_adj     => $growth_adj_value,
        growth_adj_abs => $growth_adj_abs_value,
        rationale      => \%growth_rationale
    };
}

# ==============================================================================
# SUBROUTINE: _calculate_clipping_metrics
# PURPOSE:    Implements the Multi-Stage Confidence Pipeline to detect CPU
#             clipping and estimate latent demand from a buffer of raw
#             performance data.
# ARGS:
#   1. $raw_physc_aref (array ref): A buffer of raw, high-frequency PhysC values.
#   2. $raw_runq_aref (array ref): A buffer of raw, high-frequency RunQ values.
#   3. $max_capacity (numeric): The authoritative max_capacity for the state.
#   4. $smt (numeric): The SMT value for the state, for RunQ context.
# RETURNS:
#   - A hash reference containing the clipping analysis results:
#     {
#       isClipped => 0|1,
#       clippingConfidence => 0.0-1.0,
#       clippingSeverity => 'none'|'low'|'moderate'|'high',
#       unmetDemandEstimate => 0.0,
#       platformMarkers => { aix_runq_saturation => 0.0-1.0 }
#     }
# ==============================================================================
sub _calculate_clipping_metrics
{
    my ($raw_physc_aref, $raw_runq_aref, $max_capacity, $smt) = @_;

    my %results = (
        isClipped           => 0,
        clippingConfidence  => 0.0,
        clippingSeverity    => 'none',
        unmetDemandEstimate => 0.0,
        platformMarkers     => {},
    );

    # The pipeline requires a valid capacity limit and sufficient data points.
    return \%results unless (defined $max_capacity && $max_capacity > 0.1 && @$raw_physc_aref > 10);

    # --- Stage I & II: Raw Detection & Shape Analysis ---
    # To perform the analysis, we need several key percentiles from the raw data.
    # We calculate them once here for efficiency.
    my @sorted_physc = sort { $a <=> $b } @$raw_physc_aref;
    my $p99_9 = calculate_percentile(\@sorted_physc, 99.9);
    my $p95   = calculate_percentile(\@sorted_physc, 95);
    my $p90   = calculate_percentile(\@sorted_physc, 90);
    my $p50   = calculate_percentile(\@sorted_physc, 50);

    return \%results unless (defined $p99_9 && defined $p95 && defined $p50);

    my $saturation_level = $p99_9 / $max_capacity;

    # Calculate Peak Curvature Ratio (PCR)
    my $pcr_numerator   = $p99_9 - $p95;
    my $pcr_denominator = $p95 - $p50;
    my $pcr = ($pcr_denominator > $FLOAT_EPSILON) ? ($pcr_numerator / $pcr_denominator) : 999;

    # --- Stage III: Platform Validation (AIX RunQ) ---
    my $runq_saturation_score = 0.0;
    if (@$raw_runq_aref) {
        my @sorted_runq = sort { $a <=> $b } @$raw_runq_aref;
        my $runq_p95 = calculate_percentile(\@sorted_runq, 95);

        # A simple model for RunQ saturation: the ratio of P95 RunQ to the number
        # of logical CPUs (SMT). A score > 1.0 indicates queuing.
        if (defined $runq_p95 && defined $smt && $smt > 0) {
            $runq_saturation_score = $runq_p95 / $smt;
            $results{platformMarkers}{aix_runq_saturation} = sprintf("%.2f", $runq_saturation_score);
        }
    }

    # --- Stage IV: Final Clipping Confidence Score ---
    my $confidence = 0.0;

    # Base confidence from saturation level
    if ($saturation_level >= $CLIPPING_DEFINITE_THRESHOLD) {
        $confidence += 0.6; # High base confidence for definite saturation
    } elsif ($saturation_level >= $CLIPPING_POTENTIAL_THRESHOLD) {
        $confidence += 0.3; # Moderate base confidence for potential saturation
    }

    # Add confidence from shape analysis (low PCR)
    if ($pcr < $PCR_HIGH_CONFIDENCE_THRESHOLD) {
        $confidence += 0.35; # High confidence for very flat peaks
    } elsif ($pcr < $PCR_LOW_CONFIDENCE_THRESHOLD) {
        $confidence += 0.15; # Moderate confidence for somewhat flat peaks
    }

    # Boost confidence if platform markers confirm pressure
    if ($runq_saturation_score > 1.5) {
        $confidence += 0.2; # Significant boost for high RunQ
    }

    # Cap confidence at 1.0
    $results{clippingConfidence} = min(1.0, $confidence);

    # --- Latent Demand Estimation ---
    if ($results{clippingConfidence} > 0.65) {
        $results{isClipped} = 1;

        # Estimate unmet demand as a function of confidence and saturation severity.
        # This is a simple model: the more confident we are, and the closer to the
        # ceiling the workload is, the higher the estimated latent demand.
        my $severity_factor = ($saturation_level - $CLIPPING_POTENTIAL_THRESHOLD) * 10; # Normalise 0.9-1.0 range
        $severity_factor = max(0, min(1.0, $severity_factor));

        # The unmet demand is a fraction of the P90 value, scaled by our confidence and severity.
        $results{unmetDemandEstimate} = $p90 * $results{clippingConfidence} * $severity_factor * 0.25; # 0.25 is a tuning factor

        # Determine severity string for reporting
        if ($results{clippingConfidence} > 0.9) {
            $results{clippingSeverity} = 'high';
        } elsif ($results{clippingConfidence} > 0.75) {
            $results{clippingSeverity} = 'moderate';
        } else {
            $results{clippingSeverity} = 'low';
        }
    }

    return \%results;
}

# ==============================================================================
# SUBROUTINE: aggregate_decay_metrics
# PURPOSE:    Applies the final recency-weighted aggregation for decay models.
#             It takes the per-state metrics, synthesises a daily time series,
#             and calculates a single, final value for each profile.
# ==============================================================================
sub aggregate_decay_metrics {
    my ($metrics_href, $manifest_href, $states_by_vm_href, $transform_states_href, $raw_peak_tracker_ref, $args_ref) = @_;

    my %aggregated_results;

    # Determine the analysis reference date for the calculations.
    my $analysis_ref_obj;
    if ($args_ref->{analysis_reference_date}) {
        eval { $analysis_ref_obj = Time::Piece->strptime($args_ref->{analysis_reference_date}, "%Y-%m-%d"); };
    } else {
        # Fallback: find the latest date from all states if no date is provided.
        my $latest_epoch = 0;
        foreach my $vm_states (values %$states_by_vm_href) {
            foreach my $state (@$vm_states) {
                $latest_epoch = $state->{end_epoch} if $state->{end_epoch} > $latest_epoch;
            }
        }
        $analysis_ref_obj = Time::Piece->new($latest_epoch);
    }
    $analysis_ref_obj = $analysis_ref_obj->truncate(to => 'day');

    foreach my $vm_name (keys %$metrics_href) {
        # Find all unique profiles for this VM
        my %profiles_for_vm;
        foreach my $state_id (keys %{$metrics_href->{$vm_name}}) {
            next unless $state_id =~ /^\d+$/; # Only process numeric state IDs
            foreach my $key (keys %{$metrics_href->{$vm_name}{$state_id}}) {
                $profiles_for_vm{$_} = 1 for keys %{$manifest_href->{$key}{profiles} || {}};
            }
        }

        foreach my $profile_name (keys %profiles_for_vm) {
            # Find PhysC transform for this profile
            my ($physc_transform_key) = grep {
                $manifest_href->{$_}{metric} eq 'PhysC' &&
                exists $manifest_href->{$_}{profiles}{$profile_name}
            } keys %$manifest_href;

            next unless $physc_transform_key;

            # Build time series for PhysC
            my @physc_timeseries;
            my $last_state_id;
            foreach my $state_id (sort { $a <=> $b } keys %{$metrics_href->{$vm_name}}) {
                next unless $state_id =~ /^\d+$/;
                $last_state_id = $state_id;
                
                if (exists $metrics_href->{$vm_name}{$state_id}{$physc_transform_key}) {
                    my $perc_metrics = $metrics_href->{$vm_name}{$state_id}{$physc_transform_key};
                    my $profile_perc = $manifest_href->{$physc_transform_key}{profiles}{$profile_name}{percentiles}[0];
                    my $perc_key = "P" . $profile_perc;
                    
                    if (defined $perc_metrics->{$perc_key}) {
                        my $state_obj = $states_by_vm_href->{$vm_name}[$state_id];
                        push @physc_timeseries, {
                            value => $perc_metrics->{$perc_key},
                            date  => $state_obj->{end_time}
                        };
                    }
                }
            }

            # Calculate aggregated PhysC value
            if (@physc_timeseries) {
                my $aggregated_physc = calculate_recency_weighted_average(
                    \@physc_timeseries, 
                    $analysis_ref_obj, 
                    $args_ref->{decay_half_life_days}
                );
                
                $aggregated_results{$vm_name}{$profile_name}{aggregated_value} = $aggregated_physc;
            }

            # For each profile, extract any associated RunQ metrics from the LAST state.
            if (defined $last_state_id) {
                # Find the NormRunQ transform associated with this specific profile.
                my ($norm_rq_key) = grep { $manifest_href->{$_}{metric} eq 'NormRunQ' && exists $manifest_href->{$_}{profiles}{$profile_name} } keys %$manifest_href;
                if ($norm_rq_key && exists $metrics_href->{$vm_name}{$last_state_id}{$norm_rq_key}) {
                    my $norm_metrics = $metrics_href->{$vm_name}{$last_state_id}{$norm_rq_key};
                    # Add this profile's RunQ metrics to the aggregated results hash, nested under a temporary key.
                    $aggregated_results{$vm_name}{_RunQ}{normalized}{$profile_name} = $norm_metrics;
                }

                # Find the AbsRunQ transform associated with this specific profile.
                my ($abs_rq_key) = grep { $manifest_href->{$_}{metric} eq 'AbsRunQ' && exists $manifest_href->{$_}{profiles}{$profile_name} } keys %$manifest_href;
                if ($abs_rq_key && exists $metrics_href->{$vm_name}{$last_state_id}{$abs_rq_key}) {
                    my $abs_metrics = $metrics_href->{$vm_name}{$last_state_id}{$abs_rq_key};
                    # Add this profile's RunQ metrics to the aggregated results hash.
                    $aggregated_results{$vm_name}{_RunQ}{absolute}{$profile_name} = $abs_metrics;
                 }
               
                # Also store Peak value from last state
                my ($peak_key) = grep {
                    $manifest_href->{$_}{metric} eq 'PhysC' &&
                    exists $manifest_href->{$_}{profiles}{$profile_name} &&
                    $manifest_href->{$_}{profiles}{$profile_name}{calculate_peak}
               } keys %$manifest_href;

                if ($peak_key) {
                    my $transform_state_data = $transform_states_href->{$vm_name}{$last_state_id}{$peak_key};
                    if ($transform_state_data && exists $transform_state_data->{peak_value}) {
                        $aggregated_results{$vm_name}{Peak} = $transform_state_data->{peak_value};
                    }
                }
            }
        }
    }
    
    # --- Inject the TRUE raw peak into the aggregated results ---
    foreach my $vm_name (keys %aggregated_results) {
        if (exists $raw_peak_tracker_ref->{$vm_name}) {
            $aggregated_results{$vm_name}{Peak} = $raw_peak_tracker_ref->{$vm_name};
        }
    }
    return \%aggregated_results;

}

# ==============================================================================
# SUBROUTINE: apply_growth_prediction_to_metrics
# PURPOSE:    Applies a growth adjustment factor to the final metric values for
#             profiles that have growth prediction enabled in the manifest.
# ==============================================================================
sub apply_growth_prediction_to_metrics {
    my ($agg_metrics_href, $manifest_href, $analysis_data_href, $vm_name, $args_ref) = @_;

    # Build a map from profile name to its corresponding PhysC transform key
    # but only for profiles that have growth prediction enabled.
    my %profile_to_transform;
    foreach my $transform_key (keys %$manifest_href) {
        my $transform_info = $manifest_href->{$transform_key};
        next unless $transform_info->{metric} eq 'PhysC';

        foreach my $profile_name (keys %{$transform_info->{profiles}}) {
            my $directives = $transform_info->{profiles}{$profile_name};
            if ($directives->{enable_growth}) {
                $profile_to_transform{$profile_name} = $transform_key;
            }
        }
    }

    # Process the first profile found that has growth enabled.
    # In this aggregated model, the growth trend is VM-wide, so one calculation is sufficient.
    foreach my $profile_name (sort keys %profile_to_transform) {
        my $transform_key = $profile_to_transform{$profile_name};
        my $baseline_val = $agg_metrics_href->{$profile_name};

        # Build the complete timeseries from the analysis data using the correct transform key.
        my @timeseries;
        foreach my $state_id (sort { $a <=> $b } keys %{$analysis_data_href->{$vm_name}}) {
            if (exists $analysis_data_href->{$vm_name}{$state_id}{$transform_key}) {
                push @timeseries, unpack('f<*', $analysis_data_href->{$vm_name}{$state_id}{$transform_key});
            }
        }

        unless (@timeseries) {
            warn "Warning: No timeseries data found for growth calculation on profile $profile_name (transform: $transform_key)\n";
            next;
        }

        my $growth_result = calculate_growth_prediction(\@timeseries, $baseline_val);

        if ($growth_result) {
            # Store the single, authoritative growth adjustment and rationale for the entire VM.
            $agg_metrics_href->{GrowthAdj} = $growth_result->{growth_adj};
            $agg_metrics_href->{GrowthRationale} = $growth_result->{rationale};

#            if ($args_ref->{verbose}) {
#                print STDERR "DEBUG: Growth calculation succeeded for VM $vm_name (using profile $profile_name): adj=" . $growth_result->{growth_adj} . "\n";
#            }
        } else {
            warn "Warning: Growth calculation returned no result for profile $profile_name\n";
        }

        # Since growth is VM-wide in this model, we only need to calculate it once.
        return;
    }
}

# ==============================================================================
#                      Single-Pass Engine Subroutines
# ==============================================================================

sub run_single_pass_from_manifest {
    my ($args_ref) = @_;

    # 1. Unpack all necessary arguments for the entire run.
    my $manifest_file                = $args_ref->{manifest_file};
    my $nmon_dir                     = $args_ref->{nmon_dir};
    my $target_vm_name               = $args_ref->{target_vm_name};
    my $start_date_str               = $args_ref->{start_date_str};
    my $end_date_str                 = $args_ref->{end_date_str};
    my $include_states_selector      = $args_ref->{include_states};
    my $smt_value                    = $args_ref->{smt_value};
    my $runq_avg_method_str          = $args_ref->{runq_avg_method_str};
    my $verbose                      = $args_ref->{verbose};
    
    # Rounding Arguments
    my $round_arg                    = $args_ref->{round_arg};
    my $roundup_arg                  = $args_ref->{roundup_arg};
    
    # Analysis Model Flags
    my $enable_windowed_decay        = $args_ref->{enable_windowed_decay};
    my $decay_over_states_flag       = $args_ref->{decay_over_states};
    my $enable_growth_prediction     = $args_ref->{enable_growth_prediction};
    my $enable_clipping_detection    = $args_ref->{enable_clipping_detection};

    # Decay Model Tunables
    my $process_window_unit_str      = $args_ref->{process_window_unit};
    my $process_window_size_val      = $args_ref->{process_window_size};
    my $decay_half_life_days_val     = $args_ref->{decay_half_life_days};
    my $analysis_reference_date_str  = $args_ref->{analysis_reference_date};

    # Growth Model Tunables
    my $growth_projection_days       = $args_ref->{growth_projection_days};
    my $max_growth_inflation_percent = $args_ref->{max_growth_inflation_percent};

    # 2. Read and decode the manifest
    my $json_text = do { open my $fh, '<:encoding(utf8)', $manifest_file or die "Cannot open manifest $manifest_file: $!"; local $/; <$fh> };
    my $manifest = decode_json($json_text);

    # 3. Handle L2 Caching
    # Generate a unique key from the manifest and all relevant global arguments.
    my $canonical_key = generate_manifest_key($args_ref, $manifest);
    my $results_cache_path = File::Spec->catfile($nmon_dir, ".nfit.cache.results");
    
    my $cached_result_aref = lookup_cached_result($canonical_key, $results_cache_path);
    if (defined $cached_result_aref) {
        print STDERR "L2 Cache HIT. Returning cached result.\n" if $verbose;
        my $json_encoder = JSON->new->utf8;
        my @json_lines = map { $json_encoder->encode($_) } @$cached_result_aref;
        print STDOUT join("\n", @json_lines) . "\n";
        return; # Exit the function immediately on a cache hit.
    }

    # 4. Load and "Hydrate" Configuration States
    my $states_cache_file = File::Spec->catfile($nmon_dir, $CACHE_STATES_FILE);
    my $states_json_text = do { open my $fh, '<:encoding(utf8)', $states_cache_file; local $/; <$fh> };
    my $states_by_vm = decode_json($states_json_text);

    # This block modifies the %states_by_vm hash IN-PLACE, adding derived
    # metadata and Time::Piece objects that are used by all downstream functions.
    print STDERR "Hydrating configuration states...\n" if $verbose;
    foreach my $vm_name (keys %$states_by_vm) {
        if (ref($states_by_vm->{$vm_name}) eq 'ARRAY') {
            for (my $i = 0; $i < @{$states_by_vm->{$vm_name}}; $i++) {
                my $state = $states_by_vm->{$vm_name}[$i];
                $state->{state_id} = $i + 1;
                $state->{vm_name}  = $vm_name;
                if (defined $state->{start_epoch}) { $state->{start_time} = gmtime($state->{start_epoch}); }
                if (defined $state->{end_epoch})   { $state->{end_time}   = gmtime($state->{end_epoch}); }

                my $md = $state->{metadata};
                $md->{smt} //= $smt_value; # USE the unpacked global SMT as a fallback.

                my $ent       = $md->{entitlement} // 0;
                my $vcpu      = $md->{virtual_cpus} // 0;
                my $poolcpu   = $md->{pool_cpu} // 0;
                my $is_capped = $md->{capped} // 0;
                my $max_cpu_calc = $is_capped ? $ent : ($poolcpu > 0 && $vcpu > 0 ? min($vcpu, $poolcpu) : $vcpu);
                $md->{max_cpu} = $max_cpu_calc > 0 ? $max_cpu_calc : ($vcpu > 0 ? $vcpu : 0);
            }
        }
    }

    # 6. Global Filter Logic
    # This logic uses the hydrated states and global flags to determine the exact
    # scope of the analysis before the main data pass begins.
    my %vms_to_process;
    if (defined $target_vm_name) {
        %vms_to_process = map { $_ => 1 } split /,/, $target_vm_name;
    } else {
        %vms_to_process = map { $_ => 1 } keys %$states_by_vm;
    }

    if (defined $include_states_selector && lc($include_states_selector) ne 'all') {
        foreach my $vm_name (keys %$states_by_vm) {
            # Filter the list of states for each VM in-place.
            $states_by_vm->{$vm_name} = [ parse_state_selector($include_states_selector, $states_by_vm->{$vm_name}) ];
        }
    }

    # 7. Prepare for and execute the single pass
    my %analysis_data;    # Holds the packed binary data strings
    my %raw_data;         # Holds raw data for clipping
    my %transform_states; # Holds the in-memory state for rolling averages
    # --- Persistent state & raw peak tracker ---
    # This hash holds the calculator state (SMA queue, etc.) across all
    # configuration states for a given VM, ensuring continuity.
    my %persistent_transform_states;
    my %raw_peak_tracker; # Track true raw peak per VM
    my $data_cache_file = File::Spec->catfile($nmon_dir, $CACHE_DATA_FILE);
    open my $data_fh, '<', $data_cache_file or die "FATAL: Cannot open $data_cache_file: $!";
    my $header = <$data_fh>; # Read header to determine column indices if needed

    # --- Progress Indicator Initialisation ---
    my $record_counter = 0;
    my $progress_interval = 50_000;
    my $show_progress = ($verbose || -t STDERR); # Show progress if verbose or interactive
    printf STDERR "\r  -> Processing data cache ..." if ($show_progress);

    # Figure out total size once (works for regular files)
    my $total_bytes = do {
        my @st = stat($data_fh);
        $st[7] // 0  # size in bytes (0 if not a regular file/unknown)
    };

    while (my $line = <$data_fh>) {
        $record_counter++;

        if ($show_progress && ($record_counter % $progress_interval == 0)) {
            my ($ts_str) = split ',', $line, 2;
            my $tp;
            eval { $tp = Time::Piece->strptime($ts_str, "%Y-%m-%d %H:%M:%S"); };

            my $pct = 0;
            if ($total_bytes) {
                # tell($data_fh) is bytes consumed from start for normal files
                my $pos = tell($data_fh);
                $pct = $pos ? (100.0 * $pos / $total_bytes) : 0;
            }

            printf STDERR "\r  -> Processing data cache: %-19s (%.1f%%)",
                (defined $tp ? $tp->strftime("%Y-%m-%d %H:%M:%S") : ''), $pct;
        }

        process_data_point($line, $manifest, $states_by_vm, \%analysis_data, \%raw_data, \%transform_states, \%persistent_transform_states, {
            vms_to_process            => \%vms_to_process,
            start_date_str            => $start_date_str,
            end_date_str              => $end_date_str,
            smt_fallback              => $smt_value,
            raw_peak_tracker          => \%raw_peak_tracker,
            enable_clipping_detection => $enable_clipping_detection
        });
    }
    close $data_fh;

    # Print a final newline to clear the progress line
    printf STDERR "\rProcessing data cache: %-34s\n", "done (100.0%)" if $show_progress;
    
    # 8. Finalisation (calculates per-state/per-transform metrics)
    my $final_metrics = finalize_results(
        \%analysis_data, 
        \%raw_data, 
        $manifest, 
        \%transform_states, 
        $states_by_vm,
        { enable_clipping_detection => $enable_clipping_detection }
    );

    # 9. Determine if this is an aggregated run and choose the correct output path
    my @output_objects;
    my $is_aggregated_run = $args_ref->{enable_windowed_decay} || $args_ref->{decay_over_states};

    if ($is_aggregated_run) {
        # --- PATH A: AGGREGATED RUNS (Decay / Growth) ---
        
        # 1. Calculate recency-weighted baselines (WITHOUT growth)
        my $aggregated_results = aggregate_decay_metrics($final_metrics, $manifest, $states_by_vm, \%transform_states, \%raw_peak_tracker, $args_ref);
        
        # 2. Calculate per-profile growth adjustments using the baselines
        my $growth_results_by_profile = {};
        if ($args_ref->{enable_growth_prediction}) {
           $growth_results_by_profile = calculate_per_profile_growth_adjustments(
                $aggregated_results, \%analysis_data, $final_metrics, $manifest, $states_by_vm, $args_ref
            );
        }
        
        # 3. Assemble the final JSON, combining baselines and growth
        foreach my $vm_name (sort keys %$aggregated_results) {
            my $output_obj = assemble_aggregated_output(
                $vm_name,
                $aggregated_results->{$vm_name},
                $growth_results_by_profile, # Pass the new growth data
                $states_by_vm,
                $args_ref
            );
            push @output_objects, $output_obj if defined $output_obj;
        }

    } else {
        # --- PATH B: STANDARD PER-STATE RUNS ---
        @output_objects = @{ assemble_output($final_metrics, $manifest, $states_by_vm, \%transform_states, $args_ref) };
    }

    # 10. Save new result to L2 Cache and print to STDOUT
    my $lock_file = File::Spec->catfile($nmon_dir, $CACHE_LOCK_FILE);
    save_result_to_cache($canonical_key, \@output_objects, $results_cache_path, $lock_file) if @output_objects;
    
    my $json_encoder = JSON->new->utf8;
    foreach my $obj (@output_objects) {
        print STDOUT $json_encoder->encode($obj) . "\n";
    }
}

# ==============================================================================
# SUBROUTINE: process_data_point
# PURPOSE:    Processes a single line of data from the L1 cache. It applies
#             all global filters (VM, date) and per-transform time filters,
#             finds the correct configuration state, and updates all relevant
#             transform streams.
# ==============================================================================
sub process_data_point {
    my ($line, $manifest, $states_by_vm, $analysis_data, $raw_data, $transform_states, $persistent_transform_states, $filters) = @_;

    
    chomp $line;
    my ($ts_str, $vm, $physc, $runq) = split ',', $line, 4;

    # --- FIX: TRACK TRUE RAW PEAK ---
    # For every valid data point, before any filtering, update the absolute max
    # raw PhysC value seen for this VM. This is the new source for the 'Peak' value.
    my $raw_peak_tracker = $filters->{raw_peak_tracker};
    $raw_peak_tracker->{$vm} = $physc if (looks_like_number($physc) && $physc > ($raw_peak_tracker->{$vm} // 0));

    # Add a robust guard to skip any line with non-numeric or missing data.
    return unless (defined $physc && defined $runq && looks_like_number($physc) && looks_like_number($runq));

    # 1. Apply global VM filter passed from the orchestrator.
    return unless (exists $filters->{vms_to_process}{$vm} && exists $states_by_vm->{$vm});

    # 2. Apply global date filters.
    my $tp;
    eval { $tp = Time::Piece->strptime($ts_str, "%Y-%m-%d %H:%M:%S"); };
    return if $@; # Skip malformed timestamps.
    if (defined $filters->{start_date_str} && $tp->ymd lt $filters->{start_date_str}) { return; }
    if (defined $filters->{end_date_str}   && $tp->ymd gt $filters->{end_date_str})   { return; }
    
    # 3. Find the correct state for this timestamp using the hydrated state objects.
    my $state_obj;
    my $state_id;
    # This loop is optimised by assuming states are sorted by time.
    for (my $i=0; $i < @{$states_by_vm->{$vm}}; $i++) {
        my $s = $states_by_vm->{$vm}[$i];
        # Check if the timestamp falls within the state's epoch range.
        if ($tp->epoch >= $s->{start_time}->epoch && $tp->epoch <= $s->{end_time}->epoch) {
            $state_obj = $s;
            $state_id = $i;
            last;
        }
    }
    return unless $state_obj;

    # 4. Iterate through all required transforms for this data point.
    foreach my $key (keys %$manifest) {
        my $entry = $manifest->{$key};
        
        # --- COMPLETE PER-TRANSFORM TIME FILTER LOGIC ---
        # This block applies the specific time filters for THIS transform.
        my $line_time = substr($tp->hms(':'), 0, 5);

        # 4a. Check for --no-weekends filter.
        if ($entry->{no_weekends}) {
            my $day_of_week = $tp->day_of_week; # Sunday=1, Saturday=7 in Time::Piece
            next if ($day_of_week == 1 || $day_of_week == 7);
        }

        # 4b. Check for time-of-day filters (online/batch).
        my $time_filter_type = $entry->{time_filter};
        if ($time_filter_type ne 'none') {
            my $include_line = 0;
            if ($time_filter_type eq 'online') {
                $include_line = 1 if ($line_time ge '08:00' && $line_time lt '17:00');
            } elsif ($time_filter_type eq 'batch') {
                $include_line = 1 if ($line_time ge '18:00' || $line_time lt '06:00');
            }
            # Skip to the next transform if this data point is filtered out.
            next unless $include_line;
        }

        # --- Data processing continues only if the data point was not filtered out ---
        # This tracks the PEAK of the rolling average, which IS state-specific.
        my $t_state = $transform_states->{$vm}{$state_id}{$key} //= initialise_transform_state($entry);

        # --- Use the PERSISTENT state for the rolling average calculation ---
        my $p_state = $persistent_transform_states->{$vm}{$key} //= initialise_transform_state($entry);
        
        # 4c. Select the correct metric and calculate NormRunQ if needed.
        my $smt = $state_obj->{metadata}{smt} // $filters->{smt_fallback};
        my $value_to_process;
        if ($entry->{metric} eq 'PhysC') {
            $value_to_process = $physc;
        } elsif ($entry->{metric} eq 'AbsRunQ') {
            $value_to_process = $runq;
        } elsif ($entry->{metric} eq 'NormRunQ') {
            $value_to_process = (defined $physc && $physc > $ACTIVE_PHYSC_THRESHOLD && $smt > 0) ? ($runq / ($physc * $smt)) : undef;
        }
        
        # 4d. Calculate the rolling average for the selected metric.
        my $rolling_avg = update_transform_state($value_to_process, $p_state);

        
        # 4e. Store the rolling average in the packed array for percentile calculation.
        if (defined $rolling_avg) {
            update_packed_array($analysis_data, $vm, $state_id, $key, $rolling_avg, $t_state);
        }

        # 4f. Conditionally gather raw data if clipping detection is enabled.
        if ($filters->{enable_clipping_detection} && $entry->{metric} eq 'PhysC') {
            # Use a different hash to store raw data streams.
            update_packed_array($raw_data, $vm, $state_id, $key."_raw_physc", $physc);
            update_packed_array($raw_data, $vm, $state_id, $key."_raw_runq", $runq);
        }
    }
}

sub initialise_transform_state {
    my ($manifest_entry) = @_;
    my $state = {
        method => $manifest_entry->{method},
        window => $manifest_entry->{window},
    };
    if ($state->{method} eq 'SMA') {
        $state->{queue} = [];
    } elsif ($state->{method} eq 'EMA') {
        $state->{prev_value} = undef;
        $state->{alpha} = $EMA_ALPHAS{$manifest_entry->{decay}} // $EMA_ALPHAS{'medium'};
        $state->{warmup_queue} = []; # For initial EMA value
    }
    $state->{peak_value} = undef;
    return $state;
}

sub update_transform_state {
    my ($value, $state_obj) = @_;
    return undef unless defined $value;

    if ($state_obj->{method} eq 'SMA') {
        push @{$state_obj->{queue}}, $value;
        shift @{$state_obj->{queue}} while @{$state_obj->{queue}} > $state_obj->{window};
        return (scalar @{$state_obj->{queue}} == $state_obj->{window})
            ? (sum0(@{$state_obj->{queue}}) / $state_obj->{window})
            : undef;
    } elsif ($state_obj->{method} eq 'EMA') {
        if (!defined $state_obj->{prev_value}) {
            # Use SMA for the first window to seed the EMA
            push @{$state_obj->{warmup_queue}}, $value;
            if (@{$state_obj->{warmup_queue}} == $state_obj->{window}) {
                $state_obj->{prev_value} = sum0(@{$state_obj->{warmup_queue}}) / $state_obj->{window};
                return $state_obj->{prev_value};
            }
            return undef;
        }
        $state_obj->{prev_value} = ($value * $state_obj->{alpha}) + ($state_obj->{prev_value} * (1 - $state_obj->{alpha}));
        return $state_obj->{prev_value};
    }
    return $value; # Default for 'none' method
}

sub update_packed_array {
    my ($hash_ref, $vm, $state_id, $transform_key, $value, $transform_state_obj) = @_;
    unless (defined $value && looks_like_number($value)) {
        warn "Skipping invalid non-numeric value for $vm/$state_id/$transform_key: '$value'\n";
        return;
    }

    # Track the peak value
    if (!defined $transform_state_obj->{peak_value} || $value > $transform_state_obj->{peak_value}) {
        $transform_state_obj->{peak_value} = $value;
    }

    $hash_ref->{$vm}{$state_id}{$transform_key} //= '';
    $hash_ref->{$vm}{$state_id}{$transform_key} .= pack('f<', $value);
}

# ==============================================================================
# SUBROUTINE: finalize_results
# PURPOSE:    Iterates through all processed data, calculates final percentile
#             metrics from the packed binary arrays, and conditionally runs
#             additional analysis like clipping detection.
# ==============================================================================
sub finalize_results {
    my ($analysis_data, $raw_data, $manifest, $transform_states, $states_by_vm, $args_ref) = @_;
    my %final_metrics;

    my $enable_clipping_detection = $args_ref->{enable_clipping_detection};

    foreach my $vm (keys %$analysis_data) {
        foreach my $state_id (keys %{$analysis_data->{$vm}}) {
            foreach my $key (keys %{$analysis_data->{$vm}{$state_id}}) {
                my $packed_data = $analysis_data->{$vm}{$state_id}{$key};
                my $transform_info = $manifest->{$key};

                my @values = unpack('f<*', $analysis_data->{$vm}{$state_id}{$key});
                next unless @values; # Skip if there's no data for this transform.

                # Apply the --filter-above-perc logic BEFORE percentile calculation
                my $filter_perc = $transform_info->{filter_perc};
                my @final_values_for_percentile;

                my @sorted_values = sort { $a <=> $b } @values;

                if (defined $filter_perc && $filter_perc > 0) {
                    my $threshold = calculate_percentile(\@sorted_values, $filter_perc);
                    if (defined $threshold) {
                        # Filter the already sorted array to include values at or above the threshold.
                        @final_values_for_percentile = grep { $_ >= ($threshold - $FLOAT_EPSILON) } @sorted_values;
                    } else {
                        @final_values_for_percentile = @sorted_values;
                    }
                } else {
                    @final_values_for_percentile = @sorted_values; # Use all values if no filter.
                }
                
                # Get all unique percentiles needed for this transform
                my %percentiles_needed;
                foreach my $prof (values %{$transform_info->{profiles}}) {
                    $percentiles_needed{$_} = 1 for @{$prof->{percentiles}};
                }
                my @p_list = sort { $a <=> $b } keys %percentiles_needed;
                
                my $p_results = calculate_percentiles_from_packed(\@final_values_for_percentile, \@p_list);
                
                foreach my $p (keys %$p_results) {
                    $final_metrics{$vm}{$state_id}{$key}{"P$p"} = $p_results->{$p};
                }

                # --- USE of enable_clipping_detection flag ---
                # After calculating standard percentiles, run clipping detection if enabled.
                if ($enable_clipping_detection && $transform_info->{metric} eq 'PhysC') {
                    my $raw_physc_packed = $raw_data->{$vm}{$state_id}{$key."_raw_physc"} // '';
                    my $raw_runq_packed  = $raw_data->{$vm}{$state_id}{$key."_raw_runq"} // '';
                    
                    my @raw_physc = unpack('f<*', $raw_physc_packed);
                    my @raw_runq  = unpack('f<*', $raw_runq_packed);

                    # Get max_capacity and SMT from the state object for this calculation
                    my $max_cap = $states_by_vm->{$vm}[$state_id]{metadata}{max_cpu};
                    my $smt_val = $states_by_vm->{$vm}[$state_id]{metadata}{smt};

                    my $clipping_results = _calculate_clipping_metrics(\@raw_physc, \@raw_runq, $max_cap, $smt_val);

                    # Merge clipping results into the final metrics hash
                    $final_metrics{$vm}{$state_id}{$key}{Clipping} = $clipping_results;
                }
            }
        }
    }
    return \%final_metrics;
}

sub calculate_percentiles_from_packed {
    my ($values_ref, $percentiles_ref) = @_;
    
    # This function receives a pre-sorted array reference.
    my @values = @$values_ref;
    my $n = scalar @values;
    
    return {} if $n == 0;
    
    my %results;
    foreach my $p (@$percentiles_ref) {
        # Standard linear interpolation for percentile calculation.
        my $rank = ($p / 100) * ($n - 1);
        my $k = int($rank);
        my $d = $rank - $k;
        
        if ($k + 1 < $n) {
            $results{$p} = $values[$k] + $d * ($values[$k+1] - $values[$k]);
        } else {
            # Handle edge case where rank is at the very end.
            $results{$p} = $values[$k];
        }
    }
    
    return \%results;
}

# ==============================================================================
# SUBROUTINE: assemble_output
# PURPOSE:    Assembles the final JSON output. It correctly handles both standard
#             (per-state) and aggregated (decay/growth) result structures,
#             ensuring the output format is correct for the analysis model used.
# ==============================================================================
sub assemble_output {
    my ($final_metrics_href, $manifest_href, $states_by_vm_href, $transform_states_href, $raw_peak_tracker_ref, $args_ref) = @_;

    my @output_objects;

    my $rounding_method = 'none';
    my $round_increment;
    if (defined $args_ref->{round_arg}) {
        $rounding_method = 'standard';
        $round_increment = (length $args_ref->{round_arg} && $args_ref->{round_arg} =~ /^[0-9.]*$/) ? $args_ref->{round_arg} : $DEFAULT_ROUND_INCREMENT;
    } elsif (defined $args_ref->{roundup_arg}) {
        $rounding_method = 'up';
        $round_increment = (length $args_ref->{roundup_arg} && $args_ref->{roundup_arg} =~ /^[0-9.]*$/) ? $args_ref->{roundup_arg} : $DEFAULT_ROUND_INCREMENT;
    }

    foreach my $vm_name (sort keys %$final_metrics_href) {
        # Determine if the result for this VM is aggregated or per-state.
        # An aggregated result has profile names as the top-level keys under the VM.
        # A per-state result has numeric state IDs as the keys.
        my $is_aggregated_run = !grep { /^\d+$/ } keys %{$final_metrics_href->{$vm_name}};

        if ($is_aggregated_run) {
            # --- PATH A: Assemble a single, aggregated JSON object for a decay/growth run ---
            my $last_state_obj = $states_by_vm_href->{$vm_name}[-1];
            next unless $last_state_obj;
            
            my $last_state_id = $last_state_obj->{state_id} - 1; # Convert to zero-based index

            my $output_obj = {
                vmName       => $vm_name,
                analysisType => 'aggregated',
                state        => { stateCount => scalar(@{$states_by_vm_href->{$vm_name}}) },
                metadata     => $last_state_obj->{metadata},
                metrics      => { physc => {}, runq => { normalized => {}, absolute => {} }, growth => {} },
                debug        => { growthRationale => {} }
            };

            # Extract Peak from transform_states for the last state
            my ($peak_transform_key) = grep {
                $manifest_href->{$_}{metric} eq 'PhysC' &&
                exists $manifest_href->{$_}{profiles}{'P-99W1'}
            } keys %$manifest_href;
            
            if ($peak_transform_key && 
                exists $transform_states_href->{$vm_name} &&
                exists $transform_states_href->{$vm_name}{$last_state_id} &&
                exists $transform_states_href->{$vm_name}{$last_state_id}{$peak_transform_key}{peak_value}) {
                $output_obj->{metadata}{peakPhyscFromLatestState} = 
                    $transform_states_href->{$vm_name}{$last_state_id}{$peak_transform_key}{peak_value};
            }
            
            # Extract RunQ metrics from the last state (NOT from aggregated profiles)
            my ($norm_rq_key) = grep {
                $manifest_href->{$_}{metric} eq 'NormRunQ' &&
                exists $manifest_href->{$_}{profiles}{'P-99W1'}
            } keys %$manifest_href;
            
            my ($abs_rq_key) = grep {
                $manifest_href->{$_}{metric} eq 'AbsRunQ' &&
                exists $manifest_href->{$_}{profiles}{'P-99W1'}
            } keys %$manifest_href;
            
            if ($norm_rq_key && 
                exists $final_metrics_href->{$vm_name}{$last_state_id}{$norm_rq_key}) {
                my $norm_metrics = $final_metrics_href->{$vm_name}{$last_state_id}{$norm_rq_key};
                foreach my $perc_key (keys %$norm_metrics) {
                    if ($perc_key =~ /^P(\d+\.?\d*)$/) {
                        $output_obj->{metrics}{runq}{normalized}{'P-99W1'}{$perc_key} = 
                            apply_rounding($norm_metrics->{$perc_key}, $round_increment, $rounding_method);
                    }
                }
            }
            
            if ($abs_rq_key && 
                exists $final_metrics_href->{$vm_name}{$last_state_id}{$abs_rq_key}) {
                my $abs_metrics = $final_metrics_href->{$vm_name}{$last_state_id}{$abs_rq_key};
                foreach my $perc_key (keys %$abs_metrics) {
                    if ($perc_key =~ /^P(\d+\.?\d*)$/) {
                        $output_obj->{metrics}{runq}{absolute}{'P-99W1'}{$perc_key} = 
                            apply_rounding($abs_metrics->{$perc_key}, $round_increment, $rounding_method);
                    }
                }
            }
            
            # Process PhysC profiles from aggregated results
            foreach my $profile_name (keys %{$final_metrics_href->{$vm_name}}) {
                next if $profile_name =~ /^\d+$/; # Skip numeric (state ID) keys
                next unless ref($final_metrics_href->{$vm_name}{$profile_name}) eq 'HASH';
                
                my $profile_metrics = $final_metrics_href->{$vm_name}{$profile_name};
                my $final_value = $profile_metrics->{aggregated_value};
                next unless defined $final_value;
                
                $output_obj->{metrics}{physc}{$profile_name}{FinalValue} = 
                    apply_rounding($final_value, $round_increment, $rounding_method);
                
                if (exists $profile_metrics->{GrowthAdj}) {
                    $output_obj->{metrics}{growth}{adjustment} = $profile_metrics->{GrowthAdj};
                }
                if (exists $profile_metrics->{GrowthRationale}) {
                    $output_obj->{debug}{growthRationale} = $profile_metrics->{GrowthRationale};
                }
            }
            
            push @output_objects, $output_obj;
        } else {
            # --- PATH B: Assemble a standard, per-state JSON object ---
            foreach my $state_id (sort { $a <=> $b } keys %{$final_metrics_href->{$vm_name}}) {
                my $state_obj = $states_by_vm_href->{$vm_name}[$state_id];
                next unless $state_obj;

                my $output_obj = {
                    vmName       => $vm_name,
                    analysisType => 'state_based',
                    state        => {
                        id           => $state_id + 1,
                        durationDays => $state_obj->{duration_days} || 0,
                        startTimeISO => $state_obj->{start_time}->datetime . 'Z',
                        endTimeISO   => $state_obj->{end_time}->datetime . 'Z',
                    },
                    metadata     => $state_obj->{metadata},
                    metrics      => { physc => {}, runq => { normalized => {}, absolute => {} } },
                };

                foreach my $transform_key (keys %{$final_metrics_href->{$vm_name}{$state_id}}) {
                    my $metric_results = $final_metrics_href->{$vm_name}{$state_id}{$transform_key};
                    my $transform_info = $manifest_href->{$transform_key};
                    my $profiles_for_transform = $transform_info->{profiles};
                    my $metric_type = $transform_info->{metric};

                    foreach my $profile_name (keys %$profiles_for_transform) {
                        my $directives = $profiles_for_transform->{$profile_name};
                        
                        foreach my $p_val (@{ $directives->{percentiles} }) {
                            my $p_key = "P" . clean_perc_label($p_val);
                            my $value = $metric_results->{$p_key};
                            my $rounded_value = apply_rounding($value, $round_increment, $rounding_method);

                            if ($metric_type eq 'PhysC') {
                                $output_obj->{metrics}{physc}{$profile_name}{$p_key} = $rounded_value;
                            } elsif ($metric_type eq 'NormRunQ') {
                                $output_obj->{metrics}{runq}{normalized}{$profile_name}{$p_key} = $rounded_value;
                            } elsif ($metric_type eq 'AbsRunQ') {
                                $output_obj->{metrics}{runq}{absolute}{$profile_name}{$p_key} = $rounded_value;
                            }
                        }
                        
                        if ($metric_type eq 'PhysC' && $directives->{calculate_peak}) {
                            my $peak_value = $transform_states_href->{$vm_name}{$state_id}{$transform_key}{peak_value};
                            if (defined $peak_value) {
                                $output_obj->{metrics}{physc}{$profile_name}{'Peak'} = apply_rounding($peak_value, $round_increment, $rounding_method);
                            }
                        }

                        if ($metric_type eq 'PhysC' && exists $metric_results->{Clipping}) {
                            $output_obj->{metrics}{physc}{$profile_name}{Clipping} = $metric_results->{Clipping};
                        }
                    }
                }
                push @output_objects, $output_obj;
            }
            # --- Inject the TRUE raw peak into the per-state output ---
            # The true peak is a VM-level metric, not a state-level one. For consistency,
            # we add it to the metadata of the LAST state reported for the VM.
            if (@output_objects && exists $raw_peak_tracker_ref->{$vm_name}) {
                $output_objects[-1]->{metadata}{peakPhyscFromLatestState} = apply_rounding($raw_peak_tracker_ref->{$vm_name}, $round_increment, $rounding_method);
            }
        }
    }
    return \@output_objects;
}

# ==============================================================================
# SUBROUTINE: generate_manifest_key
# PURPOSE:    Creates a unique, reproducible key for L2 caching by hashing
#             the contents of the manifest and all relevant global arguments
#             that affect the final calculation.
# ==============================================================================
sub generate_manifest_key {
    my ($args_href, $manifest_href) = @_;
    
    # Create a canonical data structure containing ONLY the elements
    # that influence the final numerical result.
    my $data_for_key = {
        manifest => $manifest_href,
        args => {
            # --- Data Scoping & Filtering Flags ---
            target_vm_name          => $args_href->{target_vm_name},
            start_date_str          => $args_href->{start_date_str},
            end_date_str            => $args_href->{end_date_str},
            include_states_selector => $args_href->{include_states_selector},

            # --- Global Analysis Model Flags & Tunables ---
            smt_value                    => $args_href->{smt_value},
            runq_avg_method_str          => $args_href->{runq_avg_method_str},
            enable_windowed_decay        => $args_href->{enable_windowed_decay},
            decay_over_states_flag       => $args_href->{decay_over_states_flag},
            process_window_unit_str      => $args_href->{process_window_unit_str},
            process_window_size_val      => $args_href->{process_window_size_val},
            decay_half_life_days_val     => $args_href->{decay_half_life_days_val},
            analysis_reference_date_str  => $args_href->{analysis_reference_date_str},
            enable_growth_prediction     => $args_href->{enable_growth_prediction},
            growth_projection_days       => $args_href->{growth_projection_days},
            max_growth_inflation_percent => $args_href->{max_growth_inflation_percent},
            enable_clipping_detection    => $args_href->{enable_clipping_detection},

            # --- Output Formatting Flags ---
            round_arg                    => $args_href->{round_arg},
            roundup_arg                  => $args_href->{roundup_arg},
        }
    };
    
    # Use Storable's 'freeze' to create a canonical binary representation
    # of the data structure. This is essential for a consistent hash.
    my $canonical_string = freeze($data_for_key);
    
    # Return a SHA-256 hash of the string for a clean, fixed-length key.
    return sha256_hex($canonical_string);
}

# Helper for hash references (with safe fallback)
sub _safe_get {
    my ($href, $key, $fallback) = @_;
    return (ref($href) eq 'HASH' && exists $href->{$key}) ? $href->{$key} : $fallback;
}

sub _safe_dig {
    my ($href, @path) = @_;
    my $cur = $href;
    for my $k (@path) {   
        return undef unless ref($cur) eq 'HASH' && exists $cur->{$k};
        $cur = $cur->{$k};
    }       
    return $cur;
}

# ==============================================================================
# SUBROUTINE: assemble_aggregated_output
# PURPOSE:    Assembles the final JSON for a single VM from fully aggregated results.
#             This is the dedicated output path for decay/growth models.
# ==============================================================================
sub assemble_aggregated_output {
    my ($vm_name, $aggregated_metrics_href, $growth_results_href, 
        $states_by_vm_href, $args_ref) = @_;

    my $rounding_method = 'none';
    my $round_increment;
    if (defined $args_ref->{round_arg}) {
        $rounding_method = 'standard';
        $round_increment = (length $args_ref->{round_arg} && $args_ref->{round_arg} =~ /^[0-9.]*$/) ? $args_ref->{round_arg} : $DEFAULT_ROUND_INCREMENT;
    } elsif (defined $args_ref->{roundup_arg}) {
        $rounding_method = 'up';
        $round_increment = (length $args_ref->{roundup_arg} && $args_ref->{roundup_arg} =~ /^[0-9.]*$/) ? $args_ref->{roundup_arg} : $DEFAULT_ROUND_INCREMENT;
    }

    my $last_state_obj = $states_by_vm_href->{$vm_name}[-1];
    return undef unless $last_state_obj;

    my $output_obj = {
        vmName       => $vm_name,
        analysisType => ($args_ref->{decay_over_states}) ? 'hybrid_decay_aggregated' : 'windowed_decay_aggregated',
        state        => { stateCount => scalar(@{$states_by_vm_href->{$vm_name}}) },
        metadata     => dclone($last_state_obj->{metadata}),
        metrics      => { physc => {}, runq => { normalized => {}, absolute => {} } },
        debug        => {}
    };
    
    # Populate PhysC metrics for each profile
    foreach my $profile_name (keys %{$aggregated_metrics_href}) {
        next if $profile_name eq '_RunQ' or $profile_name eq 'Peak';
        
        my $baseline_val = _safe_get($aggregated_metrics_href, $profile_name, 0)->{aggregated_value};
        
        # Get growth adjustment for this profile (default to 0)
        my $growth_adj = 0;
        my $growth_rationale = {};

        # Do NOT apply growth adjustments to the P-99W1 profile (similar as for Peak - this is an empirical value, not a prediction)
        if ($profile_name ne $MANDATORY_PEAK_PROFILE_FOR_HINT) {
            if (my $growth_res = _safe_dig($growth_results_href, $vm_name, $profile_name)) {
                $growth_adj = $growth_res->{growth_adj} // 0;
                $growth_rationale = $growth_res->{rationale} // {};
            }
        }
        
        my $final_val_with_growth = $baseline_val + $growth_adj;
        
        # Store all components in the output structure for transparency
        $output_obj->{metrics}{physc}{$profile_name} = {
            BaseValue       => apply_rounding($baseline_val, $round_increment, $rounding_method),
            GrowthAdj       => sprintf("%.4f", $growth_adj),
            FinalValue      => apply_rounding($final_val_with_growth, $round_increment, $rounding_method),
        };
        
        # Place the rationale in the top-level debug hash for this profile if it exists
        if (scalar keys %$growth_rationale > 0) {
            $output_obj->{debug}{growthRationale} = $growth_rationale;
        }
    }

    # Populate Peak and RunQ metrics
    $output_obj->{metadata}{peakPhyscFromLatestState} = apply_rounding($aggregated_metrics_href->{Peak}, $round_increment, $rounding_method) if exists $aggregated_metrics_href->{Peak};
    $output_obj->{metrics}{runq} = $aggregated_metrics_href->{_RunQ} if exists $aggregated_metrics_href->{_RunQ};

    return $output_obj;
}

# ==============================================================================
# SUBROUTINE: calculate_per_profile_growth_adjustments (REVISED)
# PURPOSE:    Calculates growth adjustment for each profile that requested it.
#             Must be called AFTER aggregate_decay_metrics but BEFORE output assembly.
# ARGUMENTS:
#   1. $aggregated_results_href - Hash of baseline values per VM/profile (WITHOUT growth)
#   2. $analysis_data_href - The %analysis_data hash containing packed timeseries
#   3. $final_metrics_href - Per-state aggregated metrics (NEW REQUIREMENT)
#   4. $manifest_href - The manifest to identify which profiles need growth
#   5. $states_by_vm_href - The hydrated states hash for time context
#   6. $args_ref - Hash with growth parameters (projection_days, etc.)
# RETURNS:
#   Hash ref: {$vm_name}{$profile_name} = {growth_adj => $value, rationale => {...}}
# ==============================================================================
sub calculate_per_profile_growth_adjustments {
    my ($aggregated_results_href, $analysis_data_href, $final_metrics_href, 
        $manifest_href, $states_by_vm_href, $args_ref) = @_;
    
    my %growth_results;

    foreach my $vm_name (keys %{$aggregated_results_href}) {
       foreach my $profile_name (keys %{$aggregated_results_href->{$vm_name}}) {

            my ($transform_key) = grep {
                $manifest_href->{$_}{metric} eq 'PhysC' &&
                exists $manifest_href->{$_}{profiles}{$profile_name} &&
                exists $manifest_href->{$_}{profiles}{$profile_name}{enable_growth}
            } keys %$manifest_href;
            next unless $transform_key;

            my $baseline_val = $aggregated_results_href->{$vm_name}{$profile_name}{aggregated_value};
            next unless (defined $baseline_val && $baseline_val > $FLOAT_EPSILON);

            my @timeseries_for_growth;

            # --- KEY CHANGE: Select the correct time-series source based on the model ---
            if ($args_ref->{decay_over_states}) {
                # HYBRID MODEL (--decay-over-states): Synthesize a clean, daily time-series.
                @timeseries_for_growth = @{synthesize_daily_timeseries_for_growth(
                    $vm_name, $profile_name, $final_metrics_href, 
                    $states_by_vm_href, $manifest_href
                )};
           } else {
                # WINDOWED DECAY MODEL (--enable-windowed-decay): Use the raw, continuous rolling average data.
                for (my $i = 0; $i < @{ $states_by_vm_href->{$vm_name} }; $i++) {
                    if (exists $analysis_data_href->{$vm_name}{$i}{$transform_key}) {
                        push @timeseries_for_growth, unpack('f<*', 
                            $analysis_data_href->{$vm_name}{$i}{$transform_key});
                    }
                }
            }

            if (@timeseries_for_growth >= $GROWTH_MIN_HISTORICAL_PERIODS) {
                my $growth_result = calculate_growth_prediction(\@timeseries_for_growth, $baseline_val);
                $growth_results{$vm_name}{$profile_name} = $growth_result;
            }
        }
    }
    return \%growth_results;
}

# ==============================================================================
# SUBROUTINE: synthesize_daily_timeseries_for_growth
# PURPOSE:    Converts per-state metrics into a daily time-series for growth analysis.
#             Replicates the legacy engine's methodology for --decay-over-states.
# ARGUMENTS:
#   1. $vm_name - The VM being processed
#   2. $profile_name - The profile being processed
#   3. $final_metrics_href - Per-state aggregated metrics (from finalize_results)
#   4. $states_by_vm_href - Hydrated state objects with date ranges
#   5. $manifest_href - To find the correct transform key
#   6. $analysis_ref_date - The reference date for the analysis (Time::Piece object)
# RETURNS:
#   Array ref of daily metric values, ordered chronologically
# ==============================================================================
sub synthesize_daily_timeseries_for_growth {
    my ($vm_name, $profile_name, $final_metrics_href, $states_by_vm_href, 
        $manifest_href, $analysis_ref_date) = @_;

    # Find the PhysC transform key for this profile
    my ($transform_key) = grep {
        $manifest_href->{$_}{metric} eq 'PhysC' &&
        exists $manifest_href->{$_}{profiles}{$profile_name}
    } keys %$manifest_href;
    return [] unless $transform_key;

    # Get the percentile key for this profile
    my $percentiles = _safe_dig($manifest_href, $transform_key, 'profiles', $profile_name, 'percentiles');
    # GUARD: Ensure the percentiles array exists and is not empty
    return [] unless (defined $percentiles && ref $percentiles eq 'ARRAY' && @$percentiles);

    my $p_val = $percentiles->[0]; # Assumes first percentile is the primary one
    my $p_key = "P" . clean_perc_label($p_val);

    # Build a hash of dates to metric values
    my %date_to_value;
    
    foreach my $state (@{ $states_by_vm_href->{$vm_name} }) {
        my $state_id = $state->{state_id} - 1; # Convert to zero-based index

        # Get the aggregated metric for this state
        my $state_metrics = $final_metrics_href->{$vm_name}{$state_id}{$transform_key};
        next unless $state_metrics;
        
        my $metric_value = $state_metrics->{$p_key};
        next unless defined $metric_value;
        
        # Iterate through each day this state was active
        my $current_date = $state->{start_time}->truncate(to => 'day');
        my $end_date = $state->{end_time}->truncate(to => 'day');
        
        while ($current_date <= $end_date && $current_date <= $analysis_ref_date) {
            my $date_key = $current_date->ymd;
            $date_to_value{$date_key} = $metric_value;
            $current_date += ONE_DAY;
        }
    }

    # Convert to chronologically sorted array
    my @daily_timeseries;
    foreach my $date_key (sort keys %date_to_value) {
        push @daily_timeseries, $date_to_value{$date_key};
    }
    
    return \@daily_timeseries;
}

# --- usage ---
# Generates and returns the usage/help message for the script.
#
sub usage
{
    my $script_name = $0;
    $script_name =~ s{.*/}{};
    my $valid_decays_usage = join("|", sort keys %EMA_ALPHAS);
    return <<END_USAGE;
Usage: $script_name --nmondir <cache_directory> [options]
   or: $script_name --mgsys <serial_number> [options]

Analyses performance data from a pre-built nFit cache directory. This tool
calculates key metrics based on percentile, rolling average, and optional
recency-weighted decay models to provide right-sizing recommendations.

Cache Location (provide one):
  --nmondir <directory>    : The full path to a valid nFit cache directory
                             (e.g., /path/to/stage/12345ABC).
  --mgsys <serial>         : The serial number of the managed system to analyse.
                             If --nmondir is not specified, this will default to
                             looking for a cache at './stage/<serial>/'.

Averaging Method (used within each window if decay enabled):
  --avg-method <method>    : Averaging method for PhysC: 'sma' or 'ema'. (Default: $DEFAULT_AVG_METHOD)
  --decay <level>          : If 'ema' for PhysC, specifies decay level: $valid_decays_usage.
                           (Default: $DEFAULT_DECAY_LEVEL).
  --runq-decay <level>     : Optional. If 'ema' for RunQ, specifies its decay level.
  -w, --window <minutes>     : Window for SMA/EMA calculations. (Default: $DEFAULT_WINDOW_MINUTES min).

RunQ Data Options:
  --smt <N>                : SMT level for RunQ normalisation (Default: $DEFAULT_SMT).
                             Note: This is a fallback; SMT from the cache is preferred.
  --runq-norm-perc <list>  : Comma-separated percentiles for Normalised RunQ.
  --runq-abs-perc <list>   : Comma-separated percentiles for Absolute RunQ.
  --runq-avg-method <none|sma|ema> : Averaging method for RunQ data.

Filtering Options (Applied BEFORE windowing if decay enabled):
  -s, --startdate <YYYY-MM-DD> : Ignore data before this date.
  -ed, --enddate <YYYY-MM-DD>  : Ignore data after this date.
  -startt <HH:MM> / -endt <HH:MM> : Daily time filter.
  -online / -batch           : Shortcut daily time filters.
  -no-weekends               : Exclude data from Saturdays and Sundays.
  -vm, --lpar <name>         : Analyse only the specified VM/LPAR name(s), comma-separated.

PhysC Calculation Options:
  -p, --percentile <value>   : Final percentile of PhysC (0-100) (Default: $DEFAULT_PERCENTILE).
  -k, --peak                 : Calculate peak PhysC value.
  --filter-above-perc <N>    : Optional. Filter rolling PhysC values before PXX calc.

Windowed Recency Decay & Growth Prediction:
  --enable-windowed-decay    : Enable internal windowed processing with recency decay.
  --decay-over-states        : Enable Hybrid State-Time Decay Model.
  --enable-growth-prediction : Enable growth prediction (requires a decay model).
  (See documentation for more detail on decay and growth parameters)

Rounding Options:
  -r[=increment] / -u[=increment] : Round results (Default increment: $DEFAULT_ROUND_INCREMENT).

Other:
  -h, --help                 : Display this help message.
  -v, --verbose              : Enable verbose output for debugging.
  --version                  : Display script version.
END_USAGE
}
