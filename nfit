#!/usr/bin/env perl
# NAME     : nfit
# AUTHOR   : Niël Lambrechts (https://github.com/niel-lambrechts)
# PURPOSE  : Analyse NMON PhysC and optionally RunQ data for AIX and Linux on Power
#            VM right-sizing recommendations. Calculates rolling average (SMA or EMA)
#            percentiles, optionally absolute peaks, and specified percentiles of
#            normalised and absolute run-queue statistics.
#            RunQ data can now also be smoothed using SMA or EMA before percentile calculation.
#            If windowed decay is enabled, metrics are calculated per window,
#            weighted by recency, and then aggregated.
#            NEW: Can predict future growth based on windowed data trends.
#            Supports filtering by date (start and end), time, VM, weekends,
#            and percentile threshold, plus rounding options.
# REQUIRES : Perl, Time::Piece, POSIX (for ceil), List::Util (for sum0, max, min), Getopt::Long, File::Temp, version, File::Spec

use strict;
use warnings;
use Getopt::Long qw(GetOptions);
use File::Temp qw(tempfile tempdir);
use List::Util qw(sum0 max min);
use Scalar::Util qw(looks_like_number);
use POSIX qw(ceil);
use Time::Piece;
use Time::Seconds;
use version;
use File::Spec;
use File::Find;
use List::MoreUtils qw(uniq);
use JSON;
use Fcntl qw(:flock);
use File::Basename qw(dirname);
use Cwd 'abs_path';
use Digest::SHA qw(sha256_hex);
use Storable qw(freeze dclone);
use File::Copy;

# Canonical name for the mandatory peak helper profile (used for peak analysis and as the
# source of RunQ evidence for credibility checks in downstream tools).
my $MANDATORY_PEAK_PROFILE_FOR_HINT = "Peak_P-99W1";

# Back-compat: legacy name used by standalone/older workflows.
my $LEGACY_MANDATORY_PEAK_PROFILE_FOR_HINT = "P-99W1";

# --- Force STDERR to be unbuffered for real-time progress updates ---
# This is critical for the --force-progress flag to work correctly when
# this script is called from another process like nfit-profile.
select STDERR;
$| = 1;
select STDOUT;

# --- Version ---
my $VERSION = '6.25.306.0';

# --- Configuration ---
my $DEFAULT_AVG_METHOD     = 'ema';
my $DEFAULT_DECAY_LEVEL    = 'medium';
my $DEFAULT_WINDOW_MINUTES = 15;
my $DEFAULT_PERCENTILE     = 95;
my $DEFAULT_ROUND_INCREMENT= 0.05;
my $DEFAULT_SMT            = 8;
my $DEFAULT_RUNQ_NORM_PERC = "50,90";
my $DEFAULT_RUNQ_ABS_PERC  = "90";
my $DEFAULT_RUNQ_AVG_METHOD = "ema";

my $FLOAT_EPSILON          = 1e-9; # Small number for float comparisons
my $ACTIVE_PHYSC_THRESHOLD = 0.15; # Original threshold for RunQ normalization
my $SAFETY_MARGIN_FOR_THRESHOLD_CONST = 1.05; # Safety margin for ACTIVE_PHYSC_THRESHOLD calculation
my $PLACEHOLDER_ENTITLEMENT_ADJUSTMENT_FACTOR = 1.0; # Placeholder in ACTIVE_PHYSC_THRESHOLD calc

# Windowed Decay Defaults (if enabled for nfit itself)
my $DEFAULT_PROCESS_WINDOW_UNIT = "weeks";
my $DEFAULT_PROCESS_WINDOW_SIZE = 1;
my $DEFAULT_DECAY_HALF_LIFE_DAYS = 30;

# EMA Alpha values based on decay level
my %EMA_ALPHAS = (
    'low'        => 0.03,
    'medium'     => 0.08,
    'high'       => 0.15,
    'very-high'  => 0.30,
    'extreme'    => 0.40,
);

# --- Growth Prediction Configuration (NEW) ---
my $DEFAULT_GROWTH_PROJECTION_DAYS         = 90;
my $DEFAULT_MAX_GROWTH_INFLATION_PERCENT   = 25;

# --- Cache Configuration ---
my $CACHE_MANIFEST_FILE = ".nfit.cache.manifest";
my $CACHE_STATES_FILE   = ".nfit.cache.states";
my $CACHE_DATA_FILE     = ".nfit.cache.data";
my $CACHE_INDEX_FILE    = ".nfit.cache.idx";
my $CACHE_LOCK_FILE     = ".nfit.cache.lock";

# Internal constants for growth heuristics (not user-configurable initially)
my $GROWTH_MIN_HISTORICAL_PERIODS       = 5;    # Min number of windowed periods to attempt trend
my $GROWTH_MAX_CV_THRESHOLD             = 0.50; # Max Coefficient of Variation (StdDev/Mean); if > this, data too volatile
my $GROWTH_MIN_POSITIVE_SLOPE_THRESHOLD = 0.01; # Min slope (units/period) to consider as actual growth for inflation
my $GROWTH_MAX_PROJECTION_HISTORY_RATIO = 2.0;  # Max ratio of projection duration to history duration used for trend

# --- Clipping Detection Configuration ---
my $CLIPPING_DEFINITE_THRESHOLD = 0.95; # P99.75 > 95% of max_capacity = definite clip
my $CLIPPING_POTENTIAL_THRESHOLD = 0.90; # P99.75 > 90% of max_capacity = potential clip
my $PCR_LOW_CONFIDENCE_THRESHOLD = 0.4;  # Peak Curvature Ratio below this adds confidence
my $PCR_HIGH_CONFIDENCE_THRESHOLD = 0.2; # Peak Curvature Ratio below this adds high confidence

# --- Argument Parsing ---
my $manifest_file;
my $physc_csv_file;
my $physc_csv_dirname;
my $runq_csv_file;
my $vm_config_file;
my $nmon_dir;
my $mgsys_filter;
my $start_date_str;
my $end_date_str;
my $target_vm_name;
my $exclude_vms_str;
my $round_arg;
my $roundup_arg;
my $smt_value = $DEFAULT_SMT;
my $runq_norm_perc_str = $DEFAULT_RUNQ_NORM_PERC;
my $runq_abs_perc_str  = $DEFAULT_RUNQ_ABS_PERC;
my $runq_avg_method_str = $DEFAULT_RUNQ_AVG_METHOD;

# Windowed Decay options
my $enable_windowed_decay = 0;
my $process_window_unit_str = $DEFAULT_PROCESS_WINDOW_UNIT;
my $process_window_size_val = $DEFAULT_PROCESS_WINDOW_SIZE;
my $decay_half_life_days_val = $DEFAULT_DECAY_HALF_LIFE_DAYS;
my $analysis_reference_date_str;
my $decay_over_states_flag = 0;

# Growth Prediction options
my $enable_growth_prediction = 0;
my $growth_projection_days = $DEFAULT_GROWTH_PROJECTION_DAYS;
my $growth_projection_days_user_override = 0;  # Track explicit user specification
my $max_growth_inflation_percent = $DEFAULT_MAX_GROWTH_INFLATION_PERCENT;

my $show_states_flag = 0;
my $include_states_selector = 'all'; # Default value
my $reset_cache = 0;
my $enable_clipping_detection = 0;
my $profile_label;

# Legacy flag support ---
my $percentile = $DEFAULT_PERCENTILE;
my $window_minutes = $DEFAULT_WINDOW_MINUTES;
my $avg_method     = $DEFAULT_AVG_METHOD;
my $decay_level    = $DEFAULT_DECAY_LEVEL;
my $runq_decay_level_arg;
my $filter_above_perc_arg;
my $peak_flag = 0;
my $online_flag = 0;
my $batch_flag  = 0;
my $no_weekends_flag = 0;

my ($help, $show_version, $verbose, $quiet, $show_progress_flag) = (0, 0, 0, 0, 0);

GetOptions(
    'manifest=s'          => \$manifest_file,
    'physc-data|pc=s'     => \$physc_csv_file,
    'runq-data|rq=s'      => \$runq_csv_file,
    # Legacy flag support ---
    'percentile|p=f'          => \$percentile,
    'window|w=i'              => \$window_minutes,
    'avg-method=s'            => \$avg_method,
    'decay=s'                 => \$decay_level,
    'runq-decay=s'            => \$runq_decay_level_arg,
    'filter-above-perc=f'     => \$filter_above_perc_arg,
    'peak|k'                  => \$peak_flag,
    'online'                  => \$online_flag,
    'batch'                   => \$batch_flag,
    'no-weekends'             => \$no_weekends_flag,
    'runq-norm-perc=s'    => \$runq_norm_perc_str,
    'runq-abs-perc=s'     => \$runq_abs_perc_str,
    # --- End of legacy flags ---
    'config=s'            => \$vm_config_file,
    'nmondir=s'           => \$nmon_dir,
    'mgsys|system|serial|host=s' => \$mgsys_filter,
    'startdate|s=s'       => \$start_date_str,
    'enddate|ed=s'        => \$end_date_str,
    'round|r:f'           => \$round_arg,
    'roundup|u:f'         => \$roundup_arg,
    'vms|vm|lpar|lpars=s' => \$target_vm_name,
    'exclude-vms|exclude-vm=s' => \$exclude_vms_str,
    'smt=i'               => \$smt_value,
    'runq-avg-method=s'   => \$runq_avg_method_str,
    # Windowed Decay Options
    'enable-windowed-decay'     => \$enable_windowed_decay,
    'process-window-unit=s'     => \$process_window_unit_str,
    'process-window-size=i'     => \$process_window_size_val,
    'decay-half-life-days=i'    => \$decay_half_life_days_val,
    'analysis-reference-date=s' => \$analysis_reference_date_str,
    'decay-over-states'         => \$decay_over_states_flag,
    # Growth Prediction Options (NEW)
    'enable-growth-prediction'       => \$enable_growth_prediction,
    'growth-projection-days=i'       => sub {
        $growth_projection_days = $_[1];
        $growth_projection_days_user_override = 1;  # User explicitly set this value
    },
    'max-growth-inflation-percent=i' => \$max_growth_inflation_percent,
    'show-states'               => \$show_states_flag,
    'include-states=s'          => \$include_states_selector,
    'reset-cache'               => \$reset_cache,
    # Clipping
    'enable-clipping-detection' => \$enable_clipping_detection,
    'profile-label=s'           => \$profile_label,
    # General Options
    'help|h'              => \$help,
    'verbose|v'           => \$verbose,
    'version'             => \$show_version,
    'q|quiet'             => \$quiet,
    'show-progress'       => \$show_progress_flag,
) or die usage();

# Enforce mutual exclusivity for decay models
if ($enable_windowed_decay && $decay_over_states_flag) {
    die "FATAL: Incompatible arguments. --enable-windowed-decay and --decay-over-states are mutually exclusive.\n" .
        "       Please select only one decay model for the analysis run.\n";
}

# --- Validation ---
if ($show_version)
{
    print STDERR "nfit version $VERSION\n";
    exit 0;
}

if ($help)
{
    print STDERR usage();
    exit 0;
}

# This script now requires a cache directory to be specified, either directly
# via --nmondir or implicitly via --mgsys.
if (!$nmon_dir && !$mgsys_filter)
{
    print STDERR "Error: You must specify a data source cache via --nmondir or --mgsys.\n\n";
    print STDERR usage();
    exit 0;
}

# --- Smart Dispatcher Logic ---
# This logic determines the final, absolute path to the cache directory that
# will be used for analysis. It provides flexibility for the user.
my $resolved_cache_dir;
my $DEFAULT_BASE_STAGE_DIR = File::Spec->catfile(dirname(abs_path($0)), 'stage');

if (defined $nmon_dir)
{
    # Case 1: User provides --nmondir. It could be a base path or a specific cache.
    if (defined $mgsys_filter && -d File::Spec->catfile($nmon_dir, $mgsys_filter))
    {
        # The user gave a base path and a serial, and the combined path exists.
        $resolved_cache_dir = File::Spec->catfile($nmon_dir, $mgsys_filter);
    }
    else
    {
        # Assume --nmondir is the full path to the cache.
        $resolved_cache_dir = $nmon_dir;
    }
}
elsif (defined $mgsys_filter)
{
    # Case 2: User provides --mgsys only. Use the new default location.
    $resolved_cache_dir = File::Spec->catfile($DEFAULT_BASE_STAGE_DIR, $mgsys_filter);
}

# --- Final Validation of the Resolved Cache Directory ---
unless (defined $resolved_cache_dir && -d $resolved_cache_dir)
{
    die "Error: Could not find a valid cache directory at the resolved path: '$resolved_cache_dir'\n" .
    "       Please ensure the path is correct and the cache was created by an nfit-stage tool.\n";
}

# Check for the manifest file to confirm it's a valid cache.
my $manifest_file_check = File::Spec->catfile($resolved_cache_dir, $CACHE_MANIFEST_FILE);
unless (-f $manifest_file_check)
{
    die "Error: The directory '$resolved_cache_dir' does not appear to be a valid nfit cache (missing '$CACHE_MANIFEST_FILE').\n";
}

# Re-assign the validated, absolute path back to the nmon_dir variable for use throughout the script.
$nmon_dir = abs_path($resolved_cache_dir);

$runq_avg_method_str = lc($runq_avg_method_str);
if ($runq_avg_method_str ne 'none' && $runq_avg_method_str ne 'sma' && $runq_avg_method_str ne 'ema')
{
    die "Error: --runq-avg-method must be 'none', 'sma', or 'ema'. Got '$runq_avg_method_str'.\n";
}

if ($physc_csv_file)
{
    die "Error: PhysC data file (--physc-data) not found: $physc_csv_file\n" if (! -f $physc_csv_file);
    $physc_csv_dirname = dirname($physc_csv_file);
}

if (defined $runq_csv_file && ! -f $runq_csv_file)
{
    die "Error: RunQ data file (--runq-data) not found: $runq_csv_file\n";
}

if ($smt_value <= 0)
{
    die "Error: --smt value must be a positive integer.\n";
}
if (defined $start_date_str && $start_date_str !~ /^\d{4}-\d{2}-\d{2}$/)
{
    die "Error: Invalid startdate (-s) format '$start_date_str'. Use YYYY-MM-DD.\n";
}
if (defined $end_date_str && $end_date_str !~ /^\d{4}-\d{2}-\d{2}$/)
{
    die "Error: Invalid enddate (-ed) format '$end_date_str'. Use YYYY-MM-DD.\n";
}
if (defined $start_date_str && defined $end_date_str)
{
    my ($s_tp_val, $e_tp_val);
    eval { $s_tp_val = Time::Piece->strptime($start_date_str, "%Y-%m-%d"); };
    if ($@ || (defined $start_date_str && !$s_tp_val) )
    {
        die "Error parsing startdate '$start_date_str': $@\n";
    }
    eval { $e_tp_val = Time::Piece->strptime($end_date_str, "%Y-%m-%d"); };
    if ($@ || (defined $end_date_str && !$e_tp_val) )
    {
        die "Error parsing enddate '$end_date_str': $@\n";
    }
    if ($s_tp_val && $e_tp_val && $e_tp_val < $s_tp_val)
    {
        die "Error: --enddate ($end_date_str) cannot be before --startdate ($start_date_str).\n";
    }
}
if (defined($round_arg) && defined($roundup_arg))
{
    die "Error: -round (-r) and -roundup (-u) options are mutually exclusive.\n";
}
if ($enable_windowed_decay)
{
    if ($process_window_unit_str ne "days" && $process_window_unit_str ne "weeks")
    {
        die "Error: --process-window-unit must be 'days' or 'weeks'. Got '$process_window_unit_str'.\n";
    }
    if ($process_window_size_val < 1)
    {
        die "Error: --process-window-size must be at least 1. Got '$process_window_size_val'.\n";
    }
    if ($decay_half_life_days_val < 1)
    {
        die "Error: --decay-half-life-days must be at least 1. Got '$decay_half_life_days_val'.\n";
    }
    if (defined $analysis_reference_date_str && $analysis_reference_date_str !~ /^\d{4}-\d{2}-\d{2}$/)
    {
        die "Error: Invalid --analysis-reference-date format '$analysis_reference_date_str'. Use YYYY-MM-DD.\n";
    }
}

# Growth Prediction Option Validation (NEW)
if ($enable_growth_prediction)
{
    # Growth prediction now works with EITHER standard windowed decay OR the new hybrid model.
    unless ($enable_windowed_decay || $decay_over_states_flag)
    {
        print STDERR "Warning: --enable-growth-prediction requires either --enable-windowed-decay or --decay-over-states to be active. Growth prediction will be SKIPPED.\n";
        $enable_growth_prediction = 0;
    }

    if ($growth_projection_days < 1)
    {
        die "Error: --growth-projection-days must be at least 1.\n";
    }
    if ($max_growth_inflation_percent < 0 || $max_growth_inflation_percent > 200) # Cap inflation percentage
    {
        die "Error: --max-growth-inflation-percent must be between 0 and 200.\n";
    }
}

my @runq_norm_percentiles_to_calc = parse_percentile_list($runq_norm_perc_str, "runq-norm-perc");
my @runq_abs_percentiles_to_calc  = parse_percentile_list($runq_abs_perc_str,  "runq-abs-perc");

my $rounding_method = 'none';
my $round_increment = undef;
my $output_dp = 4;
if (defined $round_arg)
{
    $rounding_method = 'standard';
    $round_increment = (length $round_arg && $round_arg =~ /^[0-9.]*$/) ? $round_arg : $DEFAULT_ROUND_INCREMENT;
    print STDERR "Applying standard rounding to nearest $round_increment\n";
}
elsif (defined $roundup_arg)
{
    $rounding_method = 'up';
    $round_increment = (length $roundup_arg && $roundup_arg =~ /^[0-9.]*$/) ? $roundup_arg : $DEFAULT_ROUND_INCREMENT;
    print STDERR "Applying ceiling rounding up to nearest $round_increment\n";
}
if ($rounding_method ne 'none')
{
    if (!defined $round_increment || $round_increment <= $FLOAT_EPSILON)
    {
        die "Error: Rounding increment must be positive (got '$round_increment').\n";
    }
    $output_dp = get_decimal_places($round_increment);
}

{
    # Block to keep calculation-specific variables local
    # Restoring original logic as requested, with a note about unreachable code.
    my %smt_adjustment_map = (
        1 => 1.00,
        2 => 0.95,
        4 => 0.90,
        8 => 1.10,
    );

    my $smt_adjustment_factor = $smt_adjustment_map{$smt_value} // 1.00;  # default to 1.00

    my $entitlement_adjustment_factor = $PLACEHOLDER_ENTITLEMENT_ADJUSTMENT_FACTOR;
    my $calculated_threshold = $ACTIVE_PHYSC_THRESHOLD *
    $smt_adjustment_factor *
    $entitlement_adjustment_factor *
    $SAFETY_MARGIN_FOR_THRESHOLD_CONST;
    my $MIN_ACTIVE_PHYSC_THRESHOLD = 0.10;
    $ACTIVE_PHYSC_THRESHOLD = ($calculated_threshold < $MIN_ACTIVE_PHYSC_THRESHOLD) ? $MIN_ACTIVE_PHYSC_THRESHOLD : $calculated_threshold;
}

# --- Main Execution Path ---
if (!$quiet) {
    print STDERR "\nnfit version $VERSION\n";
    print STDERR "-------------------------\n";
}

# --- Legacy Invocation Handler ---
# If this script is called without a manifest, it means it's being used by an
# older workflow (like --update-history). We build a manifest in-memory from
# the legacy command-line flags to remain compatible with the new engine.
unless ($manifest_file) {
    # A profile label is required in this mode to identify the output.
    die "Error: --profile-label is required when nfit is run without a manifest.\n" unless defined $profile_label;

    # Back-compat: normalise legacy peak helper name to the canonical profile label.
    # This prevents mixed JSON keys (P-99W1 vs Peak_P-99W1) when nfit is run standalone.
    if (defined $profile_label && $profile_label eq $LEGACY_MANDATORY_PEAK_PROFILE_FOR_HINT) {
        $profile_label = $MANDATORY_PEAK_PROFILE_FOR_HINT;
        print STDERR "  [WARN] Profile '$LEGACY_MANDATORY_PEAK_PROFILE_FOR_HINT' is deprecated; treating it as '$MANDATORY_PEAK_PROFILE_FOR_HINT'.\n";
    }

    # --- 1. Set Defaults and Determine Parameters ---
    $percentile            //= $DEFAULT_PERCENTILE;
    $window_minutes        //= $DEFAULT_WINDOW_MINUTES;
    $avg_method            //= $DEFAULT_AVG_METHOD;
    $decay_level           //= $DEFAULT_DECAY_LEVEL;
    $filter_above_perc_arg //= 0;

    # Determine time filter from flags, mirroring old behavior.
    my $time_filter = 'none';
    $time_filter = 'online' if $online_flag;
    $time_filter = 'batch'  if $batch_flag;

    # Determine the effective RunQ decay level, falling back to the main decay level.
    # This exactly replicates the logic in nfit.old.pl.
    my $runq_decay_to_use = $runq_decay_level_arg // $decay_level;

    # --- 2. Build the Multi-Transform Manifest Data Structure ---
    my %temp_manifest;

    # --- 2a. Build the PhysC Transform ---
    my $physc_key = join(":", 'PhysC', $avg_method, $window_minutes, $decay_level, $filter_above_perc_arg, $time_filter, ($no_weekends_flag ? 1 : 0));
    $temp_manifest{$physc_key} = {
        metric      => 'PhysC',
        method      => $avg_method,
        window      => $window_minutes,
        decay       => $decay_level,
        filter_perc => $filter_above_perc_arg,
        time_filter => $time_filter,
        no_weekends => ($no_weekends_flag ? 1 : 0),
        profiles    => {
            $profile_label => {
                percentiles     => [$percentile],
                calculate_peak  => ($peak_flag ? 1 : 0),
                enable_growth   => ($enable_growth_prediction ? 1 : 0),
                enable_clipping => ($enable_clipping_detection ? 1 : 0)
            }
        }
    };

    # --- 2b. Build the NormRunQ Transform ---
    my @norm_percs_to_calc = parse_percentile_list($runq_norm_perc_str, "runq-norm-perc");
    if (@norm_percs_to_calc) {
        my $norm_runq_key = join(":", 'NormRunQ', $runq_avg_method_str, $window_minutes, $runq_decay_to_use, 0, $time_filter, ($no_weekends_flag ? 1 : 0));
        $temp_manifest{$norm_runq_key} = {
            metric      => 'NormRunQ',
            method      => $runq_avg_method_str,
            window      => $window_minutes,
            decay       => $runq_decay_to_use,
            filter_perc => 0,
            time_filter => $time_filter,
            no_weekends => ($no_weekends_flag ? 1 : 0),
            profiles    => {
                $profile_label => {
                    percentiles => \@norm_percs_to_calc
                }
            }
        };
    }

    # --- 2c. Build the AbsRunQ Transform ---
    my @abs_percs_to_calc = parse_percentile_list($runq_abs_perc_str, "runq-abs-perc");
    if (@abs_percs_to_calc) {
        my $abs_runq_key = join(":", 'AbsRunQ', $runq_avg_method_str, $window_minutes, $runq_decay_to_use, 0, $time_filter, ($no_weekends_flag ? 1 : 0));
        $temp_manifest{$abs_runq_key} = {
            metric      => 'AbsRunQ',
            method      => $runq_avg_method_str,
            window      => $window_minutes,
            decay       => $runq_decay_to_use,
            filter_perc => 0,
            time_filter => $time_filter,
            no_weekends => ($no_weekends_flag ? 1 : 0),
            profiles    => {
                $profile_label => {
                    percentiles => \@abs_percs_to_calc
                }
            }
        };
    }

    # --- 3. Write Manifest to a Temporary File ---
    my ($fh, $filename) = tempfile(UNLINK => 1);
    print $fh JSON->new->encode(\%temp_manifest);
    close $fh;

    # Set the $manifest_file variable, so the main engine uses our generated file.
    $manifest_file = $filename;
}
# --- End: Legacy Handler ---

if ($manifest_file) {
    # Backup manifest and invocation arguments to cache dir for inspection
    if (defined $nmon_dir && -d $nmon_dir) {
        my $debug_manifest_path = File::Spec->catfile($nmon_dir, '.nfit.engine.last_manifest');
        copy($manifest_file, $debug_manifest_path) or warn "Warning: Failed to create debug manifest at $debug_manifest_path: $!\n";
        chmod 0644, $debug_manifest_path; # Ensure it is readable

        # Backup Global Arguments to cache dir for inspection
        # This captures the Start/End dates and Analysis Reference Date which are NOT in the manifest.
        my $debug_args_path = File::Spec->catfile($nmon_dir, '.nfit.engine.last_args');
        if (open my $dbg_fh, '>', $debug_args_path) {
            # Manually format key args for readability (avoiding Data::Dumper dependency if preferred,
            # though Data::Dumper is standard).
            print $dbg_fh "Start Date:                      " . ($start_date_str // 'undef') . "\n";
            print $dbg_fh "End Date:                        " . ($end_date_str // 'undef') . "\n";
            print $dbg_fh "Analysis Reference Date:         " . ($analysis_reference_date_str // 'undef') . "\n";
            print $dbg_fh "Windowed Decay:                  " . ($enable_windowed_decay // '0') . "\n";
            print $dbg_fh "Growth Enabled:                  " . ($enable_growth_prediction // '0') . "\n";
            print $dbg_fh "Target VM:                       " . ($target_vm_name // 'all') . "\n";
            print $dbg_fh "RunQ Avg Method:                 " . ($runq_avg_method_str // 'undef') . "\n";
            print $dbg_fh "Clipping Detection:              " . ($enable_clipping_detection // 'undef') . "\n";
            print $dbg_fh "Decay Settings:\n";
            print $dbg_fh "process_window_unit:             " . ($process_window_unit_str // 'undef') . "\n";
            print $dbg_fh "process_window_size:             " . ($process_window_size_val // 'undef') . "\n";
            print $dbg_fh "decay_half_life_days:            " . ($decay_half_life_days_val // 'undef') . "\n";
            print $dbg_fh "Growth Prediction Settings:\n";
            print $dbg_fh "growth_projection_days:          " . ($growth_projection_days // 'undef') . "\n";
            print $dbg_fh "growth_projection_override:      " . ($growth_projection_days_user_override // 'undef') . "\n";
            print $dbg_fh "max_growth_inflation_percent:    " . ($max_growth_inflation_percent // 'undef') . "\n";
            print $dbg_fh "Rounding:\n";
            print $dbg_fh "round_arg:                       " . ($round_arg // 'undef') . "\n";
            print $dbg_fh "roundup_arg:                     " . ($roundup_arg // 'undef') . "\n";
            close $dbg_fh;
        }
    }

    # New single-pass engine entry point.
    # We now construct a single hash reference containing all necessary global
    # flags to pass to the engine. This keeps the function call clean and
    # ensures all original functionality is preserved.
    run_single_pass_from_manifest({
        # --- File/Cache Arguments ---
        manifest_file           => $manifest_file,
        nmon_dir                => $nmon_dir, # This is the resolved path from the Smart Dispatcher

        # --- Filtering Arguments ---
        target_vm_name          => $target_vm_name,
        exclude_vms_str         => $exclude_vms_str,
        start_date_str          => $start_date_str,
        end_date_str            => $end_date_str,
        include_states          => $include_states_selector,

        # --- RunQ and SMT Arguments ---
        smt_value               => $smt_value,
        runq_avg_method_str     => $runq_avg_method_str,

        # --- Analysis Model Flags ---
        enable_windowed_decay   => $enable_windowed_decay,
        decay_over_states       => $decay_over_states_flag,
        enable_growth_prediction=> $enable_growth_prediction,
        enable_clipping_detection=> $enable_clipping_detection,

        # --- Decay Model Tunables ---
        process_window_unit     => $process_window_unit_str,
        process_window_size     => $process_window_size_val,
        decay_half_life_days    => $decay_half_life_days_val,
        analysis_reference_date => $analysis_reference_date_str,

        # --- Growth Model Tunables ---
        growth_projection_days  => $growth_projection_days,
        growth_projection_days_user_override => $growth_projection_days_user_override,
        max_growth_inflation_percent => $max_growth_inflation_percent,

        # --- Rounding Arguments ---
        round_arg               => $round_arg,
        roundup_arg             => $roundup_arg,

        # --- Utility/Verbosity Flags ---
        verbose                 => $verbose,
        quiet                   => $quiet,
    });

    # The new engine handles everything, so we exit cleanly.
    exit 0;
}

# ==============================================================================
# Subroutines
# ==============================================================================

# --- clean_perc_label ---
# Helper to format a percentile number into a clean string for metric keys.
sub clean_perc_label {
    my ($p) = @_;
    my $label = sprintf("%.2f", $p);
    $label =~ s/\.?0+$//;
    $label = "0" if $label eq "" && abs($p-0)<0.001;
    return $label;
}

# --- calculate_rolling_average ---
# Generic sub to calculate a single rolling average point (SMA or EMA).
sub calculate_rolling_average {
    my ($value, $method, $sma_queue_aref, $prev_ema_sref, $window, $alpha) = @_;

    my $avg_to_return;
    if ($method eq 'sma') {
        push @$sma_queue_aref, $value;
        shift @$sma_queue_aref while @$sma_queue_aref > $window;
        if (@$sma_queue_aref == $window) {
            $avg_to_return = calculate_average(@$sma_queue_aref);
        }
    } else { # ema
        if (defined $value) {
            if (!defined $$prev_ema_sref) { $$prev_ema_sref = $value; }
            else { $$prev_ema_sref = ($value * $alpha) + ($$prev_ema_sref * (1 - $alpha)); }
        }
        push @$sma_queue_aref, $value; # Use sma_queue as a simple counter
        shift @$sma_queue_aref while @$sma_queue_aref > $window;
        if (@$sma_queue_aref == $window) {
            $avg_to_return = $$prev_ema_sref;
        }
    }
    return $avg_to_return;
}

# --- print_state_windows_report ---
# Takes the data structure returned by define_configuration_states and
# prints a formatted, compact report to STDOUT.
#
# Args:
#   1. $states_href (hash ref): The state windows data structure.
#
sub print_state_windows_report
{
    my ($states_href) = @_;

    my $header_format = "%-20s %-10s %-20s %-20s %-10s %-45s %s\n";
    my $row_format    = "%-20s %-10s %-20s %-20s %-10.2f %-45s %s\n";

    printf $header_format, "VM_Name", "State_ID", "Start_Time", "End_Time", "Duration", "Config_Summary", "Processor";

    my $separator = ("-" x 20) . " " . ("-" x 10) . " " . ("-" x 20) . " " . ("-" x 20) . " " . ("-" x 10) . " " . ("-" x 45) . " " . ("-" x 30) . "\n";
    print STDOUT $separator;

    foreach my $vm_name (sort keys %$states_href)
    {
        my @vm_states = @{$states_href->{$vm_name}};
        my $total_states = scalar(@vm_states);
        foreach my $state (@vm_states)
        {
            my $md = $state->{metadata};

            # Build the compact configuration summary string
            my $capped_str = ($md->{capped} // 0) ? "Capped" : "Uncapped";
            my $config_summary = sprintf("Ent:%.2f vCPU:%d PoolCPU:%d SMT:%d %s",
                $md->{entitlement} // 0,
                $md->{virtual_cpus} // 0,
                $md->{pool_cpu} // 0,
                $md->{smt} // 0,
                $capped_str
            );

            printf $row_format,
            $state->{vm_name},
            $state->{state_id} . "/" . $total_states,
            $state->{start_time}->strftime('%Y-%m-%d %H:%M'),
            $state->{end_time}->strftime('%Y-%m-%d %H:%M'),
            $state->{duration_days},
            $config_summary,
            $md->{processor_state};
        }
    }
}

# --- parse_state_selector ---
# Parses the user's state selection string (e.g., "1,3,5-7,-1") and returns
# an array of the actual state window objects that match the selection.
#
# Args:
#   1. $selector_str (string): The raw string from the --include-states flag.
#   2. $available_states_aref (array ref): An array of all state window hashes for a VM.
#
# Returns:
#   - An array containing the state window hashes that were selected.
#
sub parse_state_selector
{
    my ($selector_str, $available_states_aref) = @_;
    my @selected_states;
    return @$available_states_aref if (lc($selector_str) eq 'all');
    return () if (scalar(@$available_states_aref) == 0);
    my %selected_ids;
    my $total_states = scalar(@$available_states_aref);
    my @parts = split ',', $selector_str;
    foreach my $part (@parts) {
        $part =~ s/\s+//g;
        if ($part =~ /^(\d+)-(\d+)$/) {
            for my $id ($1 .. $2) { $selected_ids{$id} = 1; }
        } elsif ($part =~ /^-(\d+)$/) {
            my $offset = $1;
            if ($total_states - $offset >= 0) { $selected_ids{ $total_states - ($offset - 1) } = 1; }
        } elsif ($part =~ /^\d+$/) {
            $selected_ids{$part} = 1;
        }
    }
    foreach my $state (@$available_states_aref) {
        push @selected_states, $state if exists $selected_ids{$state->{state_id}};
    }
    return @selected_states;
}

# Calculate basic statistics (mean, stddev, CV) for a list of values
# Input: reference to an array of numeric values
# Output: hashref { mean, stddev, cv, count } or undef if insufficient data
sub calculate_statistics_for_trend
{
    my ($values_aref) = @_;
    my @defined_values = grep { defined $_ && $_ =~ /^-?[0-9.]+$/ } @{$values_aref};
    my $count = scalar @defined_values;

    return undef if $count == 0;

    my $sum = sum0(@defined_values);
    my $mean = $sum / $count;
    my $stddev = 0;
    my $cv = undef;

    if ($count > 1)
    {
        my $sum_sq_diff = 0;
        foreach my $val (@defined_values)
        {
            $sum_sq_diff += ($val - $mean)**2;
        }
        $stddev = sqrt($sum_sq_diff / ($count - 1));
    }

    if (abs($mean) > $FLOAT_EPSILON)
    {
        $cv = $stddev / $mean;
    }
    elsif ($stddev < $FLOAT_EPSILON && abs($mean) < $FLOAT_EPSILON)
    {
        $cv = 0;
    }

    return {
        mean   => $mean,
        stddev => $stddev,
        cv     => $cv,
        count  => $count,
    };
}

# Calculate linear regression (slope, intercept, R-squared) manually
# Input: reference to an array of [x, y] points, where x and y are numeric.
#        x is typically a time index (0, 1, 2,...), y is the metric value.
# Output: hashref { slope, intercept, r_squared, n_points } or undef if
#         insufficient data (n < 2) or if slope cannot be determined (e.g., all x values are identical).
sub calculate_manual_linear_regression
{
    my ($points_aref) = @_;
    my $n = scalar @{$points_aref};

    # Regression requires at least 2 distinct points to define a line.
    return undef if $n < 2;

    my $sum_x = 0;
    my $sum_y = 0;
    my $sum_xy = 0;
    my $sum_x_squared = 0;
    my $sum_y_squared = 0;

    foreach my $point (@{$points_aref})
    {
        my ($x_val, $y_val) = @{$point};

        # Assuming $x_val and $y_val are already numeric based on upstream processing.
        # If strict validation is needed here, it can be added, but the input
        # preparation logic should ensure numeric data.

        $sum_x += $x_val;
        $sum_y += $y_val;
        $sum_xy += $x_val * $y_val;
        $sum_x_squared += $x_val**2;
        $sum_y_squared += $y_val**2;
    }

    my $slope_calc     = undef;
    my $intercept_calc = undef;
    my $r_squared_calc = undef;

    # Denominator for slope calculation: N * sum(x^2) - (sum(x))^2
    # This is also N * SS_xx (where SS_xx is sum of squares for x)
    my $denominator_slope = ($n * $sum_x_squared) - ($sum_x**2);

    # Check if denominator is too close to zero (implies x values are not distinct enough
    # or only one unique x value if n > 1, which makes slope undefined or infinite).
    if (abs($denominator_slope) > $FLOAT_EPSILON)
    {
        $slope_calc = (($n * $sum_xy) - ($sum_x * $sum_y)) / $denominator_slope;
        $intercept_calc = ($sum_y - ($slope_calc * $sum_x)) / $n;

        # Calculate R-squared (Coefficient of Determination)
        # R^2 = (N * sum(xy) - sum(x) * sum(y))^2 / ((N * sum(x^2) - (sum(x))^2) * (N * sum(y^2) - (sum(y))^2))
        my $numerator_r_sq_squared = (($n * $sum_xy) - ($sum_x * $sum_y))**2;
        my $denominator_r_sq_part_yy = ($n * $sum_y_squared) - ($sum_y**2);

        if (abs($denominator_r_sq_part_yy) > $FLOAT_EPSILON) # Avoid division by zero if all y values are the same
        {
            $r_squared_calc = $numerator_r_sq_squared / ($denominator_slope * $denominator_r_sq_part_yy);
            # Clamp R-squared to [0, 1] due to potential floating point inaccuracies
            $r_squared_calc = 0.0 if defined $r_squared_calc && $r_squared_calc < 0;
            $r_squared_calc = 1.0 if defined $r_squared_calc && $r_squared_calc > 1.0;
        }
        elsif (abs($numerator_r_sq_squared) < $FLOAT_EPSILON**2) # All y are same, and slope is ~0
        {
            # If all y values are identical, the line should perfectly predict them if slope is also ~0.
            $r_squared_calc = 1.0;
        }
        else
        {
            # This case implies an issue or perfect vertical correlation if all x were same (but ss_xx_calc > 0 here)
            # If y is constant, variance of y is 0. If model doesn't perfectly predict this constant, R^2 can be odd.
            # Given y is constant, and slope is non-zero, it means the line isn't horizontal.
            # SS_tot would be 0. SS_res would be >0. R^2 = 1 - SS_res/SS_tot is undefined.
            # However, if $denominator_r_sq_part_yy is near zero, it implies SS_tot is near zero.
            # If the numerator $numerator_r_sq_squared is also near zero, it suggests the model fits perfectly (R^2 = 1).
            # If numerator is not zero but denominator_yy is, it's effectively a poor fit for variation that doesn't exist.
            $r_squared_calc = 0.0; # Default to 0 for poor/undefined fit in this edge scenario
        }
    }
    else
    {
        # Denominator for slope is zero or too small.
        # This happens if all x values are (nearly) identical.
        # Cannot reliably determine a linear trend.
        return undef;
    }

    return {
        slope     => $slope_calc,
        intercept => $intercept_calc,
        r_squared => $r_squared_calc,
        n_points  => $n,
    };
}

# ==============================================================================
# ROBUST TREND ANALYSIS MODULE
# ==============================================================================
# PURPOSE: Non-parametric trend detection for CPU time-series using Sen's slope
#          and Mann-Kendall test (standard version, no autocorrelation correction)
# AUTHOR: nFit Development Team
# VERSION: 1.0 (Simplified - Phase 1)
# DATE: 2025-11-04
#
# IMPLEMENTATION NOTES:
# - This is the v1 simplified implementation
# - Uses standard Mann-Kendall (autocorrelation correction deferred to Phase 2)
# - Designed for integration into nfit.pl
# - Follows existing nFit coding standards
# ==============================================================================

# ==============================================================================
# SUBROUTINE: calculate_sens_slope
# PURPOSE:    Calculate Theil-Sen slope estimator (median of pairwise slopes)
# ARGUMENTS:
#   1. $series_aref - Array ref of [x, y] pairs (time series points)
# RETURNS:
#   Hash ref: { slope => $value, n_slopes => $count, method => 'exact_pairs' }
#   Returns undef if insufficient data (n < 2)
# NOTES:
#   - Robust to outliers (29% breakdown point)
#   - Computational complexity: O(n²) for n points
#   - For n=90 (daily over 90 days), this is 4,005 pairwise slopes (fast)
# ==============================================================================
sub calculate_sens_slope {
    my ($series_aref) = @_;
    my $n = scalar @{$series_aref};

    # Need at least 2 points to calculate a slope
    return undef if $n < 2;

    my @slopes;

    # Calculate all pairwise slopes: slope_ij = (y_j - y_i) / (x_j - x_i)
    for my $i (0 .. $n-2) {
        for my $j ($i+1 .. $n-1) {
            my ($x1, $y1) = @{$series_aref->[$i]};
            my ($x2, $y2) = @{$series_aref->[$j]};

            my $delta_x = $x2 - $x1;

            # Only calculate slope if x values are distinct
            if (abs($delta_x) > $FLOAT_EPSILON) {
                push @slopes, ($y2 - $y1) / $delta_x;
            }
        }
    }

    # If no valid slopes could be calculated, return undef
    return undef if @slopes == 0;

    # Sen's slope is the median of all pairwise slopes
    my $median_slope = _calculate_median(\@slopes);

    return {
        slope     => $median_slope,
        n_slopes  => scalar(@slopes),
        method    => 'exact_pairs',
    };
}

# ==============================================================================
# SUBROUTINE: _calculate_median
# PURPOSE:    Calculate median of an array of numbers
# ARGUMENTS:
#   1. $values_aref - Array ref of numeric values
# RETURNS:
#   Median value (scalar)
# NOTES:
#   - Sorts the array (does not modify original)
#   - Returns middle value for odd n, average of two middle values for even n
# ==============================================================================
sub _calculate_median {
    my ($values_aref) = @_;

    # Sort values in ascending order
    my @sorted = sort { $a <=> $b } @{$values_aref};
    my $n = scalar @sorted;

    # For odd n, return middle value
    if ($n % 2 == 1) {
        return $sorted[int($n/2)];
    }
    # For even n, return average of two middle values
    else {
        return ($sorted[$n/2 - 1] + $sorted[$n/2]) / 2;
    }
}

# ==============================================================================
# SUBROUTINE: mann_kendall_test_standard
# PURPOSE:    Perform standard Mann-Kendall trend test (no autocorr correction)
# ARGUMENTS:
#   1. $series_aref - Array ref of [x, y] pairs (time series points)
#   2. $alpha       - Significance level (optional, default 0.05)
# RETURNS:
#   Hash ref with test results:
#     { statistic, tau, z_score, p_value, trend, passed, alpha }
#   Returns undef if insufficient data (n < 4)
# NOTES:
#   - Tests for monotonic trend in time series
#   - Non-parametric test (no distribution assumptions)
#   - V1: Standard version, autocorrelation correction deferred to Phase 2
#   - Minimum 4 points recommended for meaningful test
# ==============================================================================
sub mann_kendall_test_standard {
    my ($series_aref, $alpha) = @_;
    $alpha //= 0.05;  # Default significance level

    my $n = scalar @{$series_aref};

    # Need at least 4 points for meaningful trend test
    return undef if $n < 4;

    # Extract y-values (the actual measurements)
    my @y_values = map { $_->[1] } @{$series_aref};

    # ─────────────────────────────────────────────────────────────────────────
    # STEP 1: Calculate Kendall's S statistic
    # ─────────────────────────────────────────────────────────────────────────
    # S = sum of sgn(y_j - y_i) for all pairs i < j
    # where sgn(x) = +1 if x > 0, -1 if x < 0, 0 if x = 0

    my $S = 0;
    my $n_ties = 0;  # Track ties for optional reporting

    for my $i (0 .. $n-2) {
        for my $j ($i+1 .. $n-1) {
            my $diff = $y_values[$j] - $y_values[$i];

            if (abs($diff) < $FLOAT_EPSILON) {
                # Tie: contributes 0 to S
                $n_ties++;
            } elsif ($diff > 0) {
                $S++;  # Concordant pair (increasing)
            } else {
                $S--;  # Discordant pair (decreasing)
            }
        }
    }

    # ─────────────────────────────────────────────────────────────────────────
    # STEP 2: Calculate variance of S (standard formula, no autocorr correction)
    # ─────────────────────────────────────────────────────────────────────────
    # Var(S) = n(n-1)(2n+5) / 18
    # Note: This assumes independent observations
    # V2 will add Hamed-Rao correction: Var(S)* = Var(S) / (1 + 2·sum(rho_k))

    my $var_S = ($n * ($n - 1) * (2 * $n + 5)) / 18;

    # Handle edge case: variance should never be zero for n >= 4
    if ($var_S < $FLOAT_EPSILON) {
        warn "Warning: Mann-Kendall variance is zero for n=$n. This should not happen.\n";
        return undef;
    }

    # ─────────────────────────────────────────────────────────────────────────
    # STEP 3: Calculate Z-statistic with continuity correction
    # ─────────────────────────────────────────────────────────────────────────
    # Z = (S - 1) / sqrt(Var(S))  if S > 0
    #     (S + 1) / sqrt(Var(S))  if S < 0
    #     0                        if S = 0

    my $Z;
    if ($S > 0) {
        $Z = ($S - 1) / sqrt($var_S);
    } elsif ($S < 0) {
        $Z = ($S + 1) / sqrt($var_S);
    } else {
        $Z = 0;
    }

    # ─────────────────────────────────────────────────────────────────────────
    # STEP 4: Calculate two-tailed p-value from Z-score
    # ─────────────────────────────────────────────────────────────────────────
    # p-value = 2 * P(Z > |z|) = 2 * (1 - Φ(|z|))
    # where Φ is the standard normal cumulative distribution function

    my $p_value = 2 * (1 - _standard_normal_cdf(abs($Z)));

    # Ensure p-value is in valid range [0, 1] due to numerical precision
    $p_value = 0.0 if $p_value < 0;
    $p_value = 1.0 if $p_value > 1;

    # ─────────────────────────────────────────────────────────────────────────
    # STEP 5: Calculate Kendall's tau (correlation coefficient)
    # ─────────────────────────────────────────────────────────────────────────
    # tau = S / (n(n-1)/2) = normalised S statistic in range [-1, +1]

    my $tau = $S / ($n * ($n - 1) / 2);

    # ─────────────────────────────────────────────────────────────────────────
    # STEP 6: Determine trend direction and significance
    # ─────────────────────────────────────────────────────────────────────────

    my $passed = ($p_value < $alpha) ? 1 : 0;

    my $trend;
    if ($passed) {
        # Significant trend detected
        $trend = ($tau > 0) ? 'increasing' : 'decreasing';
    } else {
        # No significant trend
        $trend = 'none';
    }

    # ─────────────────────────────────────────────────────────────────────────
    # STEP 7: Return complete test results
    # ─────────────────────────────────────────────────────────────────────────

    return {
        statistic  => $S,           # Kendall's S statistic
        tau        => $tau,         # Kendall's tau (-1 to +1)
        z_score    => $Z,           # Standardised test statistic
        p_value    => $p_value,     # Two-tailed p-value
        trend      => $trend,       # 'increasing', 'decreasing', or 'none'
        passed     => $passed,      # 1 if significant at alpha, 0 otherwise
        alpha      => $alpha,       # Significance level used
        n_ties     => $n_ties,      # Number of tied pairs (for diagnostics)
    };
}

# ==============================================================================
# SUBROUTINE: _standard_normal_cdf
# PURPOSE:    Calculate standard normal cumulative distribution function Φ(z)
# ARGUMENTS:
#   1. $z - Z-score (standardised value)
# RETURNS:
#   P(Z <= z) for standard normal distribution
# NOTES:
#   - Uses Abramowitz and Stegun approximation (accurate to 7 decimal places)
#   - Reference: Abramowitz & Stegun (1964), formula 26.2.17
# ==============================================================================
sub _standard_normal_cdf {
    my ($z) = @_;

    # Use absolute value and adjust at end for symmetry
    my $t = 1 / (1 + 0.2316419 * abs($z));

    # Probability density at z
    my $d = 0.3989423 * exp(-$z * $z / 2);

    # Polynomial approximation
    my $p = $d * $t * (0.3193815 + $t * (-0.3565638 + $t *
            (1.781478 + $t * (-1.821256 + $t * 1.330274))));

    # Adjust for sign of z (using symmetry of normal distribution)
    return ($z > 0) ? (1 - $p) : $p;
}

# ==============================================================================
# SUBROUTINE: calculate_growth_trend_robust_v1
# PURPOSE:    Main entry point for robust growth prediction (simplified version)
# ARGUMENTS:
#   1. $vm_name           - VM in scope
#   2. $profile_name      - Profile name in scope
#   3. $time_series_aref  - Time series from SPE (already aggregated)
#   4. $projection_days   - Number of days to project growth
#   5. $args_ref          - Hash ref with options:
#        - cv_threshold: Volatility threshold (default 0.50)
#        - mk_alpha: Significance level (default 0.05)
#        - decay_over_states: Boolean (for aggregation_basis determination)
#        - enable_windowed_decay: Boolean
#        - process_window_unit: 'days' or 'weeks'
#        - process_window_size: Integer
# RETURNS:
#   Hash ref with complete growth rationale for JSON storage
# NOTES:
#   - Time series from SPE is already in correct format (no aggregation needed)
#   - Hybrid model: daily indices [0, val], [1, val], ... (90 points)
#   - Windowed model: window indices [0, val], [1, val], ... (12-22 points)
# ==============================================================================
sub calculate_growth_trend_robust_v1 {
    my ($vm_name, $profile_name, $time_series_aref, $projection_days, $args_ref) = @_;

    # ─────────────────────────────────────────────────────────────────────────
    # STAGE 0: Input validation and setup
    # ─────────────────────────────────────────────────────────────────────────

    my $n = scalar @{$time_series_aref};
    my $cv_threshold = $args_ref->{cv_threshold} // $GROWTH_MAX_CV_THRESHOLD;
    my $mk_alpha = $args_ref->{mk_alpha} // 0.05;
    my $use_hamed_rao = $args_ref->{use_hamed_rao} // 0;

    # Determine aggregation basis from model type
    my $aggregation_basis;
    if ($args_ref->{decay_over_states} || $args_ref->{enable_windowed_decay}) {
        $aggregation_basis = 'daily';
    } else {
        $aggregation_basis = 'unknown';
    }

    # ─────────────────────────────────────────────────────────────────────────
    # STAGE 1: Volatility check (existing logic preserved)
    # ─────────────────────────────────────────────────────────────────────────

    # Extract y-values for CV calculation
    my @values = map { $_->[1] } @{$time_series_aref};
    my $stats = calculate_statistics_for_trend(\@values);

    my $cv_passed = (defined $stats->{cv} && $stats->{cv} < $cv_threshold) ? 1 : 0;

    # If volatility too high, suppress growth prediction
    if (!$cv_passed) {
        return {
            # Metadata
            method_used => 'none',
            aggregation_basis => $aggregation_basis,
            sample_points => $n,
            volatility_cv => $stats->{cv},
            volatility_check_passed => 0,

            # Sen's slope + MK (not calculated due to high volatility)
            sen_slope => undef,
            sen_trend => 'none',
            sen_p_value => undef,
            sen_is_significant => 0,
            sen_tau => undef,

            # OLS (not calculated)
            ols_slope => undef,
            ols_intercept => undef,
            ols_r2 => undef,

            # Final result
            growth_adj => 0,

            # Diagnostic
            skip_reason => 'High volatility (CV > threshold)',
        };
    }

    # ─────────────────────────────────────────────────────────────────────────
    # STAGE 2: Calculate Sen's slope (robust magnitude estimator)
    # ─────────────────────────────────────────────────────────────────────────

    my $sen_result = calculate_sens_slope($time_series_aref);
    my $sen_slope = $sen_result ? $sen_result->{slope} : undef;

    # ─────────────────────────────────────────────────────────────────────────
    # STAGE 3: Mann-Kendall significance test (standard version)
    # ─────────────────────────────────────────────────────────────────────────

    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    # Mann-Kendall Significance Test Selection
    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    # The Hamed-Rao variant includes intelligent routing:
    #   1. Trend-dominance pre-check: If |τ| ≥ 0.4, use standard MK
    #   2. Autocorrelation correction: If trend moderate, apply Hamed-Rao
    #   3. Reactive fallback: If adjustment unstable, fallback to standard MK
    #
    # The 'variant' field in the result indicates which path was taken:
    #   - 'standard_mk_trend_dominant'           : Strong trend detected (skipped H-R)
    #   - 'hamed-rao'                            : H-R correction applied successfully
    #   - 'standard_mk_via_hamed_rao_fallback'   : H-R unstable, fell back
    #   - 'standard'                             : Standard MK (H-R not requested)
    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

    my $mk_result;
    if ($use_hamed_rao) {
        $mk_result = mann_kendall_test_hamed_rao($vm_name, $profile_name, $time_series_aref, $mk_alpha);
    } else {
        $mk_result = mann_kendall_test_standard($vm_name, $profile_name, $time_series_aref, $mk_alpha);
    }

    # ─────────────────────────────────────────────────────────────────────────
    # STAGE 4: Gate growth prediction by statistical significance
    # ─────────────────────────────────────────────────────────────────────────

    my $growth_adj = 0;
    my $method_used = 'none';
    my $skip_reason = undef;

    if (!defined $sen_slope) {
        # Sen's slope calculation failed (e.g., all x values identical)
        $method_used = 'none';
        $skip_reason = 'Sen slope calculation failed';
    } elsif (!$mk_result) {
        # Mann-Kendall test failed (e.g., insufficient data)
        $method_used = 'none';
        $skip_reason = 'Mann-Kendall test failed';
   } elsif ($mk_result->{passed}) {
        # A significant trend can be negative, flat, or positive.
        $growth_adj = $sen_slope * $projection_days;
        $method_used = 'sen_slope';

        if ($sen_slope < $GROWTH_MIN_POSITIVE_SLOPE_THRESHOLD && $sen_slope > -$GROWTH_MIN_POSITIVE_SLOPE_THRESHOLD) {
            # Trend is significant but slope is trivial
            $growth_adj = 0; # Do not apply negative or zero growth
            $method_used = 'sen_slope_not_significant';
            $skip_reason = sprintf("Trend is significant (p=%.4f) but trivial (slope=%.6f)", $mk_result->{p_value}, $sen_slope);
        }
   } else {
        # Sen's slope calculated but trend not statistically significant
        $method_used = 'sen_slope_not_significant';
        $skip_reason = sprintf("Trend not significant (p=%.4f > alpha=%.2f)",
                              $mk_result->{p_value}, $mk_alpha);
    }

    # ─────────────────────────────────────────────────────────────────────────
    # STAGE 5: Calculate OLS for comparison (existing function)
    # ─────────────────────────────────────────────────────────────────────────

    my $ols_result = calculate_manual_linear_regression($time_series_aref);

    # ─────────────────────────────────────────────────────────────────────────
    # STAGE 6: Return complete rationale for JSON storage
    # ─────────────────────────────────────────────────────────────────────────

    my %result = (
        # ---- METADATA ----
        method_used => $method_used,
        aggregation_basis => $aggregation_basis,
        sample_points => $n,
        volatility_cv => $stats->{cv},
        volatility_check_passed => $cv_passed,

        #  ---- SEN'S SLOPE + MANN-KENDALL (Combined System) ----
        sen_slope => $sen_slope,
        sen_trend => $mk_result ? $mk_result->{trend} : 'none',
        sen_p_value => $mk_result ? $mk_result->{p_value} : undef,
        sen_is_significant => $mk_result ? $mk_result->{passed} : 0,
        sen_tau => $mk_result ? $mk_result->{tau} : undef,

        # ---- MANN-KENDALL VARIANT METADATA ----
        mk_variant => $mk_result ? ($mk_result->{variant} // 'standard') : 'none',
        mk_n_effective => $mk_result ? ($mk_result->{n_eff} // $n) : undef,

        # ---- HAMED-RAO SPECIFIC FIELDS (when applicable) ----
        ($mk_result && $mk_result->{variant} && $mk_result->{variant} =~ /hamed.rao|fallback|trend_dominant/ ? (
            hamed_rao_adjustment_factor => $mk_result->{adjustment_factor} // $mk_result->{hamed_rao_adjustment_factor},
            hamed_rao_autocorr_max_lag => $mk_result->{autocorr_max_lag},
            hamed_rao_lag1_autocorr => $mk_result->{lag1_autocorr},
        ) : ()),

        # Include skip/fallback reasons when present
        ($mk_result && $mk_result->{hamed_rao_skip_reason} ? (
            hamed_rao_skip_reason => $mk_result->{hamed_rao_skip_reason}
        ) : ()),
        ($mk_result && $mk_result->{hamed_rao_fallback_reason} ? (
            hamed_rao_fallback_reason => $mk_result->{hamed_rao_fallback_reason}
        ) : ()),

        # ---- OLS (Legacy Comparison) ----
        ols_slope => $ols_result ? $ols_result->{slope} : undef,
        ols_intercept => $ols_result ? $ols_result->{intercept} : undef,
        ols_r2 => $ols_result ? $ols_result->{r_squared} : undef,

        # ---- FINAL RESULT ----
        growth_adj => $growth_adj,

        # ---- DIAGNOSTIC ----
        skip_reason => $skip_reason,

        # ---- PROJECTION METADATA (Adaptive Growth Projection v6.x) ----
        projection_days => $projection_days,
        projection_days_source => $args_ref->{growth_projection_days_source} // 'default',
        analysis_days => $args_ref->{analysis_days} // $n,
        avg_sampling_interval_mins => $args_ref->{avg_sampling_interval_mins} // undef,
    );

    # ---- Hamed–Rao MK diagnostics ----
    if ($use_hamed_rao && $mk_result && ($mk_result->{variant} // '') eq 'hamed-rao') {
        @result{qw(mk_variant mk_neff mk_autocorr_k mk_adjustment_factor mk_rho_1)} =
            (
                $mk_result->{variant},
                $mk_result->{n_eff},
                $mk_result->{autocorr_k},
                $mk_result->{adjustment_factor},
                $mk_result->{rho_1},
            );
    }

    return \%result;
}

# ==============================================================================
# END OF ROBUST TREND ANALYSIS MODULE
# ==============================================================================

# ==============================================================================
# SUBROUTINE: calculate_adaptive_projection_days
# PURPOSE:    Dynamically adjust growth projection horizon based on analysis
#             window and sampling density to prevent statistical over-extrapolation
#
# ALGORITHM:
#   - Short windows (<30 days): Proportional projection to avoid 3:1 ratios
#   - Long windows (90+ days): Capped at quarterly maximum (90 days)
#   - Density adjustment: Sparse data (15-60 min intervals) reduces projection
#   - Conservative approach: Never exceeds 1:1 extrapolation for typical use
#
# INPUTS:
#   $analysis_days     - Number of days in the analysis window
#   $avg_interval_mins - Average sampling interval in minutes
#
# RETURNS: Number of days to project growth (integer, minimum 1)
#
# EXAMPLES:
#   30 days @ 1-min → 30 days (1:1 ratio)
#   90 days @ 1-min → 90 days (1:1 ratio)
#   30 days @ 15-min → 22 days (0.75 density factor applied)
# ==============================================================================
sub calculate_adaptive_projection_days {
    my ($analysis_days, $avg_interval_mins) = @_;

    # Guard against invalid inputs
    return 1 if ($analysis_days < 1);
    $avg_interval_mins //= 60;  # Default to hourly if undefined

    # Density factor: Higher frequency data supports longer projections
    # Typical NMON (1-5 min) gets full projection window
    # Sparse data (15-60 min) gets reduced projection to maintain statistical validity
    my $density_factor = 1.0;
    if ($avg_interval_mins > 30) {
        $density_factor = 0.5;   # Hourly or sparser: significantly reduce projection
    } elsif ($avg_interval_mins > 10) {
        $density_factor = 0.75;  # 15-30 min intervals: moderate reduction
    }
    # else: 1-10 min intervals use full density_factor = 1.0

    # Base projection determined by analysis window length
    # Philosophy: Match projection to observation window (1:1 ratio)
    my $base_projection;
    if ($analysis_days < 7) {
        # Very short analysis: Only 1 day projection (indicative only)
        $base_projection = 1;
    } elsif ($analysis_days < 14) {
        # Short analysis: Project half the window (conservative)
        $base_projection = int($analysis_days * 0.5);
    } elsif ($analysis_days < 30) {
        # Medium analysis: Project weekly (7 days)
        $base_projection = 7;
    } elsif ($analysis_days < 60) {
        # Monthly analysis: Project monthly (30 days)
        $base_projection = 30;
    } elsif ($analysis_days < 90) {
        # Bimonthly analysis: Project bimonthly (60 days)
        $base_projection = 60;
    } else {
        # Quarterly or longer: Cap at quarterly maximum (90 days)
        $base_projection = 90;
    }

    # Apply density adjustment to base projection
    my $adjusted_projection = int($base_projection * $density_factor);

    # Enforce minimum floor: Always project at least 1 day
    $adjusted_projection = 1 if ($adjusted_projection < 1);

    return $adjusted_projection;
}

# --- Existing Subroutines from Original Script ---
sub parse_percentile_list
{
    my ($perc_str, $arg_name) = @_;
    my @percentiles;
    if (defined $perc_str && $perc_str ne '')
    {
        my @raw_percentiles = split /,\s*/, $perc_str;
        foreach my $p (@raw_percentiles)
        {
            if ($p !~ /^[0-9]+(?:\.[0-9]+)?$/ || $p < 0 || $p > 100)
            {
                die "Error: Invalid percentile value '$p' in --$arg_name list. Must be numeric between 0 and 100.\n";
            }
            push @percentiles, $p + 0;
        }
    }
    return @percentiles;
}

# ==============================================================================
# SUBROUTINE: generate_processing_time_windows
# PURPOSE:    Subdivides an analysis period into regular time windows.
#
# ARGUMENTS:
#   1. $period_start_obj (Time::Piece): The effective start of the analysis.
#   2. $period_end_obj (Time::Piece): The effective end of the analysis.
#   3. $unit_str (string): "days" or "weeks".
#   4. $size_val (integer): The number of units in each window.
#
# RETURNS:
#   - An array reference of window hashes:
#     [ { id => "YYYYMMDD", start_obj => ..., end_obj => ... }, ... ]
#     or [ { id => "YYYYWww", start_obj => ..., end_obj => ... }, ... ]
#
# NOTES:
#   - Implements [start, end) boundaries (start inclusive, end exclusive)
#     to prevent double-counting data points that fall on a midnight boundary.
# ==============================================================================
sub generate_processing_time_windows
{
    my ($period_start_obj, $period_end_obj, $unit_str, $size_val) = @_;

    my @windows;
    return \@windows unless (
        defined $period_start_obj && defined $period_end_obj &&
        $period_start_obj->isa('Time::Piece') && $period_end_obj->isa('Time::Piece') &&
        $period_start_obj <= $period_end_obj
    );

    # Start from the beginning of the day for clean boundaries
    my $current_window_start = $period_start_obj->truncate(to => 'day');

    while ($current_window_start <= $period_end_obj)
    {
        my $current_window_end;
        my $window_id;

        if ($unit_str eq "days")
        {
            # A 1-day window starting today ends tomorrow (exclusive)
            $current_window_end = $current_window_start + (ONE_DAY() * $size_val);
            # ID is the start date
            $window_id = $current_window_start->strftime('%Y%m%d');
        }
        elsif ($unit_str eq "weeks")
        {
            # A 1-week window starting today ends 7 days from now (exclusive)
            $current_window_end = $current_window_start + (ONE_WEEK() * $size_val);
            # ID is the ISO week (Monday-based) of the *start* of the window
            $window_id = $current_window_start->strftime('%Y_W%V');
        }
        else
        {
            die "Unsupported window unit: $unit_str\n";
        }

        # Do not create a window that starts after the period has ended
        last if $current_window_start > $period_end_obj;

        # The last window must not go past the end of the analysis period.
        # Note: We do *not* truncate the period_end_obj, as it may be intra-day.
        if ($current_window_end > $period_end_obj) {
             $current_window_end = $period_end_obj + 1; # +1 second to make it exclusive
        }

        # Create the window hash
        push @windows, {
            id        => $window_id,
            start_obj => Time::Piece->new($current_window_start->epoch),
            # end_obj is exclusive
            end_obj   => Time::Piece->new($current_window_end->epoch)
        };

        # The next window starts exactly where this one ended
        $current_window_start = $current_window_end;
    }
    return \@windows;
}

sub get_window_key_for_timestamp
{
    my ($timestamp_obj, $windows_aref, $hint_idx) = @_;
    my $timestamp_day_obj = $timestamp_obj->truncate(to => 'day');
    if (defined $hint_idx && $hint_idx >=0 && $hint_idx < @{$windows_aref})
    {
        my ($win_start, $win_end) = @{$windows_aref->[$hint_idx]};
        if ($timestamp_day_obj >= $win_start && $timestamp_day_obj <= $win_end)
        {
            return $windows_aref->[$hint_idx][2]->ymd('') . "_" . $hint_idx;
        }
    }
    for (my $i=0; $i < @{$windows_aref}; $i++)
    {
        my ($win_start, $win_end) = @{$windows_aref->[$i]};
        if ($timestamp_day_obj >= $win_start && $timestamp_day_obj <= $win_end)
        {
            return $windows_aref->[$i][2]->ymd('') . "_" . $i;
        }
    }
    return undef;
}

sub get_weighted_metric_for_vm
{
    my ($vm_data_by_window_href, $vm_name_local, $metric_key, # Use local var
        $processing_windows_aref, $analysis_ref_obj_arg, $decay_hl_days_arg, $sprintf_fmt_optional) = @_;

    my @metric_window_values;
    if (exists $vm_data_by_window_href->{$vm_name_local})
    {
        foreach my $win_key (sort {
                my ($ad, $ai) = ($a =~ /^(\d{8})_(\d+)$/);
                my ($bd, $bi) = ($b =~ /^(\d{8})_(\d+)$/);
                return ($ad cmp $bd) || ($ai <=> $bi);
            } keys %{$vm_data_by_window_href->{$vm_name_local}})
        {
            if (exists $vm_data_by_window_href->{$vm_name_local}{$win_key}{$metric_key} &&
                defined $vm_data_by_window_href->{$vm_name_local}{$win_key}{$metric_key} &&
                $vm_data_by_window_href->{$vm_name_local}{$win_key}{$metric_key} ne "N/A" )
            {
                my ($win_date_str_part, $win_idx_part) = ($win_key =~ /^(\d{8})_(\d+)$/);
                if (defined $win_idx_part && $win_idx_part < @{$processing_windows_aref})
                {
                    my $rep_date_obj = $processing_windows_aref->[$win_idx_part][2];
                    unless (defined $rep_date_obj && $rep_date_obj->isa('Time::Piece'))
                    {
                        print STDERR "Warning: Representative date for window key '$win_key' is invalid for VM '$vm_name_local', metric '$metric_key'. Skipping this window point.\n";
                        next;
                    }
                    push @metric_window_values, {
                        value => $vm_data_by_window_href->{$vm_name_local}{$win_key}{$metric_key},
                        date  => $rep_date_obj
                    };
                }
                else
                {
                    print STDERR "Warning: Could not accurately map window key '$win_key' to a processing window definition for VM '$vm_name_local', metric '$metric_key'. Skipping this window point.\n";
                }
            }
        }
    }

    if (@metric_window_values)
    {
        my $final_val_str;
        if ($metric_key eq 'Peak') {
            # For the Peak, we want the absolute maximum value found across all windows.
            my @values = map { $_->{value} } grep { defined $_->{value} && looks_like_number($_->{value}) } @metric_window_values;
            $final_val_str = @values ? max(@values) : "N/A";
        } else {
            # For all other metrics, perform the recency-weighted average.
            $final_val_str = calculate_recency_weighted_average(
                \@metric_window_values, $analysis_ref_obj_arg, $decay_hl_days_arg
            );
        }
        if (defined $sprintf_fmt_optional && defined $final_val_str && $final_val_str ne "N/A" && $final_val_str =~ /^-?[0-9.]+$/)
        {
            return sprintf($sprintf_fmt_optional, $final_val_str + 0);
        }
        return $final_val_str;
    }
    return "N/A";
}

sub calculate_recency_weighted_average
{
    my ($windowed_data_ref, $analysis_ref_obj_arg, $half_life_days_arg) = @_;
    my $sum_weighted_values = 0;
    my $sum_weights = 0;
    return "N/A" if (!defined $analysis_ref_obj_arg || !$analysis_ref_obj_arg->isa('Time::Piece'));
    return "N/A" if (!defined $half_life_days_arg || $half_life_days_arg <= 0);

    my $lambda = log(2) / $half_life_days_arg;

    foreach my $dp_ref (@{$windowed_data_ref})
    {
        my $value_str = $dp_ref->{value};
        next if (!defined $value_str || $value_str eq "N/A" || $value_str !~ /^-?[0-9.]+$/);
        my $value = $value_str + 0;
        my $date_obj = $dp_ref->{date};
        next if (!defined $date_obj || !$date_obj->isa('Time::Piece'));

        my $date_obj_day = $date_obj->truncate(to => 'day');
        my $analysis_ref_day = $analysis_ref_obj_arg->truncate(to => 'day');

        my $days_diff_seconds = $analysis_ref_day->epoch - $date_obj_day->epoch;
        my $days_diff = $days_diff_seconds / ONE_DAY(); # Use seconds from Time::Seconds object
        $days_diff = 0 if $days_diff < 0;

        my $weight = exp(-$lambda * $days_diff);
        $sum_weighted_values += $value * $weight;
        $sum_weights += $weight;
    }

    if ($sum_weights > $FLOAT_EPSILON)
    {
        return sprintf("%.4f", $sum_weighted_values / $sum_weights);
    }
    else
    {
        return "N/A";
    }
}

sub get_vm_index_by_name
{
    my ($vm_name_to_find, $vm_names_list_ref) = @_;
    for (my $i=0; $i < @{$vm_names_list_ref}; $i++)
    {
        if ($vm_names_list_ref->[$i] eq $vm_name_to_find)
        {
            return $i;
        }
    }
    return undef;
}

sub calculate_average
{
    my (@data) = @_;
    my @numbers = grep { defined $_ && looks_like_number($_) } @data;
    return 0 if !@numbers;
    return sum0(@numbers) / scalar(@numbers);
}

sub calculate_percentile
{
    my ($data_ref, $p) = @_;
    my @data_input = @{$data_ref};
    my @data = grep { defined($_) && $_ =~ /^-?[0-9]+(?:\.[0-9]+)?$/ } @data_input;
    my $n = scalar @data;
    return undef if $n == 0;
    @data = sort { $a <=> $b } @data;
    return $data[0] if $n == 1;

    my $rank_fractional = ($p / 100) * ($n - 1);
    my $k = int($rank_fractional);
    my $d = $rank_fractional - $k;

    if ($p == 0) { return $data[0]; }
    if ($p == 100) { return $data[$n-1]; }

    if ($k >= $n - 1)
    {
        return $data[$n - 1];
    }
    elsif ($k < 0)
    {
        return $data[0];
    }
    else
    {
        my $val_k = $data[$k];
        my $val_k_plus_1 = ($k + 1 < $n) ? $data[$k + 1] : $data[$k];
        return $val_k + $d * ($val_k_plus_1 - $val_k);
    }
}

sub calculate_rolling_average_series
{
    my ($data_series_ref, $method, $window_size, $alpha_val) = @_;
    my @output_series;
    my @current_window_sma;
    my $current_ema;

    if ($method eq 'none')
    {
        return $data_series_ref;
    }

    foreach my $value (@{$data_series_ref})
    {
        my $avg_val_to_add = undef;
        if ($method eq 'sma')
        {
            if (defined $value && $value =~ /^-?[0-9.]+$/)
            {
                push @current_window_sma, ($value + 0);
            }
            else
            {
                push @current_window_sma, undef;
            }
            shift @current_window_sma while scalar @current_window_sma > $window_size;
            if (scalar @current_window_sma == $window_size)
            {
                $avg_val_to_add = calculate_average(@current_window_sma);
            }
        }
        elsif ($method eq 'ema')
        {
            if (defined $value && $value =~ /^-?[0-9.]+$/)
            {
                my $numeric_value = $value + 0;
                if (!defined $current_ema)
                {
                    $current_ema = $numeric_value;
                }
                else
                {
                    $current_ema = ($numeric_value * $alpha_val) + ($current_ema * (1 - $alpha_val));
                }
            }
            push @current_window_sma, $value;
            shift @current_window_sma while scalar @current_window_sma > $window_size;
            if (scalar @current_window_sma == $window_size)
            {
                $avg_val_to_add = $current_ema;
            }
        }
        push @output_series, $avg_val_to_add if defined $avg_val_to_add;
    }
    return \@output_series;
}

sub apply_rounding
{
    my ($value, $increment, $method) = @_;
    return $value unless (defined $value && $value =~ /^-?[0-9]+(?:\.[0-9]+)?$/);
    return $value if $method eq 'none' || !defined $increment || $increment <= $FLOAT_EPSILON;

    my $rounded_value;
    if ($method eq 'standard')
    {
        $rounded_value = int( ($value / $increment) + ( ($value >= 0) ? 0.5 : -0.5) ) * $increment;
    }
    elsif ($method eq 'up')
    {
        $rounded_value = ceil( $value / $increment ) * $increment;
    }
    else
    {
        $rounded_value = $value;
    }
    return $rounded_value;
}

sub get_decimal_places
{
    my ($number_str) = @_;
    $number_str = sprintf("%.15f", $number_str) if ($number_str =~ /e/i);
    if ($number_str =~ /\.(\d+)$/)
    {
        return length($1);
    }
    else
    {
        return 0;
    }
}

# --- define_time_windows_for_state ---
# Subdivides a configuration state window into smaller, regular time-based
# windows for decay and growth analysis.
#
sub define_time_windows_for_state {
    my ($state_obj, $ref_date_str, $unit, $size) = @_;
    my @windows;
    my $start = $state_obj->{start_time};
    my $end   = $state_obj->{end_time};

    my $current_win_start = $start->truncate(to => 'day');

    while ($current_win_start < $end) {
        my $current_win_end;
        if ($unit eq 'days') {
            $current_win_end = $current_win_start + ($size * ONE_DAY()) - 1;
        } else { # weeks
            $current_win_end = $current_win_start + ($size * ONE_WEEK()) - 1;
        }

        $current_win_end = $end if $current_win_end > $end;

        push @windows, {
            start => $current_win_start,
            end   => $current_win_end,
            rep_date => $current_win_end, # Representative date for weighting is the end of the window
        };

        $current_win_start = $current_win_end->truncate(to => 'day') + ONE_DAY();
    }
    return @windows;
}

# ==============================================================================
# SUBROUTINE: build_cache_index
# PURPOSE:    Builds a date-to-byte-offset index for the L1 cache data file.
#             This enables O(1) seeking to specific dates instead of sequential
#             scanning from the start of the file.
#
# ARGUMENTS:
#   1. $data_file (string): Path to the .nfit.cache.data file
#
# RETURNS:
#   Hash reference: { "YYYY-MM-DD" => byte_offset, ... }
#
# PERFORMANCE:
#   - Optimised with substr() for date extraction (5-10x faster than split)
#   - Uses :raw layer (no UTF-8 decoding overhead)
#   - For 350MB / 8.9M lines: ~2-3 seconds
# ==============================================================================
sub build_cache_index {
    my ($data_file) = @_;

    my %index;
    keys(%index) = 100;  # Pre-allocate for ~90 days

    my $current_date = '';
    my $line_count = 0;

    # Open with :raw for performance (VM names are ASCII on UNIX platforms)
    open my $fh, '<:raw', $data_file or die "FATAL: Cannot open $data_file: $!\n";

    # Skip header line (Timestamp,VMName,PhysC,RunQ)
    my $header = <$fh>;

    # Record position before first data line
    my $line_start_pos = tell($fh);

    while (my $line = <$fh>) {
        # Extract YYYY-MM-DD (first 10 characters)
        # This is 5-10x faster than split() operations
        my $date = substr($line, 0, 10);

        # Validate date format (detect corrupted data)
        unless ($date =~ /^\d{4}-\d{2}-\d{2}$/) {
            warn "Warning: Invalid date format at line $line_count: '$date'\n";
            $line_start_pos = tell($fh);
            next;
        }

        # Record byte offset when date changes
        if ($date ne $current_date) {
            $index{$date} = $line_start_pos;
            $current_date = $date;
        }

        # Save position for next iteration
        $line_start_pos = tell($fh);

        # Progress indicator for large files
        if (++$line_count % 1_000_000 == 0) {
            printf STDERR "\r  Building cache index: %.1fM lines...", $line_count / 1_000_000;
        }
    }

    if ($line_count >= 1_000_000) {
        printf STDERR "\r  Building cache index: %.1fM lines... Done (%d days)    \n",
            $line_count / 1_000_000, scalar keys %index;
    }

    close $fh;

    return \%index;
}

# ==============================================================================
# SUBROUTINE: load_or_build_cache_index
# PURPOSE:    Loads an existing cache index if valid, otherwise builds a new one.
#             Index validity is based on source file size and modification time.
#
# ARGUMENTS:
#   1. $cache_dir (string): Path to the cache directory
#   2. $data_file (string): Path to the .nfit.cache.data file
#
# RETURNS:
#   Hash reference: { "YYYY-MM-DD" => byte_offset, ... }
# ==============================================================================
sub load_or_build_cache_index {
    my ($cache_dir, $data_file) = @_;

    my $idx_file  = File::Spec->catfile($cache_dir, $CACHE_INDEX_FILE);
    my $lock_file = File::Spec->catfile($cache_dir, $CACHE_LOCK_FILE);

    my @st = stat($data_file) or die "FATAL: Cannot stat $data_file: $!\n";
    my ($data_size, $data_mtime) = @st[7, 9];

    # Try to load existing index
    if (-f $idx_file) {
        my ($index_size, $index_mtime);
        my $cached_index;
        my $cache_ok = 0;

        eval {
            open my $fh, '<', $idx_file or die "Cannot open index file: $!\n";
            flock($fh, LOCK_SH) or die "Cannot lock index file: $!\n";
            local $/; my $raw = <$fh>;
            close $fh;

            my $idx = decode_json($raw);

            die "Invalid index structure\n" unless ref($idx) eq 'HASH';
            die "Missing metadata\n"     unless ref($idx->{metadata}) eq 'HASH';
            die "Missing index\n"        unless ref($idx->{index})    eq 'HASH';

            # Coerce to numeric to avoid any string/dualvar surprises
            $index_size  = 0 + ($idx->{metadata}{source_size}  // -1);
            $index_mtime = 0 + ($idx->{metadata}{source_mtime} // -1);

            if ($index_size == $data_size && $index_mtime == $data_mtime) {
                $cached_index = $idx->{index};
                $cache_ok = 1;  # mark as a valid cache hit; return after eval
            }
            1;
        } or do {
            warn "Warning: Cache index corrupted or invalid: $@\n";
        };

        return $cached_index if $cache_ok;
        # Fall through to rebuild if we didnâ€™t get a valid cache hit
    }

    # Cache miss or stale - build new index
    print STDERR "Building cache index for date-range optimisation...\n";
    my $index = build_cache_index($data_file);

    # Save the index with metadata
    my $idx_data = {
        metadata => {
            source_file  => $CACHE_DATA_FILE,
            source_size  => $data_size + 0,
            source_mtime => $data_mtime + 0,
            created_at   => gmtime()->datetime(),
            nfit_version => $VERSION,
        },
        index => $index,
    };

    # Exclusive lock + atomic write (temp then rename)
    open my $lock_fh, '>>', $lock_file or die "FATAL: Cannot create lock file: $!\n";
    flock($lock_fh, LOCK_EX) or die "FATAL: Cannot acquire exclusive lock: $!\n";

    my $tmp = "$idx_file.tmp.$$";
    eval {
        open my $out, '>', $tmp or die "Cannot write temp index file: $!\n";
        print {$out} JSON->new->pretty->canonical->encode($idx_data);
        close $out or die "Cannot close temp index file: $!\n";
        rename $tmp, $idx_file or die "Cannot atomically replace index file: $!\n";
        1;
    } or do {
        warn "Warning: Failed to save cache index: $@\n";
        unlink $tmp;
    };

    flock($lock_fh, LOCK_UN);
    close $lock_fh;

    return $index;
}

# ==============================================================================
# SUBROUTINE: seek_to_start_date
# PURPOSE:    Seeks to the appropriate position in the data file based on the
#             start date, using the byte-offset index.
#
# ARGUMENTS:
#   1. $fh (filehandle): Open filehandle to the data file
#   2. $index (hash ref): Date-to-byte-offset index
#   3. $start_date_str (string): Start date in YYYY-MM-DD format
#
# RETURNS:
#   Byte offset where seeking occurred (for progress calculation)
# ==============================================================================
sub seek_to_start_date {
    my ($fh, $index, $start_date_str) = @_;

    return 0 unless defined $start_date_str && $start_date_str ne '';

    # Try exact match first
    if (exists $index->{$start_date_str}) {
        seek($fh, $index->{$start_date_str}, 0);
        return $index->{$start_date_str};
    }

    # No exact match - find closest earlier date
    my @dates = sort keys %$index;
    for (my $i = $#dates; $i >= 0; $i--) {
        if ($dates[$i] le $start_date_str) {
            seek($fh, $index->{$dates[$i]}, 0);
            return $index->{$dates[$i]};
        }
    }

    # Start date is before all indexed data - start from beginning
    return 0;
}

sub get_window_index_for_timestamp {
    my ($timestamp_obj, $windows_aref) = @_;
    my $ts_epoch = $timestamp_obj->epoch;
    for (my $i=0; $i < @{$windows_aref}; $i++) {
        my ($win_start, $win_end) = @{$windows_aref->[$i]};
        return $i if ($ts_epoch >= $win_start->epoch && $ts_epoch <= $win_end->epoch);
    }
    return undef;
}


sub parse_csv_line {
    my ($line) = @_;
    my @fields;

    while (length($line)) {
        if ($line =~ s/^"((?:[^"]|"")*)"[,]*//) {
            my $field = $1;
            $field =~ s/""/"/g;  # unescape double quotes
            push @fields, $field;
        } elsif ($line =~ s/^([^",]*)[,]*//) {
            push @fields, $1;
        } else {
            # fallback (shouldn't happen with consistent CSV)
            last;
        }
    }

    return @fields;
}

## --- BEGIN L2 RESULTS CACHE SUBROUTINES --- ##

# ==============================================================================
# Subroutine to look up a result from the L2 cache file.
# Returns: An array reference of result hashes, or undef if not found.
# ==============================================================================
sub lookup_cached_result
{
    my ($key, $cache_file) = @_;

    return undef unless (-f $cache_file && -s $cache_file);

    # Read the entire file content as a single string.
    my $json_text = do {
        open my $fh, '<:encoding(utf8)', $cache_file or do {
            warn "Warning: Could not open results cache '$cache_file' for reading: $!";
            return undef;
        };
        local $/;
        my $content = <$fh>;
        close $fh;
        $content;
    };

    # Decode the single JSON object from the file.
    my $json_decoder = JSON->new->allow_nonref;
    my $cache_data = eval { decode_json($json_text) };
    if ($@ || !defined($cache_data) || ref($cache_data) ne 'HASH') {
        print STDERR "Warning: Results cache '$cache_file' is invalid. Resetting.\n";
        unlink $cache_file or warn "Warning: Could not remove corrupt cache file '$cache_file': $!";
        return undef;
    }

    # The value is now an array reference of hashes.
    return $cache_data->{$key};
}

# ==============================================================================
# Subroutine to save a result to the L2 cache file under an exclusive lock.
# ==============================================================================
sub save_result_to_cache
{
    my ($key, $result_objects_aref, $cache_file, $lock_file) = @_;

    # This subroutine now performs a safe read-modify-write of the entire JSON cache file.
    # Acquire an exclusive lock to prevent race conditions when writing.
    open my $lock_fh, '>', $lock_file or do {
        warn "Warning: Could not create lock file '$lock_file' for writing cache: $!. Skipping cache write.";
        return;
    };
    flock($lock_fh, LOCK_EX) or do {
        warn "Warning: Could not acquire exclusive lock on '$lock_file': $!. Skipping cache write.";
        close $lock_fh;
        return;
    };
    # Read the existing cache file into a Perl hash.
    my $cache_data = {};
    if (-f $cache_file && -s $cache_file) {
        my $json_text = do {
            open my $read_fh, '<:encoding(utf8)', $cache_file;
            local $/;
            <$read_fh>;
        };
        my $json_decoder = JSON->new->allow_nonref;
        my $decoded = eval { $json_decoder->decode($json_text) };
        # If the file is valid JSON hash, use it. Otherwise, start fresh.
        $cache_data = $decoded if (ref($decoded) eq 'HASH');
    }

    # Add or update the result for the current key with the array of result objects.
    $cache_data->{$key} = $result_objects_aref;

    # Encode the entire updated hash back to a pretty-printed JSON string.
    my $json_encoder = JSON->new->pretty->canonical;
    my $output_json = $json_encoder->encode($cache_data);

    # Overwrite the cache file with the new content.
    open my $write_fh, '>:encoding(utf8)', $cache_file or do {
        warn "Warning: Could not open results cache '$cache_file' for writing: $!";
        close $lock_fh;
        return;
    };

    print $write_fh $output_json;
    close $write_fh;
    close $lock_fh; # Releases the lock automatically on close.
}

# Subroutine for the Hybrid State-Time Decay Model
sub synthesize_hybrid_timeseries {
    my ($state_results_aref, $metric_key) = @_;
    my @timeseries_points;

    foreach my $state_result (@$state_results_aref) {
        my $start_obj = $state_result->{state_obj}{start_time}->truncate(to => 'day');
        my $end_obj   = $state_result->{state_obj}{end_time}->truncate(to => 'day');
        my $metric_val = $state_result->{metrics}{$metric_key};

        # Skip if this state didn't have a valid metric
        next unless defined $metric_val && looks_like_number($metric_val);

        my $current_day = $start_obj;
        while ($current_day <= $end_obj) {
            # Add a data point for every day the state was active
            push @timeseries_points, {
                value => $metric_val,
                date  => Time::Piece->new($current_day->epoch) # Use a copy
            };
            $current_day += ONE_DAY;
        }
    }
    return @timeseries_points;
}

# ==============================================================================
# SUBROUTINE: _calculate_clipping_metrics
# PURPOSE:    Implements the Multi-Stage Confidence Pipeline to detect CPU
#             clipping and estimate latent demand from a buffer of raw
#             performance data.
# ARGS:
#   1. $raw_physc_aref (array ref): A buffer of raw, high-frequency PhysC values.
#   2. $raw_runq_aref (array ref): A buffer of raw, high-frequency RunQ values.
#   3. $max_capacity (numeric): The authoritative max_capacity for the state.
#   4. $smt (numeric): The SMT value for the state, for RunQ context.
# RETURNS:
#   - A hash reference containing the clipping analysis results:
#     {
#       isClipped => 0|1,
#       clippingConfidence => 0.0-1.0,
#       clippingSeverity => 'none'|'low'|'moderate'|'high',
#       unmetDemandEstimate => 0.0,
#       platformMarkers => { aix_runq_saturation => 0.0-1.0 }
#     }
# ==============================================================================
sub _calculate_clipping_metrics
{
    my ($raw_physc_aref, $raw_runq_aref, $max_capacity, $smt) = @_;

    my %results = (
        isClipped           => 0,
        clippingConfidence  => 0.0,
        clippingSeverity    => 'none',
        unmetDemandEstimate => 0.0,
        platformMarkers     => {},
    );

    # The pipeline requires a valid capacity limit and sufficient data points.
    return \%results unless (defined $max_capacity && $max_capacity > 0.1 && @$raw_physc_aref > 10);

    # --- Stage I & II: Raw Detection & Shape Analysis ---
    # To perform the analysis, we need several key percentiles from the raw data.
    # We calculate them once here for efficiency.
    my @sorted_physc = sort { $a <=> $b } @$raw_physc_aref;
    my $p99_9 = calculate_percentile(\@sorted_physc, 99.9);
    my $p95   = calculate_percentile(\@sorted_physc, 95);
    my $p90   = calculate_percentile(\@sorted_physc, 90);
    my $p50   = calculate_percentile(\@sorted_physc, 50);

    return \%results unless (defined $p99_9 && defined $p95 && defined $p50);

    my $saturation_level = $p99_9 / $max_capacity;

    # Calculate Peak Curvature Ratio (PCR)
    my $pcr_numerator   = $p99_9 - $p95;
    my $pcr_denominator = $p95 - $p50;
    my $pcr = ($pcr_denominator > $FLOAT_EPSILON) ? ($pcr_numerator / $pcr_denominator) : 999;

    # --- Stage III: Platform Validation (AIX RunQ) ---
    my $runq_saturation_score = 0.0;
    if (@$raw_runq_aref) {
        my @sorted_runq = sort { $a <=> $b } @$raw_runq_aref;
        my $runq_p95 = calculate_percentile(\@sorted_runq, 95);

        # A simple model for RunQ saturation: the ratio of P95 RunQ to the number
        # of logical CPUs (SMT). A score > 1.0 indicates queuing.
        if (defined $runq_p95 && defined $smt && $smt > 0) {
            $runq_saturation_score = $runq_p95 / $smt;
            $results{platformMarkers}{aix_runq_saturation} = sprintf("%.2f", $runq_saturation_score);
        }
    }

    # --- Stage IV: Final Clipping Confidence Score ---
    my $confidence = 0.0;

    # Base confidence from saturation level
    if ($saturation_level >= $CLIPPING_DEFINITE_THRESHOLD) {
        $confidence += 0.6; # High base confidence for definite saturation
    } elsif ($saturation_level >= $CLIPPING_POTENTIAL_THRESHOLD) {
        $confidence += 0.3; # Moderate base confidence for potential saturation
    }

    # Add confidence from shape analysis (low PCR)
    if ($pcr < $PCR_HIGH_CONFIDENCE_THRESHOLD) {
        $confidence += 0.35; # High confidence for very flat peaks
    } elsif ($pcr < $PCR_LOW_CONFIDENCE_THRESHOLD) {
        $confidence += 0.15; # Moderate confidence for somewhat flat peaks
    }

    # Boost confidence if platform markers confirm pressure
    if ($runq_saturation_score > 1.5) {
        $confidence += 0.2; # Significant boost for high RunQ
    }

    # Cap confidence at 1.0
    $results{clippingConfidence} = min(1.0, $confidence);

    # --- Latent Demand Estimation ---
    if ($results{clippingConfidence} > 0.65) {
        $results{isClipped} = 1;

        # Estimate unmet demand as a function of confidence and saturation severity.
        # This is a simple model: the more confident we are, and the closer to the
        # ceiling the workload is, the higher the estimated latent demand.
        my $severity_factor = ($saturation_level - $CLIPPING_POTENTIAL_THRESHOLD) * 10; # Normalise 0.9-1.0 range
        $severity_factor = max(0, min(1.0, $severity_factor));

        # The unmet demand is a fraction of the P90 value, scaled by our confidence and severity.
        $results{unmetDemandEstimate} = $p90 * $results{clippingConfidence} * $severity_factor * 0.25; # 0.25 is a tuning factor

        # Determine severity string for reporting
        if ($results{clippingConfidence} > 0.9) {
            $results{clippingSeverity} = 'high';
        } elsif ($results{clippingConfidence} > 0.75) {
            $results{clippingSeverity} = 'moderate';
        } else {
            $results{clippingSeverity} = 'low';
        }
    }

    return \%results;
}

# ==============================================================================
# SUBROUTINE: aggregate_decay_metrics (CORRECTED VERSION)
# PURPOSE:    Aggregates per-bin metrics using recency weighting (decay) and
#             extracts RunQ metrics from the most recent bin WITH ACTUAL DATA.
#
# Important:
#   - Previous versions assumed the chronologically last time window exists for
#     all VMs, causing failures when certain VMs have incomplete data.
#   - This version finds the most recent bin that actually has data for each VM.
# ==============================================================================
sub aggregate_decay_metrics {
    my ($vm_name, $final_metrics_href, $manifest_href,
        $model_is_windowed, $window_context_aref, $state_context_aref,
        $analysis_ref_obj, $args_ref) = @_;

    my %aggregated_results_for_vm;

    # Get all per-bin metrics for this specific VM
    my $metrics_for_vm = $final_metrics_href->{$vm_name};
    return {} unless (defined $metrics_for_vm && ref($metrics_for_vm) eq 'HASH');

    # Find all unique profiles that have metrics for this VM
    my %profiles_for_vm;
    foreach my $bin_id (keys %{$metrics_for_vm}) {
        foreach my $key (keys %{$metrics_for_vm->{$bin_id}}) {
            $profiles_for_vm{$_} = 1 for keys %{$manifest_href->{$key}{profiles} || {}};
        }
    }

    foreach my $profile_name (keys %profiles_for_vm) {
        # Find the PhysC transform key for this profile
        my ($physc_transform_key) = grep {
            $manifest_href->{$_}{metric} eq 'PhysC' &&
            exists $manifest_href->{$_}{profiles}{$profile_name}
        } keys %$manifest_href;
        next unless $physc_transform_key;

        # Get the percentile key for this profile
        my $percentiles = _safe_dig($manifest_href, $physc_transform_key, 'profiles', $profile_name, 'percentiles');
        next unless (defined $percentiles && ref $percentiles eq 'ARRAY' && @$percentiles);
        my $p_val = $percentiles->[0]; # Assumes first percentile is the primary one
        my $p_key = "P" . clean_perc_label($p_val);

        # --- Build Time Series for PhysC ---
        # This series is built from the *aggregated metrics* of each bin.
        my @physc_timeseries;

        # Create a bin iterator for this loop
        my $bin_iterator = _create_bin_iterator(
            $model_is_windowed, $window_context_aref, $state_context_aref
        );

        while (my $bin_meta = $bin_iterator->()) {
            my $bin_id = $bin_meta->{id};

            # Get the aggregated metric for this bin
            my $metric_val = _safe_dig($metrics_for_vm, $bin_id, $physc_transform_key, $p_key);

            if (defined $metric_val && looks_like_number($metric_val)) {
                push @physc_timeseries, {
                    value => $metric_val,
                    date  => $bin_meta->{end_obj} # Use bin end-date for recency
                };
            }
        }

        # Calculate aggregated PhysC value
        if (@physc_timeseries) {
            my $aggregated_physc = calculate_recency_weighted_average(
                \@physc_timeseries,
                $analysis_ref_obj,
                $args_ref->{decay_half_life_days}
            );
            $aggregated_results_for_vm{$profile_name}{aggregated_value} = $aggregated_physc;
        }
    } # end foreach profile

    # ===========================================================================
    # CRITICAL FIX: Extract RunQ metrics from the MOST RECENT BIN WITH DATA
    # ===========================================================================
    # PROBLEM: VMs may go offline or have incomplete data in the chronologically
    #          last time window due to InfluxDB ingestion lag or system downtime.
    #          The previous code blindly used the last chronological window,
    #          causing undef lookups and cascading failures.
    #
    # SOLUTION: Build a map of bin end times, then find the most recent bin that
    #           actually exists in metrics_for_vm for THIS SPECIFIC VM.
    # ===========================================================================

    my $last_valid_bin_id = undef;
    my $last_valid_bin_end_epoch = 0;

    # Build a lookup map of bin_id -> end_epoch for O(1) access
    my %bin_end_times;
    my $bin_iterator_for_times = _create_bin_iterator(
        $model_is_windowed, $window_context_aref, $state_context_aref
    );
    while (my $bin_meta = $bin_iterator_for_times->()) {
        $bin_end_times{$bin_meta->{id}} = $bin_meta->{end_obj}->epoch;
    }

    # Find the most recent bin that actually has data for this VM
    foreach my $bin_id (keys %{$metrics_for_vm}) {
        if (exists $bin_end_times{$bin_id}) {
            my $bin_end = $bin_end_times{$bin_id};
            if ($bin_end > $last_valid_bin_end_epoch) {
                $last_valid_bin_end_epoch = $bin_end;
                $last_valid_bin_id = $bin_id;
            }
        }
    }

    # Extract RunQ metrics from the identified bin
    if (defined $last_valid_bin_id) {
        # Optional diagnostic output for debugging
        if ($args_ref->{verbose} && $args_ref->{verbose} > 1) {
            warn "DEBUG: Using bin '$last_valid_bin_id' for RunQ metrics extraction (VM: $vm_name)\n";
        }

        foreach my $profile_name (keys %profiles_for_vm) {
            # Find NormRunQ transform for this profile
            my ($norm_rq_key) = grep {
                $manifest_href->{$_}{metric} eq 'NormRunQ' &&
                exists $manifest_href->{$_}{profiles}{$profile_name}
            } keys %$manifest_href;

            if ($norm_rq_key) {
                my $norm_metrics = _safe_dig($metrics_for_vm, $last_valid_bin_id, $norm_rq_key);

                # Validate the retrieved data structure
                if (defined $norm_metrics &&
                    ref($norm_metrics) eq 'HASH' &&
                    keys %$norm_metrics) {
                    $aggregated_results_for_vm{_RunQ}{normalized}{$profile_name} = $norm_metrics;
                }
            }

            # Find AbsRunQ transform for this profile
            my ($abs_rq_key) = grep {
                $manifest_href->{$_}{metric} eq 'AbsRunQ' &&
                exists $manifest_href->{$_}{profiles}{$profile_name}
            } keys %$manifest_href;

            if ($abs_rq_key) {
                my $abs_metrics = _safe_dig($metrics_for_vm, $last_valid_bin_id, $abs_rq_key);

                # Validate the retrieved data structure
                if (defined $abs_metrics &&
                    ref($abs_metrics) eq 'HASH' &&
                    keys %$abs_metrics) {
                    $aggregated_results_for_vm{_RunQ}{absolute}{$profile_name} = $abs_metrics;
                }
            }
        }
    } else {
        # This should be extremely rare - only if VM has no data at all
        warn "WARNING: No bins with valid data found for VM '$vm_name' - RunQ metrics unavailable\n";
    }

    return \%aggregated_results_for_vm;
}

# ==============================================================================
# SUBROUTINE: apply_growth_prediction_to_metrics
# PURPOSE:    Applies a growth adjustment factor to the final metric values for
#             profiles that have growth prediction enabled in the manifest.
# ==============================================================================
sub apply_growth_prediction_to_metrics {
    my ($agg_metrics_href, $manifest_href, $analysis_data_href, $vm_name, $args_ref) = @_;

    # Build a map from profile name to its corresponding PhysC transform key
    # but only for profiles that have growth prediction enabled.
    my %profile_to_transform;
    foreach my $transform_key (keys %$manifest_href) {
        my $transform_info = $manifest_href->{$transform_key};
        next unless $transform_info->{metric} eq 'PhysC';

        foreach my $profile_name (keys %{$transform_info->{profiles}}) {
            my $directives = $transform_info->{profiles}{$profile_name};
            if ($directives->{enable_growth}) {
                $profile_to_transform{$profile_name} = $transform_key;
            }
        }
    }

    # Process the first profile found that has growth enabled.
    # In this aggregated model, the growth trend is VM-wide, so one calculation is sufficient.
    foreach my $profile_name (sort keys %profile_to_transform) {
        my $transform_key = $profile_to_transform{$profile_name};
        my $baseline_val = $agg_metrics_href->{$profile_name};

        # Build the complete timeseries from the analysis data using the correct transform key.
        my @timeseries;
        foreach my $state_id (sort { $a <=> $b } keys %{$analysis_data_href->{$vm_name}}) {
            if (exists $analysis_data_href->{$vm_name}{$state_id}{$transform_key}) {
                push @timeseries, unpack('f<*', $analysis_data_href->{$vm_name}{$state_id}{$transform_key});
            }
        }

        unless (@timeseries) {
            warn "Warning: No timeseries data found for growth calculation on profile $profile_name (transform: $transform_key)\n";
            next;
        }

        my $growth_result = calculate_growth_prediction(\@timeseries, $baseline_val);

        if ($growth_result) {
            # Store the single, authoritative growth adjustment and rationale for the entire VM.
            $agg_metrics_href->{GrowthAdj} = $growth_result->{growth_adj};
            $agg_metrics_href->{GrowthRationale} = $growth_result->{rationale};

#            if ($args_ref->{verbose}) {
#                print STDERR "DEBUG: Growth calculation succeeded for VM $vm_name (using profile $profile_name): adj=" . $growth_result->{growth_adj} . "\n";
#            }
        } else {
            warn "Warning: Growth calculation returned no result for profile $profile_name\n";
        }

        # Since growth is VM-wide in this model, we only need to calculate it once.
        return;
    }
}

# ==============================================================================
#                      Single-Pass Engine Subroutines
# ==============================================================================

sub run_single_pass_from_manifest {
    my ($args_ref) = @_;

    # 1. Unpack all necessary arguments for the entire run.
    my $manifest_file                = $args_ref->{manifest_file};
    my $nmon_dir                     = $args_ref->{nmon_dir};
    my $target_vm_name               = $args_ref->{target_vm_name};
    my $exclude_vms_str              = $args_ref->{exclude_vms_str};
    my $start_date_str               = $args_ref->{start_date_str};
    my $end_date_str                 = $args_ref->{end_date_str};
    my $include_states_selector      = $args_ref->{include_states};
    my $smt_value                    = $args_ref->{smt_value};
    my $runq_avg_method_str          = $args_ref->{runq_avg_method_str};
    my $verbose                      = $args_ref->{verbose};

    # Rounding Arguments
    my $round_arg                    = $args_ref->{round_arg};
    my $roundup_arg                  = $args_ref->{roundup_arg};

    # Analysis Model Flags
    my $enable_windowed_decay        = $args_ref->{enable_windowed_decay};
    my $decay_over_states_flag       = $args_ref->{decay_over_states};
    my $enable_growth_prediction     = $args_ref->{enable_growth_prediction};
    my $enable_clipping_detection    = $args_ref->{enable_clipping_detection};

    # Decay Model Tunables
    my $process_window_unit_str      = $args_ref->{process_window_unit};
    my $process_window_size_val      = $args_ref->{process_window_size};
    my $decay_half_life_days_val     = $args_ref->{decay_half_life_days};
    my $analysis_reference_date_str  = $args_ref->{analysis_reference_date};

    # Growth Model Tunables
    my $growth_projection_days       = $args_ref->{growth_projection_days};
    my $max_growth_inflation_percent = $args_ref->{max_growth_inflation_percent};

    # 2. Read and decode the manifest
    my $json_text = do { open my $fh, '<:encoding(utf8)', $manifest_file or die "Cannot open manifest $manifest_file: $!"; local $/; <$fh> };
    my $manifest = decode_json($json_text);

    # 3. Handle L2 Caching
    # Generate a unique key from the manifest and all relevant global arguments.
    my $canonical_key = generate_manifest_key($args_ref, $manifest);
    my $results_cache_path = File::Spec->catfile($nmon_dir, ".nfit.cache.results");

    my $cached_result_aref = lookup_cached_result($canonical_key, $results_cache_path);
    if (defined $cached_result_aref) {
        print STDERR "L2 Cache HIT. Returning cached result.\n" if $verbose;
        my $json_encoder = JSON->new->utf8;
        my @json_lines = map { $json_encoder->encode($_) } @$cached_result_aref;
        print STDOUT join("\n", @json_lines) . "\n";
        return; # Exit the function immediately on a cache hit.
    }

    # 4. Load and "Hydrate" Configuration States
    my $states_cache_file = File::Spec->catfile($nmon_dir, $CACHE_STATES_FILE);
    my $states_json_text = do { open my $fh, '<:encoding(utf8)', $states_cache_file; local $/; <$fh> };
    my $states_by_vm = decode_json($states_json_text);

    # ======================================================================
    # == 1. Generate Analysis Context (Windows or States)
    # ======================================================================
    # This block determines the binning strategy for the entire run.

    my $time_windows_aref = []; # Default to empty array
    my $binning_context_source; # For logging
    my $data_cache_file = File::Spec->catfile($nmon_dir, $CACHE_DATA_FILE);

    if ($enable_windowed_decay) {
        # --- Windowed-Decay Model ---
        # Find the true start/end of the cache data and intersect with user flags.
        my ($eff_start, $eff_end) = _get_effective_cache_range(
            $data_cache_file, $start_date_str, $end_date_str
        );

        if ($eff_start && $eff_end) {
            # Generate the time-window bins for the effective analysis period.
            $time_windows_aref = generate_processing_time_windows(
                $eff_start, $eff_end,
                $process_window_unit_str, $process_window_size_val
            );
            $binning_context_source = "TimeWindows (" . scalar(@$time_windows_aref) . " bins)";
        } else {
            warn "Warning: Could not determine effective date range for windowed decay. No data will be processed.\n";
        }
    } else {
        # --- Hybrid-Decay or Standard Model ---
        # The binning context will be the VM's Configuration States.
        # This is handled dynamically inside process_data_point.
        $binning_context_source = "ConfigStates";
    }

    print STDERR "  Binning Strategy: $binning_context_source\n" if $verbose;

    # This block modifies the %states_by_vm hash IN-PLACE, adding derived
    # metadata and Time::Piece objects that are used by all downstream functions.
    print STDERR "Hydrating configuration states...\n" if $verbose;
    foreach my $vm_name (keys %$states_by_vm) {
        if (ref($states_by_vm->{$vm_name}) eq 'ARRAY') {
            for (my $i = 0; $i < @{$states_by_vm->{$vm_name}}; $i++) {
                my $state = $states_by_vm->{$vm_name}[$i];
                $state->{state_id} = $i + 1;
                $state->{vm_name}  = $vm_name;
                if (defined $state->{start_epoch}) { $state->{start_time} = gmtime($state->{start_epoch}); }
                if (defined $state->{end_epoch})   { $state->{end_time}   = gmtime($state->{end_epoch}); }

                # Populate duration_days for reporting/metadata consistency
                if (defined $state->{start_epoch} && defined $state->{end_epoch}) {
                    my $diff = $state->{end_epoch} - $state->{start_epoch};
                    $state->{duration_days} = ($diff >= 0) ? (int($diff / 86400) + 1) : 0;
                } else {
                    $state->{duration_days} = 0 unless defined $state->{duration_days};
                }

                my $md = $state->{metadata};
                $md->{smt} //= $smt_value; # USE the unpacked global SMT as a fallback.

                my $ent       = $md->{entitlement} // 0;
                my $vcpu      = $md->{virtual_cpus} // 0;
                my $poolcpu   = $md->{pool_cpu} // 0;
                my $is_capped = $md->{capped} // 0;
                my $max_cpu_calc = $is_capped ? $ent : ($poolcpu > 0 && $vcpu > 0 ? min($vcpu, $poolcpu) : $vcpu);
                $md->{max_cpu} = $max_cpu_calc > 0 ? $max_cpu_calc : ($vcpu > 0 ? $vcpu : 0);
            }
        }
    }

    # 6. Global Filter Logic
    # This logic uses the hydrated states and global flags to determine the exact
    # scope of the analysis before the main data pass begins.
    my %vms_to_process;
    if (defined $target_vm_name) {
        %vms_to_process = map { $_ => 1 } split /,/, $target_vm_name;
    } else {
        %vms_to_process = map { $_ => 1 } keys %$states_by_vm;
    }

    # Phase 3.3: Apply VM exclusion filter (--exclude-vms)
    if (defined $exclude_vms_str && $exclude_vms_str =~ /\S/) {
        my @excluded_vms;
        foreach my $vm_raw (split /,/, $exclude_vms_str) {
            my $vm = $vm_raw;
            $vm =~ s/^\s+//;  # Trim leading whitespace
            $vm =~ s/\s+$//;  # Trim trailing whitespace
            next unless length($vm);

            if (exists $vms_to_process{$vm}) {
                delete $vms_to_process{$vm};
                push @excluded_vms, $vm;
            } else {
                print STDERR "[INFO] Exclusion target '$vm' not in processing set (already excluded or not present)\n"
                    unless $quiet;
            }
        }

        if (@excluded_vms && !$quiet) {
            my $count = scalar(@excluded_vms);
            print STDERR "[SCOPE] Excluded $count VM(s) via --exclude-vms: " . join(', ', sort @excluded_vms) . "\n";
        }

        # Safety check: ensure at least one VM remains
        unless (%vms_to_process) {
            die "[ERROR] All VMs excluded by --exclude-vms. No VMs remain for analysis.\n";
        }
    }

    if (defined $include_states_selector && lc($include_states_selector) ne 'all') {
        my @excluded_vms;
        foreach my $vm_raw (split /,/, $exclude_vms_str) {
            my $vm = $vm_raw;
            $vm =~ s/^\s+//;  # Trim leading whitespace
            $vm =~ s/\s+$//;  # Trim trailing whitespace
            next unless length($vm);

            if (exists $vms_to_process{$vm}) {
                delete $vms_to_process{$vm};
                push @excluded_vms, $vm;
            } else {
                print STDERR "[INFO] Exclusion target '$vm' not in processing set (already excluded or not present)\n" if $verbose;
            }
        }

        if (@excluded_vms && !$quiet) {
            my $count = scalar(@excluded_vms);
            print STDERR "[-] Excluded $count VM(s) via --exclude-vms: " . join(', ', sort @excluded_vms) . "\n";
        }

        # Safety check: ensure at least one VM remains
        unless (%vms_to_process) {
            die "[ERROR] All VMs excluded by --exclude-vms. No VMs remain for analysis.\n";
        }
    }

    if (defined $include_states_selector && lc($include_states_selector) ne 'all') {
        foreach my $vm_name (keys %$states_by_vm) {
            # Filter the list of states for each VM in-place.
            $states_by_vm->{$vm_name} = [ parse_state_selector($include_states_selector, $states_by_vm->{$vm_name}) ];
        }
    }

    # 7. Prepare for and execute the single pass
    my %analysis_data;    # Holds the packed binary data strings
    my %raw_data;         # Holds raw data for clipping
    my %transform_states; # Holds the in-memory state for rolling averages
    # --- Persistent state & raw peak tracker ---
    # This hash holds the calculator state (SMA queue, etc.) across all
    # configuration states for a given VM, ensuring continuity.
    my %persistent_transform_states;
    my %raw_peak_tracker; # Track true raw peak per VM

    # Out-of-Band Storage for Daily Series
    # Structure: $daily_raw_data{$vm}{$transform_key}{$date_str} = packed_binary
    my %daily_raw_data;

    # --- Load or build cache index for date-range optimisation ---
    my $cache_index;
    my $seek_offset = 0;

    if (defined $start_date_str && $start_date_str ne '') {
        $cache_index = load_or_build_cache_index($nmon_dir, $data_cache_file);
    }

    # ==========================================================================
    # SAFETY CHECK: Data Resolution Detection & Window Clamping
    # ==========================================================================
    # Uses ported robust detection to prevent Nyquist violations (e.g. 5m window on 15m data)
    my $detected_interval_secs = detect_sampling_interval($data_cache_file) // 60;

    # Convert to minutes (rounded) for internal logic
    my $detected_interval_mins = sprintf("%.0f", $detected_interval_secs / 60);
    $detected_interval_mins = 1 if $detected_interval_mins < 1;

    # Store for downstream logic (e.g. Growth Projection / Adaptive Horizons)
    $args_ref->{avg_sampling_interval_mins} = $detected_interval_mins;

    # Adjust Manifest Windows based on Resolution
    # This logic converts "Minutes" (CLI request) to "Samples" (Engine requirement)
    # and clamps to 1 sample if the window is physically impossible given the resolution.
    foreach my $key (keys %$manifest) {
        my $entry = $manifest->{$key};
        next unless defined $entry->{window};

        my $req_window_mins = $entry->{window};

        # Calculate effective sample count needed for this window size
        # e.g. Req=5m, Data=1m  -> 5 samples
        # e.g. Req=5m, Data=15m -> 0.33 samples -> Clamps to 1
        my $effective_samples = int($req_window_mins / $detected_interval_mins);

        if ($effective_samples < 1) {
            $effective_samples = 1; # Clamp to disable smoothing
            if ($verbose) {
                 warn "Warning: Data resolution (${detected_interval_mins}m) exceeds requested window (${req_window_mins}m) for '$key'.\n" .
                      "         Clamping window to 1 sample (smoothing disabled).\n";
            }
        }

        # Update the manifest logic to use SAMPLES, which the engine expects
        $entry->{window} = $effective_samples;
    }
    # --- end of safety check ---

    # --- Open data cache ---
    open my $data_fh, '<:raw', $data_cache_file or die "FATAL: Cannot open $data_cache_file: $!";
    my $header = <$data_fh>; # Read header to determine column indices if needed

    # --- Seek to start date if index available ---
    if ($cache_index) {
        $seek_offset = seek_to_start_date($data_fh, $cache_index, $start_date_str);
        if ($seek_offset > 0) {
            my $seek_mb = sprintf("%.1f", $seek_offset / 1_048_576);
            print STDERR "  Seeked to $start_date_str (offset: ${seek_mb}MB)\n" if $verbose;
        }
    }

    # --- Progress Indicator Initialisation ---
    my $record_counter = 0;
    my $progress_interval = 50_000;
    my $show_progress = ($verbose || -t STDERR); # Show progress if verbose or interactive
    printf STDERR "\r  -> Processing data cache ..." if ($show_progress && !$quiet);

    # Figure out total size and calculate actual bytes to process
    my $total_bytes = do {
        my @st = stat($data_fh);
        $st[7] // 0  # size in bytes (0 if not a regular file/unknown)
    };

    # Calculate end offset for accurate progress (if we have index and end date)
    my $end_offset = $total_bytes;
    if ($cache_index && defined $end_date_str && $end_date_str ne '') {
        # Find the next date after end_date in index
        my @dates = sort keys %$cache_index;
        for my $date (@dates) {
            if ($date gt $end_date_str) {
                $end_offset = $cache_index->{$date};
                last;
            }
        }
    }

    my $bytes_to_process = $end_offset - $seek_offset;

    # ==============================================================================
    # EXCLUSION LOOKUP TABLE CONSTRUCTION (Before the main loop)
    # ==============================================================================
    # Extract exclusions from manifest and remove from iteration set
    my $manifest_exclusions = delete $manifest->{_exclusions};

    # Build O(1) lookup tables - MANDATORY for performance
    my %global_exclusion_lookup;
    my %vm_exclusion_lookup;

    if (defined $manifest_exclusions) {
        # Global exclusions: date -> 1
        %global_exclusion_lookup = map { $_ => 1 } @{$manifest_exclusions->{global} // []};

        # VM-specific exclusions: vm -> { date -> 1 }
        foreach my $vm (keys %{$manifest_exclusions->{vm_specific} // {}}) {
            $vm_exclusion_lookup{$vm} = {
                map { $_ => 1 } @{$manifest_exclusions->{vm_specific}{$vm}}
            };
        }

        my $global_count = scalar keys %global_exclusion_lookup;
        my $vm_count = scalar keys %vm_exclusion_lookup;
        print STDERR "  Exclusions loaded: $global_count global date(s), $vm_count VM rule(s)\n" if $verbose;
    }

    while (my $line = <$data_fh>) {
        $record_counter++;

        chomp $line;
        my ($ts_str, $vm, $physc, $runq) = split ',', $line, 4;

        # early exit for non-numeric/corrupt data
        next unless (defined $physc && defined $runq && looks_like_number($physc) && looks_like_number($runq));

        # parse timee-stamp
        my $tp;
        eval { $tp = Time::Piece->strptime($ts_str, "%Y-%m-%d %H:%M:%S"); };
        next if $@; # Skip malformed/unparseable timestamps

        # Global date filters
        if (defined $end_date_str) {
            # Use string compare on date, it's faster than object methods
            my $line_date = substr($ts_str, 0, 10);
            last if ($line_date gt $end_date_str);
        }
        if (defined $start_date_str) {
            my $line_date = substr($ts_str, 0, 10);
            next if ($line_date lt $start_date_str);
        }

        # --- EXCLUSION CHECK (O(1) hash lookup) ---
        # Check global exclusions first (most common case)
        my $line_date_excl = substr($ts_str, 0, 10);
        next if exists $global_exclusion_lookup{$line_date_excl};
        # Check VM-specific exclusions
        next if exists $vm_exclusion_lookup{$vm} && exists $vm_exclusion_lookup{$vm}{$line_date_excl};

        if ($show_progress && ($record_counter % $progress_interval == 0)) {
            my $pct = 0;
            if ($bytes_to_process > 0) {
                my $pos = tell($data_fh);
                my $bytes_processed = $pos - $seek_offset;
                $pct = ($bytes_processed / $bytes_to_process) * 100.0;
                # Clamp to 100% in case we overshoot
                $pct = 100.0 if $pct > 100.0;
            }

            if ($show_progress && !$quiet) {
                if (defined $start_date_str && defined $end_date_str) {
                    printf STDERR "\r  ↳  Processing data cache ($start_date_str - $end_date_str): %-19s (%.1f%%)", $ts_str, $pct;
                } elsif (defined $start_date_str) {
                    printf STDERR "\r  ↳  Processing data cache (from $start_date_str): %-19s (%.1f%%)", $ts_str, $pct;
                } elsif (defined $end_date_str) {
                    printf STDERR "\r  ↳  Processing data cache (to $end_date_str): %-19s (%.1f%%)", $ts_str, $pct;
                } else {
                    printf STDERR "\r  ↳  Processing data cache: %-19s (%.1f%%)", $ts_str, $pct;
                }
            }
        }

        process_data_point($tp, $vm, $physc, $runq, $manifest, $states_by_vm, \%analysis_data, \%raw_data, \%transform_states, \%persistent_transform_states, \%daily_raw_data, {
            vms_to_process            => \%vms_to_process,
            start_date_str            => $start_date_str,
            end_date_str              => $end_date_str,
            smt_fallback              => $smt_value,
            raw_peak_tracker          => \%raw_peak_tracker,
            enable_clipping_detection => $enable_clipping_detection,

            # --- NEW CONTEXT FLAGS ---
            enable_windowed_decay     => $enable_windowed_decay,
            time_windows_aref         => $time_windows_aref, # Pass the generated windows
        });
    }
    close $data_fh;

    # Print a final newline to clear the progress line
    if ($show_progress && !$quiet) {
        if (defined $start_date_str && defined $end_date_str) {
            printf STDERR "\r✓ Processed data cache ($start_date_str - $end_date_str): %-40s\n", "completed (100.0%)";
        } elsif (defined $start_date_str) {
            printf STDERR "\r✓ Processed data cache (from $start_date_str): %-40s\n", "completed (100.0%)";
        } elsif (defined $end_date_str) {
            printf STDERR "\r✓ Processed data cache (to $end_date_str): %-40s\n", "completed (100.0%)";
        } else {
            printf STDERR "\r✓ Processed data cache: %-40s\n", "completed (100.0%)";
        }
    }

    # 8. Finalisation (calculates per-state/per-transform metrics)
    my $final_metrics = finalize_results(
        \%analysis_data,
        \%raw_data,
        $manifest,
        \%transform_states,
        $states_by_vm,
        { enable_clipping_detection => $enable_clipping_detection }
    );
    # --- Finalise Daily Metrics separately ---
    my $daily_final_metrics = finalize_daily_metrics(\%daily_raw_data, $manifest, $args_ref);

    # 9. Determine if this is an aggregated run and choose the correct output path
    my @output_objects;
    my $is_aggregated_run = $args_ref->{enable_windowed_decay} || $args_ref->{decay_over_states};

    if ($is_aggregated_run) {
        # --- PATH A: AGGREGATED RUNS (Decay / Growth) ---

        my $aggregated_results_all_vms = {};
        my $growth_results_all_vms = {};

        # Determine the analysis reference date (used by both models)
        my $analysis_ref_obj;
        if ($args_ref->{analysis_reference_date}) {
            eval { $analysis_ref_obj = Time::Piece->strptime($args_ref->{analysis_reference_date}, "%Y-%m-%d"); };
        } else {
            # Fallback: find the latest date from all states
            my $latest_epoch = 0;
            foreach my $vm_states (values %$states_by_vm) {
                foreach my $state (@$vm_states) {
                    $latest_epoch = $state->{end_epoch} if $state->{end_epoch} > $latest_epoch;
                }
            }
            $analysis_ref_obj = Time::Piece->new($latest_epoch);
        }
        $analysis_ref_obj = $analysis_ref_obj->truncate(to => 'day');


        # Iterate over each VM to create its specific context and run aggregations
        foreach my $vm_name (keys %$final_metrics) {

            # --- FIX: Pass raw context arrays, not an iterator ---
            # Pass the model selector flag and the two potential data contexts.
            # The consumer functions will create their own iterators as needed.

            # 2. Calculate recency-weighted baselines (WITHOUT growth)
            my $aggregated_results_for_vm = aggregate_decay_metrics(
                $vm_name,
                $final_metrics,
                $manifest,
                $enable_windowed_decay,      # Pass model selector
                $time_windows_aref,          # Pass window context
                $states_by_vm->{$vm_name},   # Pass state context
                $analysis_ref_obj,
                $args_ref
            );
            $aggregated_results_all_vms->{$vm_name} = $aggregated_results_for_vm;

            # 3. Calculate per-profile growth adjustments
            if ($args_ref->{enable_growth_prediction}) {
                $growth_results_all_vms->{$vm_name} = calculate_per_profile_growth_adjustments(
                    $vm_name,
                    $aggregated_results_for_vm, # Pass this VM's baseline
                    \%analysis_data,
                    $manifest,
                    $enable_windowed_decay,      # Pass model selector
                    $time_windows_aref,          # Pass window context
                    $states_by_vm->{$vm_name},   # Pass state context
                    $args_ref,
                    $analysis_ref_obj
                );
            }
        } # End foreach $vm_name

        # 4. Assemble the final JSON
        foreach my $vm_name (sort keys %$aggregated_results_all_vms) {
            my $output_obj = assemble_aggregated_output(
                $vm_name,
                $aggregated_results_all_vms->{$vm_name},
                $growth_results_all_vms->{$vm_name} || {}, # Pass this VM's growth
                $states_by_vm,
                $args_ref,
                $final_metrics,
                $manifest,
                \%raw_peak_tracker, # Pass raw peak tracker
                $daily_final_metrics
            );
            push @output_objects, $output_obj if defined $output_obj;
        }
    } else {
        # --- PATH B: STANDARD PER-STATE RUNS ---
        @output_objects = @{ assemble_output(
            $final_metrics,
            $manifest,
            $states_by_vm,
            \%transform_states,
            \%raw_peak_tracker, # Note: ensure you are passing raw_peak_tracker if not already
            $args_ref,
            $daily_final_metrics
        ) };
    }

    # 10. Save new result to L2 Cache and print to STDOUT
    my $lock_file = File::Spec->catfile($nmon_dir, $CACHE_LOCK_FILE);
    save_result_to_cache($canonical_key, \@output_objects, $results_cache_path, $lock_file) if @output_objects;

    my $json_encoder = JSON->new->utf8;
    foreach my $obj (@output_objects) {
        print STDOUT $json_encoder->encode($obj) . "\n";
    }
}

# ==============================================================================
# SUBROUTINE: process_data_point
# PURPOSE:    Processes a single line of data from the L1 cache. It applies
#             all global filters (VM, date) and per-transform time filters,
#             finds the correct configuration state, and updates all relevant
#             transform streams.
# ==============================================================================
sub process_data_point {
    my ($tp, $vm, $physc, $runq, $manifest, $states_by_vm, $analysis_data, $raw_data, $transform_states, $persistent_transform_states, $daily_raw_data_ref, $filters) = @_;

    # --- FIX: TRACK TRUE RAW PEAK ---
    # For every valid data point, before any filtering, update the absolute max
    # raw PhysC value seen for this VM. This is the new source for the 'Peak' value.
    my $raw_peak_tracker = $filters->{raw_peak_tracker};
    $raw_peak_tracker->{$vm} = $physc if (looks_like_number($physc) && $physc > ($raw_peak_tracker->{$vm} // 0));

    # 1. Apply global VM filter passed from the orchestrator.
    return unless (exists $filters->{vms_to_process}{$vm} && exists $states_by_vm->{$vm});

    # 2. Apply global date filters -> This is already done in the caller loop.

    # 3. Find the correct bin for this data point based on the analysis model
    my $bin_key;
    my $state_obj; # State object is still needed for metadata (SMT, max_cpu)
    my $ts_epoch = $tp->epoch;

    if ($filters->{enable_windowed_decay}) {
        # --- Windowed-Decay Model ---
        # Bin key is the Time Window ID (e.g., "2025_W42")
        # Use the fast binary search helper.
        $bin_key = _binary_search_window_index($ts_epoch, $filters->{time_windows_aref});

        # We still need the state_obj for SMT/config, so find it.
        # This is a linear scan, but only runs *after* we know the timestamp
        # is in a valid time window.
        if (defined $bin_key) {
             for (my $i=0; $i < @{$states_by_vm->{$vm}}; $i++) {
                my $s = $states_by_vm->{$vm}[$i];
                if ($ts_epoch >= $s->{start_time}->epoch && $ts_epoch <= $s->{end_time}->epoch) {
                    $state_obj = $s;
                    last;
                }
            }
        }
    } else {
        # --- Hybrid-Decay or Standard Model ---
        # Bin key is the numeric State ID (0, 1, 2...)
        for (my $i=0; $i < @{$states_by_vm->{$vm}}; $i++) {
            my $s = $states_by_vm->{$vm}[$i];
            if ($ts_epoch >= $s->{start_time}->epoch && $ts_epoch <= $s->{end_time}->epoch) {
                $state_obj = $s;
                $bin_key   = $i; # Use the zero-based index as the key
                last;
            }
        }
    }

    # If no valid bin (state or window) was found, or if state metadata
    # is missing, we cannot process this data point.
    return unless (defined $bin_key && defined $state_obj);

    # 4. Iterate through all required transforms for this data point.
    foreach my $key (keys %$manifest) {
        my $entry = $manifest->{$key};

        # --- COMPLETE PER-TRANSFORM TIME FILTER LOGIC ---
        # This block applies the specific time filters for THIS transform.
        my $line_time = substr($tp->hms(':'), 0, 5);

        # 4a. Check for --no-weekends filter.
        if ($entry->{no_weekends}) {
            my $day_of_week = $tp->day_of_week; # Sunday=1, Saturday=7 in Time::Piece
            next if ($day_of_week == 1 || $day_of_week == 7);
        }

        # 4b. Check for time-of-day filters (online/batch).
        my $time_filter_type = $entry->{time_filter};
        if ($time_filter_type ne 'none') {
            my $include_line = 0;
            if ($time_filter_type eq 'online') {
                $include_line = 1 if ($line_time ge '08:00' && $line_time lt '17:00');
            } elsif ($time_filter_type eq 'batch') {
                $include_line = 1 if ($line_time ge '18:00' || $line_time lt '06:00');
            }
            # Skip to the next transform if this data point is filtered out.
            next unless $include_line;
        }

        # --- Data processing continues only if the data point was not filtered out ---
        # This tracks the PEAK of the rolling average, which is bin-specific.
        my $t_state = $transform_states->{$vm}{$bin_key}{$key} //= initialise_transform_state($entry);

        # --- Use the PERSISTENT state for the rolling average calculation ---
        my $p_state = $persistent_transform_states->{$vm}{$key} //= initialise_transform_state($entry);

        # 4c. Select the correct metric and calculate NormRunQ if needed.
        my $smt = $state_obj->{metadata}{smt} // $filters->{smt_fallback};
        my $value_to_process;
        if ($entry->{metric} eq 'PhysC') {
            $value_to_process = $physc;
        } elsif ($entry->{metric} eq 'AbsRunQ') {
            $value_to_process = $runq;
        } elsif ($entry->{metric} eq 'NormRunQ') {
            $value_to_process = (defined $physc && $physc > $ACTIVE_PHYSC_THRESHOLD && $smt > 0) ? ($runq / ($physc * $smt)) : undef;
        }

        # 4d. Calculate the rolling average for the selected metric.
        my $rolling_avg = update_transform_state($value_to_process, $p_state);


        # 4e. Store the rolling average in the packed array for percentile calculation.
        if (defined $rolling_avg) {
            update_packed_array($analysis_data, $vm, $bin_key, $key, $rolling_avg, $t_state);

            # --- NEW: Out-of-Band Daily Bucketing ---
            # We store this in a completely separate structure to avoid polluting the main analysis hash.
            if ($entry->{metric} eq 'PhysC') {
                my $date_key = $tp->ymd;
                # Store directly: VM -> TransformKey -> Date -> PackedData
                $daily_raw_data_ref->{$vm}{$key}{$date_key} //= '';
                $daily_raw_data_ref->{$vm}{$key}{$date_key} .= pack('f<', $rolling_avg);
            }
        }

        # 4f. Conditionally gather raw data if clipping detection is enabled.
        if ($filters->{enable_clipping_detection} && $entry->{metric} eq 'PhysC') {
            # Use a different hash to store raw data streams.
            update_packed_array($raw_data, $vm, $bin_key, $key."_raw_physc", $physc);
            update_packed_array($raw_data, $vm, $bin_key, $key."_raw_runq", $runq);
        }
    }
}

sub initialise_transform_state {
    my ($manifest_entry) = @_;
    my $state = {
        method => $manifest_entry->{method},
        window => $manifest_entry->{window},
    };
    if ($state->{method} eq 'SMA') {
        $state->{queue} = [];
    } elsif ($state->{method} eq 'EMA') {
        $state->{prev_value} = undef;
        $state->{alpha} = $EMA_ALPHAS{$manifest_entry->{decay}} // $EMA_ALPHAS{'medium'};
        $state->{warmup_queue} = []; # For initial EMA value
    }
    $state->{peak_value} = undef;
    return $state;
}

sub update_transform_state {
    my ($value, $state_obj) = @_;
    return undef unless defined $value;

    if ($state_obj->{method} eq 'SMA') {
        push @{$state_obj->{queue}}, $value;
        shift @{$state_obj->{queue}} while @{$state_obj->{queue}} > $state_obj->{window};
        return (scalar @{$state_obj->{queue}} == $state_obj->{window})
            ? (sum0(@{$state_obj->{queue}}) / $state_obj->{window})
            : undef;
    } elsif ($state_obj->{method} eq 'EMA') {
        if (!defined $state_obj->{prev_value}) {
            # Use SMA for the first window to seed the EMA
            push @{$state_obj->{warmup_queue}}, $value;
            if (@{$state_obj->{warmup_queue}} == $state_obj->{window}) {
                $state_obj->{prev_value} = sum0(@{$state_obj->{warmup_queue}}) / $state_obj->{window};
                return $state_obj->{prev_value};
            }
            return undef;
        }
        $state_obj->{prev_value} = ($value * $state_obj->{alpha}) + ($state_obj->{prev_value} * (1 - $state_obj->{alpha}));
        return $state_obj->{prev_value};
    }
    return $value; # Default for 'none' method
}

sub update_packed_array {
    my ($hash_ref, $vm, $state_id, $transform_key, $value, $transform_state_obj) = @_;
    unless (defined $value && looks_like_number($value)) {
        warn "Skipping invalid non-numeric value for $vm/$state_id/$transform_key: '$value'\n";
        return;
    }

    # Track the peak value
    if (!defined $transform_state_obj->{peak_value} || $value > $transform_state_obj->{peak_value}) {
        $transform_state_obj->{peak_value} = $value;
    }

    $hash_ref->{$vm}{$state_id}{$transform_key} //= '';
    $hash_ref->{$vm}{$state_id}{$transform_key} .= pack('f<', $value);
}

# Processes %daily_raw_data for DailyProfileSeries
sub finalize_daily_metrics {
    my ($daily_raw_data, $manifest, $args_ref) = @_;
    my %daily_final;

    my $round_increment = (defined $args_ref->{round_arg}) ? $args_ref->{round_arg} : $DEFAULT_ROUND_INCREMENT;
    my $rounding_method = (defined $args_ref->{round_arg}) ? 'standard' : 'none'; # Simplified for brevity

    foreach my $vm (keys %$daily_raw_data) {
        foreach my $transform_key (keys %{$daily_raw_data->{$vm}}) {
            my $transform_info = $manifest->{$transform_key};
            next unless $transform_info; # Should always exist, but safe to check

            foreach my $date_str (keys %{$daily_raw_data->{$vm}{$transform_key}}) {
                my $packed = $daily_raw_data->{$vm}{$transform_key}{$date_str};
                my @values = unpack('f<*', $packed);
                next unless @values;

                # Process each profile attached to this transform
                foreach my $profile_name (keys %{$transform_info->{profiles}}) {
                    my $directives = $transform_info->{profiles}{$profile_name};

                    # Use the first configured percentile
                    if (defined $directives->{percentiles} && @{$directives->{percentiles}}) {
                        my $p_val = $directives->{percentiles}->[0];

                        # Calculate simple percentile for the day
                        # (Re-using existing calculation logic)
                        my $val = calculate_percentile(\@values, $p_val);

                        if (defined $val) {
                             # Apply rounding
                             $val = apply_rounding($val, $round_increment, $rounding_method);
                             $daily_final{$vm}{$profile_name}{$date_str} = $val;
                        }
                    }
                }
            }
        }
    }
    return \%daily_final;
}

# ==============================================================================
# SUBROUTINE: finalize_results
# PURPOSE:    Iterates through all processed data, calculates final percentile
#             metrics from the packed binary arrays, and conditionally runs
#             additional analysis like clipping detection.
# ==============================================================================
sub finalize_results {
    my ($analysis_data, $raw_data, $manifest, $transform_states, $states_by_vm, $args_ref) = @_;
    my %final_metrics;

    my $enable_clipping_detection = $args_ref->{enable_clipping_detection};

    foreach my $vm (keys %$analysis_data) {
        foreach my $state_id (keys %{$analysis_data->{$vm}}) {
            foreach my $key (keys %{$analysis_data->{$vm}{$state_id}}) {

                my $packed_data = $analysis_data->{$vm}{$state_id}{$key};
                my $transform_info = $manifest->{$key};

                my @values = unpack('f<*', $analysis_data->{$vm}{$state_id}{$key});
                next unless @values; # Skip if there's no data for this transform.

                # Apply the --filter-above-perc logic BEFORE percentile calculation
                my $filter_perc = $transform_info->{filter_perc};
                my @final_values_for_percentile;

                my @sorted_values = sort { $a <=> $b } @values;

                if (defined $filter_perc && $filter_perc > 0) {
                    my $threshold = calculate_percentile(\@sorted_values, $filter_perc);
                    if (defined $threshold) {
                        # Filter the already sorted array to include values at or above the threshold.
                        @final_values_for_percentile = grep { $_ >= ($threshold - $FLOAT_EPSILON) } @sorted_values;
                    } else {
                        @final_values_for_percentile = @sorted_values;
                    }
                } else {
                    @final_values_for_percentile = @sorted_values; # Use all values if no filter.
                }

                # Get all unique percentiles needed for this transform
                my %percentiles_needed;
                foreach my $prof (values %{$transform_info->{profiles}}) {
                    $percentiles_needed{$_} = 1 for @{$prof->{percentiles}};
                }
                my @p_list = sort { $a <=> $b } keys %percentiles_needed;

                my $p_results = calculate_percentiles_from_packed(\@final_values_for_percentile, \@p_list);

                foreach my $p (keys %$p_results) {
                    $final_metrics{$vm}{$state_id}{$key}{"P$p"} = $p_results->{$p};
                }

                # --- USE of enable_clipping_detection flag ---
                # After calculating standard percentiles, run clipping detection if enabled.
                if ($enable_clipping_detection && $transform_info->{metric} eq 'PhysC') {
                    my $raw_physc_packed = $raw_data->{$vm}{$state_id}{$key."_raw_physc"} // '';
                    my $raw_runq_packed  = $raw_data->{$vm}{$state_id}{$key."_raw_runq"} // '';

                    my @raw_physc = unpack('f<*', $raw_physc_packed);
                    my @raw_runq  = unpack('f<*', $raw_runq_packed);

                    # Get max_capacity and SMT from the state object for this calculation
                    my $max_cap = $states_by_vm->{$vm}[$state_id]{metadata}{max_cpu};
                    my $smt_val = $states_by_vm->{$vm}[$state_id]{metadata}{smt};

                    my $clipping_results = _calculate_clipping_metrics(\@raw_physc, \@raw_runq, $max_cap, $smt_val);

                    # Merge clipping results into the final metrics hash
                    $final_metrics{$vm}{$state_id}{$key}{Clipping} = $clipping_results;
                }
            }
        }
    }
    return \%final_metrics;
}

sub calculate_percentiles_from_packed {
    my ($values_ref, $percentiles_ref) = @_;

    # This function receives a pre-sorted array reference.
    my @values = @$values_ref;
    my $n = scalar @values;

    return {} if $n == 0;

    my %results;
    foreach my $p (@$percentiles_ref) {
        # Standard linear interpolation for percentile calculation.
        my $rank = ($p / 100) * ($n - 1);
        my $k = int($rank);
        my $d = $rank - $k;

        if ($k + 1 < $n) {
            $results{$p} = $values[$k] + $d * ($values[$k+1] - $values[$k]);
        } else {
            # Handle edge case where rank is at the very end.
            $results{$p} = $values[$k];
        }
    }

    return \%results;
}

# ==============================================================================
# SUBROUTINE: assemble_output
# PURPOSE:    Assembles the final JSON output. It correctly handles both standard
#             (per-state) and aggregated (decay/growth) result structures,
#             ensuring the output format is correct for the analysis model used.
# ==============================================================================
sub assemble_output {
    my ($final_metrics_href, $manifest_href, $states_by_vm_href, $transform_states_href, $raw_peak_tracker_ref, $args_ref, $daily_metrics_href) = @_;

    my @output_objects;

    my $rounding_method = 'none';
    my $round_increment;
    if (defined $args_ref->{round_arg}) {
        $rounding_method = 'standard';
        $round_increment = (length $args_ref->{round_arg} && $args_ref->{round_arg} =~ /^[0-9.]*$/) ? $args_ref->{round_arg} : $DEFAULT_ROUND_INCREMENT;
    } elsif (defined $args_ref->{roundup_arg}) {
        $rounding_method = 'up';
        $round_increment = (length $args_ref->{roundup_arg} && $args_ref->{roundup_arg} =~ /^[0-9.]*$/) ? $args_ref->{roundup_arg} : $DEFAULT_ROUND_INCREMENT;
    }

    foreach my $vm_name (sort keys %$final_metrics_href) {
        # Determine if the result for this VM is aggregated or per-state.
        # An aggregated result has profile names as the top-level keys under the VM.
        # A per-state result has numeric state IDs as the keys.
        my $is_aggregated_run = !grep { /^\d+$/ } keys %{$final_metrics_href->{$vm_name}};

        if ($is_aggregated_run) {
            # --- PATH A: Assemble a single, aggregated JSON object for a decay/growth run ---
            my $last_state_obj = $states_by_vm_href->{$vm_name}[-1];
            next unless $last_state_obj;

            my $last_state_id = $last_state_obj->{state_id} - 1; # Convert to zero-based index

            my $output_obj = {
                vmName       => $vm_name,
                analysisType => 'aggregated',
                profileLabel => $profile_label,
                state        => { stateCount => scalar(@{$states_by_vm_href->{$vm_name}}) },
                metadata     => $last_state_obj->{metadata},
                metrics      => { physc => {}, runq => { normalized => {}, absolute => {} }, growth => {} },
                debug        => { growthRationale => {} }
            };

            # Extract Peak from transform_states for the last state
            my ($peak_transform_key) = grep {
                $manifest_href->{$_}{metric} eq 'PhysC' &&
                (
                    exists $manifest_href->{$_}{profiles}{$MANDATORY_PEAK_PROFILE_FOR_HINT} ||
                    exists $manifest_href->{$_}{profiles}{$LEGACY_MANDATORY_PEAK_PROFILE_FOR_HINT}
                )
            } keys %$manifest_href;

            if ($peak_transform_key &&
                exists $transform_states_href->{$vm_name} &&
                exists $transform_states_href->{$vm_name}{$last_state_id} &&
                exists $transform_states_href->{$vm_name}{$last_state_id}{$peak_transform_key}{peak_value}) {
                $output_obj->{metadata}{peakPhyscFromLatestState} =
                    $transform_states_href->{$vm_name}{$last_state_id}{$peak_transform_key}{peak_value};
            }

            # Extract RunQ metrics from the last state (NOT from aggregated profiles)
            my ($norm_rq_key) = grep {
                $manifest_href->{$_}{metric} eq 'NormRunQ' &&
                (
                    exists $manifest_href->{$_}{profiles}{$MANDATORY_PEAK_PROFILE_FOR_HINT} ||
                    exists $manifest_href->{$_}{profiles}{$LEGACY_MANDATORY_PEAK_PROFILE_FOR_HINT}
                )
            } keys %$manifest_href;

            my ($abs_rq_key) = grep {
                $manifest_href->{$_}{metric} eq 'AbsRunQ' &&
                (
                    exists $manifest_href->{$_}{profiles}{$MANDATORY_PEAK_PROFILE_FOR_HINT} ||
                    exists $manifest_href->{$_}{profiles}{$LEGACY_MANDATORY_PEAK_PROFILE_FOR_HINT}
                )
            } keys %$manifest_href;

            if ($norm_rq_key &&
                exists $final_metrics_href->{$vm_name}{$last_state_id}{$norm_rq_key}) {
                my $norm_metrics = $final_metrics_href->{$vm_name}{$last_state_id}{$norm_rq_key};
                foreach my $perc_key (keys %$norm_metrics) {
                    if ($perc_key =~ /^P(\d+\.?\d*)$/) {
                        $output_obj->{metrics}{runq}{normalized}{$MANDATORY_PEAK_PROFILE_FOR_HINT}{$perc_key} =
                            apply_rounding($norm_metrics->{$perc_key}, $round_increment, $rounding_method);
                    }
                }
            }

            if ($abs_rq_key &&
                exists $final_metrics_href->{$vm_name}{$last_state_id}{$abs_rq_key}) {
                my $abs_metrics = $final_metrics_href->{$vm_name}{$last_state_id}{$abs_rq_key};
                foreach my $perc_key (keys %$abs_metrics) {
                    if ($perc_key =~ /^P(\d+\.?\d*)$/) {
                        $output_obj->{metrics}{runq}{absolute}{$MANDATORY_PEAK_PROFILE_FOR_HINT}{$perc_key} =
                            apply_rounding($abs_metrics->{$perc_key}, $round_increment, $rounding_method);

                        # Real W1 emission: for the peak helper profile (window=1), expose an explicit
                        # "P99W1" key so downstream consumers can bind to the correct statistic rather
                        # than whatever profile happened to populate AbsRunQ_P99 first.
                        if ($perc_key eq 'P99') {
                            $output_obj->{metrics}{runq}{absolute}{$MANDATORY_PEAK_PROFILE_FOR_HINT}{'P99W1'} =
                                $output_obj->{metrics}{runq}{absolute}{$MANDATORY_PEAK_PROFILE_FOR_HINT}{$perc_key};
                        }
                    }
                }
            }

            # Process PhysC profiles from aggregated results
            foreach my $profile_name (keys %{$final_metrics_href->{$vm_name}}) {
                next if $profile_name =~ /^\d+$/; # Skip numeric (state ID) keys
                next unless ref($final_metrics_href->{$vm_name}{$profile_name}) eq 'HASH';

                my $profile_metrics = $final_metrics_href->{$vm_name}{$profile_name};
                my $final_value = $profile_metrics->{aggregated_value};
                next unless defined $final_value;

                $output_obj->{metrics}{physc}{$profile_name}{FinalValue} =
                    apply_rounding($final_value, $round_increment, $rounding_method);

                if (exists $profile_metrics->{GrowthAdj}) {
                    $output_obj->{metrics}{growth}{adjustment} = $profile_metrics->{GrowthAdj};
                }
                if (exists $profile_metrics->{GrowthRationale}) {
                    $output_obj->{debug}{growthRationale} = $profile_metrics->{GrowthRationale};
                }
            }

            push @output_objects, $output_obj;
        } else {
            # --- PATH B: Assemble a standard, per-state JSON object ---
            foreach my $state_id (sort { $a <=> $b } keys %{$final_metrics_href->{$vm_name}}) {
                my $state_obj = $states_by_vm_href->{$vm_name}[$state_id];
                next unless $state_obj;

                my $output_obj = {
                    vmName       => $vm_name,
                    analysisType => 'state_based',
                    state        => {
                        id           => $state_id + 1,
                        durationDays => $state_obj->{duration_days} || 0,
                        startTimeISO => $state_obj->{start_time}->datetime . 'Z',
                        endTimeISO   => $state_obj->{end_time}->datetime . 'Z',
                    },
                    metadata     => $state_obj->{metadata},
                    metrics      => { physc => {}, runq => { normalized => {}, absolute => {} } },
                };

                foreach my $transform_key (keys %{$final_metrics_href->{$vm_name}{$state_id}}) {
                    my $metric_results = $final_metrics_href->{$vm_name}{$state_id}{$transform_key};
                    my $transform_info = $manifest_href->{$transform_key};
                    my $profiles_for_transform = $transform_info->{profiles};
                    my $metric_type = $transform_info->{metric};

                    foreach my $profile_name (keys %$profiles_for_transform) {
                        my $directives = $profiles_for_transform->{$profile_name};

                        foreach my $p_val (@{ $directives->{percentiles} }) {
                            my $p_key = "P" . clean_perc_label($p_val);
                            my $value = $metric_results->{$p_key};
                            my $rounded_value = apply_rounding($value, $round_increment, $rounding_method);

                            if ($metric_type eq 'PhysC') {
                                $output_obj->{metrics}{physc}{$profile_name}{$p_key} = $rounded_value;
                            } elsif ($metric_type eq 'NormRunQ') {
                                $output_obj->{metrics}{runq}{normalized}{$profile_name}{$p_key} = $rounded_value;
                            } elsif ($metric_type eq 'AbsRunQ') {
                                $output_obj->{metrics}{runq}{absolute}{$profile_name}{$p_key} = $rounded_value;
                            }
                        }

                        if ($metric_type eq 'PhysC' && $directives->{calculate_peak}) {
                            my $peak_value = $transform_states_href->{$vm_name}{$state_id}{$transform_key}{peak_value};
                            if (defined $peak_value) {
                                $output_obj->{metrics}{physc}{$profile_name}{'Peak'} = apply_rounding($peak_value, $round_increment, $rounding_method);
                            }
                        }

                        if ($metric_type eq 'PhysC' && exists $metric_results->{Clipping}) {
                            # The history priming logic expects this exact block name.
                            $output_obj->{metrics}{physc}{$profile_name}{ClippingInfo} = $metric_results->{Clipping};
                        }
                    }
                }
                push @output_objects, $output_obj;
            }
            # --- Inject the TRUE raw peak into the per-state output ---
            # The true peak is a VM-level metric, not a state-level one. For consistency,
            # we add it to the metadata of the LAST state reported for the VM.
            if (@output_objects && exists $raw_peak_tracker_ref->{$vm_name}) {
                $output_objects[-1]->{metadata}{peakPhyscFromLatestState} = apply_rounding($raw_peak_tracker_ref->{$vm_name}, $round_increment, $rounding_method);
            }

            # --- NEW: Inject Daily Profile Series into the latest state ---
            # Since standard runs break data into states, we attach the VM-wide daily series
            # to the final state object for the VM. This ensures it is accessible to
            # nfit-profile without duplicating it across every state object.
            if (@output_objects && defined $daily_metrics_href && exists $daily_metrics_href->{$vm_name}) {
                my $last_obj = $output_objects[-1];
                # Safety check to ensure we are modifying the correct VM object
                if ($last_obj->{vmName} eq $vm_name) {
                    foreach my $profile_name (keys %{$daily_metrics_href->{$vm_name}}) {
                        # Only inject if we have metrics for this profile in the final object
                        # (Avoids autovivification of empty profile keys)
                        if (exists $last_obj->{metrics}{physc}{$profile_name}) {
                            $last_obj->{metrics}{physc}{$profile_name}{DailySeries} = $daily_metrics_href->{$vm_name}{$profile_name};
                        }
                    }
                }
            }
        }
    }
    return \@output_objects;
}

# ==============================================================================
# SUBROUTINE: generate_manifest_key
# PURPOSE:    Creates a unique, reproducible key for L2 caching by hashing
#             the contents of the manifest and all relevant global arguments
#             that affect the final calculation.
# ==============================================================================
sub generate_manifest_key {
    my ($args_href, $manifest_href) = @_;

    # Create a canonical data structure containing ONLY the elements
    # that influence the final numerical result.
    my $data_for_key = {
        manifest => $manifest_href,
        args => {
            # --- Data Scoping & Filtering Flags ---
            target_vm_name          => $args_href->{target_vm_name},
            exclude_vms_str         => $args_href->{exclude_vms_str},
            start_date_str          => $args_href->{start_date_str},
            end_date_str            => $args_href->{end_date_str},
            include_states_selector => $args_href->{include_states_selector},

            # --- Global Analysis Model Flags & Tunables ---
            smt_value                    => $args_href->{smt_value},
            runq_avg_method_str          => $args_href->{runq_avg_method_str},
            enable_windowed_decay        => $args_href->{enable_windowed_decay},
            decay_over_states_flag       => $args_href->{decay_over_states_flag},
            process_window_unit_str      => $args_href->{process_window_unit_str},
            process_window_size_val      => $args_href->{process_window_size_val},
            decay_half_life_days_val     => $args_href->{decay_half_life_days_val},
            analysis_reference_date_str  => $args_href->{analysis_reference_date_str},
            enable_growth_prediction     => $args_href->{enable_growth_prediction},
            growth_projection_days       => $args_href->{growth_projection_days},
            max_growth_inflation_percent => $args_href->{max_growth_inflation_percent},
            enable_clipping_detection    => $args_href->{enable_clipping_detection},

            # --- Output Formatting Flags ---
            round_arg                    => $args_href->{round_arg},
            roundup_arg                  => $args_href->{roundup_arg},
        }
    };


    # Use canonical JSON encoding (instead of Storable::freeze, as it ensures deterministic key ordering)
    my $json_encoder = JSON->new->canonical;
    my $canonical_string = $json_encoder->encode($data_for_key);

    # Return a SHA-256 hash of the string for a clean, fixed-length key.
    return sha256_hex($canonical_string);
}

# ==============================================================================
# SUBROUTINE: _create_bin_iterator
# PURPOSE:    Creates a bin-agnostic iterator (a coderef) that yields
#             bin metadata in chronological order, regardless of whether
#             the bins are time-windows or configuration states.
#
# ARGUMENTS:
#   1. $model_is_windowed (boolean): True if in --enable-windowed-decay mode.
#   2. $window_context_aref (array ref): The @time_windows array.
#   3. $state_context_aref (array ref): The $states_by_vm->{$vm} array.
#
# RETURNS:
#   - A coderef. When called, it returns the next bin's metadata hash:
#     { id => "key", start_obj => Time::Piece, end_obj => Time::Piece }
#     or undef if iteration is complete.
# ==============================================================================
sub _create_bin_iterator {
    my ($model_is_windowed, $window_context_aref, $state_context_aref) = @_;

    my $index = 0;
    my $context_aref;
    my $is_state_based;

    if ($model_is_windowed) {
        # --- Windowed-Decay Model ---
        # The context is the array of time windows.
        $context_aref = $window_context_aref;
        $is_state_based = 0;
    } else {
        # --- Hybrid-Decay or Standard Model ---
        # The context is the array of configuration states.
        $context_aref = $state_context_aref;
        $is_state_based = 1;
    }

    # Return an iterator (a closure)
    return sub {
        return undef if $index > $#{$context_aref};

        my $bin_data = $context_aref->[$index];
        $index++;

        if ($is_state_based) {
            # Adapt the state object to the iterator's interface
            return {
                id        => $bin_data->{state_id} - 1, # Use 0-based index as key
                start_obj => $bin_data->{start_time},
                end_obj   => $bin_data->{end_time}
            };
        } else {
            # The window object already matches the interface (id, start_obj, end_obj)
            # Note: window end_obj is already exclusive, which is fine
            return $bin_data;
        }
    };
}

# Helper for hash references (with safe fallback)
sub _safe_get {
    my ($href, $key, $fallback) = @_;
    return (ref($href) eq 'HASH' && exists $href->{$key}) ? $href->{$key} : $fallback;
}

sub _safe_dig {
    my ($href, @path) = @_;
    my $cur = $href;
    for my $k (@path) {
        return undef unless ref($cur) eq 'HASH' && exists $cur->{$k};
        $cur = $cur->{$k};
    }
    return $cur;
}

# ==============================================================================
# SUBROUTINE: assemble_aggregated_output
# PURPOSE:    Assembles the final JSON for a single VM from fully aggregated results.
#             This is the dedicated output path for decay/growth models.
# ==============================================================================
sub assemble_aggregated_output {
    my ($vm_name, $aggregated_metrics_href, $growth_results_href, $states_by_vm_href, $args_ref, $final_metrics_href, $manifest_href, $raw_peak_tracker_ref, $daily_metrics_href) = @_;

    my $rounding_method = 'none';
    my $round_increment;
    if (defined $args_ref->{round_arg}) {
        $rounding_method = 'standard';
        $round_increment = (length $args_ref->{round_arg} && $args_ref->{round_arg} =~ /^[0-9.]*$/) ? $args_ref->{round_arg} : $DEFAULT_ROUND_INCREMENT;
    } elsif (defined $args_ref->{roundup_arg}) {
        $rounding_method = 'up';
        $round_increment = (length $args_ref->{roundup_arg} && $args_ref->{roundup_arg} =~ /^[0-9.]*$/) ? $args_ref->{roundup_arg} : $DEFAULT_ROUND_INCREMENT;
    }

    my $last_state_obj = $states_by_vm_href->{$vm_name}[-1];
    return undef unless $last_state_obj;

    # Correctly get the zero-based index of the last state.
    my $last_state_id = @{$states_by_vm_href->{$vm_name}} - 1;

    my $output_obj = {
        vmName       => $vm_name,
        analysisType => ($args_ref->{decay_over_states}) ? 'hybrid_decay_aggregated' : 'windowed_decay_aggregated',
        state        => { stateCount => scalar(@{$states_by_vm_href->{$vm_name}}) },
        metadata     => dclone($last_state_obj->{metadata}),
        metrics      => { physc => {}, runq => { normalized => {}, absolute => {} } },
        debug        => {}
    };

    # --- Add Bin-Basis Metadata (Point 11) ---
    if ($args_ref->{enable_windowed_decay}) {
        $output_obj->{metadata}{binBasis} = "window_" . $args_ref->{process_window_unit} . "_" . $args_ref->{process_window_size};
    } else {
        $output_obj->{metadata}{binBasis} = "state";
    }

    # Populate profileLabel (when it is defined)
    if (defined $profile_label && $profile_label ne '') {
        $output_obj->{profileLabel} = $profile_label;
    }

    # Populate PhysC metrics for each profile
    foreach my $profile_name (keys %{$aggregated_metrics_href}) {
        next if $profile_name eq '_RunQ' or $profile_name eq 'Peak';

        my $baseline_val = _safe_get($aggregated_metrics_href, $profile_name, 0)->{aggregated_value};

        # Get growth adjustment for this profile (default to 0)
        my $growth_adj = 0;
        my $growth_rationale = {};

        # Do NOT apply growth adjustments to the peak helper profile (empirical peak measurement, not a prediction)
        # Accept both the canonical and legacy name here for backwards compatibility with older manifests.
        if ($profile_name ne $MANDATORY_PEAK_PROFILE_FOR_HINT && $profile_name ne $LEGACY_MANDATORY_PEAK_PROFILE_FOR_HINT) {
            if (my $growth_res = _safe_dig($growth_results_href, $profile_name)) {
                $growth_adj = $growth_res->{growth_adj} // 0;
                $growth_rationale = $growth_res->{rationale} // {};
            }
        }

        my $final_val_with_growth = $baseline_val + $growth_adj;

        # Store all components in the output structure for transparency
        $output_obj->{metrics}{physc}{$profile_name} = {
            BaseValue       => apply_rounding($baseline_val, $round_increment, $rounding_method),
            GrowthAdj       => sprintf("%.4f", $growth_adj),
            FinalValue      => apply_rounding($final_val_with_growth, $round_increment, $rounding_method),
        };

        # --- DailyProfileSeries block ---
        # Inject the daily granular data into the profile block if available.
        if (defined $daily_metrics_href && exists $daily_metrics_href->{$vm_name}{$profile_name}) {
            $output_obj->{metrics}{physc}{$profile_name}{DailySeries} = $daily_metrics_href->{$vm_name}{$profile_name};
        }

        # --- GrowthRationale block ---
        # Inject the rationale directly into this profile's block,
        # not the top-level debug hash.
        if (scalar keys %$growth_rationale > 0) {
            $output_obj->{metrics}{physc}{$profile_name}{growthRationale} = $growth_rationale;
        }

        # --- ClippingInfo block ---
        # Find the transform key for this profile to locate its clipping data
        # in the results from the final state, ensuring it's saved to the L2 cache.
        my ($transform_key) = grep {
            $manifest_href->{$_}{metric} eq 'PhysC' &&
            exists $manifest_href->{$_}{profiles}{$profile_name}
        } keys %$manifest_href;

        if ($transform_key) {
            my $clipping_data = _safe_dig($final_metrics_href, $vm_name, $last_state_id, $transform_key, 'Clipping');
            if ($clipping_data && ref($clipping_data) eq 'HASH') {
                $output_obj->{metrics}{physc}{$profile_name}{ClippingInfo} = $clipping_data;
            }
        }

    }

    # Populate Peak and RunQ metrics
    # Use the TRUE raw peak from the tracker (Point 10, Peak Invariant)
    if (exists $raw_peak_tracker_ref->{$vm_name}) {
        $output_obj->{metadata}{peakPhyscFromLatestState} = apply_rounding($raw_peak_tracker_ref->{$vm_name}, $round_increment, $rounding_method);
    }
    $output_obj->{metrics}{runq} = $aggregated_metrics_href->{_RunQ} if exists $aggregated_metrics_href->{_RunQ};

    return $output_obj;
}

# ==============================================================================
# SUBROUTINE: calculate_per_profile_growth_adjustments
# PURPOSE:    Calculate statistically robust growth adjustments for each profile
#             using Theil-Sen robust trend analysis with Hamed-Rao autocorrelation
#             correction on logically-chunked time-series data.
#
# ARGUMENTS:
#   1. $vm_name              - VM identifier (string)
#   2. $aggregated_results   - Hash ref: profile results from aggregation phase
#   3. $analysis_data_href   - Hash ref: packed binary arrays of smoothed data per bin
#   4. $manifest_href        - Hash ref: transform metadata including time filters
#   5. $model_is_windowed    - Boolean: true for windowed decay, false for hybrid
#   6. $window_context_aref  - Array ref: window bins (if windowed model)
#   7. $state_context_aref   - Array ref: state bins (if hybrid model)
#   8. $args_ref             - Hash ref: analysis parameters and flags
#   9. $analysis_ref_obj     - Time::Piece object: reference date for analysis
#
# RETURNS:
#   Hash ref: { profile_name => { growth_adj => N, rationale => {...} } }
#
# METHODOLOGY: "LOGICAL DAY CHUNKING"
#   This implementation uses a statistically robust approach that abandons the
#   flawed "temporal smearing" algorithm in favour of direct sample chunking:
#
#   1. COLLATE: Gather all smoothed samples from all bins into a time-ordered stream.
#   2. DEFINE LOGICAL DAY: Calculate expected samples per "logical day" based on
#      the profile's time filter (e.g., 'online' = 9h × 60 samples/hr = 540 samples).
#   3. CHUNK: Iterate through the sample stream in chunks of this logical day size.
#   4. QC & AGGREGATE: Apply 75% completeness check to each chunk. Valid chunks
#      are aggregated into a single percentile value (one data point per logical day).
#   5. STATISTICAL ANALYSIS: Pass the resulting clean, autocorrelated time-series
#      to Theil-Sen robust trend analysis with Hamed-Rao correction.
#   6. CAPPING: Cap positive growth to configured maximum percentage.
#
# WHY THIS APPROACH IS CORRECT:
#   - Preserves Natural Autocorrelation: Correctly "stitches" active workload
#     periods together, maintaining the day-to-day correlation that Hamed-Rao
#     requires (typically ρ ≈ 0.3-0.5 for real workloads).
#   - Handles Gaps Gracefully: Data gaps (VM shutdowns, NMON errors) become
#     partial chunks that fail QC and are correctly excluded.
#   - Statistically Valid: Avoids the "temporal smearing" bug that created
#     pathological gappy series (e.g., 46 points followed by 45 NULLs).
#   - Works for All Profiles: Single algorithm handles 'none' (24h), 'online' (9h),
#     and 'batch' (12h) profiles correctly.
#
# PROFILE TIME FILTERS:
#   Time filtering is applied upstream in process_data_point, so the packed
#   samples already contain only active-period data:
#   - 'online': 08:00-17:00 (9 hours)  → ~540 samples/day @ 1-min interval
#   - 'batch':  18:00-06:00 (12 hours) → ~720 samples/day @ 1-min interval
#   - 'none':   00:00-24:00 (24 hours) → ~1440 samples/day @ 1-min interval
#
# NOTES:
#   - Requires minimum $GROWTH_MIN_HISTORICAL_PERIODS logical days for trend analysis.
#   - Growth adjustments are capped per profile's max_growth_inflation_percent.
#   - All decisions logged in rationale hash for audit trail.
#   - Compatible with both windowed and hybrid (state-based) decay models.
# ==============================================================================
sub calculate_per_profile_growth_adjustments {
    my ($vm_name, $aggregated_results_href, $analysis_data_href,
        $manifest_href, $model_is_windowed, $window_context_aref, $state_context_aref,
        $args_ref, $analysis_ref_obj) = @_;

    my %growth_results_for_vm;

    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    # STAGE 1: Calculate analysis bounds
    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

    my $earliest_epoch = undef;
    my $latest_epoch = undef;

    my $bin_iterator = _create_bin_iterator(
        $model_is_windowed, $window_context_aref, $state_context_aref
    );
    while (my $bin_meta = $bin_iterator->()) {
        my $start_epoch = $bin_meta->{start_obj}->epoch;
        my $end_epoch = $bin_meta->{end_obj}->epoch;
        if (!defined $earliest_epoch || $start_epoch < $earliest_epoch) {
            $earliest_epoch = $start_epoch;
        }
        if (!defined $latest_epoch || $end_epoch > $latest_epoch) {
            $latest_epoch = $end_epoch;
        }
    }
    return {} unless (defined $earliest_epoch && defined $latest_epoch);

    my $analysis_days = int(($latest_epoch - $earliest_epoch) / 86400) + 1;

    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    # STAGE 2: Determine expected sampling interval
    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

    my $detected_interval_mins = $args_ref->{avg_sampling_interval_mins} // 60;
    my $expected_interval_seconds;

    if (defined $detected_interval_mins &&
        $detected_interval_mins >= 0.5 &&
        $detected_interval_mins <= 30) {
        $expected_interval_seconds = $detected_interval_mins * 60;
    } else {
        if ($args_ref->{verbose} && defined $detected_interval_mins) {
            warn sprintf(
                "WARNING: Detected sampling interval (%.2f min) is outside " .
                "reasonable range [0.5-30]. Using 60s default for growth logic.\n",
                $detected_interval_mins
            );
        }
        $expected_interval_seconds = 60;
    }

    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    # STAGE 3: Calculate adaptive projection days
    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

    my $projection_days;
    my $projection_days_source;
    if (exists $args_ref->{growth_projection_days_user_override} &&
        $args_ref->{growth_projection_days_user_override}) {
        $projection_days = $args_ref->{growth_projection_days};
        $projection_days_source = 'user_specified';
    } else {
        # Use a reasonable fallback for avg_interval_mins in adaptive calculation
        my $avg_interval_mins = int($expected_interval_seconds / 60) || 1;
        $projection_days = calculate_adaptive_projection_days($analysis_days, $avg_interval_mins);
        $projection_days_source = 'adaptive';
    }

    my %growth_args = (
        %{$args_ref},
        growth_projection_days_calculated => $projection_days,
        growth_projection_days_source     => $projection_days_source,
        analysis_days                     => $analysis_days,
        avg_sampling_interval_mins        => int($expected_interval_seconds / 60),
        use_hamed_rao                     => 1,
        aggregation_basis                 => 'per_day',
    );

    # Profile active duration lookup table (in seconds per logical day)
    my %profile_active_duration_seconds = (
        'none'   => 86400, # 24 hours
        'online' => 32400, # 9 hours (08:00-17:00)
        'batch'  => 43200, # 12 hours (18:00-06:00)
    );

    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    # STAGE 4: Per-profile logical day chunking and trend analysis
    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

    foreach my $profile_name (keys %{$aggregated_results_href}) {
        next if $profile_name eq '_RunQ' or $profile_name eq 'Peak';

        # Find the transform key for this profile
        my ($transform_key) = grep {
            $manifest_href->{$_}{metric} eq 'PhysC' &&
            exists $manifest_href->{$_}{profiles}{$profile_name} &&
            exists $manifest_href->{$_}{profiles}{$profile_name}{enable_growth}
        } keys %$manifest_href;
        next unless $transform_key;

        my $baseline_val = $aggregated_results_href->{$profile_name}{aggregated_value};
        next unless (defined $baseline_val && $baseline_val > $FLOAT_EPSILON);

        my $max_growth_perc = _safe_dig($manifest_href, $transform_key, 'profiles',
                                        $profile_name, 'max_growth_inflation_percent')
                              // $max_growth_inflation_percent;
        my $percentiles = _safe_dig($manifest_href, $transform_key, 'profiles',
                                    $profile_name, 'percentiles');
        next unless (defined $percentiles && ref $percentiles eq 'ARRAY' && @$percentiles);
        my $p_val = $percentiles->[0];

        # ──────────────────────────────────────────────────────────────────────
        # Define profile's "logical day" parameters based on time filter
        # ──────────────────────────────────────────────────────────────────────

        my $transform_info = $manifest_href->{$transform_key};
        my $time_filter = _safe_dig($transform_info, 'time_filter') // 'none';

        my $active_duration_per_day_seconds = $profile_active_duration_seconds{$time_filter}
                                              // 86400;
        my $samples_per_logical_day = int($active_duration_per_day_seconds /
                                          $expected_interval_seconds);

        # Quality control threshold: require 75% completeness (minimum 1 sample)
        my $min_samples_threshold = max(1, int($samples_per_logical_day * 0.75));

        # ──────────────────────────────────────────────────────────────────────
        # Collate all smoothed samples from all bins into a single time-ordered stream
        # ──────────────────────────────────────────────────────────────────────

        my @all_smoothed_samples;

        $bin_iterator = _create_bin_iterator(
            $model_is_windowed, $window_context_aref, $state_context_aref
        );

        while (my $bin_meta = $bin_iterator->()) {
            my $bin_id = $bin_meta->{id};
            my $packed_data = _safe_dig($analysis_data_href, $vm_name, $bin_id, $transform_key);
            next unless defined $packed_data;

            # Unpack all samples from this bin
            my @bin_samples = unpack('f<*', $packed_data);
            my $bin_sample_count = scalar(@bin_samples);

            if ($bin_sample_count > 0) {

                # ═══════════════════════════════════════════════════════════════
                # CRITICAL: Check for and trim trailing zeros (InfluxDB lag handling)
                # ═══════════════════════════════════════════════════════════════
                # Scan backwards from end to find last non-zero sample
                my $last_valid_idx = $#bin_samples;
                while ($last_valid_idx >= 0 && abs($bin_samples[$last_valid_idx]) < 1e-6) {
                    $last_valid_idx--;
                }

                my $trimmed_count = 0;
                if ($last_valid_idx < $#bin_samples) {
                    $trimmed_count = $#bin_samples - $last_valid_idx;
                    @bin_samples = @bin_samples[0 .. $last_valid_idx];
                }

                # Append valid samples to the stream
                push @all_smoothed_samples, @bin_samples;
            }
        }

        # ──────────────────────────────────────────────────────────────────────
        # Chunk the sample stream by logical day size and build time series
        # ──────────────────────────────────────────────────────────────────────

        my @series_for_trend;
        my $total_samples = scalar(@all_smoothed_samples);
        my $days_processed = 0;

        for (my $i = 0; $i < $total_samples; $i += $samples_per_logical_day) {
            # Extract chunk (may be partial at the end of the data stream)
            my $chunk_end = min($i + $samples_per_logical_day - 1, $total_samples - 1);
            my @chunk = @all_smoothed_samples[$i .. $chunk_end];

            my $chunk_size = scalar(@chunk);

            # Quality control: require minimum 75% completeness for this logical day
            if ($chunk_size >= $min_samples_threshold) {
                my $daily_percentile = calculate_percentile(\@chunk, $p_val);

                # Add this logical day's aggregated value to the time series
                push @series_for_trend, [$days_processed, $daily_percentile];
                $days_processed++;
            } elsif ($args_ref->{verbose}) {
                warn sprintf(
                    "INFO: Logical day %d for profile '%s' (filter=%s) has insufficient data: " .
                    "%d samples < %d threshold (%.0f%% of %d expected). Excluding from trend.\n",
                    $days_processed,
                    $profile_name,
                    $time_filter,
                    $chunk_size,
                    $min_samples_threshold,
                    75,
                    $samples_per_logical_day
                );
            }
        }

        # ──────────────────────────────────────────────────────────────────────
        # Statistical trend analysis with Theil-Sen + Hamed-Rao
        # ──────────────────────────────────────────────────────────────────────

        if (scalar(@series_for_trend) >= $GROWTH_MIN_HISTORICAL_PERIODS) {
            my $robust_rationale_hash = calculate_growth_trend_robust_v1(
                $vm_name,
                $profile_name,
                \@series_for_trend,
                $projection_days,
                \%growth_args
            );

            # Retrieve the raw calculated growth from the statistical engine
            my $raw_growth_adj = $robust_rationale_hash->{growth_adj} // 0;
            my $sen_slope = $robust_rationale_hash->{sen_slope} // 0;

            # Default final growth to the raw value (which may be negative)
            my $final_growth_adj = $raw_growth_adj;

            # ── Capping Logic: Apply Symmetric Caps (Growth and Decline) ──

            # Calculate the absolute cap limit based on the baseline value
            my $cap_abs = $baseline_val * ($max_growth_perc / 100.0);

            # Check if the magnitude of the raw adjustment exceeds the cap
            if (abs($raw_growth_adj) > $cap_abs) {

                # Apply the cap while preserving the trend direction
                if ($raw_growth_adj > 0) {
                    $final_growth_adj = $cap_abs;
                } else {
                    $final_growth_adj = -$cap_abs;
                }

                # Record rationale metadata for the cap (consumed by nfit-profile logging)
                $robust_rationale_hash->{was_capped} = 1;
                $robust_rationale_hash->{original_growth_adj} = $raw_growth_adj;
                $robust_rationale_hash->{cap_value_applied} = $cap_abs;
                $robust_rationale_hash->{cap_percent_applied} = $max_growth_perc;
            }

            # Store the daily slope for diagnostics (ensure this is recorded for all significant trends)
            $robust_rationale_hash->{sen_slope_per_day} = $sen_slope;

            # Update the rationale with the final (potentially capped) growth value
            $robust_rationale_hash->{growth_adj} = $final_growth_adj;

            # Store results for this profile
            $growth_results_for_vm{$profile_name} = {
                growth_adj => $final_growth_adj,
                rationale  => $robust_rationale_hash
            };
        } else {
            # Insufficient data points for statistical trend analysis
            $growth_results_for_vm{$profile_name} = {
                growth_adj => 0,
                rationale  => {
                    method_used => 'none',
                    skip_reason => 'Insufficient data points for trend analysis (n=' .
                                   scalar(@series_for_trend) . ' < ' .
                                   $GROWTH_MIN_HISTORICAL_PERIODS . ')',
                    sample_points => scalar(@series_for_trend),
                    aggregation_basis => $growth_args{aggregation_basis} || 'unknown',
                }
            };
        }
    } # end foreach profile

    return \%growth_results_for_vm;
}

# ==============================================================================
# HAMED-RAO AUTOCORRELATION CORRECTION FOR MANN-KENDALL TEST
# ==============================================================================

# ==============================================================================
# SUBROUTINE: calculate_lag_autocorrelation
# PURPOSE:    Calculate lag-k autocorrelation coefficient for time series
# ARGUMENTS:
#   1. $series_aref - Array ref of [x, y] pairs (time series points)
#   2. $lag - Lag value k (1, 2, 3, ...)
# RETURNS:
#   Autocorrelation coefficient ρₖ (scalar), or undef if cannot calculate
# NOTES:
#   - Formula: ρₖ = Σ(y_t - ȳ)(y_{t+k} - ȳ) / Σ(y_t - ȳ)²
#   - Used in Hamed-Rao variance correction for Mann-Kendall test
#   - Returns undef if lag >= n or denominator is zero
# ==============================================================================
sub calculate_lag_autocorrelation {
    my ($series_aref, $lag) = @_;

    my $n = scalar @{$series_aref};
    return undef if $lag >= $n || $lag < 1;

    # Extract y-values (the actual measurements)
    my @y_values = map { $_->[1] } @{$series_aref};

    # Calculate mean
    my $mean = sum0(@y_values) / $n;

    # Calculate lag-k autocorrelation
    # ρₖ = Σ(y_t - ȳ)(y_{t+k} - ȳ) / Σ(y_t - ȳ)²

    my $numerator = 0;
    my $denominator = 0;

    # Numerator: sum from t=0 to n-k-1
    for my $t (0 .. $n - $lag - 1) {
        $numerator += ($y_values[$t] - $mean) * ($y_values[$t + $lag] - $mean);
    }

    # Denominator: sum from t=0 to n-1
    for my $t (0 .. $n - 1) {
        $denominator += ($y_values[$t] - $mean) ** 2;
    }

    # Guard against division by zero
    return undef if abs($denominator) < $FLOAT_EPSILON;

    return $numerator / $denominator;
}

# ==============================================================================
# SUBROUTINE: mann_kendall_test_hamed_rao
# PURPOSE:    Mann-Kendall trend test with Hamed-Rao autocorrelation correction
# ARGUMENTS:
#   1. $vm_name           - VM in scope
#   2. $profile_name      - Profile name in scope
#   3. $series_aref       - Array ref of [x, y] pairs (time series points)
#   4. $alpha             - Significance level (optional, default 0.05)
# RETURNS:
#   Hash ref with test results including Hamed-Rao corrections:
#     { statistic, tau, z_score, p_value, trend, passed, alpha,
#       variant, n_eff, autocorr_k, adjustment_factor }
#   Returns undef if insufficient data (n < 4)
# NOTES:
#   - Corrects Mann-Kendall variance for autocorrelated time series
#   - Uses Hamed & Rao (1998) variance adjustment method
#   - Calculates effective sample size n_eff = n / adjustment_factor
#   - More conservative than standard MK for autocorrelated data
#   - Recommended for daily aggregated data from 1-minute samples
# REFERENCE:
#   Hamed, K.H. & Rao, A.R. (1998). "A modified Mann-Kendall trend test
#   for autocorrelated data." Journal of Hydrology, 204(1-4), 182-196.
# ==============================================================================
sub mann_kendall_test_hamed_rao {
    my ($vm_name, $profile_name, $series_aref, $alpha) = @_;
    $alpha //= 0.05;  # Default significance level

    my $n = scalar @{$series_aref};

    # Need at least 4 points for meaningful trend test
    return undef if $n < 4;

    # Extract y-values (the actual measurements)
    my @y_values = map { $_->[1] } @{$series_aref};

    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    # STEP 1: Calculate Kendall's S statistic (same as standard MK)
    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

    my $S = 0;
    my $n_ties = 0;

    for my $i (0 .. $n-2) {
        for my $j ($i+1 .. $n-1) {
            my $diff = $y_values[$j] - $y_values[$i];

            if (abs($diff) < $FLOAT_EPSILON) {
                $n_ties++;
            } elsif ($diff > 0) {
                $S++;
            } else {
                $S--;
            }
        }
    }

    # Calculate Kendall's tau for trend-dominance assessment
    my $tau = $S / ($n * ($n - 1) / 2);

    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    # STEP 2: Trend-Dominance Pre-Check
    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    # For strongly trending data (|τ| ≥ 0.4), Hamed-Rao's stationarity assumption
    # is violated. Higher-lag autocorrelations become negative due to trend,
    # causing unstable adjustment factors. Skip H-R and use standard MK instead.
    #
    # RATIONALE: When trend dominates, autocorrelation correction is inappropriate.
    # Standard MK is the correct test for monotonic trending data.
    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

    if (abs($tau) >= 0.4) {
        # Calculate standard variance (no autocorrelation correction)
        my $var_S_standard = ($n * ($n - 1) * (2 * $n + 5)) / 18;

        # Calculate Z-statistic
        my $Z;
        if ($S > 0) {
            $Z = ($S - 1) / sqrt($var_S_standard);
        } elsif ($S < 0) {
            $Z = ($S + 1) / sqrt($var_S_standard);
        } else {
            $Z = 0;
        }

        # Calculate p-value
        my $p_value = 2 * (1 - _standard_normal_cdf(abs($Z)));
        $p_value = 0.0 if $p_value < 0;
        $p_value = 1.0 if $p_value > 1;

        # Determine trend
        my $passed = ($p_value < $alpha) ? 1 : 0;
        my $trend;
        if ($passed) {
            $trend = ($tau > 0) ? 'increasing' : 'decreasing';
        } else {
            $trend = 'none';
        }

        return {
            # Standard Mann-Kendall fields
            statistic  => $S,
            tau        => $tau,
            z_score    => $Z,
            p_value    => $p_value,
            trend      => $trend,
            passed     => $passed,
            alpha      => $alpha,
            n_ties     => $n_ties,

            # Metadata indicating trend-dominance path
            variant    => 'standard_mk_trend_dominant',
            n_eff      => $n,
            hamed_rao_skipped => 1,
            hamed_rao_skip_reason => sprintf('Strong trend detected (|τ|=%.4f ≥ 0.4)', abs($tau)),
            lag1_autocorr => undef,  # Not calculated for trend-dominant series
        };
    }

    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    # STEP 3: Calculate standard variance (unadjusted)
    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

    my $var_S_standard = ($n * ($n - 1) * (2 * $n + 5)) / 18;

    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    # STEP 4: Calculate Hamed-Rao autocorrelation adjustment
    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

    # Maximum lag K (typically n/3 as per Hamed & Rao)
    my $K = int($n / 3);
    $K = max(1, min($K, $n - 1));  # Ensure valid range: 1 ≤ K < n

    my $autocorr_sum = 0;
    my @rho_values;

    # Calculate autocorrelations for lags 1 to K
    for my $k (1 .. $K) {
        my $rho_k = calculate_lag_autocorrelation($series_aref, $k);
        if (defined $rho_k) {
            push @rho_values, $rho_k;
            # Weighted sum: ρₖ * (n-k) / n
            $autocorr_sum += $rho_k * ($n - $k) / $n;
        }
    }

    # Hamed-Rao adjustment factor: 1 + 2·Σ[ρₖ·(n-k)/n]
    my $adjustment_factor = 1 + 2 * $autocorr_sum;

    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    # STEP 5: Guard - Fallback to Standard MK for Unstable Adjustment
    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    # When adjustment_factor ≤ 0.5, higher-lag autocorrelations are strongly
    # negative due to residual trend effects. Hamed-Rao's stationarity assumption
    # is violated. Fallback to standard MK which handles trending data correctly.
    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

    if ($adjustment_factor <= 0.5) {
        # Calculate Z with unadjusted variance
        my $Z;
        if ($S > 0) {
            $Z = ($S - 1) / sqrt($var_S_standard);
        } elsif ($S < 0) {
            $Z = ($S + 1) / sqrt($var_S_standard);
        } else {
            $Z = 0;
        }

        # Calculate p-value
        my $p_value = 2 * (1 - _standard_normal_cdf(abs($Z)));
        $p_value = 0.0 if $p_value < 0;
        $p_value = 1.0 if $p_value > 1;

        # Determine trend
        my $passed = ($p_value < $alpha) ? 1 : 0;
        my $trend;
        if ($passed) {
            $trend = ($tau > 0) ? 'increasing' : 'decreasing';
        } else {
            $trend = 'none';
        }

        # Emit warning for diagnostics
        if ($adjustment_factor <= 0 && $verbose) {
            warn sprintf(
                "Warning: Hamed-Rao adjustment factor is non-positive (%.4f) for VM '%s', Profile '%s'.\n" .
                "         Falling back to standard Mann-Kendall (appropriate for trending data).\n",
                $adjustment_factor, $vm_name // 'unknown', $profile_name // 'unknown'
            );
        }

        return {
            # Standard Mann-Kendall fields
            statistic  => $S,
            tau        => $tau,
            z_score    => $Z,
            p_value    => $p_value,
            trend      => $trend,
            passed     => $passed,
            alpha      => $alpha,
            n_ties     => $n_ties,

            # Metadata indicating fallback path
            variant    => 'standard_mk_via_hamed_rao_fallback',
            n_eff      => $n,
            hamed_rao_attempted => 1,
            hamed_rao_adjustment_factor => sprintf("%.4f", $adjustment_factor),
            hamed_rao_fallback_reason => 'Adjustment factor unstable (≤ 0.5)',
            autocorr_max_lag => $K,
            lag1_autocorr => scalar(@rho_values) > 0 ? sprintf("%.4f", $rho_values[0]) : undef,
        };
    }

    # Additional guard: Clamp adjustment factor to reasonable upper bound [0.5, 3.0]
    # This prevents extreme corrections while preserving validity
    if ($adjustment_factor > 3.0) {
        $adjustment_factor = 3.0;
    }

    # Adjusted variance: Var(S)* = Var(S) / adjustment_factor
    my $var_S_adjusted = $var_S_standard / $adjustment_factor;

    # Effective sample size
    my $n_eff = $n / $adjustment_factor;

    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    # STEP 6: Calculate Z-statistic with adjusted variance
    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

    my $Z;
    if ($S > 0) {
        $Z = ($S - 1) / sqrt($var_S_adjusted);
    } elsif ($S < 0) {
        $Z = ($S + 1) / sqrt($var_S_adjusted);
    } else {
        $Z = 0;
    }

    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    # STEP 7: Calculate two-tailed p-value
    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

    my $p_value = 2 * (1 - _standard_normal_cdf(abs($Z)));

    # Ensure p-value is in valid range [0, 1]
    $p_value = 0.0 if $p_value < 0;
    $p_value = 1.0 if $p_value > 1;

    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    # STEP 8: Determine trend direction and significance
    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

    my $passed = ($p_value < $alpha) ? 1 : 0;

    my $trend;
    if ($passed) {
        $trend = ($tau > 0) ? 'increasing' : 'decreasing';
    } else {
        $trend = 'none';
    }

    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    # STEP 9: Return complete test results with Hamed-Rao metadata
    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

    return {
        # Standard Mann-Kendall fields
        statistic  => $S,
        tau        => $tau,
        z_score    => $Z,
        p_value    => $p_value,
        trend      => $trend,
        passed     => $passed,
        alpha      => $alpha,
        n_ties     => $n_ties,

        # Hamed-Rao specific fields
        variant    => 'hamed-rao',
        n_eff      => sprintf("%.2f", $n_eff),
        autocorr_max_lag => $K,
        adjustment_factor => sprintf("%.4f", $adjustment_factor),
        lag1_autocorr => scalar(@rho_values) > 0 ? sprintf("%.4f", $rho_values[0]) : undef,
    };
}

# ==============================================================================
# SUBROUTINE: _get_effective_cache_range
# PURPOSE:    Finds the first and last timestamps in the L1 cache and intersects
#             them with the user's --startdate/--enddate flags to determine
#             the actual period for analysis.
#
# ARGUMENTS:
#   1. $data_file (string): Path to the .nfit.cache.data file
#   2. $start_date_str (string, optional): User's --startdate
#   3. $end_date_str (string, optional): User's --enddate
#
# RETURNS:
#   - A list of two Time::Piece objects: (effective_start_obj, effective_end_obj)
#   - Returns (undef, undef) on failure.
#
# NOTES:
#   This is a port of the logic from nfit-profile.pl, adapted for nfit.pl
# ==============================================================================
sub _get_effective_cache_range {
    my ($data_file, $start_date_str, $end_date_str) = @_;

    return (undef, undef) unless (-f $data_file && -s $data_file);

    my ($cache_start_obj, $cache_end_obj);
    my $first_valid_ts;

    # --- Find first timestamp ---
    open my $fh, '<:raw', $data_file or die "FATAL: Could not open $data_file: $!";
    my $header = <$fh>; # Skip header

    while (my $line = <$fh>) {
        # Fast check for YYYY-MM-DD HH:MM:SS format
        if ($line =~ /^(\d{4}-\d{2}-\d{2}\s\d{2}:\d{2}:\d{2}),/) {
            $first_valid_ts = $1;
            last;
        }
    }
    # If we found a start, seek to end. Otherwise, file is empty/corrupt.
    return (undef, undef) unless $first_valid_ts;

    # --- Find last timestamp (fast seek) ---
    my $last_valid_ts;
    my $pos = -s $fh;
    my $line_buffer = '';
    my $chunk_size = 1024;

    while ($pos > 0 && !$last_valid_ts) {
        my $seek_pos = ($pos < $chunk_size) ? 0 : $pos - $chunk_size;
        $chunk_size = $pos if $seek_pos == 0;
        last if $chunk_size <= 0;

        seek $fh, $seek_pos, 0;
        my $chunk;
        read $fh, $chunk, $chunk_size;
        $pos = $seek_pos; # Update position for next iteration

        $line_buffer = $chunk . $line_buffer;
        my @lines = split /\n/, $line_buffer;
        $line_buffer = shift @lines if $pos > 0; # Keep partial line

        for (my $i = $#lines; $i >= 0; $i--) {
            if ($lines[$i] =~ /^(\d{4}-\d{2}-\d{2}\s\d{2}:\d{2}:\d{2}),/) {
                $last_valid_ts = $1;
                last;
            }
        }
    }
    close $fh;
    return (undef, undef) unless $last_valid_ts;

    # --- Parse cache bounds ---
    eval {
        $cache_start_obj = Time::Piece->strptime($first_valid_ts, "%Y-%m-%d %H:%M:%S");
        $cache_end_obj   = Time::Piece->strptime($last_valid_ts,   "%Y-%m-%d %H:%M:%S");
    };
    if ($@) {
        warn "Warning: Could not parse cache boundary timestamps ($first_valid_ts, $last_valid_ts). Error: $@";
        return (undef, undef);
    }

    # --- Intersect with user flags ---
    my $effective_start = $cache_start_obj;
    my $effective_end   = $cache_end_obj;

    if ($start_date_str) {
        my $user_start_obj = Time::Piece->strptime($start_date_str, "%Y-%m-%d")->truncate(to => 'day');
        $effective_start = $user_start_obj if $user_start_obj > $effective_start;
    }
    if ($end_date_str) {
        # User end date is inclusive, so we find the end of that day
        my $user_end_obj = (Time::Piece->strptime($end_date_str, "%Y-%m-%d") + ONE_DAY) - 1;
        $effective_end = $user_end_obj if $user_end_obj < $effective_end;
    }

    return ($effective_start, $effective_end) if $effective_start <= $effective_end;
    return (undef, undef); # No valid overlapping range
}

# ==============================================================================
# SUBROUTINE: _binary_search_window_index
# PURPOSE:    Performs a fast binary search to find the correct time window
#             for a given timestamp.
#
# ARGUMENTS:
#   1. $ts_epoch (integer): The epoch seconds of the current data point.
#   2. $windows_aref (array ref): The pre-sorted array of window hashes.
#
# RETURNS:
#   - The window's string ID (e.g., "2025W42") if found.
#   - undef if not found in any window (e.g., data point is in a gap).
#
# NOTES:
#   Assumes [start, end) boundaries (start inclusive, end exclusive).
# ==============================================================================
sub _binary_search_window_index {
    my ($ts_epoch, $windows_aref) = @_;

    my $low = 0;
    my $high = $#{$windows_aref};
    my $found_id = undef;

    while ($low <= $high) {
        my $mid = int(($low + $high) / 2);
        my $win = $windows_aref->[$mid];

        # Check against [start, end) boundaries
        if ($ts_epoch < $win->{start_obj}->epoch) {
            $high = $mid - 1; # Too high
        }
        elsif ($ts_epoch >= $win->{end_obj}->epoch) {
            $low = $mid + 1; # Too low
        }
        else {
            # Found: ts_epoch >= start_epoch AND ts_epoch < end_epoch
            $found_id = $win->{id};
            last;
        }
    }
    return $found_id;
}

# ==============================================================================
# SUBROUTINE: detect_sampling_interval
# Robustly detects the NMON sampling interval from the .nfit.cache.data file.
# It reads the start of the cache, isolates timestamps for the first VM found,
# calculates the time difference between consecutive samples, and finds the mode
# of these deltas to determine the most likely interval.
#
# Note: This version introduces jitter bucketing: Before counting the frequency,
#  it checks if a delta is close to a standard interval (60, 300, 600, 900 seconds)
#  within a tolerance ($EPS = 3). If it is, it snaps the value to that standard
#  interval before incrementing the count.
# Returns:
#   - The detected interval in seconds (e.g., 60, 300), or undef on failure.
sub detect_sampling_interval {
    my ($data_cache_file) = @_;

    my $SAMPLES_TO_GATHER       = 50;
    my $MAX_LINES_TO_SCAN       = 5000;
    my $MIN_SAMPLES_FOR_DETECTION = 5;   # required count in a single interval bucket
    my @STD = (60, 300, 600, 900);
    my $EPS = 3;                          # jitter tolerance (seconds)

    return undef unless (-f $data_cache_file && -s $data_cache_file);

    open my $fh, '<:encoding(utf8)', $data_cache_file or do {
        warn "Warning: Could not open data cache '$data_cache_file' for interval detection: $!";
        return undef;
    };

    <$fh>; # Skip header

    my $target_vm_for_detection;
    my @timestamps;
    my $lines_scanned = 0;

    while (my $line = <$fh>) {
        $lines_scanned++;
        last if $lines_scanned > $MAX_LINES_TO_SCAN;

        my ($ts_str, $vm_name) = ($line =~ /^(\d{4}-\d{2}-\d{2}\s\d{2}:\d{2}:\d{2}),([^,]+)/);
        next unless ($ts_str && $vm_name);

        $target_vm_for_detection //= $vm_name;
        next unless $vm_name eq $target_vm_for_detection;

        eval {
            push @timestamps, Time::Piece->strptime($ts_str, "%Y-%m-%d %H:%M:%S")->epoch;
        };
        last if @timestamps >= $SAMPLES_TO_GATHER;
    }
    close $fh;

    # Need enough timestamps to produce at least MIN_SAMPLES_FOR_DETECTION deltas
    return undef if @timestamps < ($MIN_SAMPLES_FOR_DETECTION + 1);

    @timestamps = sort { $a <=> $b } @timestamps;

    my %delta_counts;
    for (my $i = 1; $i < @timestamps; $i++) {
        my $delta = $timestamps[$i] - $timestamps[$i-1];
        next unless ($delta > 10 && $delta < 1810);

        # Jitter bucketing: snap near-standards to the standard value
        my $bucket = $delta;
        my $best_d = $EPS + 1;
        for my $s (@STD) {
            my $d = abs($delta - $s);
            if ($d < $best_d) { $best_d = $d; $bucket = ($d <= $EPS) ? $s : $bucket; }
        }
        $delta_counts{$bucket}++;
    }

    return undef unless %delta_counts;

    # Find max frequency
    my $max_count = 0;
    foreach my $delta (keys %delta_counts) {
        $max_count = $delta_counts{$delta} if $delta_counts{$delta} > $max_count;
    }

    # Require minimum evidence in the winning bucket
    return undef if $max_count < $MIN_SAMPLES_FOR_DETECTION;

    # Collect all tied winners
    my @tied_deltas = grep { $delta_counts{$_} == $max_count } keys %delta_counts;

    # Deterministic tie-break: closest to a standard, then smallest delta
    if (@tied_deltas > 1) {
        my %distances;
        for my $delta (@tied_deltas) {
            my $min_dist = 1e9;
            foreach my $std (@STD) {
                my $dist = abs($delta - $std);
                $min_dist = $dist if $dist < $min_dist;
            }
            $distances{$delta} = $min_dist;
        }
        @tied_deltas = sort { $distances{$a} <=> $distances{$b} || $a <=> $b } @tied_deltas;
    }

    return $tied_deltas[0];
}

# --- usage ---
# Generates and returns the usage/help message for the script.
#
sub usage
{
    my $script_name = $0;
    $script_name =~ s{.*/}{};
    my $valid_decays_usage = join("|", sort keys %EMA_ALPHAS);
    return <<END_USAGE;
Usage: $script_name --nmondir <cache_directory> [options]
   or: $script_name --mgsys <serial_number> [options]

Analyses performance data from a pre-built nFit cache directory. This tool
calculates key metrics based on percentile, rolling average, and optional
recency-weighted decay models to provide right-sizing recommendations.

Cache Location (provide one):
  --nmondir <directory>    : The full path to a valid nFit cache directory
                             (e.g., /path/to/stage/12345ABC).
  --mgsys <serial>         : The serial number of the managed system to analyse.
                             If --nmondir is not specified, this will default to
                             looking for a cache at './stage/<serial>/'.

Averaging Method (used within each window if decay enabled):
  --avg-method <method>    : Averaging method for PhysC: 'sma' or 'ema'. (Default: $DEFAULT_AVG_METHOD)
  --decay <level>          : If 'ema' for PhysC, specifies decay level: $valid_decays_usage.
                           (Default: $DEFAULT_DECAY_LEVEL).
  --runq-decay <level>     : Optional. If 'ema' for RunQ, specifies its decay level.
  -w, --window <minutes>     : Window for SMA/EMA calculations. (Default: $DEFAULT_WINDOW_MINUTES min).

RunQ Data Options:
  --smt <N>                : SMT level for RunQ normalisation (Default: $DEFAULT_SMT).
                             Note: This is a fallback; SMT from the cache is preferred.
  --runq-norm-perc <list>  : Comma-separated percentiles for Normalised RunQ.
  --runq-abs-perc <list>   : Comma-separated percentiles for Absolute RunQ.
  --runq-avg-method <none|sma|ema> : Averaging method for RunQ data.

Filtering Options (Applied BEFORE windowing if decay enabled):
  -s, --startdate <YYYY-MM-DD> : Ignore data before this date.
  -ed, --enddate <YYYY-MM-DD>  : Ignore data after this date.
  -startt <HH:MM> / -endt <HH:MM> : Daily time filter.
  -online / -batch           : Shortcut daily time filters.
  -no-weekends               : Exclude data from Saturdays and Sundays.
  -vm, --lpar <name>         : Analyse only the specified VM/LPAR name(s), comma-separated.
  --exclude-vms <list>       : Exclude the specified VM(s) from analysis, comma-separated.
                               Applied after --vm filter. Useful for ad-hoc exclusions.

PhysC Calculation Options:
  -p, --percentile <value>   : Final percentile of PhysC (0-100) (Default: $DEFAULT_PERCENTILE).
  -k, --peak                 : Calculate peak PhysC value.
  --filter-above-perc <N>    : Optional. Filter rolling PhysC values before PXX calc.

Windowed Recency Decay & Growth Prediction:
  --enable-windowed-decay    : Enable internal windowed processing with recency decay.
  --decay-over-states        : Enable Hybrid State-Time Decay Model.
  --enable-growth-prediction : Enable growth prediction (requires a decay model).
  (See documentation for more detail on decay and growth parameters)

Rounding Options:
  -r[=increment] / -u[=increment] : Round results (Default increment: $DEFAULT_ROUND_INCREMENT).

Other:
  -h, --help                 : Display this help message.
  -v, --verbose              : Enable verbose output for debugging.
  --version                  : Display script version.
END_USAGE
}
