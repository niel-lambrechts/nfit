#!/usr/bin/env perl
# NAME     : nfit
# AUTHOR   : NiÃ«l Lambrechts (https://github.com/niel-lambrechts)
# PURPOSE  : Analyse NMON PhysC and optionally RunQ data for AIX and Linux on Power
#            VM right-sizing recommendations. Calculates rolling average (SMA or EMA)
#            percentiles, optionally absolute peaks, and specified percentiles of
#            normalised and absolute run-queue statistics.
#            RunQ data can now also be smoothed using SMA or EMA before percentile calculation.
#            If windowed decay is enabled, metrics are calculated per window,
#            weighted by recency, and then aggregated.
#            NEW: Can predict future growth based on windowed data trends.
#            Supports filtering by date (start and end), time, VM, weekends,
#            and percentile threshold, plus rounding options.
# REQUIRES : Perl, Time::Piece, POSIX (for ceil), List::Util (for sum0, max, min), Getopt::Long, File::Temp, version, File::Spec

use strict;
use warnings;
use Getopt::Long qw(GetOptions);
use File::Temp qw(tempfile tempdir);
use List::Util qw(sum0 max min);
use Scalar::Util qw(looks_like_number);
use POSIX qw(ceil);
use Time::Piece;
use Time::Seconds;
use version;
use File::Spec;
use File::Find;
use List::MoreUtils qw(uniq);
use JSON;
use Fcntl qw(:flock);
use File::Basename qw(dirname);
use Cwd 'abs_path';

# --- Force STDERR to be unbuffered for real-time progress updates ---
# This is critical for the --force-progress flag to work correctly when
# this script is called from another process like nfit-profile.
select STDERR;
$| = 1;
select STDOUT;

# --- Version ---
my $VERSION = '5.25.199.0';

# --- Configuration ---
my $DEFAULT_AVG_METHOD     = 'ema';
my $DEFAULT_DECAY_LEVEL    = 'medium';
my $DEFAULT_WINDOW_MINUTES = 15;
my $DEFAULT_PERCENTILE     = 95;
my $DEFAULT_ROUND_INCREMENT= 0.05;
my $DEFAULT_SMT            = 8;
my $DEFAULT_RUNQ_NORM_PERC = "50,90";
my $DEFAULT_RUNQ_ABS_PERC  = "90";
my $DEFAULT_RUNQ_AVG_METHOD = "ema";

my $FLOAT_EPSILON          = 1e-9; # Small number for float comparisons
my $ACTIVE_PHYSC_THRESHOLD = 0.15; # Original threshold for RunQ normalization
my $SAFETY_MARGIN_FOR_THRESHOLD_CONST = 1.05; # Safety margin for ACTIVE_PHYSC_THRESHOLD calculation
my $PLACEHOLDER_ENTITLEMENT_ADJUSTMENT_FACTOR = 1.0; # Placeholder in ACTIVE_PHYSC_THRESHOLD calc

# Windowed Decay Defaults (if enabled for nfit itself)
my $DEFAULT_PROCESS_WINDOW_UNIT = "weeks";
my $DEFAULT_PROCESS_WINDOW_SIZE = 1;
my $DEFAULT_DECAY_HALF_LIFE_DAYS = 30;

# EMA Alpha values based on decay level
my %EMA_ALPHAS = (
    'low'        => 0.03,
    'medium'     => 0.08,
    'high'       => 0.15,
    'very-high'  => 0.30,
    'extreme'    => 0.40,
);

# --- Growth Prediction Configuration (NEW) ---
my $DEFAULT_GROWTH_PROJECTION_DAYS         = 90;
my $DEFAULT_MAX_GROWTH_INFLATION_PERCENT   = 25;

# --- Cache Configuration ---
my $CACHE_MANIFEST_FILE = ".nfit.cache.manifest";
my $CACHE_STATES_FILE   = ".nfit.cache.states";
my $CACHE_DATA_FILE     = ".nfit.cache.data";
my $CACHE_LOCK_FILE     = ".nfit.cache.lock";

# Internal constants for growth heuristics (not user-configurable initially)
my $GROWTH_MIN_HISTORICAL_PERIODS       = 5;    # Min number of windowed periods to attempt trend
my $GROWTH_MAX_CV_THRESHOLD             = 0.50; # Max Coefficient of Variation (StdDev/Mean); if > this, data too volatile
my $GROWTH_MIN_POSITIVE_SLOPE_THRESHOLD = 0.01; # Min slope (units/period) to consider as actual growth for inflation
my $GROWTH_MAX_PROJECTION_HISTORY_RATIO = 2.0;  # Max ratio of projection duration to history duration used for trend

# --- Clipping Detection Configuration ---
my $CLIPPING_DEFINITE_THRESHOLD = 0.95; # P99.75 > 95% of max_capacity = definite clip
my $CLIPPING_POTENTIAL_THRESHOLD = 0.90; # P99.75 > 90% of max_capacity = potential clip
my $PCR_LOW_CONFIDENCE_THRESHOLD = 0.4;  # Peak Curvature Ratio below this adds confidence
my $PCR_HIGH_CONFIDENCE_THRESHOLD = 0.2; # Peak Curvature Ratio below this adds high confidence

# --- Argument Parsing ---
my $physc_csv_file;
my $physc_csv_dirname;
my $runq_csv_file;
my $vm_config_file;
my $nmon_dir;
my $avg_method     = $DEFAULT_AVG_METHOD;
my $decay_level    = $DEFAULT_DECAY_LEVEL;
my $runq_decay_level_arg = undef;
my $window_minutes = $DEFAULT_WINDOW_MINUTES;
my $percentile     = $DEFAULT_PERCENTILE;
my $start_date_str;
my $end_date_str;
my $calculate_peak = 0;
my $round_arg;
my $roundup_arg;
my $start_time_str;
my $end_time_str;
my $online_flag = 0;
my $batch_flag  = 0;
my $target_vm_name;
my $no_weekends = 0;
my $filter_above_perc_value = undef;
my $smt_value = $DEFAULT_SMT;
my $runq_norm_perc_str = $DEFAULT_RUNQ_NORM_PERC;
my $runq_abs_perc_str  = $DEFAULT_RUNQ_ABS_PERC;
my $runq_avg_method_str = $DEFAULT_RUNQ_AVG_METHOD;
my $help           = 0;
my $show_version   = 0;
my $verbose   = 0;
my $quiet   = 0;
my $show_progress_flag = 0;

# Windowed Decay options
my $enable_windowed_decay = 0;
my $process_window_unit_str = $DEFAULT_PROCESS_WINDOW_UNIT;
my $process_window_size_val = $DEFAULT_PROCESS_WINDOW_SIZE;
my $decay_half_life_days_val = $DEFAULT_DECAY_HALF_LIFE_DAYS;
my $analysis_reference_date_str;
my $decay_over_states_flag = 0;

# Growth Prediction options (NEW)
my $enable_growth_prediction = 0;
my $growth_projection_days = $DEFAULT_GROWTH_PROJECTION_DAYS;
my $max_growth_inflation_percent = $DEFAULT_MAX_GROWTH_INFLATION_PERCENT;

my $show_states_flag = 0;
my $include_states_selector = 'all'; # Default value
my $mgsys_filter;
my $reset_cache = 0;
my $enable_clipping_detection = 0;
my $profile_label;

GetOptions(
    'physc-data|pc=s'     => \$physc_csv_file,
    'runq-data|rq=s'      => \$runq_csv_file,
    'config=s'            => \$vm_config_file,
    'nmondir=s'           => \$nmon_dir,
    'avg-method=s'        => \$avg_method,
    'mgsys|system|serial|host=s' => \$mgsys_filter,
    'decay=s'             => \$decay_level,
    'runq-decay=s'        => \$runq_decay_level_arg,
    'window|w=i'          => \$window_minutes,
    'percentile|p=f'      => \$percentile,
    'startdate|s=s'       => \$start_date_str,
    'enddate|ed=s'        => \$end_date_str,
    'peak|k'              => \$calculate_peak,
    'round|r:f'           => \$round_arg,
    'roundup|u:f'         => \$roundup_arg,
    'startt=s'            => \$start_time_str,
    'endt=s'              => \$end_time_str,
    'online'              => \$online_flag,
    'batch'               => \$batch_flag,
    'vms|vm|lpar|lpars=s' => \$target_vm_name,
    'no-weekends'         => \$no_weekends,
    'filter-above-perc=f' => \$filter_above_perc_value,
    'profile-label=s'     => \$profile_label,
    'smt=i'               => \$smt_value,
    'runq-norm-perc=s'    => \$runq_norm_perc_str,
    'runq-abs-perc=s'     => \$runq_abs_perc_str,
    'runq-avg-method=s'   => \$runq_avg_method_str,
    # Windowed Decay Options
    'enable-windowed-decay'     => \$enable_windowed_decay,
    'process-window-unit=s'     => \$process_window_unit_str,
    'process-window-size=i'     => \$process_window_size_val,
    'decay-half-life-days=i'    => \$decay_half_life_days_val,
    'analysis-reference-date=s' => \$analysis_reference_date_str,
    'decay-over-states'         => \$decay_over_states_flag,
    # Growth Prediction Options (NEW)
    'enable-growth-prediction'       => \$enable_growth_prediction,
    'growth-projection-days=i'       => \$growth_projection_days,
    'max-growth-inflation-percent=i' => \$max_growth_inflation_percent,
    'show-states'               => \$show_states_flag,
    'include-states=s'          => \$include_states_selector,
    'reset-cache'               => \$reset_cache,
    # Clipping
    'enable-clipping-detection' => \$enable_clipping_detection,
    # General Options
    'help|h'              => \$help,
    'verbose|v'           => \$verbose,
    'version'             => \$show_version,
    'q|quiet'             => \$quiet,
    'show-progress'       => \$show_progress_flag,
) or die usage();

# --- Validation ---
if ($show_version)
{
    print STDERR "nfit version $VERSION\n";
    exit 0;
}

if ($help)
{
    print STDERR usage();
    exit 0;
}

# This script now requires a cache directory to be specified, either directly
# via --nmondir or implicitly via --mgsys.
if (!$nmon_dir && !$mgsys_filter)
{
    print STDERR "Error: You must specify a data source cache via --nmondir or --mgsys.\n\n";
    print STDERR usage();
    exit 0;
}

# --- Smart Dispatcher Logic ---
# This logic determines the final, absolute path to the cache directory that
# will be used for analysis. It provides flexibility for the user.
my $resolved_cache_dir;
my $DEFAULT_BASE_STAGE_DIR = File::Spec->catfile(dirname(abs_path($0)), 'stage');

if (defined $nmon_dir)
{
    # Case 1: User provides --nmondir. It could be a base path or a specific cache.
    if (defined $mgsys_filter && -d File::Spec->catfile($nmon_dir, $mgsys_filter))
    {
        # The user gave a base path and a serial, and the combined path exists.
        $resolved_cache_dir = File::Spec->catfile($nmon_dir, $mgsys_filter);
    }
    else
    {
        # Assume --nmondir is the full path to the cache.
        $resolved_cache_dir = $nmon_dir;
    }
}
elsif (defined $mgsys_filter)
{
    # Case 2: User provides --mgsys only. Use the new default location.
    $resolved_cache_dir = File::Spec->catfile($DEFAULT_BASE_STAGE_DIR, $mgsys_filter);
}

# --- Final Validation of the Resolved Cache Directory ---
unless (defined $resolved_cache_dir && -d $resolved_cache_dir)
{
    die "Error: Could not find a valid cache directory at the resolved path: '$resolved_cache_dir'\n" .
    "       Please ensure the path is correct and the cache was created by an nfit-stage tool.\n";
}

# Check for the manifest file to confirm it's a valid cache.
my $manifest_file_check = File::Spec->catfile($resolved_cache_dir, $CACHE_MANIFEST_FILE);
unless (-f $manifest_file_check)
{
    die "Error: The directory '$resolved_cache_dir' does not appear to be a valid nfit cache (missing '$CACHE_MANIFEST_FILE').\n";
}

# Re-assign the validated, absolute path back to the nmon_dir variable for use throughout the script.
$nmon_dir = abs_path($resolved_cache_dir);

$avg_method = lc($avg_method);
if ($avg_method ne 'sma' && $avg_method ne 'ema')
{
    die "Error: --avg-method must be 'sma' or 'ema'. Got '$avg_method'.\n";
}

$decay_level = lc($decay_level);
unless (exists $EMA_ALPHAS{$decay_level})
{
    my $valid_decays = join(", ", sort keys %EMA_ALPHAS);
    die "Error: --decay level '$decay_level' is invalid. Valid levels are: $valid_decays.\n";
}
my $alpha_for_physc_ema = $EMA_ALPHAS{$decay_level};

my $runq_decay_level_to_use = $decay_level;
my $runq_decay_source_for_log = "$decay_level (from PhysC decay setting)";
if (defined $runq_decay_level_arg)
{
    $runq_decay_level_arg = lc($runq_decay_level_arg);
    unless (exists $EMA_ALPHAS{$runq_decay_level_arg})
    {
        my $valid_decays = join(", ", sort keys %EMA_ALPHAS);
        die "Error: --runq-decay level '$runq_decay_level_arg' is invalid. Valid levels are: $valid_decays.\n";
    }
    $runq_decay_level_to_use = $runq_decay_level_arg;
    $runq_decay_source_for_log = "$runq_decay_level_arg (from --runq-decay)";
}
my $alpha_for_runq_ema = $EMA_ALPHAS{$runq_decay_level_to_use};

$runq_avg_method_str = lc($runq_avg_method_str);
if ($runq_avg_method_str ne 'none' && $runq_avg_method_str ne 'sma' && $runq_avg_method_str ne 'ema')
{
    die "Error: --runq-avg-method must be 'none', 'sma', or 'ema'. Got '$runq_avg_method_str'.\n";
}

if ($physc_csv_file)
{
    die "Error: PhysC data file (--physc-data) not found: $physc_csv_file\n" if (! -f $physc_csv_file);
    $physc_csv_dirname = dirname($physc_csv_file);
}

if (defined $runq_csv_file && ! -f $runq_csv_file)
{
    die "Error: RunQ data file (--runq-data) not found: $runq_csv_file\n";
}

if ($smt_value <= 0)
{
    die "Error: --smt value must be a positive integer.\n";
}
if ($window_minutes < 1)
{
    die "Error: Window size (-w) for SMA/EMA must be at least 1 minute.\n";
}
if ($percentile < 0 || $percentile > 100)
{
    die "Error: Percentile (-p) must be between 0 and 100.\n";
}
if (defined $start_date_str && $start_date_str !~ /^\d{4}-\d{2}-\d{2}$/)
{
    die "Error: Invalid startdate (-s) format '$start_date_str'. Use YYYY-MM-DD.\n";
}
if (defined $end_date_str && $end_date_str !~ /^\d{4}-\d{2}-\d{2}$/)
{
    die "Error: Invalid enddate (-ed) format '$end_date_str'. Use YYYY-MM-DD.\n";
}
if (defined $start_date_str && defined $end_date_str)
{
    my ($s_tp_val, $e_tp_val);
    eval { $s_tp_val = Time::Piece->strptime($start_date_str, "%Y-%m-%d"); };
    if ($@ || (defined $start_date_str && !$s_tp_val) )
    {
        die "Error parsing startdate '$start_date_str': $@\n";
    }
    eval { $e_tp_val = Time::Piece->strptime($end_date_str, "%Y-%m-%d"); };
    if ($@ || (defined $end_date_str && !$e_tp_val) )
    {
        die "Error parsing enddate '$end_date_str': $@\n";
    }
    if ($s_tp_val && $e_tp_val && $e_tp_val < $s_tp_val)
    {
        die "Error: --enddate ($end_date_str) cannot be before --startdate ($start_date_str).\n";
    }
}
if (defined($round_arg) && defined($roundup_arg))
{
    die "Error: -round (-r) and -roundup (-u) options are mutually exclusive.\n";
}
if ($enable_windowed_decay)
{
    if ($process_window_unit_str ne "days" && $process_window_unit_str ne "weeks")
    {
        die "Error: --process-window-unit must be 'days' or 'weeks'. Got '$process_window_unit_str'.\n";
    }
    if ($process_window_size_val < 1)
    {
        die "Error: --process-window-size must be at least 1. Got '$process_window_size_val'.\n";
    }
    if ($decay_half_life_days_val < 1)
    {
        die "Error: --decay-half-life-days must be at least 1. Got '$decay_half_life_days_val'.\n";
    }
    if (defined $analysis_reference_date_str && $analysis_reference_date_str !~ /^\d{4}-\d{2}-\d{2}$/)
    {
        die "Error: Invalid --analysis-reference-date format '$analysis_reference_date_str'. Use YYYY-MM-DD.\n";
    }
}

# Growth Prediction Option Validation (NEW)
if ($enable_growth_prediction)
{
    # Growth prediction now works with EITHER standard windowed decay OR the new hybrid model.
    unless ($enable_windowed_decay || $decay_over_states_flag)
    {
        print STDERR "Warning: --enable-growth-prediction requires either --enable-windowed-decay or --decay-over-states to be active. Growth prediction will be SKIPPED.\n";
        $enable_growth_prediction = 0;
    }

    if ($growth_projection_days < 1)
    {
        die "Error: --growth-projection-days must be at least 1.\n";
    }
    if ($max_growth_inflation_percent < 0 || $max_growth_inflation_percent > 200) # Cap inflation percentage
    {
        die "Error: --max-growth-inflation-percent must be between 0 and 200.\n";
    }
}

my @runq_norm_percentiles_to_calc = parse_percentile_list($runq_norm_perc_str, "runq-norm-perc");
my @runq_abs_percentiles_to_calc  = parse_percentile_list($runq_abs_perc_str,  "runq-abs-perc");

my $rounding_method = 'none';
my $round_increment = undef;
my $output_dp = 4;
if (defined $round_arg)
{
    $rounding_method = 'standard';
    $round_increment = (length $round_arg && $round_arg =~ /^[0-9.]*$/) ? $round_arg : $DEFAULT_ROUND_INCREMENT;
    print STDERR "Applying standard rounding to nearest $round_increment\n";
}
elsif (defined $roundup_arg)
{
    $rounding_method = 'up';
    $round_increment = (length $roundup_arg && $roundup_arg =~ /^[0-9.]*$/) ? $roundup_arg : $DEFAULT_ROUND_INCREMENT;
    print STDERR "Applying ceiling rounding up to nearest $round_increment\n";
}
if ($rounding_method ne 'none')
{
    if (!defined $round_increment || $round_increment <= $FLOAT_EPSILON)
    {
        die "Error: Rounding increment must be positive (got '$round_increment').\n";
    }
    $output_dp = get_decimal_places($round_increment);
}

my $time_filter_active = 0;
my $time_filter_start = undef;
my $time_filter_end = undef;
my $time_filter_overnight = 0;
my $time_filter_desc = "";
if ($online_flag)
{
    $time_filter_active = 1;
    $time_filter_start = "08:00";
    $time_filter_end = "17:00";
    $time_filter_desc = "ONLINE ($time_filter_start <= time < $time_filter_end)";
}
elsif ($batch_flag)
{
    $time_filter_active = 1;
    $time_filter_start = "18:00";
    $time_filter_end = "06:00";
    $time_filter_overnight = 1;
    $time_filter_desc = "BATCH (time >= $time_filter_start OR time < $time_filter_end)";
}
elsif (defined $start_time_str && defined $end_time_str)
{
    $time_filter_active = 1;
    $time_filter_start = $start_time_str;
    $time_filter_end = $end_time_str;
    if ($time_filter_end lt $time_filter_start)
    {
        $time_filter_overnight = 1;
        $time_filter_desc = "OVERNIGHT (time >= $time_filter_start OR time < $time_filter_end)";
    }
    else
    {
        $time_filter_desc = "MANUAL ($time_filter_start <= time < $time_filter_end)";
    }
}
elsif (defined $start_time_str || defined $end_time_str)
{
    die "Error: Must specify both -startt and -endt if using manual time filtering.\n";
}

{
    # Block to keep calculation-specific variables local
    # Restoring original logic as requested, with a note about unreachable code.
    my %smt_adjustment_map = (
        1 => 1.00,
        2 => 0.95,
        4 => 0.90,
        8 => 1.10,
    );

    my $smt_adjustment_factor = $smt_adjustment_map{$smt_value} // 1.00;  # default to 1.00

    my $entitlement_adjustment_factor = $PLACEHOLDER_ENTITLEMENT_ADJUSTMENT_FACTOR;
    my $calculated_threshold = $ACTIVE_PHYSC_THRESHOLD *
    $smt_adjustment_factor *
    $entitlement_adjustment_factor *
    $SAFETY_MARGIN_FOR_THRESHOLD_CONST;
    my $MIN_ACTIVE_PHYSC_THRESHOLD = 0.10;
    $ACTIVE_PHYSC_THRESHOLD = ($calculated_threshold < $MIN_ACTIVE_PHYSC_THRESHOLD) ? $MIN_ACTIVE_PHYSC_THRESHOLD : $calculated_threshold;
}

## -- Main Block -- ##

# --- Buffer for the script's final output ---
my @final_output_lines;

# --- Variables for L2 Results Cache ---
# The L2 cache stores the final text output of a run to avoid re-calculation.
my $L2_RESULTS_CACHE_FILE = ".nfit.cache.results";
my $cachedir = (defined $nmon_dir) ? $nmon_dir : (defined $physc_csv_file ? dirname($physc_csv_file) : '.');
my $stage_id_file_for_l2 = File::Spec->catfile($cachedir, '.nfit_stage_id');
my $canonical_key = generate_canonical_key(); # Generate the key for this specific run.

# --- START DEBUGGING DUMP ---
# print STDERR "DEBUG [nfit]: Generated L2 Cache Key:\n>>>$canonical_key<<<\n";
# --- END DEBUGGING DUMP ---

my $shown_cache_msg = 0;
my @processing_windows;
my $script_start_time = time();

print STDERR "\nnfit version $VERSION\n";
print STDERR "-------------------------\n";

# --- L2 Results Cache Lookup ---
# This block attempts to find a pre-calculated result to avoid all further processing.
# It checks if a result for the exact same command-line arguments already exists.
# The L2 cache is a performance optimisation to avoid re-calculating expensive
# rolling averages and percentiles for repeated analysis runs.

my $results_cache_path = File::Spec->catfile($nmon_dir, $L2_RESULTS_CACHE_FILE);
my $manifest_path      = File::Spec->catfile($nmon_dir, $CACHE_MANIFEST_FILE);
my $seasonal_cache_path = File::Spec->catfile($nmon_dir, '.nfit.cache.seasonal_snapshots.json'); # Path to the seasonal cache
my $results_cache_is_stale = 0;

if (-f $results_cache_path) {
    my $results_mtime = (stat($results_cache_path))[9];

    # Check 1: Invalidate if the main data cache is newer.
    if (-f $manifest_path && (stat($manifest_path))[9] > $results_mtime) {
        $results_cache_is_stale = 1;
        print STDERR "NOTICE: L2 Results cache is older than L1 Data cache. Ignoring L2 cache.\n" if $verbose;
    }
    # Check 2: Invalidate if the seasonal snapshot cache is newer.
    elsif (-f $seasonal_cache_path && (stat($seasonal_cache_path))[9] > $results_mtime) {
        $results_cache_is_stale = 1;
        print STDERR "NOTICE: L2 Results cache is older than Seasonal Snapshot cache. Ignoring L2 cache.\n" if $verbose;
    }
}

unless ($results_cache_is_stale)
{
    my $cached_result_aref = lookup_cached_result($canonical_key, $results_cache_path);
    # A valid cached result must be a defined array reference.
    if (defined $cached_result_aref && ref($cached_result_aref) eq 'ARRAY')
    {
        print STDERR "L2 Cache HIT. Returning cached result.\n" if $verbose;

        # Re-encode each result object from the array back into a JSON string,
        # and print each on its own line for the consumer.
        my $json_encoder = JSON->new->utf8;
        my @json_lines_for_stdout = map { $json_encoder->encode($_) } @$cached_result_aref;

        print STDOUT join("\n", @json_lines_for_stdout) . "\n";

        exit 0; # Exit immediately with the cached result.
    }
}

# --- L1 Data Loading from Cache (if L2 cache miss) ---
# This block executes only if no valid L2 result was found. It loads the
# primary data and state caches generated by an nfit-stage tool.

my %config_states_by_vm;
my $data_source_file;

# Define the paths to the two essential L1 cache files.
my $states_cache_file = File::Spec->catfile($nmon_dir, $CACHE_STATES_FILE);
my $data_cache_file   = File::Spec->catfile($nmon_dir, $CACHE_DATA_FILE);

# Validate that the required cache files exist.
unless (-f $states_cache_file && -f $data_cache_file)
{
    die "Error: The cache directory '$nmon_dir' is missing required cache files.\n" .
    "       Expected to find '$CACHE_STATES_FILE' and '$CACHE_DATA_FILE'.\n" .
    "       Please re-run the appropriate nfit-stage tool.\n";
}

# --- Load Configuration States from JSON Cache ---
print STDERR "Cache Location: $states_cache_file\n";
eval {
    # Read the entire JSON file into a string.
    local $/;
    open my $fh, '<:encoding(utf8)', $states_cache_file or die "Could not open '$states_cache_file': $!";
    my $json_text = <$fh>;
    close $fh;

    # Decode the JSON text into the main states hash.
    my $json = JSON->new->allow_nonref;
    my $retrieved_states = $json->decode($json_text);
    %config_states_by_vm = %{$retrieved_states};
};
if ($@)
{
    die "Error: Could not decode JSON from '$states_cache_file': $@\n" .
    "The cache file may be corrupt. Please rebuild it using an nfit-stage tool.\n";
}

# --- Post-load Processing and "Hydration" of State Objects ---
# This block ensures that derived metadata fields are calculated and available
# for the analysis engine, regardless of the cache's original source.
foreach my $vm_name (keys %config_states_by_vm)
{
    if (ref($config_states_by_vm{$vm_name}) eq 'ARRAY')
    {
        # Assign a sequential state_id and re-create Time::Piece objects.
        for (my $i = 0; $i < @{$config_states_by_vm{$vm_name}}; $i++)
        {
            my $state = $config_states_by_vm{$vm_name}[$i];
            $state->{state_id} = $i + 1;
            $state->{vm_name}  = $vm_name;

            if (defined $state->{start_epoch})
            {
                $state->{start_time} = gmtime($state->{start_epoch});
            }
            if (defined $state->{end_epoch})
            {
                $state->{end_time} = gmtime($state->{end_epoch});
            }

            # Calculate derived metadata fields.
            my $md = $state->{metadata};
            $md->{smt} //= $smt_value; # Provide a default for SMT if missing.

            # Calculate MaxCPU based on entitlement, vCPUs, pool CPUs, and capped status.
            my $ent       = $md->{entitlement} // 0;
            my $vcpu      = $md->{virtual_cpus} // 0;
            my $poolcpu   = $md->{pool_cpu} // 0;
            my $is_capped = $md->{capped} // 0;

            my $max_cpu_calc = $is_capped ? $ent : ($poolcpu > 0 && $vcpu > 0 ? min($vcpu, $poolcpu) : $vcpu);
            $md->{max_cpu} = $max_cpu_calc > 0 ? $max_cpu_calc : ($vcpu > 0 ? $vcpu : 0);

            # Construct the 'Processor' string from its component parts.
            my $p_type = $md->{proc_type} // 'UNK'; $p_type =~ s/PowerPC_//;
            my $p_ver  = $md->{proc_version} // 'UNK';
            my $p_clk  = $md->{proc_clock} // 'UNK';

            $md->{processor_state} = join("_", $p_type, $p_ver, $p_clk);
            $md->{processor_state} =~ s/\s+/-/g;
        }
    }
}
printf STDERR "Loaded %d cached configuration states for %d VMs.\n", (sum0 map { scalar @$_ } values %config_states_by_vm), scalar(keys %config_states_by_vm);

# Set the data source file directly to the performance data cache.
$data_source_file = $data_cache_file;

# --- Unified Analysis Engine ---
{
    # --- Prepare States for Single-Pass Analysis (note: this must support both decay and non-decay paths!) ---
    print STDERR "Unified Analysis Engine:\n";
    my @all_selected_states;

    # This block determines which VMs to process based on the --vm flag.
    # It has been modified to handle a comma-separated list of VM names.
    my @vms_to_process;
    if (defined $target_vm_name) {
        # If --vm is used, split the argument by comma into a set for efficient lookup.
        my %user_specified_vms = map { $_ => 1 } split /,/, $target_vm_name;

        # Filter the VMs found in the cache to only those specified by the user.
        @vms_to_process = grep { exists $user_specified_vms{$_} } (sort keys %config_states_by_vm);

        # This logic handles a special case where a user might analyse a single VM
        # from a file that has no corresponding configuration state.
        if (!@vms_to_process && (scalar keys %user_specified_vms) == 1) {
            @vms_to_process = keys %user_specified_vms;
        }
    } else {
        # If --vm was not specified, process all VMs found in the configuration states.
        @vms_to_process = sort keys %config_states_by_vm;
    }
    my %target_vms_lookup = map { $_ => 1 } @vms_to_process;

    foreach my $vm_name (@vms_to_process) {
        my @available = @{$config_states_by_vm{$vm_name} || []};
        my @selected_for_this_vm = parse_state_selector($include_states_selector, \@available);
        push @all_selected_states, @selected_for_this_vm;
    }

    if (scalar(@all_selected_states) == 0) {
        print STDERR "-> No matching states found for the specified selector '$include_states_selector'. No data to process.\n";
        exit 0;
    }

    my @sorted_states_to_process = sort {
        $a->{vm_name} cmp $b->{vm_name} || $a->{start_epoch} <=> $b->{start_epoch}
    } @all_selected_states;

    my %states_by_vm;
    foreach my $state (@sorted_states_to_process) {
        push @{$states_by_vm{$state->{vm_name}}}, $state;
    }
    my $total_vms_with_states = scalar(keys %config_states_by_vm);
    my $vm_progress_counter = 0;
    my $total_vms_for_progress = scalar(@vms_to_process);

    print STDERR "  -> Profiling " . scalar(@sorted_states_to_process) . " selected configuration states across $total_vms_with_states VMs.\n";

    # ------------------------------------------------------------------
    # Define the Analysis Reference Date
    # This is done once, before any analysis path is chosen, to ensure the
    # variable is available to any mode that needs it (Windowed and Hybrid).
    # ------------------------------------------------------------------
    my $analysis_ref_obj;
    if ($enable_windowed_decay || $decay_over_states_flag)
    {
        if (defined $analysis_reference_date_str)
        {
            eval { $analysis_ref_obj = Time::Piece->strptime($analysis_reference_date_str, "%Y-%m-%d"); };
            if ($@ || !$analysis_ref_obj)
            {
                die "Error: Invalid --analysis-reference-date: '$analysis_reference_date_str'. Parser error: $@";
            }
        }
        else
        {
            # Default to the end time of the latest state if no reference date is given
            my $latest_epoch = 0;
            foreach my $state (@all_selected_states)
            {
                $latest_epoch = $state->{end_time}->epoch if $state->{end_time}->epoch > $latest_epoch;
            }
            $analysis_ref_obj = Time::Piece->new($latest_epoch);
        }
        # Truncate to the start of the day for consistent calculations
        $analysis_ref_obj = $analysis_ref_obj->truncate(to => 'day');
    }

    # --- Phase 3: Main Analysis Execution ---
    my %vm_results_aggregator;

    # This single sort prepares the data for BOTH the windowed and non-windowed analysis paths.
    print STDERR "  INFO: Preparing data for analysis (sorting by VM, then Time)...\n" if $verbose;
    my ($sorted_fh, $sorted_filename) = tempfile(UNLINK => 1);
    # Use system sort for efficiency: preserve header, then sort by VM Name (col 2), then Timestamp (col 1)
    system("cat '$data_source_file' | (head -n 1 && tail -n +2 | sort -t, -k2,2 -k1,1) > '$sorted_filename'") == 0
        or die "Failed to create sorted temporary file: $!";
    # The analysis engine will now read from this correctly sorted temp file for all paths.
    $data_source_file = $sorted_filename;

    # This is the main control-break loop that reads the sorted data file once
    # and buffers data on a per-VM basis.
    my @vm_data_buffer;
    my $current_vm_name_for_buffer;

    open my $data_fh, '<', $data_source_file or die "FATAL: Could not open sorted data file '$data_source_file': $!";
    <$data_fh>; # Skip header

    while (my $line = <$data_fh>)
    {
        chomp $line;
        my ($ts, $line_vm, $physc, $runq) = split ',', $line, 4;
        next if (defined $target_vm_name && !exists $target_vms_lookup{$line_vm});
        next unless defined $line_vm && $line_vm ne '';

        $current_vm_name_for_buffer = $line_vm unless defined $current_vm_name_for_buffer;

        # On a "control break" (when VM name changes), process the buffered data
        if ($line_vm ne $current_vm_name_for_buffer && @vm_data_buffer)
        {
            my @states_for_buffered_vm = grep { $_->{vm_name} eq $current_vm_name_for_buffer } @sorted_states_to_process;
            my $state_count = scalar(@states_for_buffered_vm);
            $vm_results_aggregator{$current_vm_name_for_buffer}{'state_count'} = $state_count;

            if (-t STDERR || $show_progress_flag) {
                # Terminal output: single, updating line with state count
                my $state_label = sprintf("(%d %s)", $state_count, $state_count == 1 ? "state" : "states");
                if (exists $target_vms_lookup{$current_vm_name_for_buffer}) {
                    $vm_progress_counter++;
                    printf STDERR "\r  -> Profiling VM %d/%d: %-35s %-12s", $vm_progress_counter, $total_vms_for_progress, $current_vm_name_for_buffer, $state_label;
                    STDERR->flush();
                }
            }

            # --- Main Analysis Path Router ---
            if ($enable_windowed_decay)
            {
                # PATH A: Standard Windowed Decay
                process_vm_buffer(\@vm_data_buffer, $current_vm_name_for_buffer, \%vm_results_aggregator, \%states_by_vm, \@processing_windows);
            }
            elsif ($decay_over_states_flag)
            {
                # PATH B: Hybrid State-Time Decay Model
                my @state_results = get_metrics_for_each_state(\@vm_data_buffer, $current_vm_name_for_buffer, \@states_for_buffered_vm);

                if (@state_results)
                {
                    # Synthesize a daily time series from the state-based results
                    my @hybrid_timeseries = synthesize_hybrid_timeseries(\@state_results, "P".clean_perc_label($percentile));
                    # Apply the final decay calculation to get a single hybrid metric
                    my $final_hybrid_metric = calculate_recency_weighted_average(\@hybrid_timeseries, $analysis_ref_obj, $decay_half_life_days_val);
                    # Store the synthesized time series so the growth prediction engine can use it.
                    $vm_results_aggregator{$current_vm_name_for_buffer}{'hybrid_timeseries_data'} = \@hybrid_timeseries;
                    # Store the single result in the aggregator
                    $vm_results_aggregator{$current_vm_name_for_buffer}{'hybrid_metric'} = $final_hybrid_metric;
                    $vm_results_aggregator{$current_vm_name_for_buffer}{'source_states'} = \@state_results;
                }
            }
            else
            {
                # PATH C: Standard Non-Windowed (State-by-State)
                my @state_results = get_metrics_for_each_state(\@vm_data_buffer, $current_vm_name_for_buffer, \@states_for_buffered_vm);
                $vm_results_aggregator{$current_vm_name_for_buffer}{'states'} = \@state_results if @state_results;
            }

            @vm_data_buffer = (); # Reset buffer for the new VM
        }

        $current_vm_name_for_buffer = $line_vm;

        # Buffer the current data point
        my $tp;
        eval { $tp = Time::Piece->strptime($ts, "%Y-%m-%d %H:%M:%S"); };
        next if $@;

        # Apply the primary date filters as early as possible for performance.
        if (defined $start_date_str && $tp->ymd lt $start_date_str) { next; }
        if (defined $end_date_str   && $tp->ymd gt $end_date_str)   { next; }

        push @vm_data_buffer, {
            ts    => $ts,
            physc => (defined $physc && looks_like_number($physc)) ? $physc+0 : undef,
            runq  => (defined $runq && looks_like_number($runq)) ? $runq+0 : undef,
            tp    => $tp,
        };
    }
    close $data_fh;

    # After the loop, ensure the very last VM's buffer is processed
    if (@vm_data_buffer)
    {
        my @states_for_last_vm = grep { $_->{vm_name} eq $current_vm_name_for_buffer } @sorted_states_to_process;
        my $state_count = scalar(@states_for_last_vm);
        $vm_results_aggregator{$current_vm_name_for_buffer}{'state_count'} = $state_count;

        if (-t STDERR) {
            # Terminal output: single, updating line with state count
            my $state_label = sprintf("(%d %s)", $state_count, $state_count == 1 ? "state" : "states");
            if (exists $target_vms_lookup{$current_vm_name_for_buffer}) {
                $vm_progress_counter++;
                printf STDERR "\r  -> Profiling VM %d/%d: %-35s %-12s", $vm_progress_counter, $total_vms_for_progress, $current_vm_name_for_buffer, $state_label;
            }
        }

        # --- Final Analysis Path Router ---
        if ($enable_windowed_decay)
        {
            process_vm_buffer(\@vm_data_buffer, $current_vm_name_for_buffer, \%vm_results_aggregator, \%states_by_vm, \@processing_windows);
        }
        elsif ($decay_over_states_flag)
        {
            my @state_results = get_metrics_for_each_state(\@vm_data_buffer, $current_vm_name_for_buffer, \@states_for_last_vm);
            if (@state_results)
            {
                my @hybrid_timeseries = synthesize_hybrid_timeseries(\@state_results, "P".clean_perc_label($percentile));
                $vm_results_aggregator{$current_vm_name_for_buffer}{'hybrid_timeseries_data'} = \@hybrid_timeseries;
                my $final_hybrid_metric = calculate_recency_weighted_average(\@hybrid_timeseries, $analysis_ref_obj, $decay_half_life_days_val);
                $vm_results_aggregator{$current_vm_name_for_buffer}{'hybrid_metric'} = $final_hybrid_metric;
                $vm_results_aggregator{$current_vm_name_for_buffer}{'source_states'} = \@state_results;
            }
        }
        else
        {
            my @state_results = get_metrics_for_each_state(\@vm_data_buffer, $current_vm_name_for_buffer, \@states_for_last_vm);
            $vm_results_aggregator{$current_vm_name_for_buffer}{'states'} = \@state_results if @state_results;
        }
    }

    print STDERR "\n" if (-t STDERR); # Move to a new line only if we were using the progress bar
    print STDERR "  State Analysis complete.\n";
    # --- End of Analysis --- #

    print STDERR generate_run_summary_string();
    print STDERR "------------------------------------------------------------------\n";

    # --- Final Output Generation ---

    if (!$quiet) {
        print STDERR "\n------------------------------------------------------------------\n";
        print STDERR "Results\n";
        print STDERR "------------------------------------------------------------------\n";
    }

    my @output_header_cols = ("VM_Name", "State_ID", "Duration_Days", "Entitlement", "MaxCPU", "SMT", "Processor", "SerialNumber", "PoolID");
    my @metric_keys_in_order = ("P" . clean_perc_label($percentile));
    push @metric_keys_in_order, 'Peak' if $calculate_peak;
    foreach my $p (@runq_norm_percentiles_to_calc) { push @metric_keys_in_order, "NormRunQ_P" . clean_perc_label($p); }
    foreach my $p (@runq_abs_percentiles_to_calc) { push @metric_keys_in_order, "AbsRunQ_P" . clean_perc_label($p); }
    if ($enable_windowed_decay && $enable_growth_prediction) {
        push @metric_keys_in_order, "GrowthAdj", "GrowthAdjAbs";
    }
    push @output_header_cols, @metric_keys_in_order;

    # This will hold the raw Perl hashes for caching
    my @result_objects;

    # This is the conditional block that handles output for all three modes.
    # Path A: Output for standard Non-Windowed (State-by-State) Mode.
    if (!$enable_windowed_decay && !$decay_over_states_flag)
    {
        foreach my $vm_name (sort keys %vm_results_aggregator)
        {
            next unless (exists $vm_results_aggregator{$vm_name}{'states'} && ref($vm_results_aggregator{$vm_name}{'states'}) eq 'ARRAY');

            my @states_for_this_vm = @{$vm_results_aggregator{$vm_name}{'states'}};

            foreach my $result (@states_for_this_vm)
            {
                my $state_obj = $result->{state_obj};
                my $md        = $result->{state_obj}{metadata};

                my $metrics = $result->{metrics};

                my %output_hash = (
                    vmName       => $vm_name,
                    analysisType => 'state_based',
                    state => {
                        id           => $state_obj->{state_id},
                        durationDays => ($state_obj->{duration_days} // 0) + 0,
                        startTimeISO => $state_obj->{start_time}->datetime . 'Z',
                        endTimeISO   => $state_obj->{end_time}->datetime . 'Z',
                    },
                    metadata => {
                        platform     => 'AIX',
                        serialNumber => $md->{serial_number},
                        poolId       => defined $md->{pool_id} && looks_like_number($md->{pool_id}) ? $md->{pool_id} + 0 : undef,
                        processor    => $md->{processor_state},
                        configuration => {
                            entitlement => defined $md->{entitlement} ? ($md->{entitlement} + 0) : undef,
                            maxCpu      => defined $md->{max_cpu} ? ($md->{max_cpu} + 0) : undef,
                            smt         => defined $md->{smt} ? ($md->{smt} + 0) : undef,
                            virtualCpus => defined $md->{virtual_cpus} ? ($md->{virtual_cpus} + 0) : undef,
                            capped      => ($md->{capped} // 0) == 1 ? \1 : \0,
                            poolCpu     => defined $md->{pool_cpu} ? ($md->{pool_cpu} + 0) : undef
                        },
                    },
                    metrics => { physc => {}, runq => { normalized => {}, absolute => {} }, clipping => {} },
                );
                $output_hash{profileLabel} = $profile_label if (defined $profile_label);

                foreach my $key (keys %$metrics) {
                    my $value = $metrics->{$key};
                    my $rounded_value = defined($value) && looks_like_number($value) ? apply_rounding($value, $round_increment, $rounding_method) : undef;

                    if ($key =~ /^P(\d+\.?\d*)$/) { $output_hash{metrics}{physc}{$key} = $rounded_value; }
                    elsif ($key eq 'Peak') { $output_hash{metrics}{physc}{$key} = $rounded_value; }
                    elsif ($key =~ /^NormRunQ_(P\d+\.?\d*)$/) { $output_hash{metrics}{runq}{normalized}{$1} = $rounded_value; }
                    elsif ($key =~ /^AbsRunQ_(P\d+\.?\d*)$/) { $output_hash{metrics}{runq}{absolute}{$1} = $rounded_value; }
                    elsif ($key eq 'IsClipped') { $output_hash{metrics}{clipping}{isClipped} = $value + 0 if defined $value; }
                    elsif ($key eq 'ClipConfidence') { $output_hash{metrics}{clipping}{confidence} = $rounded_value; }
                    elsif ($key eq 'UnmetDemand') { $output_hash{metrics}{clipping}{unmetDemand} = $rounded_value; }
                    elsif ($key eq 'RunQSatScore') { $output_hash{metrics}{clipping}{runqSaturationScore} = $rounded_value; }
                }

                push @result_objects, \%output_hash;

           }
        }
    }
    else # Path B & C: Unified Output for ALL Aggregated Modes (Windowed Decay AND Hybrid)
    {
        # This block handles output for both --enable-windowed-decay and --decay-over-states

        foreach my $vm_name (sort keys %vm_results_aggregator)
        {
            # This block now builds a single, rich hash for the final JSON output
            my %output_hash = (
                vmName       => $vm_name,
                analysisType => ($enable_windowed_decay ? 'windowed_decay_aggregated' : 'hybrid_decay_aggregated'),
                metadata     => { platform => 'AIX' },
                metrics      => { physc => {}, runq => { normalized => {}, absolute => {} }, growth => {}, clipping => {} },
                debug        => { growthRationale => {}, growthParams => {} }
            );
            $output_hash{profileLabel} = $profile_label if (defined $profile_label);

            my $last_state_metadata;
            my %final_metrics = %{aggregate_decay_metrics(\%vm_results_aggregator, $vm_name, $analysis_ref_obj, $decay_half_life_days_val)};

            if ($enable_growth_prediction) {
                 apply_growth_prediction_to_metrics(\%vm_results_aggregator, $vm_name, \%final_metrics);
            }

            # Step 3: Populate the output hash with metadata and metrics.
            my ($last_state_obj) = grep { $_->{vm_name} eq $vm_name } reverse @sorted_states_to_process;
            next unless $last_state_obj;

            $last_state_metadata = $last_state_obj->{metadata} || {};
            $output_hash{metadata}{serialNumber} = $last_state_metadata->{serial_number};
            $output_hash{metadata}{poolId} = defined $last_state_metadata->{pool_id} && looks_like_number($last_state_metadata->{pool_id}) ? $last_state_metadata->{pool_id} + 0 : undef;
            $output_hash{metadata}{processor} = $last_state_metadata->{processor_state};
            $output_hash{metadata}{configuration} = {
                entitlement => defined $last_state_metadata->{entitlement} ? ($last_state_metadata->{entitlement} + 0) : undef,
                maxCpu      => defined $last_state_metadata->{max_cpu} ? ($last_state_metadata->{max_cpu} + 0) : undef,
                smt         => defined $last_state_metadata->{smt} ? ($last_state_metadata->{smt} + 0) : undef,
                virtualCpus => defined $last_state_metadata->{virtual_cpus} ? ($last_state_metadata->{virtual_cpus} + 0) : undef,
                capped      => ($last_state_metadata->{capped} // 0) == 1 ? 1 : 0,
                poolCpu     => defined $last_state_metadata->{pool_cpu} ? ($last_state_metadata->{pool_cpu} + 0) : undef
            };
            $output_hash{state}{stateCount} = $vm_results_aggregator{$vm_name}{'state_count'} if exists $vm_results_aggregator{$vm_name}{'state_count'};

            foreach my $key (keys %final_metrics) {
                my $value = $final_metrics{$key};
                my $rounded_value = defined($value) && looks_like_number($value) ? apply_rounding($value, $round_increment, $rounding_method) : undef;

                if ($key =~ /^P(\d+\.?\d*)$/) { $output_hash{metrics}{physc}{$key} = $rounded_value; }
                elsif ($key eq 'Peak') { $output_hash{metrics}{physc}{$key} = $rounded_value; }
                elsif ($key =~ /^NormRunQ_(P\d+\.?\d*)$/) { $output_hash{metrics}{runq}{normalized}{$1} = $rounded_value; }
                elsif ($key =~ /^AbsRunQ_(P\d+\.?\d*)$/) { $output_hash{metrics}{runq}{absolute}{$1} = $rounded_value; }
                elsif ($key eq 'GrowthAdj') { $output_hash{metrics}{growth}{adjustment} = $rounded_value; }
                elsif ($key eq 'GrowthAdjAbs') { $output_hash{metrics}{growth}{adjustmentAbsolute} = $rounded_value; }
                elsif ($key =~ /^GrowthDebug_(.*)$/) { $output_hash{debug}{growthRationale}{$1} = $value; }
                elsif ($key =~ /^GrowthParam_(.*)$/) { $output_hash{debug}{growthParams}{$1} = $value + 0; }
                elsif ($key eq 'IsClipped') { $output_hash{metrics}{clipping}{isClipped} = $value + 0 if defined $value; }
                elsif ($key eq 'ClipConfidence') { $output_hash{metrics}{clipping}{confidence} = $rounded_value; }
                elsif ($key eq 'UnmetDemand') { $output_hash{metrics}{clipping}{unmetDemand} = $rounded_value; }
                elsif ($key eq 'RunQSatScore') { $output_hash{metrics}{clipping}{runqSaturationScore} = $rounded_value; }
            }

            push @result_objects, \%output_hash;
        }
    }

    # --- L2 Cache Save and Final Output ---
    my $json_encoder = JSON->new->utf8;
    if (@result_objects)
    {
        # For STDOUT, create the line-by-line JSON output that nfit-profile expects.
        my @json_lines_for_stdout = map { $json_encoder->encode($_) } @result_objects;
        my $final_output_string = join("\n", @json_lines_for_stdout);

        if (-f $stage_id_file_for_l2)
        {
            my $results_cache_path = File::Spec->catfile($cachedir, $L2_RESULTS_CACHE_FILE);
            my $l2_lock_file_path  = File::Spec->catfile($cachedir, $CACHE_LOCK_FILE);
            print STDERR "L2 Results Cache MISS (caching new result).\n" if $verbose;
            save_result_to_cache($canonical_key, \@result_objects, $results_cache_path, $l2_lock_file_path);
        }
        print STDOUT $final_output_string . "\n";
    }
}

my $script_end_time = time();
my $duration = $script_end_time - $script_start_time;
print STDERR "\nCompleted in $duration second(s).\n";
exit 0;

# ==============================================================================
# Subroutines
# ==============================================================================

# --- clean_perc_label ---
# Helper to format a percentile number into a clean string for metric keys.
sub clean_perc_label {
    my ($p) = @_;
    my $label = sprintf("%.2f", $p);
    $label =~ s/\.?0+$//;
    $label = "0" if $label eq "" && abs($p-0)<0.001;
    return $label;
}

# --- calculate_rolling_average ---
# Generic sub to calculate a single rolling average point (SMA or EMA).
sub calculate_rolling_average {
    my ($value, $method, $sma_queue_aref, $prev_ema_sref, $window, $alpha) = @_;

    my $avg_to_return;
    if ($method eq 'sma') {
        push @$sma_queue_aref, $value;
        shift @$sma_queue_aref while @$sma_queue_aref > $window;
        if (@$sma_queue_aref == $window) {
            $avg_to_return = calculate_average(@$sma_queue_aref);
        }
    } else { # ema
        if (defined $value) {
            if (!defined $$prev_ema_sref) { $$prev_ema_sref = $value; }
            else { $$prev_ema_sref = ($value * $alpha) + ($$prev_ema_sref * (1 - $alpha)); }
        }
        push @$sma_queue_aref, $value; # Use sma_queue as a simple counter
        shift @$sma_queue_aref while @$sma_queue_aref > $window;
        if (@$sma_queue_aref == $window) {
            $avg_to_return = $$prev_ema_sref;
        }
    }
    return $avg_to_return;
}

# --- print_state_windows_report ---
# Takes the data structure returned by define_configuration_states and
# prints a formatted, compact report to STDOUT.
#
# Args:
#   1. $states_href (hash ref): The state windows data structure.
#
sub print_state_windows_report
{
    my ($states_href) = @_;

    my $header_format = "%-20s %-10s %-20s %-20s %-10s %-45s %s\n";
    my $row_format    = "%-20s %-10s %-20s %-20s %-10.2f %-45s %s\n";

    printf $header_format, "VM_Name", "State_ID", "Start_Time", "End_Time", "Duration", "Config_Summary", "Processor";

    my $separator = ("-" x 20) . " " . ("-" x 10) . " " . ("-" x 20) . " " . ("-" x 20) . " " . ("-" x 10) . " " . ("-" x 45) . " " . ("-" x 30) . "\n";
    print STDOUT $separator;

    foreach my $vm_name (sort keys %$states_href)
    {
        my @vm_states = @{$states_href->{$vm_name}};
        my $total_states = scalar(@vm_states);
        foreach my $state (@vm_states)
        {
            my $md = $state->{metadata};

            # Build the compact configuration summary string
            my $capped_str = ($md->{capped} // 0) ? "Capped" : "Uncapped";
            my $config_summary = sprintf("Ent:%.2f vCPU:%d PoolCPU:%d SMT:%d %s",
                $md->{entitlement} // 0,
                $md->{virtual_cpus} // 0,
                $md->{pool_cpu} // 0,
                $md->{smt} // 0,
                $capped_str
            );

            printf $row_format,
            $state->{vm_name},
            $state->{state_id} . "/" . $total_states,
            $state->{start_time}->strftime('%Y-%m-%d %H:%M'),
            $state->{end_time}->strftime('%Y-%m-%d %H:%M'),
            $state->{duration_days},
            $config_summary,
            $md->{processor_state};
        }
    }
}

# --- parse_state_selector ---
# Parses the user's state selection string (e.g., "1,3,5-7,-1") and returns
# an array of the actual state window objects that match the selection.
#
# Args:
#   1. $selector_str (string): The raw string from the --include-states flag.
#   2. $available_states_aref (array ref): An array of all state window hashes for a VM.
#
# Returns:
#   - An array containing the state window hashes that were selected.
#
sub parse_state_selector
{
    my ($selector_str, $available_states_aref) = @_;
    my @selected_states;
    return @$available_states_aref if (lc($selector_str) eq 'all');
    return () if (scalar(@$available_states_aref) == 0);
    my %selected_ids;
    my $total_states = scalar(@$available_states_aref);
    my @parts = split ',', $selector_str;
    foreach my $part (@parts) {
        $part =~ s/\s+//g;
        if ($part =~ /^(\d+)-(\d+)$/) {
            for my $id ($1 .. $2) { $selected_ids{$id} = 1; }
        } elsif ($part =~ /^-(\d+)$/) {
            my $offset = $1;
            if ($total_states - $offset >= 0) { $selected_ids{ $total_states - ($offset - 1) } = 1; }
        } elsif ($part =~ /^\d+$/) {
            $selected_ids{$part} = 1;
        }
    }
    foreach my $state (@$available_states_aref) {
        push @selected_states, $state if exists $selected_ids{$state->{state_id}};
    }
    return @selected_states;
}

# Calculate basic statistics (mean, stddev, CV) for a list of values
# Input: reference to an array of numeric values
# Output: hashref { mean, stddev, cv, count } or undef if insufficient data
sub calculate_statistics_for_trend
{
    my ($values_aref) = @_;
    my @defined_values = grep { defined $_ && $_ =~ /^-?[0-9.]+$/ } @{$values_aref};
    my $count = scalar @defined_values;

    return undef if $count == 0;

    my $sum = sum0(@defined_values);
    my $mean = $sum / $count;
    my $stddev = 0;
    my $cv = undef;

    if ($count > 1)
    {
        my $sum_sq_diff = 0;
        foreach my $val (@defined_values)
        {
            $sum_sq_diff += ($val - $mean)**2;
        }
        $stddev = sqrt($sum_sq_diff / ($count - 1));
    }

    if (abs($mean) > $FLOAT_EPSILON)
    {
        $cv = $stddev / $mean;
    }
    elsif ($stddev < $FLOAT_EPSILON && abs($mean) < $FLOAT_EPSILON)
    {
        $cv = 0;
    }

    return {
        mean   => $mean,
        stddev => $stddev,
        cv     => $cv,
        count  => $count,
    };
}

# Calculate linear regression (slope, intercept, R-squared) manually
# Input: reference to an array of [x, y] points, where x and y are numeric.
#        x is typically a time index (0, 1, 2,...), y is the metric value.
# Output: hashref { slope, intercept, r_squared, n_points } or undef if
#         insufficient data (n < 2) or if slope cannot be determined (e.g., all x values are identical).
sub calculate_manual_linear_regression
{
    my ($points_aref) = @_;
    my $n = scalar @{$points_aref};

    # Regression requires at least 2 distinct points to define a line.
    return undef if $n < 2;

    my $sum_x = 0;
    my $sum_y = 0;
    my $sum_xy = 0;
    my $sum_x_squared = 0;
    my $sum_y_squared = 0;

    foreach my $point (@{$points_aref})
    {
        my ($x_val, $y_val) = @{$point};

        # Assuming $x_val and $y_val are already numeric based on upstream processing.
        # If strict validation is needed here, it can be added, but the input
        # preparation logic should ensure numeric data.

        $sum_x += $x_val;
        $sum_y += $y_val;
        $sum_xy += $x_val * $y_val;
        $sum_x_squared += $x_val**2;
        $sum_y_squared += $y_val**2;
    }

    my $slope_calc     = undef;
    my $intercept_calc = undef;
    my $r_squared_calc = undef;

    # Denominator for slope calculation: N * sum(x^2) - (sum(x))^2
    # This is also N * SS_xx (where SS_xx is sum of squares for x)
    my $denominator_slope = ($n * $sum_x_squared) - ($sum_x**2);

    # Check if denominator is too close to zero (implies x values are not distinct enough
    # or only one unique x value if n > 1, which makes slope undefined or infinite).
    if (abs($denominator_slope) > $FLOAT_EPSILON)
    {
        $slope_calc = (($n * $sum_xy) - ($sum_x * $sum_y)) / $denominator_slope;
        $intercept_calc = ($sum_y - ($slope_calc * $sum_x)) / $n;

        # Calculate R-squared (Coefficient of Determination)
        # R^2 = (N * sum(xy) - sum(x) * sum(y))^2 / ((N * sum(x^2) - (sum(x))^2) * (N * sum(y^2) - (sum(y))^2))
        my $numerator_r_sq_squared = (($n * $sum_xy) - ($sum_x * $sum_y))**2;
        my $denominator_r_sq_part_yy = ($n * $sum_y_squared) - ($sum_y**2);

        if (abs($denominator_r_sq_part_yy) > $FLOAT_EPSILON) # Avoid division by zero if all y values are the same
        {
            $r_squared_calc = $numerator_r_sq_squared / ($denominator_slope * $denominator_r_sq_part_yy);
            # Clamp R-squared to [0, 1] due to potential floating point inaccuracies
            $r_squared_calc = 0.0 if defined $r_squared_calc && $r_squared_calc < 0;
            $r_squared_calc = 1.0 if defined $r_squared_calc && $r_squared_calc > 1.0;
        }
        elsif (abs($numerator_r_sq_squared) < $FLOAT_EPSILON**2) # All y are same, and slope is ~0
        {
            # If all y values are identical, the line should perfectly predict them if slope is also ~0.
            $r_squared_calc = 1.0;
        }
        else
        {
            # This case implies an issue or perfect vertical correlation if all x were same (but ss_xx_calc > 0 here)
            # If y is constant, variance of y is 0. If model doesn't perfectly predict this constant, R^2 can be odd.
            # Given y is constant, and slope is non-zero, it means the line isn't horizontal.
            # SS_tot would be 0. SS_res would be >0. R^2 = 1 - SS_res/SS_tot is undefined.
            # However, if $denominator_r_sq_part_yy is near zero, it implies SS_tot is near zero.
            # If the numerator $numerator_r_sq_squared is also near zero, it suggests the model fits perfectly (R^2 = 1).
            # If numerator is not zero but denominator_yy is, it's effectively a poor fit for variation that doesn't exist.
            $r_squared_calc = 0.0; # Default to 0 for poor/undefined fit in this edge scenario
        }
    }
    else
    {
        # Denominator for slope is zero or too small.
        # This happens if all x values are (nearly) identical.
        # Cannot reliably determine a linear trend.
        return undef;
    }

    return {
        slope     => $slope_calc,
        intercept => $intercept_calc,
        r_squared => $r_squared_calc,
        n_points  => $n,
    };
}

sub process_and_store_window_results
{
    my ($window_def_ref, $current_vm_win_buf_href, $vm_overall_win_data_href,
        $all_vm_names_aref, $target_vm_idx_nullable, $target_vm_name_nullable,
        $main_physc_percentile_arg, $calc_peak_arg, $has_runq_data_arg,
        $runq_norm_p_aref_arg, $runq_abs_p_aref_arg,
        $current_runq_avg_method_arg, $current_win_minutes_arg, $current_alpha_for_ema_arg_unused
    ) = @_;

    my ($win_start, $win_end, $win_rep_date) = @{$window_def_ref};
    my $win_idx = -1;
    for (my $i=0; $i < @processing_windows; $i++)
    {
        if ($processing_windows[$i][2]->epoch == $win_rep_date->epoch)
        {
            $win_idx = $i;
            last;
        }
    }
    return unless $win_idx != -1;
    my $window_key = $win_rep_date->ymd('') . "_" . $win_idx;

    my @vms_to_process_this_window_list;
    if (defined $target_vm_idx_nullable)
    {
        push @vms_to_process_this_window_list, $target_vm_name_nullable;
    }
    else
    {
        @vms_to_process_this_window_list = @{$all_vm_names_aref};
    }

    foreach my $vm_name_local (@vms_to_process_this_window_list) # Use local var
    {
        next unless (exists $current_vm_win_buf_href->{$vm_name_local} && exists $current_vm_win_buf_href->{$vm_name_local}{$window_key});
        my $vm_win_data = $current_vm_win_buf_href->{$vm_name_local}{$window_key};

        my $p_label_main = sprintf("P%.2f", $main_physc_percentile_arg);
        $p_label_main =~ s/\.?0+$//;
        $p_label_main = "0" if $p_label_main eq "" && abs($main_physc_percentile_arg-0)<0.001;

        if (exists $vm_win_data->{rolling_physc_avgs} && @{$vm_win_data->{rolling_physc_avgs}})
        {
            my @sorted_physc_avgs = sort {$a <=> $b} grep {defined $_ && $_ =~ /^-?[0-9.]+$/} @{$vm_win_data->{rolling_physc_avgs}};
            if (@sorted_physc_avgs)
            {
                my $physc_perc_val = calculate_percentile(\@sorted_physc_avgs, $main_physc_percentile_arg);
                $vm_overall_win_data_href->{$vm_name_local}{$window_key}{$p_label_main} = defined($physc_perc_val) ? $physc_perc_val : "N/A";
            }
            else
            {
                $vm_overall_win_data_href->{$vm_name_local}{$window_key}{$p_label_main} = "N/A";
            }
        }
        else
        {
            $vm_overall_win_data_href->{$vm_name_local}{$window_key}{$p_label_main} = "N/A";
        }

        if ($calc_peak_arg)
        {
            if (exists $vm_win_data->{raw_physc_for_peak} && @{$vm_win_data->{raw_physc_for_peak}})
            {
                my @defined_peaks = grep {defined $_ && $_ =~ /^-?[0-9.]+$/} @{$vm_win_data->{raw_physc_for_peak}};
                $vm_overall_win_data_href->{$vm_name_local}{$window_key}{'Peak'} = @defined_peaks ? max(@defined_peaks) : "N/A";
            }
            else
            {
                $vm_overall_win_data_href->{$vm_name_local}{$window_key}{'Peak'} = "N/A";
            }
        }
        if ($has_runq_data_arg)
        {
            my @abs_rq_for_perc_cleaned = grep {defined $_ && $_ =~ /^-?[0-9.]+$/} @{$vm_win_data->{abs_rq_values_for_perc}};
            foreach my $p_val (@{$runq_abs_p_aref_arg})
            {
                my $p_num_label = sprintf("%.2f", $p_val);
                $p_num_label =~ s/\.?0+$//;
                $p_num_label = "0" if $p_num_label eq "" && abs($p_val-0)<0.001;
                my $metric_key = "AbsRunQ_P$p_num_label";
                if (@abs_rq_for_perc_cleaned)
                {
                    my @sorted_vals = sort {$a <=> $b} @abs_rq_for_perc_cleaned;
                    my $val = calculate_percentile(\@sorted_vals, $p_val);
                    $vm_overall_win_data_href->{$vm_name_local}{$window_key}{$metric_key} = defined($val) ? sprintf("%.2f", $val) : "N/A";
                }
                else
                {
                    $vm_overall_win_data_href->{$vm_name_local}{$window_key}{$metric_key} = "N/A";
                }
            }
            my @norm_rq_for_perc_cleaned = grep {defined $_ && $_ =~ /^-?[0-9.]+$/} @{$vm_win_data->{norm_rq_values_for_perc}};
            foreach my $p_val (@{$runq_norm_p_aref_arg})
            {
                my $p_num_label = sprintf("%.2f", $p_val);
                $p_num_label =~ s/\.?0+$//;
                $p_num_label = "0" if $p_num_label eq "" && abs($p_val-0)<0.001;
                my $metric_key = "NormRunQ_P$p_num_label";
                if (@norm_rq_for_perc_cleaned)
                {
                    my @sorted_vals = sort {$a <=> $b} @norm_rq_for_perc_cleaned;
                    my $val = calculate_percentile(\@sorted_vals, $p_val);
                    $vm_overall_win_data_href->{$vm_name_local}{$window_key}{$metric_key} = defined($val) ? sprintf("%.2f", $val) : "N/A";
                }
                else
                {
                    $vm_overall_win_data_href->{$vm_name_local}{$window_key}{$metric_key} = "N/A";
                }
            }
        }
    }
}


# --- Existing Subroutines from Original Script ---
sub parse_percentile_list
{
    my ($perc_str, $arg_name) = @_;
    my @percentiles;
    if (defined $perc_str && $perc_str ne '')
    {
        my @raw_percentiles = split /,\s*/, $perc_str;
        foreach my $p (@raw_percentiles)
        {
            if ($p !~ /^[0-9]+(?:\.[0-9]+)?$/ || $p < 0 || $p > 100)
            {
                die "Error: Invalid percentile value '$p' in --$arg_name list. Must be numeric between 0 and 100.\n";
            }
            push @percentiles, $p + 0;
        }
    }
    return @percentiles;
}

sub get_nmon_overall_date_range
{
    my ($nmon_file, $global_start_filter_str, $global_end_filter_str) = @_;
    print STDERR "Scanning NMON file '$nmon_file' for overall effective date range...\n" if ($verbose);
    # Handle both compressed or uncompressed files.
    my $fh;
    if ($nmon_file =~ /\.nmon\.gz$/i) {
        open $fh, "-|", "gzip", "-dc", "--", $nmon_file or do {
            warn "Warning: Could not decompress '$nmon_file': $!. Skipping.";
            next;
        };
    }
    elsif ($nmon_file =~ /\.nmon\.(bz2|bzip2)$/i) {
        open $fh, "-|", "bzcat", "--", $nmon_file or do {
            warn "Warning: Could not bzcat '$nmon_file': $!. Skipping.";
            next;
        };
    }
    elsif ($nmon_file =~ /\.nmon$/i) {
        open $fh, '<:encoding(utf8)', $nmon_file or do {
            warn "Warning: Could not open '$nmon_file': $!. Skipping.";
            next;
        };
    }
    else {
        warn "Warning: Unsupported file type '$nmon_file'. Skipping.";
        next;
    }
    my $min_date_obj;
    my $max_date_obj;
    my $header_skipped = 0;
    my $first_data_line_checked = 0;
    my $start_filter_obj;
    my $end_filter_obj;
    if (defined $global_start_filter_str)
    {
        eval { $start_filter_obj = Time::Piece->strptime($global_start_filter_str, "%Y-%m-%d"); };
        if ($@ || (defined $global_start_filter_str && ! (defined $start_filter_obj && $start_filter_obj->isa('Time::Piece') ) ) )
        {
            die "Error parsing global start date filter '$global_start_filter_str': $@\n";
        }
    }
    if (defined $global_end_filter_str)
    {
        eval { $end_filter_obj = Time::Piece->strptime($global_end_filter_str, "%Y-%m-%d"); };
        if ($@ || (defined $global_end_filter_str && ! (defined $end_filter_obj && $end_filter_obj->isa('Time::Piece') ) ) )
        {
            die "Error parsing global end date filter '$global_end_filter_str': $@\n";
        }
    }
    while (my $line = <$fh>)
    {
        chomp $line;
        $line =~ s/\r$//;
        next if $line =~ /^\s*$/;
        if (!$header_skipped && !$first_data_line_checked)
        {
            if ($line =~ /^(Time,|Date,Time,Hostname,ZZZZ)/i)
            {
                $header_skipped = 1;
                next;
            }
            $first_data_line_checked = 1;
        }
        elsif ($header_skipped && $line =~ /^(Time,|Date,Time,Hostname,ZZZZ)/i)
        {
            next;
        }
        if ($line =~ /^(\d{4}-\d{2}-\d{2}[T\s]\d{2}:\d{2}:\d{2})/)
        {
            my $timestamp_str_local = $1;
            $timestamp_str_local =~ s/T/ /;
            my $current_tp;
            eval { $current_tp = Time::Piece->strptime($timestamp_str_local, "%Y-%m-%d %H:%M:%S"); };
            if ($@ || !$current_tp || !$current_tp->isa('Time::Piece'))
            {
                next;
            }
            my $current_date_day_obj = $current_tp->truncate(to => 'day');
            if (defined $start_filter_obj && $current_date_day_obj < $start_filter_obj) { next; }
            if (defined $end_filter_obj   && $current_date_day_obj > $end_filter_obj)   { next; }
            if (!defined $min_date_obj || !$min_date_obj->isa('Time::Piece') || $current_tp < $min_date_obj)
            {
                $min_date_obj = $current_tp;
            }
            if (!defined $max_date_obj || !$max_date_obj->isa('Time::Piece') || $current_tp > $max_date_obj)
            {
                $max_date_obj = $current_tp;
            }
        }
    }
    close $fh;
    if (defined $min_date_obj && $min_date_obj->isa('Time::Piece') && defined $max_date_obj && $max_date_obj->isa('Time::Piece'))
    {
        print STDERR "Effective NMON data range for windowing: " . $min_date_obj->datetime . " to " . $max_date_obj->datetime . "\n";
        my $ret_start = $min_date_obj->truncate(to => 'day');
        my $ret_end   = $max_date_obj->truncate(to => 'day');
        unless (defined $ret_start && $ret_start->isa('Time::Piece') && defined $ret_end && $ret_end->isa('Time::Piece'))
        {
            print STDERR "Error: Truncated date objects became invalid in get_nmon_overall_date_range.\n";
            return (undef, undef);
        }
        return ($ret_start, $ret_end);
    }
    else
    {
        print STDERR "Warning: Could not determine valid min/max dates from NMON file '$nmon_file' after global filters.\n";
        return (undef, undef);
    }
}

sub generate_processing_time_windows
{
    my ($period_start_obj, $period_end_obj, $unit_str, $size_val) = @_;
    my @windows;
    return () unless (defined $period_start_obj && defined $period_end_obj && $period_start_obj->isa('Time::Piece') && $period_end_obj->isa('Time::Piece') && $period_start_obj <= $period_end_obj);
    my $current_window_start = Time::Piece->new($period_start_obj->epoch);
    while ($current_window_start <= $period_end_obj)
    {
        my $current_window_end;
        if ($unit_str eq "days")
        {
            $current_window_end = Time::Piece->new($current_window_start->epoch) + (ONE_DAY() * ($size_val - 1));
        }
        elsif ($unit_str eq "weeks")
        {
            $current_window_end = Time::Piece->new($current_window_start->epoch) + (ONE_WEEK() * $size_val) - ONE_DAY();
        }
        else
        {
            die "Unsupported window unit: $unit_str\n";
        }
        if ($current_window_end > $period_end_obj)
        {
            $current_window_end = Time::Piece->new($period_end_obj->epoch);
        }
        my $representative_date = Time::Piece->new($current_window_end->epoch);
        push @windows, [Time::Piece->new($current_window_start->epoch), Time::Piece->new($current_window_end->epoch), $representative_date];
        # Move to the start of the next day for the next window
        $current_window_start = $current_window_end->truncate(to => 'day') + ONE_DAY;
    }
    return @windows;
}

sub get_window_key_for_timestamp
{
    my ($timestamp_obj, $windows_aref, $hint_idx) = @_;
    my $timestamp_day_obj = $timestamp_obj->truncate(to => 'day');
    if (defined $hint_idx && $hint_idx >=0 && $hint_idx < @{$windows_aref})
    {
        my ($win_start, $win_end) = @{$windows_aref->[$hint_idx]};
        if ($timestamp_day_obj >= $win_start && $timestamp_day_obj <= $win_end)
        {
            return $windows_aref->[$hint_idx][2]->ymd('') . "_" . $hint_idx;
        }
    }
    for (my $i=0; $i < @{$windows_aref}; $i++)
    {
        my ($win_start, $win_end) = @{$windows_aref->[$i]};
        if ($timestamp_day_obj >= $win_start && $timestamp_day_obj <= $win_end)
        {
            return $windows_aref->[$i][2]->ymd('') . "_" . $i;
        }
    }
    return undef;
}

sub get_weighted_metric_for_vm
{
    my ($vm_data_by_window_href, $vm_name_local, $metric_key, # Use local var
        $processing_windows_aref, $analysis_ref_obj_arg, $decay_hl_days_arg, $sprintf_fmt_optional) = @_;

    my @metric_window_values;
    if (exists $vm_data_by_window_href->{$vm_name_local})
    {
        foreach my $win_key (sort {
                my ($ad, $ai) = ($a =~ /^(\d{8})_(\d+)$/);
                my ($bd, $bi) = ($b =~ /^(\d{8})_(\d+)$/);
                return ($ad cmp $bd) || ($ai <=> $bi);
            } keys %{$vm_data_by_window_href->{$vm_name_local}})
        {
            if (exists $vm_data_by_window_href->{$vm_name_local}{$win_key}{$metric_key} &&
                defined $vm_data_by_window_href->{$vm_name_local}{$win_key}{$metric_key} &&
                $vm_data_by_window_href->{$vm_name_local}{$win_key}{$metric_key} ne "N/A" )
            {
                my ($win_date_str_part, $win_idx_part) = ($win_key =~ /^(\d{8})_(\d+)$/);
                if (defined $win_idx_part && $win_idx_part < @{$processing_windows_aref})
                {
                    my $rep_date_obj = $processing_windows_aref->[$win_idx_part][2];
                    unless (defined $rep_date_obj && $rep_date_obj->isa('Time::Piece'))
                    {
                        print STDERR "Warning: Representative date for window key '$win_key' is invalid for VM '$vm_name_local', metric '$metric_key'. Skipping this window point.\n";
                        next;
                    }
                    push @metric_window_values, {
                        value => $vm_data_by_window_href->{$vm_name_local}{$win_key}{$metric_key},
                        date  => $rep_date_obj
                    };
                }
                else
                {
                    print STDERR "Warning: Could not accurately map window key '$win_key' to a processing window definition for VM '$vm_name_local', metric '$metric_key'. Skipping this window point.\n";
                }
            }
        }
    }

    if (@metric_window_values)
    {
        my $final_val_str;
        if ($metric_key eq 'Peak') {
            # For the Peak, we want the absolute maximum value found across all windows.
            my @values = map { $_->{value} } grep { defined $_->{value} && looks_like_number($_->{value}) } @metric_window_values;
            $final_val_str = @values ? max(@values) : "N/A";
        } else {
            # For all other metrics, perform the recency-weighted average.
            $final_val_str = calculate_recency_weighted_average(
                \@metric_window_values, $analysis_ref_obj_arg, $decay_hl_days_arg
            );
        }
        if (defined $sprintf_fmt_optional && defined $final_val_str && $final_val_str ne "N/A" && $final_val_str =~ /^-?[0-9.]+$/)
        {
            return sprintf($sprintf_fmt_optional, $final_val_str + 0);
        }
        return $final_val_str;
    }
    return "N/A";
}

sub calculate_recency_weighted_average
{
    my ($windowed_data_ref, $analysis_ref_obj_arg, $half_life_days_arg) = @_;
    my $sum_weighted_values = 0;
    my $sum_weights = 0;
    return "N/A" if (!defined $analysis_ref_obj_arg || !$analysis_ref_obj_arg->isa('Time::Piece'));
    return "N/A" if (!defined $half_life_days_arg || $half_life_days_arg <= 0);

    my $lambda = log(2) / $half_life_days_arg;

    foreach my $dp_ref (@{$windowed_data_ref})
    {
        my $value_str = $dp_ref->{value};
        next if (!defined $value_str || $value_str eq "N/A" || $value_str !~ /^-?[0-9.]+$/);
        my $value = $value_str + 0;
        my $date_obj = $dp_ref->{date};
        next if (!defined $date_obj || !$date_obj->isa('Time::Piece'));

        my $date_obj_day = $date_obj->truncate(to => 'day');
        my $analysis_ref_day = $analysis_ref_obj_arg->truncate(to => 'day');

        my $days_diff_seconds = $analysis_ref_day->epoch - $date_obj_day->epoch;
        my $days_diff = $days_diff_seconds / ONE_DAY(); # Use seconds from Time::Seconds object
        $days_diff = 0 if $days_diff < 0;

        my $weight = exp(-$lambda * $days_diff);
        $sum_weighted_values += $value * $weight;
        $sum_weights += $weight;
    }

    if ($sum_weights > $FLOAT_EPSILON)
    {
        return sprintf("%.4f", $sum_weighted_values / $sum_weights);
    }
    else
    {
        return "N/A";
    }
}

sub get_vm_index_by_name
{
    my ($vm_name_to_find, $vm_names_list_ref) = @_;
    for (my $i=0; $i < @{$vm_names_list_ref}; $i++)
    {
        if ($vm_names_list_ref->[$i] eq $vm_name_to_find)
        {
            return $i;
        }
    }
    return undef;
}

sub calculate_average
{
    my (@data) = @_;
    my @numbers = grep { defined $_ && looks_like_number($_) } @data;
    return 0 if !@numbers;
    return sum0(@numbers) / scalar(@numbers);
}

sub calculate_percentile
{
    my ($data_ref, $p) = @_;
    my @data_input = @{$data_ref};
    my @data = grep { defined($_) && $_ =~ /^-?[0-9]+(?:\.[0-9]+)?$/ } @data_input;
    my $n = scalar @data;
    return undef if $n == 0;
    @data = sort { $a <=> $b } @data;
    return $data[0] if $n == 1;

    my $rank_fractional = ($p / 100) * ($n - 1);
    my $k = int($rank_fractional);
    my $d = $rank_fractional - $k;

    if ($p == 0) { return $data[0]; }
    if ($p == 100) { return $data[$n-1]; }

    if ($k >= $n - 1)
    {
        return $data[$n - 1];
    }
    elsif ($k < 0)
    {
        return $data[0];
    }
    else
    {
        my $val_k = $data[$k];
        my $val_k_plus_1 = ($k + 1 < $n) ? $data[$k + 1] : $data[$k];
        return $val_k + $d * ($val_k_plus_1 - $val_k);
    }
}

sub calculate_rolling_average_series
{
    my ($data_series_ref, $method, $window_size, $alpha_val) = @_;
    my @output_series;
    my @current_window_sma;
    my $current_ema;

    if ($method eq 'none')
    {
        return $data_series_ref;
    }

    foreach my $value (@{$data_series_ref})
    {
        my $avg_val_to_add = undef;
        if ($method eq 'sma')
        {
            if (defined $value && $value =~ /^-?[0-9.]+$/)
            {
                push @current_window_sma, ($value + 0);
            }
            else
            {
                push @current_window_sma, undef;
            }
            shift @current_window_sma while scalar @current_window_sma > $window_size;
            if (scalar @current_window_sma == $window_size)
            {
                $avg_val_to_add = calculate_average(@current_window_sma);
            }
        }
        elsif ($method eq 'ema')
        {
            if (defined $value && $value =~ /^-?[0-9.]+$/)
            {
                my $numeric_value = $value + 0;
                if (!defined $current_ema)
                {
                    $current_ema = $numeric_value;
                }
                else
                {
                    $current_ema = ($numeric_value * $alpha_val) + ($current_ema * (1 - $alpha_val));
                }
            }
            push @current_window_sma, $value;
            shift @current_window_sma while scalar @current_window_sma > $window_size;
            if (scalar @current_window_sma == $window_size)
            {
                $avg_val_to_add = $current_ema;
            }
        }
        push @output_series, $avg_val_to_add if defined $avg_val_to_add;
    }
    return \@output_series;
}

sub apply_rounding
{
    my ($value, $increment, $method) = @_;
    return $value unless (defined $value && $value =~ /^-?[0-9]+(?:\.[0-9]+)?$/);
    return $value if $method eq 'none' || !defined $increment || $increment <= $FLOAT_EPSILON;

    my $rounded_value;
    if ($method eq 'standard')
    {
        $rounded_value = int( ($value / $increment) + ( ($value >= 0) ? 0.5 : -0.5) ) * $increment;
    }
    elsif ($method eq 'up')
    {
        $rounded_value = ceil( $value / $increment ) * $increment;
    }
    else
    {
        $rounded_value = $value;
    }
    return $rounded_value;
}

sub get_decimal_places
{
    my ($number_str) = @_;
    $number_str = sprintf("%.15f", $number_str) if ($number_str =~ /e/i);
    if ($number_str =~ /\.(\d+)$/)
    {
        return length($1);
    }
    else
    {
        return 0;
    }
}

# --- define_time_windows_for_state ---
# Subdivides a configuration state window into smaller, regular time-based
# windows for decay and growth analysis.
#
sub define_time_windows_for_state {
    my ($state_obj, $ref_date_str, $unit, $size) = @_;
    my @windows;
    my $start = $state_obj->{start_time};
    my $end   = $state_obj->{end_time};

    my $current_win_start = $start->truncate(to => 'day');

    while ($current_win_start < $end) {
        my $current_win_end;
        if ($unit eq 'days') {
            $current_win_end = $current_win_start + ($size * ONE_DAY()) - 1;
        } else { # weeks
            $current_win_end = $current_win_start + ($size * ONE_WEEK()) - 1;
        }

        $current_win_end = $end if $current_win_end > $end;

        push @windows, {
            start => $current_win_start,
            end   => $current_win_end,
            rep_date => $current_win_end, # Representative date for weighting is the end of the window
        };

        $current_win_start = $current_win_end->truncate(to => 'day') + ONE_DAY();
    }
    return @windows;
}

# --- calculate_metrics_for_period ---
# A generic routine to calculate all relevant metrics (PhysC, RunQ, Peak)
# for a given set of data points and a specific configuration state.
#
sub calculate_metrics_for_period {
    my ($physc_avgs_ref, $peak_physc_val, $norm_runq_raw_ref, $abs_runq_raw_ref) = @_;
    my %metrics;

    # --- Optimised PhysC Percentile Calculation ---
    if (@$physc_avgs_ref) {
        # Sort the rolling averages array only ONCE.
        my @sorted_physc_avgs = sort {$a <=> $b} grep {defined $_ && looks_like_number($_)} @$physc_avgs_ref;
        my @final_physc_to_process = @sorted_physc_avgs;

        # If the filter is active, create a sorted slice without re-sorting.
        if (defined $filter_above_perc_value && @sorted_physc_avgs) {
            my $ft = calculate_percentile(\@sorted_physc_avgs, $filter_above_perc_value);
            if (defined $ft) {
                @final_physc_to_process = grep { $_ >= ($ft - $FLOAT_EPSILON) } @sorted_physc_avgs;
            }
        }

        # Calculate the final percentile on the (already sorted) final list.
        my $p_val_physc = @final_physc_to_process ? calculate_percentile(\@final_physc_to_process, $percentile) : undef;
        $metrics{"P".clean_perc_label($percentile)} = $p_val_physc;
    }

    # --- Peak and RunQ Calculations (already efficient) ---
    if ($calculate_peak) {
        $metrics{'Peak'} = ($peak_physc_val > 0) ? $peak_physc_val : undef;
    }

    # The RunQ logic now operates on the pre-populated arrays.
    my $norm_data_ref = \@$norm_runq_raw_ref;
    my $abs_data_ref = \@$abs_runq_raw_ref;
    if ($runq_avg_method_str ne 'none') {
        $norm_data_ref = calculate_rolling_average_series($norm_data_ref, $runq_avg_method_str, $window_minutes, $alpha_for_runq_ema);
        $abs_data_ref = calculate_rolling_average_series($abs_data_ref, $runq_avg_method_str, $window_minutes, $alpha_for_runq_ema);
    }

    my @sorted_norm = sort {$a <=> $b} @$norm_data_ref;
    my @sorted_abs = sort {$a <=> $b} @$abs_data_ref;
    foreach my $p (@runq_norm_percentiles_to_calc) { $metrics{"NormRunQ_P".clean_perc_label($p)} = @sorted_norm ? calculate_percentile(\@sorted_norm, $p) : undef; }
    foreach my $p (@runq_abs_percentiles_to_calc) { $metrics{"AbsRunQ_P".clean_perc_label($p)} = @sorted_abs ? calculate_percentile(\@sorted_abs, $p) : undef; }

    return \%metrics;
}

sub generate_run_summary_string {
    my @summary_lines;
    my $label_width = 25;

    push @summary_lines, "\n------------------------------------------------------------------";
    push @summary_lines, "Run Summary";
    push @summary_lines, "------------------------------------------------------------------";

    # Input Method
    my $input_method = defined($nmon_dir) ? "--nmondir ($nmon_dir)" : "--physc-data ($physc_csv_file)";
    push @summary_lines, sprintf("%-".$label_width."s: %s", "Input Method", $input_method);

    # --- NEW: Add Date Period to summary if applicable ---
    if (defined $start_date_str && defined $end_date_str) {
        push @summary_lines, sprintf("%-".$label_width."s: %s to %s", "Date Period", $start_date_str, $end_date_str);
    }

    # Primary Metric
    my $p_label = sprintf("P%.2f", $percentile);
    $p_label =~ s/\.?0+$//;
    my $primary_metric = sprintf("%s (Percentile: %.2f)", $p_label, $percentile);
    push @summary_lines, sprintf("%-".$label_width."s: %s", "Primary Metric", $primary_metric);

    # Averaging
    my $avg_details = uc($avg_method);
    $avg_details .= sprintf(" (Window: %d min, PhysC Decay: %s)", $window_minutes, $decay_level) if $avg_method eq 'ema';
    $avg_details .= sprintf(" (Window: %d min)", $window_minutes) if $avg_method eq 'sma';
    push @summary_lines, sprintf("%-".$label_width."s: %s", "Averaging Method", $avg_details);

    # RunQ
    if (defined($runq_csv_file) || defined($nmon_dir)) {
        my $runq_decay_str = ($runq_avg_method_str eq 'ema') ? ", Decay: $runq_decay_level_to_use" : "";
        my $runq_details = sprintf("Enabled (SMT: %d, Avg Method: %s%s)", $smt_value, $runq_avg_method_str, $runq_decay_str);
        push @summary_lines, sprintf("%-".$label_width."s: %s", "RunQ Analysis", $runq_details);

        my $norm_perc_str = $runq_norm_perc_str eq '' ? 'none' : $runq_norm_perc_str;
        my $abs_perc_str = $runq_abs_perc_str eq '' ? 'none' : $runq_abs_perc_str;
        push @summary_lines, sprintf("%-".$label_width."s: Norm(%s), Abs(%s)", "RunQ Percentiles", $norm_perc_str, $abs_perc_str);
    }

    # Filters
    my @filters;
    push @filters, "Time ($time_filter_desc)" if $time_filter_active;
    push @filters, "Weekends Excluded" if $no_weekends;
    if (defined $filter_above_perc_value) {
        my $perc_filter_label = sprintf("P%.2f", $filter_above_perc_value);
        $perc_filter_label =~ s/\.?0+$//;
        push @filters, "PhysC Perc (>= $perc_filter_label)";
    }
    push @summary_lines, sprintf("%-".$label_width."s: %s", "Filters Applied", join(', ', @filters)) if @filters;

    # Decay / Growth / Analysis Model
    my $analysis_model_str;
    if ($enable_windowed_decay) {
        $analysis_model_str = "Time-Based Windowed Decay ON";
    } elsif ($decay_over_states_flag) {
        $analysis_model_str = "Hybrid State-Time Decay ON";
    } else {
        $analysis_model_str = "State-Based (Decay OFF)";
    }

    my $growth_status = $enable_growth_prediction ? "Growth Prediction ON" : "Growth Prediction OFF";
    push @summary_lines, sprintf("%-".$label_width."s: %s", "Analysis Model", $analysis_model_str);
    push @summary_lines, sprintf("%-".$label_width."s: %s", "Growth Prediction", $growth_status);
    push @summary_lines, sprintf("%-".$label_width."s: %s", "Reference Date", $analysis_reference_date_str) if (defined $analysis_reference_date_str);

    return join("\n", @summary_lines) . "\n";
}

sub get_window_index_for_timestamp {
    my ($timestamp_obj, $windows_aref) = @_;
    my $ts_epoch = $timestamp_obj->epoch;
    for (my $i=0; $i < @{$windows_aref}; $i++) {
        my ($win_start, $win_end) = @{$windows_aref->[$i]};
        return $i if ($ts_epoch >= $win_start->epoch && $ts_epoch <= $win_end->epoch);
    }
    return undef;
}


sub parse_csv_line {
    my ($line) = @_;
    my @fields;

    while (length($line)) {
        if ($line =~ s/^"((?:[^"]|"")*)"[,]*//) {
            my $field = $1;
            $field =~ s/""/"/g;  # unescape double quotes
            push @fields, $field;
        } elsif ($line =~ s/^([^",]*)[,]*//) {
            push @fields, $1;
        } else {
            # fallback (shouldn't happen with consistent CSV)
            last;
        }
    }

    return @fields;
}

# ==============================================================================
# Analyses the buffered data for a single VM using the time-based windowed decay model.
# FIX: This version now correctly processes the data buffer to calculate rolling averages
# and other metrics before passing them to the final calculation function.
# ==============================================================================
sub process_vm_buffer {
    my ($vm_buffer_aref, $vm_name, $results_agg_href, $states_by_vm, $windows_aref) = @_;

    # --- Time filter logic ---
    # This block ensures that granular time filters (--no-weekends, --online, etc.)
    # are correctly applied to the data before any windowed analysis occurs.
    my @filtered_vm_buffer;
    if ($no_weekends || $time_filter_active) {
        foreach my $dp (@$vm_buffer_aref) {
            if ($no_weekends) {
                # Sunday is 1, Saturday is 7 in Time::Piece
                my $day_of_week = $dp->{tp}->day_of_week;
                next if ($day_of_week == 1 || $day_of_week == 7);
            }
            if ($time_filter_active) {
                my $line_time = substr($dp->{tp}->hms(':'), 0, 5);
                my $include_line = 0;
                if ($time_filter_overnight) {
                    $include_line = 1 if ($line_time ge $time_filter_start || $line_time lt $time_filter_end);
                } else {
                    $include_line = 1 if ($line_time ge $time_filter_start && $line_time lt $time_filter_end);
                }
                next unless $include_line;
            }
            push @filtered_vm_buffer, $dp;
        }
    } else {
        @filtered_vm_buffer = @$vm_buffer_aref;
    }

    print STDERR "INFO: Processing " . scalar(@$vm_buffer_aref) . " data points for VM '$vm_name' (Windowed Decay Mode)...\n" if $verbose;

    my @vm_states = @{$states_by_vm->{$vm_name} || []};
    # Use the newly filtered buffer for all subsequent logic.
    return unless @vm_states && @filtered_vm_buffer > 0;

    my $vm_start_obj = $filtered_vm_buffer[0]->{tp};
    my $vm_end_obj   = $filtered_vm_buffer[-1]->{tp};

    my @vm_processing_windows = generate_processing_time_windows(
        $vm_start_obj,
        $vm_end_obj,
        $process_window_unit_str,
        $process_window_size_val
    );

    return unless @vm_processing_windows;

    my %window_data_groups;
    # Use the filtered buffer to group data into windows.
    foreach my $dp (@filtered_vm_buffer) {
        my $win_idx = get_window_index_for_timestamp($dp->{tp}, \@vm_processing_windows);
        next unless defined $win_idx;
        push @{$window_data_groups{$win_idx}}, $dp;
    }

    foreach my $win_idx (sort { $a <=> $b } keys %window_data_groups) {
        my @window_buffer = @{$window_data_groups{$win_idx}};
        next unless @window_buffer;

        my $sample_tp = $window_buffer[0]->{tp};
        my ($current_state) = grep { $sample_tp->epoch >= $_->{start_epoch} && $sample_tp->epoch <= $_->{end_epoch} } @vm_states;
        next unless $current_state;

        # --- START: Added single-pass processing logic ---
        # This logic was missing and is the source of the bug. It now correctly
        # calculates rolling averages and gathers raw data before metric calculation.
        my @physc_avgs;
        my ($physc_sma_q, $physc_ema_s) = ([], undef);
        my $peak_physc = 0;
        my (@norm_runq_raw, @abs_runq_raw);
        my $smt_for_period = $current_state->{metadata}{smt} // $DEFAULT_SMT;

        foreach my $dp (@window_buffer) {
            my $avg = calculate_rolling_average($dp->{physc}, $avg_method, $physc_sma_q, \$physc_ema_s, $window_minutes, $alpha_for_physc_ema);
            push @physc_avgs, $avg if defined $avg;

            $peak_physc = $dp->{physc} if (defined $dp->{physc} && $dp->{physc} > $peak_physc);

            if (defined $dp->{runq} && defined $dp->{physc} && $dp->{physc} >= $ACTIVE_PHYSC_THRESHOLD) {
                push @abs_runq_raw, $dp->{runq};
                my $eff_lcpus = $dp->{physc} * $smt_for_period;
                if ($eff_lcpus > $FLOAT_EPSILON) {
                    push @norm_runq_raw, ($dp->{runq} / $eff_lcpus);
                }
            }
        }

        my $metrics = calculate_metrics_for_period(
            \@physc_avgs,
            $peak_physc,
            \@norm_runq_raw,
            \@abs_runq_raw
        );
        # --- END: Added single-pass processing logic ---

        my $rep_date = $vm_processing_windows[$win_idx][2];
        my $win_key = $rep_date->ymd('') . "_" . $win_idx;
        $results_agg_href->{$vm_name}{'time_windows'}{$win_key} = { metrics => $metrics, date_obj => $rep_date };
    }
}

## --- BEGIN L2 RESULTS CACHE SUBROUTINES --- ##

# ==============================================================================
# Subroutine to generate a canonical key from script arguments for caching.
# ==============================================================================
sub generate_canonical_key
{
    my @key_parts;

    # This subroutine should be called AFTER GetOptions has populated the variables.
    # It systematically checks all options that affect the output.

    # Input Method
    push @key_parts, "--nmondir $nmon_dir" if defined $nmon_dir;
    push @key_parts, "--physc-data $physc_csv_file" if defined $physc_csv_file;
    push @key_parts, "--runq-data $runq_csv_file" if defined $runq_csv_file;
    push @key_parts, "--config $vm_config_file" if defined $vm_config_file;

    # State Management
    push @key_parts, "--include-states $include_states_selector" if (defined $include_states_selector and lc($include_states_selector) ne 'all');

    # Averaging and Decay
    push @key_parts, "--avg-method $avg_method" if defined $avg_method;
    push @key_parts, "--decay $decay_level" if defined $decay_level;
    push @key_parts, "--runq-decay $runq_decay_level_arg" if defined $runq_decay_level_arg;
    push @key_parts, "--window $window_minutes" if defined $window_minutes;

    # RunQ Options
    push @key_parts, "--smt $smt_value" if defined $smt_value;
    push @key_parts, "--runq-norm-perc '$runq_norm_perc_str'" if defined $runq_norm_perc_str;
    push @key_parts, "--runq-abs-perc '$runq_abs_perc_str'" if defined $runq_abs_perc_str;
    push @key_parts, "--runq-avg-method $runq_avg_method_str" if defined $runq_avg_method_str;

    # Filtering
    push @key_parts, "--startdate $start_date_str" if defined $start_date_str;
    push @key_parts, "--enddate $end_date_str" if defined $end_date_str;
    push @key_parts, "--startt $start_time_str" if defined $start_time_str;
    push @key_parts, "--endt $end_time_str" if defined $end_time_str;
    push @key_parts, "--online" if $online_flag;
    push @key_parts, "--batch" if $batch_flag;
    push @key_parts, "--no-weekends" if $no_weekends;
    push @key_parts, "--vm $target_vm_name" if defined $target_vm_name;
    push @key_parts, "--filter-above-perc $filter_above_perc_value" if defined $filter_above_perc_value;

    # PhysC Calculation
    push @key_parts, "--percentile $percentile" if defined $percentile;
    push @key_parts, "--peak" if $calculate_peak;

    # Windowed Recency Decay
    if ($enable_windowed_decay)
    {
        push @key_parts, "--enable-windowed-decay";
        push @key_parts, "--process-window-unit $process_window_unit_str";
        push @key_parts, "--process-window-size $process_window_size_val";
        push @key_parts, "--decay-half-life-days $decay_half_life_days_val";
    }

    # Decay over states
    push @key_parts, "--decay-over-states" if ($decay_over_states_flag);

    # Analysis reference date
    push @key_parts, "--analysis-reference-date $analysis_reference_date_str" if defined $analysis_reference_date_str;

    # Growth Prediction
    if ($enable_growth_prediction)
    {
        push @key_parts, "--enable-growth-prediction";
        push @key_parts, "--growth-projection-days $growth_projection_days";
        push @key_parts, "--max-growth-inflation-percent $max_growth_inflation_percent";
    }

    # Rounding
    push @key_parts, "-r" . (defined $round_arg && length $round_arg ? "=$round_arg" : "") if defined $round_arg;
    push @key_parts, "-u" . (defined $roundup_arg && length $roundup_arg ? "=$roundup_arg" : "") if defined $roundup_arg;

    # Normalise by sorting and joining.
    my $canonical_key = join(" ", sort @key_parts);
    return $canonical_key;
}

# ==============================================================================
# Subroutine to look up a result from the L2 cache file.
# Returns: An array reference of result hashes, or undef if not found.
# ==============================================================================
sub lookup_cached_result
{
    my ($key, $cache_file) = @_;

    return undef unless (-f $cache_file && -s $cache_file);

    # Read the entire file content as a single string.
    my $json_text = do {
        open my $fh, '<:encoding(utf8)', $cache_file or do {
            warn "Warning: Could not open results cache '$cache_file' for reading: $!";
            return undef;
        };
        local $/;
        my $content = <$fh>;
        close $fh;
        $content;
    };

    # Decode the single JSON object from the file.
    my $json_decoder = JSON->new->allow_nonref;
    my $cache_data = eval { decode_json($json_text) };
    if ($@ || !defined($cache_data) || ref($cache_data) ne 'HASH') {
        print STDERR "Warning: Results cache '$cache_file' is invalid. Resetting.\n";
        unlink $cache_file or warn "Warning: Could not remove corrupt cache file '$cache_file': $!";
        return undef;
    }

    # The value is now an array reference of hashes.
    return $cache_data->{$key};
}

# ==============================================================================
# Subroutine to save a result to the L2 cache file under an exclusive lock.
# ==============================================================================
sub save_result_to_cache
{
    my ($key, $result_objects_aref, $cache_file, $lock_file) = @_;

    # This subroutine now performs a safe read-modify-write of the entire JSON cache file.
    # Acquire an exclusive lock to prevent race conditions when writing.
    open my $lock_fh, '>', $lock_file or do {
        warn "Warning: Could not create lock file '$lock_file' for writing cache: $!. Skipping cache write.";
        return;
    };
    flock($lock_fh, LOCK_EX) or do {
        warn "Warning: Could not acquire exclusive lock on '$lock_file': $!. Skipping cache write.";
        close $lock_fh;
        return;
    };
    # Read the existing cache file into a Perl hash.
    my $cache_data = {};
    if (-f $cache_file && -s $cache_file) {
        my $json_text = do {
            open my $read_fh, '<:encoding(utf8)', $cache_file;
            local $/;
            <$read_fh>;
        };
        my $json_decoder = JSON->new->allow_nonref;
        my $decoded = eval { $json_decoder->decode($json_text) };
        # If the file is valid JSON hash, use it. Otherwise, start fresh.
        $cache_data = $decoded if (ref($decoded) eq 'HASH');
    }

    # Add or update the result for the current key with the array of result objects.
    $cache_data->{$key} = $result_objects_aref;

    # Encode the entire updated hash back to a pretty-printed JSON string.
    my $json_encoder = JSON->new->pretty->canonical;
    my $output_json = $json_encoder->encode($cache_data);

    # Overwrite the cache file with the new content.
    open my $write_fh, '>:encoding(utf8)', $cache_file or do {
        warn "Warning: Could not open results cache '$cache_file' for writing: $!";
        close $lock_fh;
        return;
    };

    print $write_fh $output_json;
    close $write_fh;
    close $lock_fh; # Releases the lock automatically on close.
}

# --- Helper subroutine for the non-windowed path ---
sub process_vm_buffer_non_windowed {
    my ($vm_buffer_aref, $vm_name, $results_agg_href, $states_for_vm_aref) = @_;

    return unless @$vm_buffer_aref && @$states_for_vm_aref;
    print STDERR "    - Finalising " . scalar(@$states_for_vm_aref) . " State(s) for VM $vm_name...\n";

    my %state_data_buffers;
    # Group all data points from the buffer into their correct state
    foreach my $dp (@$vm_buffer_aref) {
        foreach my $state (@$states_for_vm_aref) {
            if ($dp->{tp}->epoch >= $state->{start_epoch} && $dp->{tp}->epoch <= $state->{end_epoch}) {
                push @{$state_data_buffers{$state->{state_id}}}, $dp;
                last; # Found the correct state for this datapoint, move to next datapoint
            }
        }
    }

    # Now, calculate metrics for each state that had data
    foreach my $state_id (sort { $a <=> $b } keys %state_data_buffers) {
        my ($state_obj) = grep { $_->{state_id} == $state_id } @$states_for_vm_aref;
        next unless $state_obj;

        my @data_for_this_state = @{$state_data_buffers{$state_id}};

        # Apply time filters (online, batch, no-weekends) here
        my @filtered_data_for_state;
        if ($no_weekends || $time_filter_active) {
            foreach my $dp (@data_for_this_state) {
                if ($no_weekends) {
                    my $day_of_week = $dp->{tp}->day_of_week;
                    next if ($day_of_week == 1 || $day_of_week == 7);
                }
                if ($time_filter_active) {
                    my $line_time = substr($dp->{tp}->hms(':'), 0, 5);
                    my $include_line = 0;
                    if ($time_filter_overnight) {
                        $include_line = 1 if ($line_time ge $time_filter_start || $line_time lt $time_filter_end);
                    } else {
                        $include_line = 1 if ($line_time ge $time_filter_start && $line_time lt $time_filter_end);
                    }
                    next unless $include_line;
                }
                push @filtered_data_for_state, $dp;
            }
        } else {
            @filtered_data_for_state = @data_for_this_state;
        }

        # Calculate metrics using the filtered data
        my $metrics = calculate_metrics_for_period(\@filtered_data_for_state, $state_obj->{metadata});
        push @{$results_agg_href->{$vm_name}{'states'}}, { metrics => $metrics, state_obj => $state_obj };
    }
}

# Subroutine for the Hybrid State-Time Decay Model
sub synthesize_hybrid_timeseries {
    my ($state_results_aref, $metric_key) = @_;
    my @timeseries_points;

    foreach my $state_result (@$state_results_aref) {
        my $start_obj = $state_result->{state_obj}{start_time}->truncate(to => 'day');
        my $end_obj   = $state_result->{state_obj}{end_time}->truncate(to => 'day');
        my $metric_val = $state_result->{metrics}{$metric_key};

        # Skip if this state didn't have a valid metric
        next unless defined $metric_val && looks_like_number($metric_val);

        my $current_day = $start_obj;
        while ($current_day <= $end_obj) {
            # Add a data point for every day the state was active
            push @timeseries_points, {
                value => $metric_val,
                date  => Time::Piece->new($current_day->epoch) # Use a copy
            };
            $current_day += ONE_DAY;
        }
    }
    return @timeseries_points;
}

# ==============================================================================
# SUBROUTINE: get_metrics_for_each_state
# PURPOSE:    A modular subroutine for the non-windowed and hybrid analysis paths.
#             It takes a VM's buffered data and its known states, and returns
#             an array containing the calculated metrics for each state,
#             now including clipping detection metrics if enabled.
# ARGS:
#   1. $vm_buffer_aref (array ref): A buffer of all data points for a single VM.
#   2. $vm_name (string): The name of the VM being processed.
#   3. $states_for_vm_aref (array ref): The array of state objects for this VM.
# RETURNS:
#   - An array of hashes, where each hash is { metrics => \%metrics, state_obj => $state }
# ==============================================================================
sub get_metrics_for_each_state
{
    my ($vm_buffer_aref, $vm_name, $states_for_vm_aref) = @_;

    my @state_results; # Array to hold the results for each state

    return () unless @$vm_buffer_aref && @$states_for_vm_aref;
    my $count = scalar(@$states_for_vm_aref);
    my $label = $count == 1 ? 'state' : 'states';
    print STDERR "    - Profiling performance of $count configuration $label for VM $vm_name...\n" unless (-t STDERR);

    my %state_data_buffers;
    # Group all data points from the buffer into their correct state
    foreach my $dp (@$vm_buffer_aref)
    {
        foreach my $state (@$states_for_vm_aref)
        {
            if ($dp->{tp}->epoch >= $state->{start_epoch} && $dp->{tp}->epoch <= $state->{end_epoch})
            {
                push @{$state_data_buffers{$state->{state_id}}}, $dp;
                last; # Found the correct state, move to the next data point
            }
        }
    }

    # Now, calculate metrics for each state that had data
    foreach my $state_id (sort { $a <=> $b } keys %state_data_buffers)
    {
        my ($state_obj) = grep { $_->{state_id} == $state_id } @$states_for_vm_aref;
        next unless $state_obj;

        my @data_for_this_state = @{$state_data_buffers{$state_id}};

        # Apply time filters (online, batch, no-weekends)
        my @filtered_data_for_state;
        if ($no_weekends || $time_filter_active)
        {
            foreach my $dp (@data_for_this_state)
            {
                if ($no_weekends)
                {
                    my $day_of_week = $dp->{tp}->day_of_week;
                    next if ($day_of_week == 1 || $day_of_week == 7);
                }
                if ($time_filter_active)
                {
                    my $line_time = substr($dp->{tp}->hms(':'), 0, 5);
                    my $include_line = 0;
                    if ($time_filter_overnight)
                    {
                        $include_line = 1 if ($line_time ge $time_filter_start || $line_time lt $time_filter_end);
                    }
                    else
                    {
                        $include_line = 1 if ($line_time ge $time_filter_start && $line_time lt $time_filter_end);
                    }
                    next unless $include_line;
                }
                push @filtered_data_for_state, $dp;
            }
        }
        else
        {
            @filtered_data_for_state = @data_for_this_state;
        }

        next unless @filtered_data_for_state;

        # --- Start of: single-pass data processing ---
        my @physc_avgs;
        my ($physc_sma_q, $physc_ema_s) = ([], undef);
        my $peak_physc = 0;
        my (@norm_runq_raw, @abs_runq_raw);

        # NEW: Buffers for raw data for clipping detection
        my (@raw_physc_for_clipping, @raw_runq_for_clipping);

        my $smt_for_period = $state_obj->{metadata}{smt} // $DEFAULT_SMT;

        # This single loop calculates rolling averages, finds the peak, and populates RunQ arrays.
        foreach my $dp (@filtered_data_for_state) {
            # Calculate PhysC Rolling Average
            my $avg = calculate_rolling_average($dp->{physc}, $avg_method, $physc_sma_q, \$physc_ema_s, $window_minutes, $alpha_for_physc_ema);
            push @physc_avgs, $avg if defined $avg;

            # Find Peak PhysC
            $peak_physc = $dp->{physc} if (defined $dp->{physc} && $dp->{physc} > $peak_physc);

            # NEW: Buffer raw values for the clipping detection engine
            push @raw_physc_for_clipping, $dp->{physc} if (defined $dp->{physc});
            push @raw_runq_for_clipping, $dp->{runq} if (defined $dp->{runq});

            # Populate RunQ arrays if applicable
            if (defined $dp->{runq} && defined $dp->{physc} && $dp->{physc} >= $ACTIVE_PHYSC_THRESHOLD) {
                push @abs_runq_raw, $dp->{runq};
                my $eff_lcpus = $dp->{physc} * $smt_for_period;
                if ($eff_lcpus > $FLOAT_EPSILON) {
                    push @norm_runq_raw, ($dp->{runq} / $eff_lcpus);
                }
            }
        }

        # Pass the pre-processed arrays to the refactored calculation function.
        my $metrics = calculate_metrics_for_period(
            \@physc_avgs,
            $peak_physc,
            \@norm_runq_raw,
            \@abs_runq_raw
        );

        # --- NEW: Clipping Detection Integration ---
        # If the feature is enabled, call the new detection engine.
        if ($enable_clipping_detection) {
            my $max_cap = $state_obj->{metadata}{max_capacity};
            my $clipping_results_href = _calculate_clipping_metrics(
                \@raw_physc_for_clipping,
                \@raw_runq_for_clipping,
                $max_cap,
                $smt_for_period
            );

            # Merge the clipping results into the main metrics hash.
            # Use clear, simple keys for the final output string.
            $metrics->{IsClipped}       = $clipping_results_href->{isClipped};
            $metrics->{ClipConfidence}  = $clipping_results_href->{clippingConfidence};
            $metrics->{UnmetDemand}     = $clipping_results_href->{unmetDemandEstimate};
            $metrics->{RunQSatScore}    = $clipping_results_href->{platformMarkers}{aix_runq_saturation} // 'N/A';
        }
        # --- End of Clipping Integration ---

        push @state_results, { metrics => $metrics, state_obj => $state_obj };
    }

    return @state_results;
}


# ==============================================================================
# SUBROUTINE: calculate_growth_prediction
# PURPOSE:    Performs trend analysis on a time series of metric values and
#             calculates a growth adjustment factor based on a linear
#             regression projection. This function is a vital part of the
#             engine's forecasting capability.
#
# ARGS:
#   1. $timeseries_values_aref (array ref): An array of the numeric metric
#                                           values over time (e.g., from each
#                                           processing window).
#   2. $baseline_val (float): The final aggregated metric value, which serves
#                             as the base for any calculated inflation.
#
# RETURNS:
#   - A hash reference containing the results and a detailed rationale:
#     {
#       growth_adj     => The final calculated growth adjustment value (float),
#       growth_adj_abs => The absolute value of the adjustment (float),
#       rationale      => A hash ref with debug info for logging by consumers
#                         like nfit-profile.pl.
#     }
# ==============================================================================
sub calculate_growth_prediction
{
    my ($timeseries_values_aref, $baseline_val) = @_;

    # --- Pre-declare all rationale variables to ensure correct scope ---
    # Initialise to default values to handle cases where the logic exits early.
    my ($growth_adj_value, $growth_adj_abs_value) = (0.0, 0.0);
    my ($stats, $cv_val, $regression, $slope, $projected_val, $inflation_perc);
    my $num_hist_periods = 0;
    # Use explicit strings for check results to provide clearer rationale.
    my $cv_check_passed = "Skipped";
    my $slope_check_passed = "Skipped";
    my $was_capped = "N/A";

    # Growth calculation is only possible if there is a positive baseline to inflate.
    return {
        growth_adj     => 0.0,
        growth_adj_abs => 0.0,
        rationale      => { num_hist_periods => 0, stats_cv => "N/A", cv_check_passed => "Skipped (No baseline)" }
    } unless (defined $baseline_val && looks_like_number($baseline_val) && $baseline_val > $FLOAT_EPSILON);

    my @timeseries_points;
    my $time_idx = 0;

    # Create [x, y] points for regression, where x is the time index (0, 1, 2...).
    foreach my $metric_val (@$timeseries_values_aref)
    {
        if (defined $metric_val && looks_like_number($metric_val))
        {
            push @timeseries_points, [$time_idx, $metric_val];
            $time_idx++;
        }
    }

    $num_hist_periods = scalar(@timeseries_points);

    # Ensure there are enough data points to establish a meaningful trend.
    if ($num_hist_periods >= $GROWTH_MIN_HISTORICAL_PERIODS)
    {
        my @values_for_stats = map { $_->[1] } @timeseries_points;
        $stats = calculate_statistics_for_trend(\@values_for_stats);

        # Only proceed if statistics could be calculated.
        if (defined $stats && defined $stats->{cv})
        {
            $cv_val = $stats->{cv};
            # Check if the data is not excessively volatile.
            $cv_check_passed = ($cv_val < $GROWTH_MAX_CV_THRESHOLD) ? 1 : 0;

            if ($cv_check_passed)
            {
                $regression = calculate_manual_linear_regression(\@timeseries_points);
                if (defined $regression && defined $regression->{slope})
                {
                    $slope = $regression->{slope};
                    # Check if there is a clear positive growth trend.
                    $slope_check_passed = ($slope > $GROWTH_MIN_POSITIVE_SLOPE_THRESHOLD) ? 1 : 0;

                    if ($slope_check_passed)
                    {
                        # Project the value N days into the future, scaled by the window size.
                        my $projection_periods = ($process_window_unit_str eq 'weeks') ? ($growth_projection_days / 7) * (1 / $process_window_size_val) : ($growth_projection_days / $process_window_size_val);

                        # The Hybrid model provides daily data points, so its projection period is simpler.
                        if (scalar(@$timeseries_values_aref) > $num_hist_periods * 1.5) # Heuristic for daily data
                        {
                            $projection_periods = $growth_projection_days;
                        }

                        my $intercept = $regression->{intercept}; # Retrieve intercept from regression result
                        $projected_val = $slope * ($num_hist_periods - 1 + $projection_periods) + $intercept;

                        if ($projected_val > $baseline_val)
                        {
                            $inflation_perc = (($projected_val - $baseline_val) / $baseline_val) * 100;
                            $was_capped = ($inflation_perc > $max_growth_inflation_percent) ? 1 : 0;

                            # Cap the inflation to the configured maximum percentage.
                            if ($was_capped) {
                                $inflation_perc = $max_growth_inflation_percent;
                            }

                            # Calculate the final adjustment values.
                            $growth_adj_value = ($baseline_val * $inflation_perc / 100);
                            $growth_adj_abs_value = $growth_adj_value;
                        }
                    }
                }
            }
        }
    }

    # --- Assemble the final rationale hash ---
    # All variables used here were pre-declared, ensuring this is always safe to execute.
    my %growth_rationale = (
        num_hist_periods => $num_hist_periods,
        stats_cv         => defined($cv_val) ? sprintf("%.4f", $cv_val) : "N/A",
        cv_check_passed  => $cv_check_passed,
        intercept        => defined($regression->{intercept}) ? sprintf("%.4f", $regression->{intercept}) : "N/A",
        slope            => defined($slope) ? sprintf("%.4f", $slope) : "N/A",
        slope_check_passed => $slope_check_passed,
        projected_val    => defined($projected_val) ? sprintf("%.4f", $projected_val) : "N/A",
        inflation_perc   => defined($inflation_perc) ? sprintf("%.2f", $inflation_perc) : "N/A",
        was_capped       => $was_capped,
    );

    # --- Return the final result as a single hash reference ---
    return {
        growth_adj     => $growth_adj_value,
        growth_adj_abs => $growth_adj_abs_value,
        rationale      => \%growth_rationale
    };
}

# ==============================================================================
# SUBROUTINE: _calculate_clipping_metrics
# PURPOSE:    Implements the Multi-Stage Confidence Pipeline to detect CPU
#             clipping and estimate latent demand from a buffer of raw
#             performance data.
# ARGS:
#   1. $raw_physc_aref (array ref): A buffer of raw, high-frequency PhysC values.
#   2. $raw_runq_aref (array ref): A buffer of raw, high-frequency RunQ values.
#   3. $max_capacity (numeric): The authoritative max_capacity for the state.
#   4. $smt (numeric): The SMT value for the state, for RunQ context.
# RETURNS:
#   - A hash reference containing the clipping analysis results:
#     {
#       isClipped => 0|1,
#       clippingConfidence => 0.0-1.0,
#       clippingSeverity => 'none'|'low'|'moderate'|'high',
#       unmetDemandEstimate => 0.0,
#       platformMarkers => { aix_runq_saturation => 0.0-1.0 }
#     }
# ==============================================================================
sub _calculate_clipping_metrics
{
    my ($raw_physc_aref, $raw_runq_aref, $max_capacity, $smt) = @_;

    my %results = (
        isClipped           => 0,
        clippingConfidence  => 0.0,
        clippingSeverity    => 'none',
        unmetDemandEstimate => 0.0,
        platformMarkers     => {},
    );

    # The pipeline requires a valid capacity limit and sufficient data points.
    return \%results unless (defined $max_capacity && $max_capacity > 0.1 && @$raw_physc_aref > 10);

    # --- Stage I & II: Raw Detection & Shape Analysis ---
    # To perform the analysis, we need several key percentiles from the raw data.
    # We calculate them once here for efficiency.
    my @sorted_physc = sort { $a <=> $b } @$raw_physc_aref;
    my $p99_9 = calculate_percentile(\@sorted_physc, 99.9);
    my $p95   = calculate_percentile(\@sorted_physc, 95);
    my $p90   = calculate_percentile(\@sorted_physc, 90);
    my $p50   = calculate_percentile(\@sorted_physc, 50);

    return \%results unless (defined $p99_9 && defined $p95 && defined $p50);

    my $saturation_level = $p99_9 / $max_capacity;

    # Calculate Peak Curvature Ratio (PCR)
    my $pcr_numerator   = $p99_9 - $p95;
    my $pcr_denominator = $p95 - $p50;
    my $pcr = ($pcr_denominator > $FLOAT_EPSILON) ? ($pcr_numerator / $pcr_denominator) : 999;

    # --- Stage III: Platform Validation (AIX RunQ) ---
    my $runq_saturation_score = 0.0;
    if (@$raw_runq_aref) {
        my @sorted_runq = sort { $a <=> $b } @$raw_runq_aref;
        my $runq_p95 = calculate_percentile(\@sorted_runq, 95);

        # A simple model for RunQ saturation: the ratio of P95 RunQ to the number
        # of logical CPUs (SMT). A score > 1.0 indicates queuing.
        if (defined $runq_p95 && defined $smt && $smt > 0) {
            $runq_saturation_score = $runq_p95 / $smt;
            $results{platformMarkers}{aix_runq_saturation} = sprintf("%.2f", $runq_saturation_score);
        }
    }

    # --- Stage IV: Final Clipping Confidence Score ---
    my $confidence = 0.0;

    # Base confidence from saturation level
    if ($saturation_level >= $CLIPPING_DEFINITE_THRESHOLD) {
        $confidence += 0.6; # High base confidence for definite saturation
    } elsif ($saturation_level >= $CLIPPING_POTENTIAL_THRESHOLD) {
        $confidence += 0.3; # Moderate base confidence for potential saturation
    }

    # Add confidence from shape analysis (low PCR)
    if ($pcr < $PCR_HIGH_CONFIDENCE_THRESHOLD) {
        $confidence += 0.35; # High confidence for very flat peaks
    } elsif ($pcr < $PCR_LOW_CONFIDENCE_THRESHOLD) {
        $confidence += 0.15; # Moderate confidence for somewhat flat peaks
    }

    # Boost confidence if platform markers confirm pressure
    if ($runq_saturation_score > 1.5) {
        $confidence += 0.2; # Significant boost for high RunQ
    }

    # Cap confidence at 1.0
    $results{clippingConfidence} = min(1.0, $confidence);

    # --- Latent Demand Estimation ---
    if ($results{clippingConfidence} > 0.65) {
        $results{isClipped} = 1;

        # Estimate unmet demand as a function of confidence and saturation severity.
        # This is a simple model: the more confident we are, and the closer to the
        # ceiling the workload is, the higher the estimated latent demand.
        my $severity_factor = ($saturation_level - $CLIPPING_POTENTIAL_THRESHOLD) * 10; # Normalise 0.9-1.0 range
        $severity_factor = max(0, min(1.0, $severity_factor));

        # The unmet demand is a fraction of the P90 value, scaled by our confidence and severity.
        $results{unmetDemandEstimate} = $p90 * $results{clippingConfidence} * $severity_factor * 0.25; # 0.25 is a tuning factor

        # Determine severity string for reporting
        if ($results{clippingConfidence} > 0.9) {
            $results{clippingSeverity} = 'high';
        } elsif ($results{clippingConfidence} > 0.75) {
            $results{clippingSeverity} = 'moderate';
        } else {
            $results{clippingSeverity} = 'low';
        }
    }

    return \%results;
}

sub aggregate_decay_metrics {
    my ($results_agg_href, $vm_name, $analysis_ref_obj, $decay_hl_days_val) = @_;
    my %final_metrics;

    if ($enable_windowed_decay) {
        # Aggregate base metrics for Standard Windowed Decay mode
        my %all_keys;
        foreach my $win_result (values %{$results_agg_href->{$vm_name}{time_windows}}) {
            foreach my $key (keys %{$win_result->{metrics}}) { $all_keys{$key} = 1; }
        }
        foreach my $metric_key (sort keys %all_keys) {
            my @metric_values_across_windows;
            foreach my $win_result (values %{$results_agg_href->{$vm_name}{time_windows}}) {
                if (defined $win_result->{metrics}{$metric_key}) {
                    push @metric_values_across_windows, { value => $win_result->{metrics}{$metric_key}, date => $win_result->{date_obj} };
                }
            }
            if ($metric_key ne 'Peak') {
                $final_metrics{$metric_key} = calculate_recency_weighted_average(\@metric_values_across_windows, $analysis_ref_obj, $decay_hl_days_val);
            } else {
                my @values = map { $_->{value} } grep { defined $_->{value} && looks_like_number($_->{value}) } @metric_values_across_windows;
                $final_metrics{$metric_key} = @values ? max(@values) : "N/A";
            }
        }
    } elsif ($decay_over_states_flag) {
        # Populate base metrics for the Hybrid State-Time Decay mode
        my $p_label = "P" . clean_perc_label($percentile);
        $final_metrics{$p_label} = $results_agg_href->{$vm_name}{'hybrid_metric'} // "N/A";

        my $source_states_aref = $results_agg_href->{$vm_name}{'source_states'};
        if ($calculate_peak) {
            my @all_peaks;
            if ($source_states_aref && @$source_states_aref) {
                foreach my $state_res (@$source_states_aref) {
                    if (defined $state_res->{metrics}{'Peak'}) {
                        push @all_peaks, $state_res->{metrics}{'Peak'};
                    }
                }
            }
            $final_metrics{'Peak'} = @all_peaks ? max(@all_peaks) : "N/A";
        }
        if ($source_states_aref && @$source_states_aref) {
            my $last_state_metrics = $source_states_aref->[-1]{metrics};
            foreach my $p (@runq_norm_percentiles_to_calc) { my $key = "NormRunQ_P".clean_perc_label($p); $final_metrics{$key} = $last_state_metrics->{$key} // "N/A"; }
            foreach my $p (@runq_abs_percentiles_to_calc)  { my $key = "AbsRunQ_P".clean_perc_label($p);  $final_metrics{$key} = $last_state_metrics->{$key} // "N/A"; }
        }
    }
    return \%final_metrics;
}

sub apply_growth_prediction_to_metrics {
    my ($results_agg_href, $vm_name, $final_metrics_href) = @_;

    my $p_label_main = "P" . clean_perc_label($percentile);
    my @timeseries_for_growth;

    if ($enable_windowed_decay) {
        # For standard decay, build the time series from the per-window results.
        my @sorted_windows = sort { $a->{date_obj}->epoch <=> $b->{date_obj}->epoch } values %{$results_agg_href->{$vm_name}{time_windows}};
        foreach my $win_result (@sorted_windows) {
            push @timeseries_for_growth, $win_result->{metrics}{$p_label_main};
        }
    } elsif ($decay_over_states_flag && exists $results_agg_href->{$vm_name}{'hybrid_timeseries_data'}) {
        my @source_states_for_growth = @{$results_agg_href->{$vm_name}{'source_states'} || []};
        my @hybrid_timeseries_for_growth = synthesize_hybrid_timeseries(\@source_states_for_growth, $p_label_main);
        @timeseries_for_growth = map { $_->{value} } @hybrid_timeseries_for_growth;
    }

    my $growth_result = calculate_growth_prediction(\@timeseries_for_growth, $final_metrics_href->{$p_label_main});
    if (defined $growth_result && ref($growth_result) eq 'HASH') {
        my $growth_adj = $growth_result->{growth_adj};
        # Apply the adjustments to the final metrics hash.
        $final_metrics_href->{"GrowthAdj"} = $growth_adj;
        $final_metrics_href->{"GrowthAdjAbs"} = $growth_result->{growth_adj_abs};
        if (defined $final_metrics_href->{$p_label_main} && looks_like_number($final_metrics_href->{$p_label_main})) {
            $final_metrics_href->{$p_label_main} += $growth_adj;
        }

        if (defined $growth_result->{rationale}) {
            foreach my $key (keys %{$growth_result->{rationale}}) {
                $final_metrics_href->{"GrowthDebug_$key"} = $growth_result->{rationale}{$key};
            }
            $final_metrics_href->{"GrowthParam_projection_days"} = $growth_projection_days;
            $final_metrics_href->{"GrowthParam_max_inflation_perc"} = $max_growth_inflation_percent;
        }
    }
}

# --- usage ---
# Generates and returns the usage/help message for the script.
#
sub usage
{
    my $script_name = $0;
    $script_name =~ s{.*/}{};
    my $valid_decays_usage = join("|", sort keys %EMA_ALPHAS);
    return <<END_USAGE;
Usage: $script_name --nmondir <cache_directory> [options]
   or: $script_name --mgsys <serial_number> [options]

Analyses performance data from a pre-built nFit cache directory. This tool
calculates key metrics based on percentile, rolling average, and optional
recency-weighted decay models to provide right-sizing recommendations.

Cache Location (provide one):
  --nmondir <directory>    : The full path to a valid nFit cache directory
                             (e.g., /path/to/stage/12345ABC).
  --mgsys <serial>         : The serial number of the managed system to analyse.
                             If --nmondir is not specified, this will default to
                             looking for a cache at './stage/<serial>/'.

Averaging Method (used within each window if decay enabled):
  --avg-method <method>    : Averaging method for PhysC: 'sma' or 'ema'. (Default: $DEFAULT_AVG_METHOD)
  --decay <level>          : If 'ema' for PhysC, specifies decay level: $valid_decays_usage.
                           (Default: $DEFAULT_DECAY_LEVEL).
  --runq-decay <level>     : Optional. If 'ema' for RunQ, specifies its decay level.
  -w, --window <minutes>     : Window for SMA/EMA calculations. (Default: $DEFAULT_WINDOW_MINUTES min).

RunQ Data Options:
  --smt <N>                : SMT level for RunQ normalisation (Default: $DEFAULT_SMT).
                             Note: This is a fallback; SMT from the cache is preferred.
  --runq-norm-perc <list>  : Comma-separated percentiles for Normalised RunQ.
  --runq-abs-perc <list>   : Comma-separated percentiles for Absolute RunQ.
  --runq-avg-method <none|sma|ema> : Averaging method for RunQ data.

Filtering Options (Applied BEFORE windowing if decay enabled):
  -s, --startdate <YYYY-MM-DD> : Ignore data before this date.
  -ed, --enddate <YYYY-MM-DD>  : Ignore data after this date.
  -startt <HH:MM> / -endt <HH:MM> : Daily time filter.
  -online / -batch           : Shortcut daily time filters.
  -no-weekends               : Exclude data from Saturdays and Sundays.
  -vm, --lpar <name>         : Analyse only the specified VM/LPAR name(s), comma-separated.

PhysC Calculation Options:
  -p, --percentile <value>   : Final percentile of PhysC (0-100) (Default: $DEFAULT_PERCENTILE).
  -k, --peak                 : Calculate peak PhysC value.
  --filter-above-perc <N>    : Optional. Filter rolling PhysC values before PXX calc.

Windowed Recency Decay & Growth Prediction:
  --enable-windowed-decay    : Enable internal windowed processing with recency decay.
  --decay-over-states        : Enable Hybrid State-Time Decay Model.
  --enable-growth-prediction : Enable growth prediction (requires a decay model).
  (See documentation for more detail on decay and growth parameters)

Rounding Options:
  -r[=increment] / -u[=increment] : Round results (Default increment: $DEFAULT_ROUND_INCREMENT).

Other:
  -h, --help                 : Display this help message.
  -v, --verbose              : Enable verbose output for debugging.
  --version                  : Display script version.
END_USAGE
}
