#!/usr/bin/env perl

# NAME     : nfit
# AUTHOR   : NiÃ«l Lambrechts (https://github.com/niel-lambrechts)
# PURPOSE  : Analyse NMON PhysC and optionally RunQ data for AIX and Linux on Power
#            VM right-sizing recommendations. Calculates rolling average (SMA or EMA)
#            percentiles, optionally absolute peaks, and specified percentiles of
#            normalised and absolute run-queue statistics.
#            RunQ data can now also be smoothed using SMA or EMA before percentile calculation.
#            If windowed decay is enabled, metrics are calculated per window,
#            weighted by recency, and then aggregated.
#            NEW: Can predict future growth based on windowed data trends.
#            Supports filtering by date (start and end), time, VM, weekends,
#            and percentile threshold, plus rounding options.
# REQUIRES : Perl, Time::Piece, POSIX (for ceil), List::Util (for sum0, max, min), Getopt::Long, File::Temp, version, File::Spec

use strict;
use warnings;
use Getopt::Long qw(GetOptions);
use File::Temp qw(tempfile tempdir);
use List::Util qw(sum0 max min);
use Scalar::Util qw(looks_like_number);
use POSIX qw(ceil);
use Time::Piece;
use Time::Seconds;
use version;
use File::Spec;
use File::Find;
use List::MoreUtils qw(uniq);
use JSON;
use Fcntl qw(:flock);
use File::Basename qw(dirname);

# --- Version ---
my $VERSION = '5.25.185.1';

# --- Configuration ---
my $DEFAULT_AVG_METHOD     = 'ema';
my $DEFAULT_DECAY_LEVEL    = 'medium';
my $DEFAULT_WINDOW_MINUTES = 15;
my $DEFAULT_PERCENTILE     = 95;
my $DEFAULT_ROUND_INCREMENT= 0.05;
my $DEFAULT_SMT            = 8;
my $DEFAULT_RUNQ_NORM_PERC = "50,90";
my $DEFAULT_RUNQ_ABS_PERC  = "90";
my $DEFAULT_RUNQ_AVG_METHOD = "ema";

my $FLOAT_EPSILON          = 1e-9; # Small number for float comparisons
my $ACTIVE_PHYSC_THRESHOLD = 0.15; # Original threshold for RunQ normalization
my $SAFETY_MARGIN_FOR_THRESHOLD_CONST = 1.05; # Safety margin for ACTIVE_PHYSC_THRESHOLD calculation
my $PLACEHOLDER_ENTITLEMENT_ADJUSTMENT_FACTOR = 1.0; # Placeholder in ACTIVE_PHYSC_THRESHOLD calc

# Windowed Decay Defaults (if enabled for nfit itself)
my $DEFAULT_PROCESS_WINDOW_UNIT = "weeks";
my $DEFAULT_PROCESS_WINDOW_SIZE = 1;
my $DEFAULT_DECAY_HALF_LIFE_DAYS = 30;

# EMA Alpha values based on decay level
my %EMA_ALPHAS = (
    'low'        => 0.03,
    'medium'     => 0.08,
    'high'       => 0.15,
    'very-high'  => 0.30,
    'extreme'    => 0.40,
);

# --- Growth Prediction Configuration (NEW) ---
my $DEFAULT_GROWTH_PROJECTION_DAYS         = 90;
my $DEFAULT_MAX_GROWTH_INFLATION_PERCENT   = 25;

# --- Cache Configuration ---
my $CACHE_MANIFEST_FILE = ".nfit.cache.manifest";
my $CACHE_STATES_FILE   = ".nfit.cache.states";
my $CACHE_DATA_FILE     = ".nfit.cache.data";
my $CACHE_LOCK_FILE     = ".nfit.cache.lock";

# Internal constants for growth heuristics (not user-configurable initially)
my $GROWTH_MIN_HISTORICAL_PERIODS       = 5;    # Min number of windowed periods to attempt trend
my $GROWTH_MAX_CV_THRESHOLD             = 0.50; # Max Coefficient of Variation (StdDev/Mean); if > this, data too volatile
my $GROWTH_MIN_POSITIVE_SLOPE_THRESHOLD = 0.01; # Min slope (units/period) to consider as actual growth for inflation
my $GROWTH_MAX_PROJECTION_HISTORY_RATIO = 2.0;  # Max ratio of projection duration to history duration used for trend

# --- Argument Parsing ---
my $physc_csv_file;
my $physc_csv_dirname;
my $runq_csv_file;
my $vm_config_file;
my $nmon_dir;
my $avg_method     = $DEFAULT_AVG_METHOD;
my $decay_level    = $DEFAULT_DECAY_LEVEL;
my $runq_decay_level_arg = undef;
my $window_minutes = $DEFAULT_WINDOW_MINUTES;
my $percentile     = $DEFAULT_PERCENTILE;
my $start_date_str;
my $end_date_str;
my $calculate_peak = 0;
my $round_arg;
my $roundup_arg;
my $start_time_str;
my $end_time_str;
my $online_flag = 0;
my $batch_flag  = 0;
my $target_vm_name;
my $no_weekends = 0;
my $filter_above_perc_value = undef;
my $smt_value = $DEFAULT_SMT;
my $runq_norm_perc_str = $DEFAULT_RUNQ_NORM_PERC;
my $runq_abs_perc_str  = $DEFAULT_RUNQ_ABS_PERC;
my $runq_avg_method_str = $DEFAULT_RUNQ_AVG_METHOD;
my $help           = 0;
my $show_version   = 0;
my $verbose   = 0;
my $quiet   = 0;

# Windowed Decay options
my $enable_windowed_decay_internal = 0;
my $process_window_unit_str = $DEFAULT_PROCESS_WINDOW_UNIT;
my $process_window_size_val = $DEFAULT_PROCESS_WINDOW_SIZE;
my $decay_half_life_days_val = $DEFAULT_DECAY_HALF_LIFE_DAYS;
my $analysis_reference_date_str;
my $decay_over_states_flag = 0;

# Growth Prediction options (NEW)
my $enable_growth_prediction = 0;
my $growth_projection_days = $DEFAULT_GROWTH_PROJECTION_DAYS;
my $max_growth_inflation_percent = $DEFAULT_MAX_GROWTH_INFLATION_PERCENT;

my $show_states_flag = 0;
my $include_states_selector = 'all'; # Default value
my $reset_cache = 0;

GetOptions(
    'physc-data|pc=s'     => \$physc_csv_file,
    'runq-data|rq=s'      => \$runq_csv_file,
    'config=s'            => \$vm_config_file,
    'nmondir=s'           => \$nmon_dir,
    'avg-method=s'        => \$avg_method,
    'decay=s'             => \$decay_level,
    'runq-decay=s'        => \$runq_decay_level_arg,
    'window|w=i'          => \$window_minutes,
    'percentile|p=f'      => \$percentile,
    'startdate|s=s'       => \$start_date_str,
    'enddate|ed=s'        => \$end_date_str,
    'peak|k'              => \$calculate_peak,
    'round|r:f'           => \$round_arg,
    'roundup|u:f'         => \$roundup_arg,
    'startt=s'            => \$start_time_str,
    'endt=s'              => \$end_time_str,
    'online'              => \$online_flag,
    'batch'               => \$batch_flag,
    'vms|vm|lpar|lpars=s' => \$target_vm_name,
    'no-weekends'         => \$no_weekends,
    'filter-above-perc=f' => \$filter_above_perc_value,
    'smt=i'               => \$smt_value,
    'runq-norm-perc=s'    => \$runq_norm_perc_str,
    'runq-abs-perc=s'     => \$runq_abs_perc_str,
    'runq-avg-method=s'   => \$runq_avg_method_str,
    # Windowed Decay Options
    'enable-windowed-decay'     => \$enable_windowed_decay_internal,
    'process-window-unit=s'     => \$process_window_unit_str,
    'process-window-size=i'     => \$process_window_size_val,
    'decay-half-life-days=i'    => \$decay_half_life_days_val,
    'analysis-reference-date=s' => \$analysis_reference_date_str,
    'decay-over-states'         => \$decay_over_states_flag,
    # Growth Prediction Options (NEW)
    'enable-growth-prediction'       => \$enable_growth_prediction,
    'growth-projection-days=i'       => \$growth_projection_days,
    'max-growth-inflation-percent=i' => \$max_growth_inflation_percent,
    'show-states'               => \$show_states_flag,
    'include-states=s'          => \$include_states_selector,
    'reset-cache'               => \$reset_cache,
    # General Options
    'help|h'              => \$help,
    'verbose|v'           => \$verbose,
    'version'             => \$show_version,
    'q|quiet'             => \$quiet,
) or die usage();

# --- Validation ---
if ($show_version)
{
    print STDERR "nfit version $VERSION\n";
    exit 0;
}

if ($show_states_flag)
{
    # The --include-states argument might be present with its default value.
    # We only error if the user explicitly provided a different value.
    if (defined $include_states_selector and lc($include_states_selector) ne 'all')
    {
        die "Error: --show-states and --include-states are mutually exclusive.\n";
    }
}

if ( ($show_states_flag or (defined $include_states_selector and lc($include_states_selector) ne 'all')) and !$nmon_dir )
{
    die "Error: --show-states and --include-states can only be used with the --nmondir option.\n";
}

if ($help || (!$physc_csv_file && !$nmon_dir))
{
    print STDERR usage();
    exit 0;
}

if ($nmon_dir)
{
    if ($physc_csv_file || $runq_csv_file)
    {
        die "Error: --nmondir cannot be used with --physc-data or --runq-data.\n";
    }
    unless (-d $nmon_dir)
    {
        die "Error: Provided --nmondir path is not a valid directory: $nmon_dir\n";
    }
}

$avg_method = lc($avg_method);
if ($avg_method ne 'sma' && $avg_method ne 'ema')
{
    die "Error: --avg-method must be 'sma' or 'ema'. Got '$avg_method'.\n";
}

$decay_level = lc($decay_level);
unless (exists $EMA_ALPHAS{$decay_level})
{
    my $valid_decays = join(", ", sort keys %EMA_ALPHAS);
    die "Error: --decay level '$decay_level' is invalid. Valid levels are: $valid_decays.\n";
}
my $alpha_for_physc_ema = $EMA_ALPHAS{$decay_level};

my $runq_decay_level_to_use = $decay_level;
my $runq_decay_source_for_log = "$decay_level (from PhysC decay setting)";
if (defined $runq_decay_level_arg)
{
    $runq_decay_level_arg = lc($runq_decay_level_arg);
    unless (exists $EMA_ALPHAS{$runq_decay_level_arg})
    {
        my $valid_decays = join(", ", sort keys %EMA_ALPHAS);
        die "Error: --runq-decay level '$runq_decay_level_arg' is invalid. Valid levels are: $valid_decays.\n";
    }
    $runq_decay_level_to_use = $runq_decay_level_arg;
    $runq_decay_source_for_log = "$runq_decay_level_arg (from --runq-decay)";
}
my $alpha_for_runq_ema = $EMA_ALPHAS{$runq_decay_level_to_use};

$runq_avg_method_str = lc($runq_avg_method_str);
if ($runq_avg_method_str ne 'none' && $runq_avg_method_str ne 'sma' && $runq_avg_method_str ne 'ema')
{
    die "Error: --runq-avg-method must be 'none', 'sma', or 'ema'. Got '$runq_avg_method_str'.\n";
}

if ($physc_csv_file)
{
    die "Error: PhysC data file (--physc-data) not found: $physc_csv_file\n" if (! -f $physc_csv_file);
    $physc_csv_dirname = dirname($physc_csv_file);
}

if (defined $runq_csv_file && ! -f $runq_csv_file)
{
    die "Error: RunQ data file (--runq-data) not found: $runq_csv_file\n";
}

if ($smt_value <= 0)
{
    die "Error: --smt value must be a positive integer.\n";
}
if ($window_minutes < 1)
{
    die "Error: Window size (-w) for SMA/EMA must be at least 1 minute.\n";
}
if ($percentile < 0 || $percentile > 100)
{
    die "Error: Percentile (-p) must be between 0 and 100.\n";
}
if (defined $start_date_str && $start_date_str !~ /^\d{4}-\d{2}-\d{2}$/)
{
    die "Error: Invalid startdate (-s) format '$start_date_str'. Use YYYY-MM-DD.\n";
}
if (defined $end_date_str && $end_date_str !~ /^\d{4}-\d{2}-\d{2}$/)
{
    die "Error: Invalid enddate (-ed) format '$end_date_str'. Use YYYY-MM-DD.\n";
}
if (defined $start_date_str && defined $end_date_str)
{
    my ($s_tp_val, $e_tp_val);
    eval { $s_tp_val = Time::Piece->strptime($start_date_str, "%Y-%m-%d"); };
    if ($@ || (defined $start_date_str && !$s_tp_val) )
    {
        die "Error parsing startdate '$start_date_str': $@\n";
    }
    eval { $e_tp_val = Time::Piece->strptime($end_date_str, "%Y-%m-%d"); };
    if ($@ || (defined $end_date_str && !$e_tp_val) )
    {
        die "Error parsing enddate '$end_date_str': $@\n";
    }
    if ($s_tp_val && $e_tp_val && $e_tp_val < $s_tp_val)
    {
        die "Error: --enddate ($end_date_str) cannot be before --startdate ($start_date_str).\n";
    }
}
if (defined($round_arg) && defined($roundup_arg))
{
    die "Error: -round (-r) and -roundup (-u) options are mutually exclusive.\n";
}
if ($enable_windowed_decay_internal)
{
    if ($process_window_unit_str ne "days" && $process_window_unit_str ne "weeks")
    {
        die "Error: --process-window-unit must be 'days' or 'weeks'. Got '$process_window_unit_str'.\n";
    }
    if ($process_window_size_val < 1)
    {
        die "Error: --process-window-size must be at least 1. Got '$process_window_size_val'.\n";
    }
    if ($decay_half_life_days_val < 1)
    {
        die "Error: --decay-half-life-days must be at least 1. Got '$decay_half_life_days_val'.\n";
    }
    if (defined $analysis_reference_date_str && $analysis_reference_date_str !~ /^\d{4}-\d{2}-\d{2}$/)
    {
        die "Error: Invalid --analysis-reference-date format '$analysis_reference_date_str'. Use YYYY-MM-DD.\n";
    }
}

# Growth Prediction Option Validation (NEW)
if ($enable_growth_prediction)
{
    # Growth prediction now works with EITHER standard windowed decay OR the new hybrid model.
    unless ($enable_windowed_decay_internal || $decay_over_states_flag)
    {
        print STDERR "Warning: --enable-growth-prediction requires either --enable-windowed-decay or --decay-over-states to be active. Growth prediction will be SKIPPED.\n";
        $enable_growth_prediction = 0;
    }

    if ($growth_projection_days < 1)
    {
        die "Error: --growth-projection-days must be at least 1.\n";
    }
    if ($max_growth_inflation_percent < 0 || $max_growth_inflation_percent > 200) # Cap inflation percentage
    {
        die "Error: --max-growth-inflation-percent must be between 0 and 200.\n";
    }
}

my @runq_norm_percentiles_to_calc = parse_percentile_list($runq_norm_perc_str, "runq-norm-perc");
my @runq_abs_percentiles_to_calc  = parse_percentile_list($runq_abs_perc_str,  "runq-abs-perc");

my $rounding_method = 'none';
my $round_increment = undef;
my $output_dp = 4;
if (defined $round_arg)
{
    $rounding_method = 'standard';
    $round_increment = (length $round_arg && $round_arg =~ /^[0-9.]*$/) ? $round_arg : $DEFAULT_ROUND_INCREMENT;
    print STDERR "Applying standard rounding to nearest $round_increment\n";
}
elsif (defined $roundup_arg)
{
    $rounding_method = 'up';
    $round_increment = (length $roundup_arg && $roundup_arg =~ /^[0-9.]*$/) ? $roundup_arg : $DEFAULT_ROUND_INCREMENT;
    print STDERR "Applying ceiling rounding up to nearest $round_increment\n";
}
if ($rounding_method ne 'none')
{
    if (!defined $round_increment || $round_increment <= $FLOAT_EPSILON)
    {
        die "Error: Rounding increment must be positive (got '$round_increment').\n";
    }
    $output_dp = get_decimal_places($round_increment);
}

my $time_filter_active = 0;
my $time_filter_start = undef;
my $time_filter_end = undef;
my $time_filter_overnight = 0;
my $time_filter_desc = "";
if ($online_flag)
{
    $time_filter_active = 1;
    $time_filter_start = "08:00";
    $time_filter_end = "17:00";
    $time_filter_desc = "ONLINE ($time_filter_start <= time < $time_filter_end)";
}
elsif ($batch_flag)
{
    $time_filter_active = 1;
    $time_filter_start = "18:00";
    $time_filter_end = "06:00";
    $time_filter_overnight = 1;
    $time_filter_desc = "BATCH (time >= $time_filter_start OR time < $time_filter_end)";
}
elsif (defined $start_time_str && defined $end_time_str)
{
    $time_filter_active = 1;
    $time_filter_start = $start_time_str;
    $time_filter_end = $end_time_str;
    if ($time_filter_end lt $time_filter_start)
    {
        $time_filter_overnight = 1;
        $time_filter_desc = "OVERNIGHT (time >= $time_filter_start OR time < $time_filter_end)";
    }
    else
    {
        $time_filter_desc = "MANUAL ($time_filter_start <= time < $time_filter_end)";
    }
}
elsif (defined $start_time_str || defined $end_time_str)
{
    die "Error: Must specify both -startt and -endt if using manual time filtering.\n";
}

{
    # Block to keep calculation-specific variables local
    # Restoring original logic as requested, with a note about unreachable code.
	 my %smt_adjustment_map = (
		  1 => 1.00,
		  2 => 0.95,
		  4 => 0.90,
		  8 => 1.10,
	 );

	 my $smt_adjustment_factor = $smt_adjustment_map{$smt_value} // 1.00;  # default to 1.00

    my $entitlement_adjustment_factor = $PLACEHOLDER_ENTITLEMENT_ADJUSTMENT_FACTOR;
    my $calculated_threshold = $ACTIVE_PHYSC_THRESHOLD *
                               $smt_adjustment_factor *
                               $entitlement_adjustment_factor *
                               $SAFETY_MARGIN_FOR_THRESHOLD_CONST;
    my $MIN_ACTIVE_PHYSC_THRESHOLD = 0.10;
    $ACTIVE_PHYSC_THRESHOLD = ($calculated_threshold < $MIN_ACTIVE_PHYSC_THRESHOLD) ? $MIN_ACTIVE_PHYSC_THRESHOLD : $calculated_threshold;
}

## -- Main Block -- ##

my $shown_cache_msg = 0;
my @processing_windows;
my $script_start_time = time();

print STDERR "\nnfit version $VERSION\n";
print STDERR "-------------------------\n";

## -- L1/L2 Cache Reset (with safety check) ##
if ($reset_cache && $nmon_dir) {
    my $stage_id_file = File::Spec->catfile($nmon_dir, '.nfit_stage_id');

    if (-f $stage_id_file) {
        print STDERR "\nData Staging Cache:\n" if (!$shown_cache_msg++);
        print STDERR "  Resetting data cache (--reset-cache flag detected) at '$nmon_dir'...\n";

        # Add the L2 Results cache file to the list of files to be deleted.
        my @cache_files = glob(File::Spec->catfile($nmon_dir, '.nfit.cache.*'));
        my $results_cache_file = File::Spec->catfile($nmon_dir, '.nfit.cache.results');
        push @cache_files, $results_cache_file if -f $results_cache_file;

        if (@cache_files) {
            my $deleted_count = unlink @cache_files;
            print STDERR "  INFO: Deleted $deleted_count cache file(s).\n" if ($verbose);
        } else {
            print STDERR "  INFO: No cache files found to delete.\n" if ($verbose);
        }
    } else {
        warn "WARN: --reset-cache specified, but directory '$nmon_dir' is not a valid staging area (.nfit_stage_id not found).\n" .
             "      For safety, the cache was NOT reset.\n";
    }
}

# ==============================================================================
# Main Execution Block
# ==============================================================================

# --- L2 Cache Variables ---
my $L2_RESULTS_CACHE_FILE = ".nfit.cache.results";
my @final_output_lines; # Buffer for all STDOUT to enable caching the result.

my %config_states_by_vm;
my $data_source_file;
my $is_cached_run = 0;
my $cachedir = (defined $nmon_dir) ? $nmon_dir : $physc_csv_dirname;

# --- L2 Results Cache Lookup ---
# This block attempts to find a pre-calculated result to avoid all further processing.
my $canonical_key = generate_canonical_key();
my $stage_id_file_for_l2 = File::Spec->catfile($cachedir, '.nfit_stage_id');

if (-f $stage_id_file_for_l2)
{
    my $results_cache_path = File::Spec->catfile($cachedir, $L2_RESULTS_CACHE_FILE);
    my $l2_lock_file_path  = File::Spec->catfile($cachedir, $CACHE_LOCK_FILE);

    # Check if the L1 cache manifest is newer than the L2 results cache.
    # If so, the L2 cache is stale and should not be used.
    my $manifest_path = File::Spec->catfile($cachedir, $CACHE_MANIFEST_FILE);
    my $results_cache_is_stale = 0;
    if (-f $manifest_path and -f $results_cache_path) {
        if ((stat($manifest_path))[9] > (stat($results_cache_path))[9]) {
            $results_cache_is_stale = 1;
            print STDERR "  NOTICE: L2 Results cache is older than L1 Data cache. Ignoring L2 cache.\n" if $verbose;
        }
    }

    unless ($results_cache_is_stale)
    {
        my $cached_result = lookup_cached_result($canonical_key, $results_cache_path);
        if (defined $cached_result)
        {
            print STDERR "  L2 Cache HIT. Returning cached result.\n" if $verbose;
            print STDOUT $cached_result . "\n";
            exit 0;
        }
    }
}

# --- Trigger and Validate Cache ---
my $stage_id_file = File::Spec->catfile($cachedir, '.nfit_stage_id');
my $manifest_file = File::Spec->catfile($cachedir, $CACHE_MANIFEST_FILE);
my $states_cache_file = File::Spec->catfile($cachedir, $CACHE_STATES_FILE);
my $data_cache_file = File::Spec->catfile($cachedir, $CACHE_DATA_FILE);

my $has_valid_cache = 0;

# Caching is only active if the directory was created by nfit-stage
if (-f $stage_id_file) {
				print STDERR "  INFO: Staged directory detected. Checking for analysis cache...\n" if $verbose;
				if (-f $manifest_file) {
								open my $man_fh, '<', $manifest_file or die "Error: Could not read manifest file '$manifest_file': $!";
								my $success = 0;
								while (my $line = <$man_fh>) {
												if ($line =~ /^cache_build_status:\s*success/) {
																$success = 1;
																last;
												}
								}
								close $man_fh;

								if ($success) {
												print STDERR "  INFO: Valid cache found. Loading analysis data from cache.\n" if ($verbose);
												$has_valid_cache = 1;
								} else {
				        print STDERR "\nData Staging Cache:\n" if (!$shown_cache_msg++);
												print STDERR "  NOTICE: invalid cache manifest. Rebuilding cache...\n";
								}
				}
}

# --- Load Data (from Cache or Build it) ---
if ($has_valid_cache) {
				print STDERR "\nData Staging Cache:\n" if (!$shown_cache_msg++);
				# CACHE HIT: Load states from the cache file
				eval {
								local $/;
								open my $fh, '<:encoding(utf8)', $states_cache_file or die "Could not open '$states_cache_file': $!";
								my $json_text = <$fh>;
								close $fh;
								my $json = JSON->new->allow_nonref;
								my $retrieved_states = $json->decode($json_text);
								%config_states_by_vm = %{$retrieved_states};
				};
				if ($@) {
								die "Error: Could not decode JSON from '$states_cache_file': $@\n" .
												"The cache file may be corrupt or in an old format. Rebuild it by deleting '$manifest_file'.\n";
				}

				# Post-load processing to "re-hydrate" data structures from the cache.
				# This ensures consistency regardless of the cache's origin (NMON or InfluxDB).
				foreach my $vm_name (keys %config_states_by_vm) {
								if (ref($config_states_by_vm{$vm_name}) eq 'ARRAY') {
												# Add a loop with a counter to assign the missing state_id.
												for (my $i = 0; $i < @{$config_states_by_vm{$vm_name}}; $i++) {
																my $state = $config_states_by_vm{$vm_name}[$i];
																$state->{state_id} = $i + 1;

																# This ensures the vm_name key exists within each state object, matching
																# the structure created by the NMON path.
																$state->{vm_name} = $vm_name;

																# Re-create Time::Piece objects from UTC epoch timestamps.
																if (defined $state->{start_epoch}) {
																				$state->{start_time} = gmtime($state->{start_epoch});
																}
																if (defined $state->{end_epoch}) {
																				$state->{end_time} = gmtime($state->{end_epoch});
																}

																# Calculate derived metadata fields that are not stored directly in the JSON cache.
																my $md = $state->{metadata};

																# Provide a default for SMT if it is missing from the cache.
																$md->{smt} //= $smt_value;

																# Calculate MaxCPU based on entitlement, vCPUs, pool CPUs, and capped status.
																my $ent = $md->{entitlement} // 0;
																my $vcpu = $md->{virtual_cpus} // 0;
																my $poolcpu = $md->{pool_cpu} // 0;
																my $is_capped = $md->{capped} // 0;

																my $max_cpu_calc = $is_capped ? $ent : ($poolcpu > 0 && $vcpu > 0 ? min($vcpu, $poolcpu) : $vcpu);
																$md->{max_cpu} = $max_cpu_calc > 0 ? $max_cpu_calc : ($vcpu > 0 ? $vcpu : 0);

																# Construct the 'Processor' string from its component parts.
																my $p_type = $md->{proc_type} // 'UNK'; $p_type =~ s/PowerPC_//;
																my $p_ver = $md->{proc_version} // 'UNK';
																my $p_clk = $md->{proc_clock} // 'UNK';

																$md->{processor_state} = join("_", $p_type, $p_ver, $p_clk);
																$md->{processor_state} =~ s/\s+/-/g;
												}
								}
				}

				printf STDERR "  Loaded %d cached configuration states.\n", scalar keys %config_states_by_vm;
				$data_source_file = $data_cache_file;
				$is_cached_run = 1;
} else {


				print STDERR "\nData Staging Cache:\n" if (!$shown_cache_msg++);
				print STDERR "  Initialising staging cache, please be patient ...\n";

				if ($nmon_dir)
				{
								# NMONDIR CACHE MISS or non-staged directory run
								if (-f $stage_id_file) {
												# It's a staged directory with an invalid/missing cache, so build it.
												my ($states_ref, $build_success) = build_cache($nmon_dir);
												die "FATAL: Cache build failed. See previous errors." unless $build_success;
												%config_states_by_vm = %{$states_ref};
												$data_source_file = $data_cache_file;
												$is_cached_run = 1; # We are now running from the newly built cache
								} else {
												# This is a normal, non-caching --nmondir run.
												# This preserves the original behavior for non-staged directories.
												print STDERR "WARN: Running in non-caching mode ('$nmon_dir' is not a staging directory containing '$stage_id_file').\n  Use nfit-stage to set up a data staging directory and prevent sub-optimal performance.\n";

												# First, get the configuration states from the files
												my $all_states_ref = define_configuration_states($nmon_dir, $smt_value);
												%config_states_by_vm = %{$all_states_ref};

												# Create a single combined temporary file for the performance data
												my ($combined_fh, $combined_filename) = tempfile(UNLINK => 1);

												# Call our rewritten subroutine to populate it in one pass
												process_nmon_directory_to_tempfiles($nmon_dir, $combined_fh);

												# Close the handle now that it's populated
												close($combined_fh);

												# Set the global data source file to our new combined temp file
												$data_source_file = $combined_filename;
								}
				} elsif ($physc_csv_file) {

								# NON-NMONDIR CACHE MISS or non-staged directory run
								my $states_ref = build_cache_from_csv($physc_csv_file, $runq_csv_file, $vm_config_file, $cachedir, $data_cache_file);
								%config_states_by_vm = %{$states_ref};
								$data_source_file = $data_cache_file;

				}
}

# --- NEW: Data Reshaping Step ---
# The Windowed Decay path uses a control-break algorithm that expects data
# grouped by VM, but the -pc cache is sorted by timestamp. This block creates
# a correctly sorted temporary file to be used by that specific analysis path.
if ($physc_csv_file) {
    print STDERR "Preparing Cached Data for analysis...\n";
    print STDERR "  Sorting CSV-generated cache data for windowed analysis...\n";
    my ($sorted_fh, $sorted_filename) = tempfile(UNLINK => 1);

    # Use system sort for efficiency: preserve header, then sort by VM Name (col 2), then Timestamp (col 1)
    open my $sort_pipe, "-|", "cat '$data_source_file' | (head -n 1 && tail -n +2 | sort -t, -k2,2 -k1,1)"
        or die "Failed to launch sort pipe: $!";

    while (<$sort_pipe>) {
        print $sorted_fh $_;
    }
    close $sort_pipe;
    close $sorted_fh;

    # The analysis engine will now read from this correctly sorted temp file
    $data_source_file = $sorted_filename;
    print STDERR "  Data sorting complete (ready for analysis).\n";
}
# --- End of Data Reshaping Step ---

{
# --- Unified Analysis Engine ---
# --- Prepare States for Single-Pass Analysis (note: this must support both decay and non-decay paths!) ---
				print STDERR "Unified Analysis Engine:\n";
				my @all_selected_states;

	         # This block determines which VMs to process based on the --vm flag.
				# It has been modified to handle a comma-separated list of VM names.
				my @vms_to_process;
				if (defined $target_vm_name) {
				    # If --vm is used, split the argument by comma into a set for efficient lookup.
				    my %user_specified_vms = map { $_ => 1 } split /,/, $target_vm_name;

				    # Filter the VMs found in the cache to only those specified by the user.
				    @vms_to_process = grep { exists $user_specified_vms{$_} } (sort keys %config_states_by_vm);

				    # This logic handles a special case where a user might analyse a single VM
				    # from a file that has no corresponding configuration state.
				    if (!@vms_to_process && (scalar keys %user_specified_vms) == 1) {
				        @vms_to_process = keys %user_specified_vms;
				    }
				} else {
				    # If --vm was not specified, process all VMs found in the configuration states.
				    @vms_to_process = sort keys %config_states_by_vm;
				}

				foreach my $vm_name (@vms_to_process) {
								my @available = @{$config_states_by_vm{$vm_name} || []};
								my @selected_for_this_vm = parse_state_selector($include_states_selector, \@available);
								push @all_selected_states, @selected_for_this_vm;
				}

				if (scalar(@all_selected_states) == 0) {
								print STDERR "-> No matching states found for the specified selector '$include_states_selector'. No data to process.\n";
								exit 0;
				}

				my @sorted_states_to_process = sort {
								$a->{vm_name} cmp $b->{vm_name} || $a->{start_epoch} <=> $b->{start_epoch}
				} @all_selected_states;

				my %states_by_vm;
				foreach my $state (@sorted_states_to_process) {
								push @{$states_by_vm{$state->{vm_name}}}, $state;
				}
				my $total_vms_with_states = scalar(keys %config_states_by_vm);

				print STDERR "  -> Profiling " . scalar(@sorted_states_to_process) . " selected configuration states across $total_vms_with_states VMs.\n";

				# ------------------------------------------------------------------
				# Define the Analysis Reference Date
				# This is done once, before any analysis path is chosen, to ensure the
				# variable is available to any mode that needs it (Windowed and Hybrid).
				# ------------------------------------------------------------------
				my $analysis_ref_obj;
				if ($enable_windowed_decay_internal || $decay_over_states_flag)
				{
								if (defined $analysis_reference_date_str)
								{
												eval { $analysis_ref_obj = Time::Piece->strptime($analysis_reference_date_str, "%Y-%m-%d"); };
												if ($@ || !$analysis_ref_obj)
												{
																	die "Error: Invalid --analysis-reference-date: '$analysis_reference_date_str'. Parser error: $@";
												}
								}
								else
								{
												# Default to the end time of the latest state if no reference date is given
												my $latest_epoch = 0;
												foreach my $state (@all_selected_states)
												{
																$latest_epoch = $state->{end_time}->epoch if $state->{end_time}->epoch > $latest_epoch;
												}
												$analysis_ref_obj = Time::Piece->new($latest_epoch);
								}
								# Truncate to the start of the day for consistent calculations
								$analysis_ref_obj = $analysis_ref_obj->truncate(to => 'day');
				}

# --- Phase 3: Main Analysis Execution ---
				my %vm_results_aggregator;

				# This single sort prepares the data for BOTH the windowed and non-windowed analysis paths.
				print STDERR "  INFO: Preparing data for analysis (sorting by VM, then Time)...\n" if $verbose;
				my ($sorted_fh, $sorted_filename) = tempfile(UNLINK => 1);
				# Use system sort for efficiency: preserve header, then sort by VM Name (col 2), then Timestamp (col 1)
				system("cat '$data_source_file' | (head -n 1 && tail -n +2 | sort -t, -k2,2 -k1,1) > '$sorted_filename'") == 0
								or die "Failed to create sorted temporary file: $!";
				# The analysis engine will now read from this correctly sorted temp file for all paths.
				$data_source_file = $sorted_filename;

				# This is the main control-break loop that reads the sorted data file once
				# and buffers data on a per-VM basis.
				my @vm_data_buffer;
				my $current_vm_name_for_buffer;

				open my $data_fh, '<', $data_source_file or die "FATAL: Could not open sorted data file '$data_source_file': $!";
				<$data_fh>; # Skip header

				while (my $line = <$data_fh>)
				{
								chomp $line;
								my ($ts, $line_vm, $physc, $runq) = split ',', $line, 4;
								next unless defined $line_vm && $line_vm ne '';

								$current_vm_name_for_buffer = $line_vm unless defined $current_vm_name_for_buffer;

								# On a "control break" (when VM name changes), process the buffered data
								if ($line_vm ne $current_vm_name_for_buffer && @vm_data_buffer)
								{
												my @states_for_buffered_vm = grep { $_->{vm_name} eq $current_vm_name_for_buffer } @sorted_states_to_process;

												# --- Main Analysis Path Router ---
												if ($enable_windowed_decay_internal)
												{
																# PATH A: Standard Windowed Decay
																process_vm_buffer(\@vm_data_buffer, $current_vm_name_for_buffer, \%vm_results_aggregator, \%states_by_vm, \@processing_windows);
												}
												elsif ($decay_over_states_flag)
												{
																# PATH B: Hybrid State-Time Decay Model
																my @state_results = get_metrics_for_each_state(\@vm_data_buffer, $current_vm_name_for_buffer, \@states_for_buffered_vm);

																if (@state_results)
																{
																				# Synthesize a daily time series from the state-based results
																				my @hybrid_timeseries = synthesize_hybrid_timeseries(\@state_results, "P".clean_perc_label($percentile));
																				# Apply the final decay calculation to get a single hybrid metric
																				my $final_hybrid_metric = calculate_recency_weighted_average(\@hybrid_timeseries, $analysis_ref_obj, $decay_half_life_days_val);
																				# Store the single result in the aggregator
																				$vm_results_aggregator{$current_vm_name_for_buffer}{'hybrid_metric'} = $final_hybrid_metric;
																				$vm_results_aggregator{$current_vm_name_for_buffer}{'source_states'} = \@state_results;
																}
												}
												else
												{
																# PATH C: Standard Non-Windowed (State-by-State)
																my @state_results = get_metrics_for_each_state(\@vm_data_buffer, $current_vm_name_for_buffer, \@states_for_buffered_vm);
																$vm_results_aggregator{$current_vm_name_for_buffer}{'states'} = \@state_results if @state_results;
												}

												@vm_data_buffer = (); # Reset buffer for the new VM
								}

								$current_vm_name_for_buffer = $line_vm;

								# Buffer the current data point
								my $tp;
								eval { $tp = Time::Piece->strptime($ts, "%Y-%m-%d %H:%M:%S"); };
								next if $@;

								push @vm_data_buffer, {
												ts    => $ts,
												physc => (defined $physc && looks_like_number($physc)) ? $physc+0 : undef,
												runq  => (defined $runq && looks_like_number($runq)) ? $runq+0 : undef,
												tp    => $tp,
								};
				}
				close $data_fh;

				# After the loop, ensure the very last VM's buffer is processed
				if (@vm_data_buffer)
				{
								my @states_for_last_vm = grep { $_->{vm_name} eq $current_vm_name_for_buffer } @sorted_states_to_process;

								# --- Final Analysis Path Router ---
								if ($enable_windowed_decay_internal)
								{
												process_vm_buffer(\@vm_data_buffer, $current_vm_name_for_buffer, \%vm_results_aggregator, \%states_by_vm, \@processing_windows);
								}
								elsif ($decay_over_states_flag)
								{
												my @state_results = get_metrics_for_each_state(\@vm_data_buffer, $current_vm_name_for_buffer, \@states_for_last_vm);
												if (@state_results)
												{
																my @hybrid_timeseries = synthesize_hybrid_timeseries(\@state_results, "P".clean_perc_label($percentile));
																my $final_hybrid_metric = calculate_recency_weighted_average(\@hybrid_timeseries, $analysis_ref_obj, $decay_half_life_days_val);
																$vm_results_aggregator{$current_vm_name_for_buffer}{'hybrid_metric'} = $final_hybrid_metric;
																$vm_results_aggregator{$current_vm_name_for_buffer}{'source_states'} = \@state_results;
												}
								}
								else
								{
												my @state_results = get_metrics_for_each_state(\@vm_data_buffer, $current_vm_name_for_buffer, \@states_for_last_vm);
												$vm_results_aggregator{$current_vm_name_for_buffer}{'states'} = \@state_results if @state_results;
								}
				}

				print STDERR "  State Analysis complete.\n";
# --- End of Analysis --- #

				print STDERR generate_run_summary_string();
				print STDERR "------------------------------------------------------------------\n";

# --- Final Output Generation ---

    if (!$quiet) {
								print STDERR "\n------------------------------------------------------------------\n";
								print STDERR "Results\n";
								print STDERR "------------------------------------------------------------------\n";
    }

				my @output_header_cols = ("VM_Name", "State_ID", "Duration_Days", "Entitlement", "MaxCPU", "SMT", "Processor", "SerialNumber", "PoolID");
				my @metric_keys_in_order = ("P" . clean_perc_label($percentile));
				push @metric_keys_in_order, 'Peak' if $calculate_peak;
				foreach my $p (@runq_norm_percentiles_to_calc) { push @metric_keys_in_order, "NormRunQ_P" . clean_perc_label($p); }
				foreach my $p (@runq_abs_percentiles_to_calc) { push @metric_keys_in_order, "AbsRunQ_P" . clean_perc_label($p); }
				if ($enable_windowed_decay_internal && $enable_growth_prediction) {
								push @metric_keys_in_order, "GrowthAdj", "GrowthAdjAbs";
				}
				push @output_header_cols, @metric_keys_in_order;

				# This is the conditional block that handles output for all three modes.
				# Path A: Output for standard Non-Windowed (State-by-State) Mode.
				if (!$enable_windowed_decay_internal && !$decay_over_states_flag)
				{
								# This logic is preserved to print one result line per configuration state.
								foreach my $vm_name (sort keys %vm_results_aggregator)
								{
												next unless (exists $vm_results_aggregator{$vm_name}{'states'} && ref($vm_results_aggregator{$vm_name}{'states'}) eq 'ARRAY');

												my @states_for_this_vm = @{$vm_results_aggregator{$vm_name}{'states'}};
												my $total_states_for_vm = scalar(@states_for_this_vm);

												foreach my $result (@states_for_this_vm)
												{
																my $state_obj = $result->{state_obj};
																my $md        = $result->{state_obj}{metadata};

																my $state_prefix = sprintf("%s:State-%d_of_%d (%s_to_%s)",
																				$vm_name, $state_obj->{state_id}, $total_states_for_vm,
																				$state_obj->{start_time}->strftime('%Y-%m-%d'),
																				$state_obj->{end_time}->strftime('%Y-%m-%d')
																);
																my @output_parts = ($state_prefix);
																push @output_parts, sprintf("Entitlement=%.2f", $md->{entitlement} // 0);
																push @output_parts, sprintf("MaxCPU=%.2f", $md->{max_cpu} // 0);
																push @output_parts, sprintf("SMT=%d", $md->{smt} // 0);
																push @output_parts, sprintf("Processor=%s", $md->{processor_state} // 'N/A');
																push @output_parts, sprintf("SerialNumber=%s", $md->{serial_number} // '');
																push @output_parts, sprintf("PoolID=%s", $md->{pool_id} // '');

																my @metric_keys = sort keys %{$result->{metrics}};
																foreach my $key (@metric_keys)
																{
																				my $val = $result->{metrics}{$key};
																				my $val_str = defined($val) && looks_like_number($val) ? sprintf("%.${output_dp}f", apply_rounding($val, $round_increment, $rounding_method)) : "N/A";
																				push @output_parts, "$key=$val_str";
																}
																push @final_output_lines, join(" ", @output_parts);
												}
								}
				}
				else # Path B & C: Unified Output for ALL Aggregated Modes (Windowed Decay AND Hybrid)
				{
								# This block handles output for both --enable-windowed-decay and --decay-over-states
								foreach my $vm_name (sort keys %vm_results_aggregator)
								{
												my %final_metrics;
												my $last_state_metadata;

												# Step 1: Populate the base metrics depending on the analysis mode.
												if ($enable_windowed_decay_internal)
												{
																# --- Aggregate base metrics for Standard Windowed Decay mode ---
																my %all_keys;
																foreach my $win_result (values %{$vm_results_aggregator{$vm_name}{time_windows}}) {
																				foreach my $key (keys %{$win_result->{metrics}}) { $all_keys{$key} = 1; }
																}
																foreach my $metric_key (sort keys %all_keys) {
																				my @metric_values_across_windows;
																				foreach my $win_result (values %{$vm_results_aggregator{$vm_name}{time_windows}}) {
																								if (defined $win_result->{metrics}{$metric_key}) {
																												push @metric_values_across_windows, { value => $win_result->{metrics}{$metric_key}, date => $win_result->{date_obj} };
																								}
																				}
																				if ($metric_key ne 'Peak') {
																								$final_metrics{$metric_key} = calculate_recency_weighted_average(\@metric_values_across_windows, $analysis_ref_obj, $decay_half_life_days_val);
																				} else {
																								my @values = map { $_->{value} } grep { defined $_->{value} && looks_like_number($_->{value}) } @metric_values_across_windows;
																								$final_metrics{$metric_key} = @values ? max(@values) : "N/A";
																				}
																}
												}
												elsif ($decay_over_states_flag)
												{
																# --- Populate base metrics for the Hybrid State-Time Decay mode ---
																my $p_label = "P" . clean_perc_label($percentile);
																$final_metrics{$p_label} = $vm_results_aggregator{$vm_name}{'hybrid_metric'} // "N/A";

																my $source_states_aref = $vm_results_aggregator{$vm_name}{'source_states'};
																if ($calculate_peak) {
																				my @all_peaks;
																				if ($source_states_aref && @$source_states_aref) {
																								foreach my $state_res (@$source_states_aref) {
																												if (defined $state_res->{metrics}{'Peak'}) {
																																push @all_peaks, $state_res->{metrics}{'Peak'};
																												}
																								}
																				}
																				$final_metrics{'Peak'} = @all_peaks ? max(@all_peaks) : "N/A";
																}
																if ($source_states_aref && @$source_states_aref) {
																				my $last_state_metrics = $source_states_aref->[-1]{metrics};
																				foreach my $p (@runq_norm_percentiles_to_calc) { my $key = "NormRunQ_P".clean_perc_label($p); $final_metrics{$key} = $last_state_metrics->{$key} // "N/A"; }
																				foreach my $p (@runq_abs_percentiles_to_calc)  { my $key = "AbsRunQ_P".clean_perc_label($p);  $final_metrics{$key} = $last_state_metrics->{$key} // "N/A"; }
																}
												}

												# Step 2: If enabled, calculate and apply growth prediction to the base metrics.
												if ($enable_growth_prediction)
												{
																my $p_label_main = "P" . clean_perc_label($percentile);
																my @timeseries_for_growth;

																if ($enable_windowed_decay_internal)
																{
																				# For standard decay, build the time series from the per-window results.
																				my @sorted_windows = sort { $a->{date_obj}->epoch <=> $b->{date_obj}->epoch } values %{$vm_results_aggregator{$vm_name}{time_windows}};
																				foreach my $win_result (@sorted_windows)
																				{
																								push @timeseries_for_growth, $win_result->{metrics}{$p_label_main};
																				}
																}
																elsif ($decay_over_states_flag)
																{
																				# For hybrid decay, use the daily stamped values from the synthesized time series.
																				if (exists $vm_results_aggregator{$vm_name}{'hybrid_timeseries_data'})
																				{
																								@timeseries_for_growth = map { $_->{value} } @{$vm_results_aggregator{$vm_name}{'hybrid_timeseries_data'}};
																				}
																}

																# Call the reusable subroutine to get the growth adjustment values.
																my ($growth_adj, $growth_adj_abs) = calculate_growth_prediction(\@timeseries_for_growth, $final_metrics{$p_label_main});

																# Apply the adjustments to the final metrics hash.
																$final_metrics{"GrowthAdj"} = $growth_adj;
																$final_metrics{"GrowthAdjAbs"} = $growth_adj_abs;
																$final_metrics{$p_label_main} += $growth_adj if (defined $final_metrics{$p_label_main} && looks_like_number($final_metrics{$p_label_main}));
												}

												# Step 3: Common Printing Logic for All Aggregated Modes.
												my ($last_state_obj) = grep { $_->{vm_name} eq $vm_name } reverse @sorted_states_to_process;
												next unless $last_state_obj;

												# Get the most recent metadata for the VM.
												if (exists $config_states_by_vm{$vm_name} && @{$config_states_by_vm{$vm_name}})
												{
																$last_state_metadata = $config_states_by_vm{$vm_name}[-1]{metadata};
												}
												else
												{
																$last_state_metadata = {};
												}

												# Build and buffer the final output line.
												my $output_string = "$vm_name:";
												$output_string .= sprintf(" Entitlement=%.2f", $last_state_metadata->{entitlement} // 0);
												$output_string .= sprintf(" MaxCPU=%.2f", $last_state_metadata->{max_cpu} // 0);
												$output_string .= sprintf(" SMT=%s", $last_state_metadata->{smt} // '');
												$output_string .= sprintf(" Processor=%s", $last_state_metadata->{processor_state} // 'N/A');
												$output_string .= sprintf(" SerialNumber=%s", $last_state_metadata->{serial_number} // '');
												$output_string .= sprintf(" PoolID=%s", $last_state_metadata->{pool_id} // '');

												my @keys_out = sort keys %final_metrics;
												foreach my $key (@keys_out)
												{
																my $val = $final_metrics{$key};
																my $val_str = defined($val) && looks_like_number($val) ? sprintf("%.${output_dp}f", apply_rounding($val, $round_increment, $rounding_method)) : "N/A";
																$output_string .= " $key=$val_str";
												}
												push @final_output_lines, $output_string;
								}
				}

}

# --- L2 Cache Save and Final Output ---
if (@final_output_lines)
{
    my $final_output_string = join("\n", @final_output_lines);

    # If we are in a cacheable context (a staged directory), save the result.
    if (-f $stage_id_file_for_l2)
    {
        my $results_cache_path = File::Spec->catfile($cachedir, $L2_RESULTS_CACHE_FILE);
        my $l2_lock_file_path  = File::Spec->catfile($cachedir, $CACHE_LOCK_FILE);
        print STDERR "L2 Results Cache MISS (caching new result).\n" if $verbose;
        save_result_to_cache($canonical_key, $final_output_string, $results_cache_path, $l2_lock_file_path);
    }

    # Print the final, complete output to STDOUT.
    print STDOUT $final_output_string . "\n";
}

my $script_end_time = time();
my $duration = $script_end_time - $script_start_time;
print STDERR "\nCompleted in $duration second(s).\n\n";
exit 0;

# ==============================================================================
# Subroutines
# ==============================================================================

# --- build_cache ---
# Performs a single, locked pass over the NMON directory to build the data
# and state cache files. This is the primary I/O-intensive operation.
#
# Args:
#   1. $dir (string): The path to the staged --nmondir.
#
# Returns:
#   - A list: (hash_ref_of_states, success_boolean)
#
sub build_cache {
    my ($dir) = @_;

    my $lock_file = File::Spec->catfile($dir, $CACHE_LOCK_FILE);
    my $manifest_file = File::Spec->catfile($dir, $CACHE_MANIFEST_FILE);
    my $states_cache_file = File::Spec->catfile($dir, $CACHE_STATES_FILE);
    my $data_cache_file = File::Spec->catfile($dir, $CACHE_DATA_FILE);

    # --- Acquire Lock to prevent race conditions ---
    open my $lock_fh, '>', $lock_file or die "Error: Cannot create lock file '$lock_file': $!";
    unless (flock($lock_fh, LOCK_EX | LOCK_NB)) {
        print STDERR "  WARN: Another nfit process is currently building the cache in this directory. Waiting...\n";
        flock($lock_fh, LOCK_EX) or die "Error: Could not acquire exclusive lock on '$lock_file': $!";
    }
    print STDERR "  INFO: Acquired cache lock for directory '$dir'.\n" if ($verbose);

    # --- Initialize return values ---
    my $build_success = 0;
    my %all_vm_states;
    my @temp_data_files; # To hold the paths of intermediate sorted files

    # Using eval block for safe cleanup, ensuring lock is always released
    eval {
        # --- Clean up any old, partial cache files from a previous failed run ---
        unlink $_ for ($manifest_file, $states_cache_file, $data_cache_file);
        print STDERR "  INFO: Starting single-pass analysis to build cache...\n" if ($verbose);

        my @nmon_files;
        find( sub {
                return unless -f;
                return unless /\.(nmon|nmon\.gz|nmon\.bz2|nmon\.bzip2)$/i;
                push @nmon_files, $File::Find::name;
            }, $dir
        );

        unless (@nmon_files) {
            die "Error: No .nmon files found in staging directory '$dir'. Cannot build cache.\n";
        }

        print STDERR "  INFO: Found " . scalar(@nmon_files) . " NMON files to process for the cache.\n" if ($verbose);

        my %config_events_by_vm;
        my %file_boundaries;

        # This will hold all performance data points in memory before the global sort.
        # This reverts to the old, correct, but more memory-intensive method as a baseline.
        my %all_perf_data_points;

        foreach my $file_path (sort @nmon_files) {
            print STDERR "  INFO: Processing '$file_path' for data staging cache...\n" if ($verbose);

            my $fh;
            if ($file_path =~ /\.nmon\.gz$/i) {
                open $fh, "-|", "gzip -dc --", $file_path or do { warn "Warning: Could not decompress '$file_path': $!. Skipping."; next; };
            } elsif ($file_path =~ /\.nmon\.(bz2|bzip2)$/i) {
                open $fh, "-|", "bzcat --", $file_path or do { warn "Warning: Could not bzcat '$file_path': $!. Skipping."; next; };
            } else {
                open $fh, '<:encoding(utf8)', $file_path or do { warn "Warning: Could not open '$file_path': $!. Skipping."; next; };
            }

            my $current_vm;
            my %zzzz_map;
            my %file_static_config;

            while (my $line = <$fh>) {
                chomp $line; $line =~ s/\r$//;
                my @fields = split ',', $line;
                my $key = $fields[0];

                # --- This section for gathering config events is unchanged ---
                if ($key eq 'AAA' && $fields[1] eq 'host') { $current_vm = $fields[2]; }
                elsif ($key eq 'AAA' && $fields[1] eq 'SerialNumber') { $file_static_config{serial_number} = $fields[2]; }
                elsif ($key eq 'ZZZZ') {
                    my ($t_num, $time, $date_str) = @fields[1..3];
                    my $tp_date;
                    eval { $tp_date = Time::Piece->strptime($date_str, "%d-%b-%Y"); };
                    if ($@ || !$tp_date) { eval { $tp_date = Time::Piece->strptime($date_str, "%d-%B-%Y"); }; }
                    next if ($@ || !$tp_date);
                    my $timestamp = $tp_date->ymd . " " . $time;
                    $zzzz_map{$t_num} = $timestamp;
                    $file_boundaries{$current_vm}{$timestamp} = 1 if $current_vm;
                }
                elsif ($key eq 'BBBL') {
                    my ($id, $bval) = ($fields[1], $fields[3]);
                    next unless (defined $id);
                    if    ($id eq '04') { $file_static_config{virtual_cpus} = $bval + 0 if looks_like_number($bval); }
                    elsif ($id eq '06') { $file_static_config{smt} = $bval + 0 if looks_like_number($bval); }
                    elsif ($id eq '07') { $file_static_config{capped} = ($bval =~ /true|1/i ? 1 : 0); }
                    elsif ($id eq '18') { $file_static_config{pool_cpu} = $bval + 0 if looks_like_number($bval); }
                }
                elsif ($key eq 'BBBP') {
                    my $bval = $fields[3] // "";
                    if    ($bval =~ /"Processor Type:\s*(.+?)"?$/)    { $file_static_config{proc_type} = $1; }
                    elsif ($bval =~ /"Processor Version:\s*(.+?)"?$/) { $file_static_config{proc_version} = $1; }
                    elsif ($bval =~ /"Processor Clock Speed:\s*(\d+)/){ $file_static_config{proc_clock} = $1; }
                    elsif ($bval =~ /Shared Pool ID\s*:\s*(\d+)/)      { $file_static_config{pool_id} = $1; }
                }
                elsif ($key eq 'LPAR') {
                    my $t_num = $fields[1];
                    if (defined $zzzz_map{$t_num} && defined $current_vm) {
                        my $ts = $zzzz_map{$t_num};
                        # Aggregate performance data in memory instead of writing directly
                        $all_perf_data_points{$ts}{$current_vm}{physc} = $fields[2] if defined $fields[2] and looks_like_number($fields[2]);
                        my $ent_val = $fields[6];
                        if (defined $ent_val && looks_like_number($ent_val)) {
                             my $ts_obj = Time::Piece->strptime($ts, "%Y-%m-%d %H:%M:%S");
                             push @{$config_events_by_vm{$current_vm}}, { ts => $ts_obj->epoch, key => 'entitlement', value => $ent_val + 0 };
                        }
                    }
                }
                 elsif ($key eq 'PROC') {
                    my $t_num = $fields[1];
                    if (defined $zzzz_map{$t_num} && defined $current_vm) {
                        my $ts = $zzzz_map{$t_num};
                        # Aggregate performance data in memory
                        $all_perf_data_points{$ts}{$current_vm}{runq} = $fields[2] if defined $fields[2] and looks_like_number($fields[2]);
                    }
                }
            }
            close $fh;

            if (defined $current_vm && %file_static_config) {
                my @file_ts_epochs = map { Time::Piece->strptime($_, "%Y-%m-%d %H:%M:%S")->epoch } values %zzzz_map;
                if (@file_ts_epochs) {
                    my $file_start_ts = (sort {$a <=> $b} @file_ts_epochs)[0];
                    foreach my $key (keys %file_static_config) {
                        push @{$config_events_by_vm{$current_vm}}, { ts => $file_start_ts, key => $key, value => $file_static_config{$key} };
                    }
                }
            }
        }

        # --- Perform global sort and write performance data cache ---
        print STDERR "  -> Globally sorting performance data and writing cache file...\n";
        open my $data_fh, '>', $data_cache_file or die "Error: Cannot write to data cache '$data_cache_file': $!";
        print $data_fh "Timestamp,VMName,PhysC,RunQ\n";

        foreach my $ts (sort keys %all_perf_data_points) {
            foreach my $vm (sort keys %{$all_perf_data_points{$ts}}) {
                my $physc_val = $all_perf_data_points{$ts}{$vm}{physc} // '';
                my $runq_val  = $all_perf_data_points{$ts}{$vm}{runq} // '';
                if ($physc_val ne '' || $runq_val ne '') {
                     print $data_fh join(",", $ts, $vm, $physc_val, $runq_val) . "\n";
                }
            }
        }
        close $data_fh;
        print STDERR "  -> Completed data staging cache creation ('$data_cache_file').\n";

        foreach my $vm_name (sort keys %config_events_by_vm) {
            my %events_by_ts;
            foreach my $event (@{$config_events_by_vm{$vm_name}}) {
                $events_by_ts{$event->{ts}}{$event->{key}} = $event->{value};
            }

            my @sorted_timestamps = sort {$a <=> $b} keys %events_by_ts;
            next unless @sorted_timestamps;

            my %last_known_metadata;
            my $last_fingerprint = "";
            my $current_state_start_epoch;

            for (my $i = 0; $i < @sorted_timestamps; $i++) {
                my $ts = $sorted_timestamps[$i];
                my %metadata_before_change = %last_known_metadata;
                my %snapshot_metadata = %last_known_metadata;
                foreach my $key (keys %{$events_by_ts{$ts}}) {
                    $snapshot_metadata{$key} = $events_by_ts{$ts}{$key};
                }

                my $current_fingerprint = join "|", map { $snapshot_metadata{$_} // 'U' }
                                          sort qw(entitlement smt virtual_cpus pool_cpu capped proc_type proc_version proc_clock serial_number pool_id);

                if ($i == 0) {
                    $last_fingerprint = $current_fingerprint;
                    $current_state_start_epoch = $ts;
                } elsif ($current_fingerprint ne $last_fingerprint) {
                    push @{$all_vm_states{$vm_name}}, {
                        start_epoch => $current_state_start_epoch,
                        end_epoch   => $ts - 1,
                        metadata    => { %metadata_before_change }
                    };
                    $current_state_start_epoch = $ts;
                    $last_fingerprint = $current_fingerprint;
                }
                %last_known_metadata = %snapshot_metadata;
            }

            my $vm_timestamps = [sort {$a <=> $b} map {Time::Piece->strptime($_, "%Y-%m-%d %H:%M:%S")->epoch} keys %{$file_boundaries{$vm_name} || {}}];
            my $final_end_epoch = @$vm_timestamps ? $vm_timestamps->[-1] : ($current_state_start_epoch // time());

            if (defined $current_state_start_epoch) {
                push @{$all_vm_states{$vm_name}}, {
                    start_epoch => $current_state_start_epoch,
                    end_epoch   => $final_end_epoch,
                    metadata    => \%last_known_metadata
                };
            }

            for (my $i=0; $i < @{$all_vm_states{$vm_name} || []}; $i++) {
                my $state = $all_vm_states{$vm_name}[$i];
                $state->{state_id} = $i + 1;
                $state->{duration_days} = ($state->{end_epoch} - $state->{start_epoch}) / (60*60*24);
                $state->{vm_name} = $vm_name;
                $state->{metadata}{vm_name} = $vm_name;
                my $md = $state->{metadata};
                $md->{smt} //= $smt_value;
                my $ent = $md->{entitlement} // 0;
                my $vcpu = $md->{virtual_cpus} // 0;
                my $poolcpu = $md->{pool_cpu} // 0;
                my $is_capped = $md->{capped} // 0;
                $md->{lpar_state} = "capped=$is_capped";
                my $max_cpu_calc = $is_capped ? $ent : ($poolcpu > 0 && $vcpu > 0 ? min($vcpu, $poolcpu) : $vcpu);
                $md->{max_cpu} = $max_cpu_calc > 0 ? $max_cpu_calc : ($vcpu > 0 ? $vcpu : 0);
                my $p_type = $md->{proc_type} // 'UNK'; $p_type =~ s/PowerPC_//;
                my $p_ver = $md->{proc_version} // 'UNK';
                my $p_clk = $md->{proc_clock} // 'UNK';
                $md->{processor_state} = join("_", $p_type, $p_ver, $p_clk);
                $md->{processor_state} =~ s/\s+/-/g;
            }
        }

        my $json = JSON->new->pretty->canonical->allow_blessed->convert_blessed;
        my $json_text = $json->encode(\%all_vm_states);
        open my $fh, '>:encoding(utf8)', $states_cache_file or die "Error: Could not write to '$states_cache_file': $!";
        print $fh $json_text;
        close $fh;
        print STDERR "  -> Completed state configuration cache creation ('$states_cache_file').\n";

        # This is the final step in a successful build. Writing this file "commits" the cache.
        open my $man_fh, '>', $manifest_file or die "Error: Cannot write manifest file '$manifest_file': $!";
        my $timestamp = localtime->strftime('%Y-%m-%dT%H:%M:%S');
        print $man_fh "cache_build_status: success\n";
        print $man_fh "build_timestamp: $timestamp\n";
        print $man_fh "nfit_version: $VERSION\n";
        close $man_fh;

        print STDERR "  -> Cache manifest created successfully.\n" if ($verbose);

        $build_success = 1; # Signal success
    };

    if ($@) {
        warn "  FATAL: An error occurred during cache generation: $@";
        unlink $_ for ($manifest_file, $states_cache_file, $data_cache_file);
        $build_success = 0;
    }

    # --- Release Lock ---
    close $lock_fh;
    unlink $lock_file;
    print STDERR "  INFO: Released cache lock.\n" if ($verbose);

    return (\%all_vm_states, $build_success);
}

sub build_cache_from_csv {
    my ($physc_file, $runq_file, $config_file, $cachedir, $data_cache_path) = @_;

    my %all_vm_states;
    my %vm_config_data;

    my $lock_file = File::Spec->catfile($cachedir, $CACHE_LOCK_FILE);
    my $states_cache_file = File::Spec->catfile($cachedir, $CACHE_STATES_FILE);
    my $manifest_file = File::Spec->catfile($cachedir, $CACHE_MANIFEST_FILE);
    my $data_cache_file = File::Spec->catfile($cachedir, $CACHE_DATA_FILE);

    # --- Acquire Lock to prevent race conditions ---
    open my $lock_fh, '>', $lock_file or die "Error: Cannot create lock file '$lock_file': $!";
    unless (flock($lock_fh, LOCK_EX | LOCK_NB)) {
        print STDERR "  WARN: Another nfit process is currently building the cache in this directory. Waiting...\n";
        flock($lock_fh, LOCK_EX) or die "Error: Could not acquire exclusive lock on '$lock_file': $!";
    }
    print STDERR "  INFO: Acquired cache lock for directory '$cachedir'.\n" if ($verbose);

    unlink $_ for ($manifest_file, $states_cache_file, $data_cache_file);

    # --- Open cache file handles locally ---
    open my $data_fh, '>', $data_cache_path or die "Cannot write to data cache '$data_cache_path': $!";
    print $data_fh "Timestamp,VMName,PhysC,RunQ\n";

    # Precompiled regex for efficiency (OPTION 1 OPTIMIZATION)
    my $quote_strip_re = qr/^['"]|['"]$/;
    my $whitespace_re = qr/\s+/;
    my $empty_line_re = qr/^\s*$/;
    my $carriage_return_re = qr/\r$/;

    # 1. Load config-all.csv if it exists (OPTION 1 OPTIMIZED VERSION)
    if (defined $config_file && -f $config_file) {
        open my $cfg_fh, '<:encoding(utf8)', $config_file or die "Error opening config file '$config_file': $!";
        my $header_line = <$cfg_fh>;
        chomp $header_line;
        $header_line =~ s/$carriage_return_re//;

        my @headers = parse_csv_line($header_line);
        my %hmap;
        for my $i (0 .. $#headers) {
            my $col = $headers[$i];
            $col =~ s/$quote_strip_re//g;
            $hmap{lc($col)} = $i;
        }

        # Pre-check which columns exist to avoid hash lookups in the loop (OPTION 1)
        my $has_smt = exists $hmap{'smt'};
        my $has_maxcpu = exists $hmap{'maxcpu'};
        my $has_entitledcpu = exists $hmap{'entitledcpu'};
        my $has_procpool_id = exists $hmap{'procpool_id'};
        my $has_serial = exists $hmap{'serial'};
        my $has_proctype = exists $hmap{'proctype'};
        my $has_procversion = exists $hmap{'procversion'};
        my $has_procclock = exists $hmap{'procclock'};

        while (my $line = <$cfg_fh>) {
            chomp $line;
            $line =~ s/$carriage_return_re//;
            next if $line =~ /$empty_line_re/;

            my @vals = parse_csv_line($line);
            my $vm_name = $vals[$hmap{'hostname'}];
            next unless defined $vm_name && length($vm_name);

            # Direct assignment without repeated hash lookups (OPTION 1)
            my $vm_ref = $vm_config_data{$vm_name} = {};
            $vm_ref->{smt} = $vals[$hmap{'smt'}] if $has_smt;
            $vm_ref->{maxcpu} = $vals[$hmap{'maxcpu'}] if $has_maxcpu;
            $vm_ref->{entitlement} = $vals[$hmap{'entitledcpu'}] if $has_entitledcpu;
            $vm_ref->{pool_id} = $vals[$hmap{'procpool_id'}] if $has_procpool_id;
            $vm_ref->{serial_number} = $vals[$hmap{'serial'}] if $has_serial;
            $vm_ref->{proc_type} = $vals[$hmap{'proctype'}] if $has_proctype;
            $vm_ref->{proc_version} = $vals[$hmap{'procversion'}] if $has_procversion;
            $vm_ref->{proc_clock} = $vals[$hmap{'procclock'}] if $has_procclock;
        }
        close $cfg_fh;
    }

    # 2. STREAMING MERGE: Open both files simultaneously (OPTION 2 APPROACH)
    open my $pc_fh, '<:encoding(utf8)', $physc_file or die "Error: Cannot open PhysC file '$physc_file': $!";

    my $rq_fh;
    my $has_runq = 0;
    if (defined $runq_file && -s $runq_file) {
        open $rq_fh, '<:encoding(utf8)', $runq_file or die "Error: Cannot open RunQ file '$runq_file': $!";
        $has_runq = 1;
    }

    # Read headers from both files with OPTION 1 optimizations
    my $pc_header = <$pc_fh>;
    chomp $pc_header;
    $pc_header =~ s/$carriage_return_re//;
    my @pc_fields = split ',', $pc_header, -1;
    shift @pc_fields; # Remove timestamp column

    # OPTION 1 OPTIMIZATION: More efficient VM name extraction
    my @physc_vm_names = map {
        my $val = $_;
        $val =~ s/$quote_strip_re//g;
        my ($name) = split / /, $val;
        $name;
    } @pc_fields;

    # OPTION 1 OPTIMIZATION: Create hash for O(1) lookups instead of linear search
    my %physc_vm_index;
    for my $i (0 .. $#physc_vm_names) {
        $physc_vm_index{$physc_vm_names[$i]} = $i if defined $physc_vm_names[$i] && length($physc_vm_names[$i]);
    }

    my (@runq_vm_names, %runq_vm_index);
    if ($has_runq) {
        my $rq_header = <$rq_fh>;
        chomp $rq_header;
        $rq_header =~ s/$carriage_return_re//;
        my @rq_fields = split ',', $rq_header, -1;
        shift @rq_fields; # Remove timestamp column

        # OPTION 1 OPTIMIZATION: More efficient VM name extraction
        @runq_vm_names = map {
            s/$quote_strip_re//g;
            my ($name) = split / /;
            $name;
        } @rq_fields;

        # OPTION 1 OPTIMIZATION: Create index mapping for O(1) lookup
        for my $i (0 .. $#runq_vm_names) {
            $runq_vm_index{$runq_vm_names[$i]} = $i if defined $runq_vm_names[$i] && length($runq_vm_names[$i]);
        }
    }

    # Variables for streaming merge
    my ($pc_line, $rq_line, $pc_timestamp, $rq_timestamp);
    my (@pc_data, @rq_data);
    my $pc_has_data = 0;
    my $rq_has_data = 0;
    my ($min_epoch, $max_epoch);
    my %all_vms_in_file;

    # OPTION 1 OPTIMIZATION: Larger output buffer for better I/O performance
    my $output_buffer = '';
    my $buffer_size = 0;
    my $max_buffer_size = 16384; # 16KB buffer (increased from 8KB)

    # Helper function to read next PhysC line
    my $read_pc_line = sub {
        while (my $line = <$pc_fh>) {
            chomp $line;
            next if $line =~ /$empty_line_re/;
            $line =~ s/$carriage_return_re//;

            my @fields = split ',', $line, -1;

            # Validate field count against the header
            # The -1 accounts for the timestamp column that will be shifted off.
            if (scalar(@fields) - 1 != scalar(@physc_vm_names)) {
                my $ts_for_error = (split ',', $line, 2)[0] // '[unknown timestamp]';
                print STDERR "WARN: Mismatched field count in physc data at $ts_for_error. Header has ",
                             scalar(@physc_vm_names), " VMs, but line has ", scalar(@fields) - 1, " data points. Skipping line.\n";
                next; # Skip this malformed line and read the next one
            }

            $pc_timestamp = shift @fields;
            @pc_data = @fields;
            return 1;
        }
        return 0;
    };

    # Helper function to read next RunQ line
    my $read_rq_line = sub {
        return 0 unless $has_runq;
        while (my $line = <$rq_fh>) {
            chomp $line;
            next if $line =~ /$empty_line_re/;
            $line =~ s/$carriage_return_re//;

            my @fields = split ',', $line, -1;

            # Validate field count against the header ---
            if (scalar(@fields) - 1 != scalar(@runq_vm_names)) {
                my $ts_for_error = (split ',', $line, 2)[0] // '[unknown timestamp]';
                print STDERR "WARN: Mismatched field count in runq data at $ts_for_error. Header has ",
                             scalar(@runq_vm_names), " VMs, but line has ", scalar(@fields) - 1, " data points. Skipping line.\n";
                next; # Skip this malformed line and read the next one
            }

            $rq_timestamp = shift @fields;
            @rq_data = @fields;
            return 1;
        }
        return 0;
    };

    # Initialize with first data lines
    $pc_has_data = $read_pc_line->();
    $rq_has_data = $read_rq_line->() if $has_runq;

    # STREAMING MERGE LOOP with OPTION 1 optimizations
    # This loop uses a robust three-way comparison to correctly merge the two
    # time-sorted files, handling all cases to prevent the data loss seen in
    # the previous implementation. It preserves all of your optimisations.
    while ($pc_has_data || $rq_has_data) {
        # This block contains the complete and corrected logic for the streaming merge loop.
        # It correctly sets flags, validates the timestamp, processes data, and advances the file streams.

        my $current_timestamp;
        my $use_pc_data = 0;
        my $use_rq_data = 0;

        # --- Stage 1: Determine which timestamp to process and set flags ---
        if ($pc_has_data && $rq_has_data) {
            if ($pc_timestamp eq $rq_timestamp) {
                $current_timestamp = $pc_timestamp;
                $use_pc_data = 1;
                $use_rq_data = 1;
            } elsif ($pc_timestamp lt $rq_timestamp) {
                $current_timestamp = $pc_timestamp;
                $use_pc_data = 1;
            } else { # $pc_timestamp gt $rq_timestamp
                $current_timestamp = $rq_timestamp;
                $use_rq_data = 1;
            }
        } elsif ($pc_has_data) {
            $current_timestamp = $pc_timestamp;
            $use_pc_data = 1;
        } elsif ($rq_has_data) {
            $current_timestamp = $rq_timestamp;
            $use_rq_data = 1;
        }

        # --- Stage 2: Validate the timestamp before proceeding (FIX 1) ---
        my $tp;
        eval { $tp = Time::Piece->strptime($current_timestamp, "%Y-%m-%d %H:%M:%S"); };
        if ($@ || !defined($tp)) {
            print STDERR "WARN: Malformed timestamp '$current_timestamp' found. Skipping line.\n";
            # Use the flags we just set to advance past the bad line(s) and prevent an infinite loop.
            $pc_has_data = $read_pc_line->() if $use_pc_data;
            $rq_has_data = $read_rq_line->() if $use_rq_data;
            next; # Continue to the next iteration of the while loop
        }

        # --- Stage 3: Process the data for the now-validated timestamp ---
        my $epoch = $tp->epoch;
        $min_epoch = $epoch if !defined $min_epoch || $epoch < $min_epoch;
        $max_epoch = $epoch if !defined $max_epoch || $epoch > $max_epoch;

        my $ts_vm_prefix = "$current_timestamp,";
        my %processed_vms;

        if ($use_pc_data) {
            # --- Field count validation (FIX 2) is inside the helper sub $read_pc_line->()
            for my $i (0 .. $#physc_vm_names) {
                my $vm = $physc_vm_names[$i];
                next unless defined $vm && length($vm);
                $all_vms_in_file{$vm} = 1;
                $processed_vms{$vm} = 1;
                my $physc = (defined $pc_data[$i] && looks_like_number($pc_data[$i])) ? $pc_data[$i] : '';
                my $runq = '';
                if ($use_rq_data && exists $runq_vm_index{$vm}) {
                    my $rq_idx = $runq_vm_index{$vm};
                    $runq = (defined $rq_data[$rq_idx] && looks_like_number($rq_data[$rq_idx])) ? $rq_data[$rq_idx] : '';
                }
                if (length($physc) || length($runq)) {
                    my $line = $ts_vm_prefix . $vm . ',' . $physc . ',' . $runq . "\n";
                    $output_buffer .= $line;
                    $buffer_size += length($line);
                }
            }
        }

        if ($use_rq_data) {
            # --- Field count validation (FIX 2) is inside the helper sub $read_rq_line->()
            for my $i (0 .. $#runq_vm_names) {
                my $vm = $runq_vm_names[$i];
                next unless defined $vm && length($vm);
                next if $processed_vms{$vm};
                $all_vms_in_file{$vm} = 1;
                my $runq = (defined $rq_data[$i] && looks_like_number($rq_data[$i])) ? $rq_data[$i] : '';
                if (length($runq)) {
                    my $line = $ts_vm_prefix . $vm . ',,' . $runq . "\n";
                    $output_buffer .= $line;
                    $buffer_size += length($line);
                }
            }
        }

        if ($buffer_size >= $max_buffer_size) {
            print $data_fh $output_buffer;
            $output_buffer = '';
            $buffer_size = 0;
        }

        # --- Stage 4: Advance file handles for the next iteration on success ---
        if ($use_pc_data) {
            $pc_has_data = $read_pc_line->();
        }
        if ($use_rq_data) {
            $rq_has_data = $read_rq_line->();
        }
    }

    # Flush any remaining buffer
    if ($buffer_size > 0) {
        print $data_fh $output_buffer;
    }

    close $pc_fh;
    close $rq_fh if $has_runq;
    close $data_fh;

    # 4. Create a single, static state for each VM found (OPTION 1 OPTIMIZED)
    foreach my $vm_name (keys %all_vms_in_file) {
        # OPTION 1 OPTIMIZATION: Single hash lookup with fallback reference
        my $vm_config = $vm_config_data{$vm_name} || {};

        my %metadata = (
            vm_name => $vm_name,
            smt => $vm_config->{smt} // $DEFAULT_SMT,
            max_cpu => $vm_config->{maxcpu} // 0,
            entitlement => $vm_config->{entitlement} // 0,
            pool_id => $vm_config->{pool_id} // 0,
            serial_number => $vm_config->{serial_number} // '',
            capped => 0,
        );

        my $p_type = $vm_config->{proc_type} // 'UNK';
        $p_type =~ s/PowerPC_//;
        my $p_ver = $vm_config->{proc_version} // 'UNK';
        my $p_clk = $vm_config->{proc_clock} // 'UNK';

        $metadata{processor_state} = join("_", $p_type, $p_ver, $p_clk);
        $metadata{processor_state} =~ s/$whitespace_re/-/g;

        my $state_obj = {
            vm_name => $vm_name,
            state_id => 1,
            start_epoch => $min_epoch,
            end_epoch => $max_epoch,
            start_time => Time::Piece->new($min_epoch),
            end_time => Time::Piece->new($max_epoch),
            metadata => \%metadata,
        };

        push @{$all_vm_states{$vm_name}}, $state_obj;
    }

    # 5. Write the states file
    nstore(\%all_vm_states, $states_cache_file) or die "Error: Could not serialize states to '$states_cache_file': $!";
    print STDERR "  -> Completed state configuration cache creation ('$states_cache_file').\n";

    # This is the final step in a successful build. Writing this file "commits" the cache.
    open my $man_fh, '>', $manifest_file or die "Error: Cannot write manifest file '$manifest_file': $!";
    my $timestamp = localtime->strftime('%Y-%m-%dT%H:%M:%S');
    print $man_fh "cache_build_status: success\n";
    print $man_fh "build_timestamp: $timestamp\n";
    print $man_fh "nfit_version: $VERSION\n";
    close $man_fh;

    print STDERR "  -> Cache manifest created successfully.\n" if ($verbose);

    # --- Release Lock ---
    close $lock_fh;
    unlink $lock_file;
    print STDERR "  INFO: Released cache lock.\n" if ($verbose);

    return \%all_vm_states;
}

# --- clean_perc_label ---
# Helper to format a percentile number into a clean string for metric keys.
sub clean_perc_label {
    my ($p) = @_;
    my $label = sprintf("%.2f", $p);
    $label =~ s/\.?0+$//;
    $label = "0" if $label eq "" && abs($p-0)<0.001;
    return $label;
}

# --- calculate_rolling_average ---
# Generic sub to calculate a single rolling average point (SMA or EMA).
sub calculate_rolling_average {
    my ($value, $method, $sma_queue_aref, $prev_ema_sref, $window, $alpha) = @_;

    my $avg_to_return;
    if ($method eq 'sma') {
        push @$sma_queue_aref, $value;
        shift @$sma_queue_aref while @$sma_queue_aref > $window;
        if (@$sma_queue_aref == $window) {
            $avg_to_return = calculate_average(@$sma_queue_aref);
        }
    } else { # ema
        if (defined $value) {
            if (!defined $$prev_ema_sref) { $$prev_ema_sref = $value; }
            else { $$prev_ema_sref = ($value * $alpha) + ($$prev_ema_sref * (1 - $alpha)); }
        }
        push @$sma_queue_aref, $value; # Use sma_queue as a simple counter
        shift @$sma_queue_aref while @$sma_queue_aref > $window;
        if (@$sma_queue_aref == $window) {
            $avg_to_return = $$prev_ema_sref;
        }
    }
    return $avg_to_return;
}

# --- define_configuration_states ---
# Performs the first pass over all .nmon files in a directory to detect
# changes in key configuration metadata (SMT, Entitlement, vCPU, etc.).
# It builds a timeline of these changes and consolidates them into discrete
# "state windows" where the configuration for a VM was static.
#
# Args:
#   1. $dir (string): The path to the directory containing .nmon files.
#
# Returns:
#   - A hash reference where keys are VM names. Each VM name maps to an array
#     of state window hashes. Each state window hash contains:
#     {
#       vm_name      => string,
#       state_id     => int,
#       start_time   => Time::Piece object,
#       end_time     => Time::Piece object,
#       duration_days => float,
#       metadata     => {
#         smt          => int,
#         entitlement  => float,
#         virtual_cpus => int,
#         pool_cpu     => int,
#         capped       => int (0 or 1),
#         max_cpu      => float,
#         proc_type    => string,
#         proc_version => string,
#         proc_clock   => string,
#         processor_state => string
#       }
#     }
sub define_configuration_states
{
    my ($dir, $default_smt) = @_;

    my @nmon_files;
	 find(
		  sub {
				return unless -f;  # Skip if not a regular file
				return unless /\.(nmon|nmon\.gz|nmon\.bz2|nmon\.bzip2)$/i;
				push @nmon_files, $File::Find::name;
		  },
		  $dir
	 );

    unless (@nmon_files)
    {
        die "Error: No .nmon files found in directory '$dir'.\n";
    }

    @nmon_files = sort @nmon_files;
    print STDERR "  Found " . scalar(@nmon_files) . " NMON files to scan for metadata.\n";

    my %config_events_by_vm;
    my %file_boundaries;

    # --- Pass 1.1: Scan each file to gather all raw metadata events ---
    foreach my $file_path (@nmon_files)
    {
        # Handle both compressed or uncompressed files.
		  my $fh;
		  if ($file_path =~ /\.nmon\.gz$/i) {
				open $fh, "-|", "gzip", "-dc", "--", $file_path or do {
					 warn "Warning: Could not decompress '$file_path': $!. Skipping.";
					 next;
				};
		  }
		  elsif ($file_path =~ /\.nmon\.(bz2|bzip2)$/i) {
				open $fh, "-|", "bzcat", "--", $file_path or do {
					 warn "Warning: Could not bzcat '$file_path': $!. Skipping.";
					 next;
				};
		  }
		  elsif ($file_path =~ /\.nmon$/i) {
				open $fh, '<:encoding(utf8)', $file_path or do {
					 warn "Warning: Could not open '$file_path': $!. Skipping.";
					 next;
				};
		  }
		  else {
				warn "Warning: Unsupported file type '$file_path'. Skipping.";
				next;
		  }

        my $current_vm;
        my %zzzz_map;
        my %file_static_config;

        while (my $line = <$fh>)
        {
            chomp $line; $line =~ s/\r$//;
            my @fields = split ',', $line;
            my $key = $fields[0];

            if ($key eq 'AAA' && $fields[1] eq 'host') {
               $current_vm = $fields[2];
            }
            elsif ($key eq 'AAA' && $fields[1] eq 'SerialNumber') {
                $file_static_config{serial_number} = $fields[2];
            }
            elsif ($key eq 'ZZZZ') {
                my ($t_num, $time, $date_str) = @fields[1..3];
                my $tp_date;
                eval { $tp_date = Time::Piece->strptime($date_str, "%d-%b-%Y"); };
                if ($@ || !$tp_date) { eval { $tp_date = Time::Piece->strptime($date_str, "%d-%B-%Y"); }; }
                next if ($@ || !$tp_date);
                my $timestamp = $tp_date->ymd . " " . $time;
                $zzzz_map{$t_num} = $timestamp;
                $file_boundaries{$current_vm}{$timestamp} = 1;
            }
            elsif ($key eq 'BBBL') {
                my ($id, $bkey, $bval) = @fields[1..3];
                if    ($id eq '04') { $file_static_config{virtual_cpus} = $bval + 0 if looks_like_number($bval); }
                elsif ($id eq '06') { $file_static_config{smt}          = $bval + 0 if looks_like_number($bval); }
                elsif ($id eq '07') { $file_static_config{capped}       = ($bval =~ /true|1/i ? 1 : 0); }
                elsif ($id eq '18') { $file_static_config{pool_cpu}      = $bval + 0 if looks_like_number($bval); }
            }
            elsif ($key eq 'BBBP') {
                my $bval = $fields[3] // "";
                if    ($bval =~ /"Processor Type:\s*(.+?)"?$/)    { $file_static_config{proc_type} = $1; }
                elsif ($bval =~ /"Processor Version:\s*(.+?)"?$/) { $file_static_config{proc_version} = $1; }
                elsif ($bval =~ /"Processor Clock Speed:\s*(\d+)/){ $file_static_config{proc_clock} = $1; }
                elsif ($bval =~ /Shared Pool ID\s*:\s*(\d+)/) { $file_static_config{pool_id} = $1; }
            }
            elsif ($key eq 'LPAR') {
                my $t_num = $fields[1];
                if (defined $zzzz_map{$t_num}) {
                    my $ts_obj = Time::Piece->strptime($zzzz_map{$t_num}, "%Y-%m-%d %H:%M:%S");
                    my $ent_val = $fields[6]; # Entitled is 5th field after Txxxx
                    if (defined $ent_val && looks_like_number($ent_val)) {
                        push @{$config_events_by_vm{$current_vm}}, { ts => $ts_obj->epoch, key => 'entitlement', value => $ent_val + 0 };
                    }
                }
            }
        }
        close $fh;

        if (defined $current_vm && %file_static_config) {
            my @file_ts_epochs = map { Time::Piece->strptime($_, "%Y-%m-%d %H:%M:%S")->epoch } values %zzzz_map;
            next unless @file_ts_epochs;
            my $file_start_ts = (sort {$a <=> $b} @file_ts_epochs)[0];
            foreach my $key (keys %file_static_config) {
                push @{$config_events_by_vm{$current_vm}}, { ts => $file_start_ts, key => $key, value => $file_static_config{$key} };
            }
        }
    }

    # --- Pass 1.2: Process events grouped by timestamp to define state windows ---
    my %all_vm_states;
    foreach my $vm_name (sort keys %config_events_by_vm) {
        # Group all events by their exact timestamp to treat changes at one moment as atomic.
        my %events_by_ts;
        foreach my $event (@{$config_events_by_vm{$vm_name}}) {
            $events_by_ts{$event->{ts}}{$event->{key}} = $event->{value};
        }

        my @sorted_timestamps = sort {$a <=> $b} keys %events_by_ts;
        next unless @sorted_timestamps;

        my %last_known_metadata;
        my $last_fingerprint = "";
        my $current_state_start_epoch;

        # Iterate through the unique timestamps where configuration changes occurred.
        for (my $i = 0; $i < @sorted_timestamps; $i++) {
            my $ts = $sorted_timestamps[$i];

            # Create a snapshot of the current state BEFORE applying the new events
            my %metadata_before_change = %last_known_metadata;

            # Apply all events for this timestamp to a temporary hash to create the new state
            my %snapshot_metadata = %last_known_metadata;
            foreach my $key (keys %{$events_by_ts{$ts}}) {
                $snapshot_metadata{$key} = $events_by_ts{$ts}{$key};
            }

            my $current_fingerprint = join "|", map { $snapshot_metadata{$_} // 'U' }
                                      sort qw(entitlement smt virtual_cpus pool_cpu capped proc_type proc_version proc_clock);

            if ($i == 0) {
                # This is the very first event for the VM. Initialize the first state.
                $last_fingerprint = $current_fingerprint;
                $current_state_start_epoch = $ts;
            } elsif ($current_fingerprint ne $last_fingerprint) {
                # A change was detected. Finalize the PREVIOUS state using the metadata from before the change.
                push @{$all_vm_states{$vm_name}}, {
                    start_epoch => $current_state_start_epoch,
                    end_epoch   => $ts - 1, # End time is 1 second before the change.
                    metadata    => { %metadata_before_change } # CRITICAL: Use the state BEFORE the change.
                };

                # Start the new state.
                $current_state_start_epoch = $ts;
                $last_fingerprint = $current_fingerprint;
            }

            # Persist the fully updated state for the next iteration.
            %last_known_metadata = %snapshot_metadata;
        }

        # After the loop, finalize the very last state for the VM.
        my $vm_timestamps = [sort {$a <=> $b} map {Time::Piece->strptime($_, "%Y-%m-%d %H:%M:%S")->epoch} keys %{$file_boundaries{$vm_name}}];
        my $final_end_epoch = @$vm_timestamps ? $vm_timestamps->[-1] : $current_state_start_epoch;

        if (defined $current_state_start_epoch) {
            push @{$all_vm_states{$vm_name}}, {
                start_epoch => $current_state_start_epoch,
                end_epoch   => $final_end_epoch,
                metadata    => \%last_known_metadata
            };
        }

        # Post-process: Add IDs, duration, and calculated fields
        for (my $i=0; $i < @{$all_vm_states{$vm_name} || []}; $i++) {
            my $state = $all_vm_states{$vm_name}[$i];
            $state->{state_id} = $i + 1;
            $state->{start_time} = Time::Piece->new($state->{start_epoch});
            $state->{end_time} = Time::Piece->new($state->{end_epoch});
            $state->{duration_days} = ($state->{end_epoch} - $state->{start_epoch}) / (60*60*24);
            $state->{vm_name} = $vm_name;

            my $md = $state->{metadata};
            $md->{smt} //= $default_smt;
            my $ent = $md->{entitlement} // 0;
            my $vcpu = $md->{virtual_cpus} // 0;
            my $poolcpu = $md->{pool_cpu} // 0;
            my $is_capped = $md->{capped} // 0;
            $md->{lpar_state} = "capped=$is_capped";

            my $max_cpu_calc = $is_capped ? $ent : ($poolcpu > 0 && $vcpu > 0 ? min($vcpu, $poolcpu) : $vcpu);
            $md->{max_cpu} = $max_cpu_calc > 0 ? $max_cpu_calc : ($vcpu > 0 ? $vcpu : 0);

            my $p_type = $md->{proc_type} // 'UNK'; $p_type =~ s/PowerPC_//;
            my $p_ver = $md->{proc_version} // 'UNK';
            my $p_clk = $md->{proc_clock} // 'UNK';

            $md->{processor_state} = join("_", $p_type, $p_ver, $p_clk);
            $md->{processor_state} =~ s/\s+/-/g;
            $md->{duration_days} = $state->{duration_days};
        }
    }

    print STDERR "Phase 1 finished. Found states for " . scalar(keys %all_vm_states) . " VMs.\n";
    return \%all_vm_states;
}

# --- print_state_windows_report ---
# Takes the data structure returned by define_configuration_states and
# prints a formatted, compact report to STDOUT.
#
# Args:
#   1. $states_href (hash ref): The state windows data structure.
#
sub print_state_windows_report
{
    my ($states_href) = @_;

    my $header_format = "%-20s %-10s %-20s %-20s %-10s %-45s %s\n";
    my $row_format    = "%-20s %-10s %-20s %-20s %-10.2f %-45s %s\n";

    printf $header_format, "VM_Name", "State_ID", "Start_Time", "End_Time", "Duration", "Config_Summary", "Processor";

    my $separator = ("-" x 20) . " " . ("-" x 10) . " " . ("-" x 20) . " " . ("-" x 20) . " " . ("-" x 10) . " " . ("-" x 45) . " " . ("-" x 30) . "\n";
    print STDOUT $separator;

    foreach my $vm_name (sort keys %$states_href)
    {
        my @vm_states = @{$states_href->{$vm_name}};
        my $total_states = scalar(@vm_states);
        foreach my $state (@vm_states)
        {
            my $md = $state->{metadata};

            # Build the compact configuration summary string
            my $capped_str = ($md->{capped} // 0) ? "Capped" : "Uncapped";
            my $config_summary = sprintf("Ent:%.2f vCPU:%d PoolCPU:%d SMT:%d %s",
                $md->{entitlement} // 0,
                $md->{virtual_cpus} // 0,
                $md->{pool_cpu} // 0,
                $md->{smt} // 0,
                $capped_str
            );

            printf $row_format,
                $state->{vm_name},
                $state->{state_id} . "/" . $total_states,
                $state->{start_time}->strftime('%Y-%m-%d %H:%M'),
                $state->{end_time}->strftime('%Y-%m-%d %H:%M'),
                $state->{duration_days},
                $config_summary,
                $md->{processor_state};
        }
    }
}

# --- parse_state_selector ---
# Parses the user's state selection string (e.g., "1,3,5-7,-1") and returns
# an array of the actual state window objects that match the selection.
#
# Args:
#   1. $selector_str (string): The raw string from the --include-states flag.
#   2. $available_states_aref (array ref): An array of all state window hashes for a VM.
#
# Returns:
#   - An array containing the state window hashes that were selected.
#
sub parse_state_selector
{
    my ($selector_str, $available_states_aref) = @_;
    my @selected_states;
    return @$available_states_aref if (lc($selector_str) eq 'all');
    return () if (scalar(@$available_states_aref) == 0);
    my %selected_ids;
    my $total_states = scalar(@$available_states_aref);
    my @parts = split ',', $selector_str;
    foreach my $part (@parts) {
        $part =~ s/\s+//g;
        if ($part =~ /^(\d+)-(\d+)$/) {
            for my $id ($1 .. $2) { $selected_ids{$id} = 1; }
        } elsif ($part =~ /^-(\d+)$/) {
            my $offset = $1;
            if ($total_states - $offset >= 0) { $selected_ids{ $total_states - ($offset - 1) } = 1; }
        } elsif ($part =~ /^\d+$/) {
            $selected_ids{$part} = 1;
        }
    }
    foreach my $state (@$available_states_aref) {
        push @selected_states, $state if exists $selected_ids{$state->{state_id}};
    }
    return @selected_states;
}

sub process_nmon_directory
{
    my ($dir, $physc_out_fh, $runq_out_fh) = @_;

    opendir(my $dh, $dir) or die "Error: Cannot open directory '$dir': $!";
    my @nmon_files = grep { /\.nmon$/ && -f File::Spec->catfile($dir, $_) } readdir($dh);
    closedir($dh);

    if (!@nmon_files)
    {
        die "Error: No .nmon files found in directory '$dir'.\n";
    }

    my %data; # {timestamp} -> {vm_name} -> {physc => val, runq => val}
    my %vm_names;
    my $smt_value_from_file;

    # First pass: Get SMT from the first file
    my $first_file_path = File::Spec->catfile($dir, $nmon_files[0]);
    open(my $fh_first, '<:encoding(utf8)', $first_file_path) or die "Error opening $first_file_path: $!";
    while (my $line = <$fh_first>)
    {
        if ($line =~ /^BBBL,06,smt threads,(\d+)/)
        {
            $smt_value_from_file = $1;
            last;
        }
    }
    close $fh_first;

    if (defined $smt_value_from_file)
    {
        $smt_value = $smt_value_from_file;
        print STDERR "  Using SMT value of $smt_value from the first NMON file.\n";
    }
    else
    {
        print STDERR "Warning: Could not determine SMT from first file. Using default of $DEFAULT_SMT.\n";
    }

    # Second pass: Process all files
    foreach my $file (@nmon_files)
    {
        my $file_path = File::Spec->catfile($dir, $file);
        print STDERR "Processing file: $file\n" if $verbose;
        open(my $fh, '<:encoding(utf8)', $file_path) or die "Error opening $file_path: $!";

        my %timestamps;
        my $vm_name;

        while (my $line = <$fh>)
        {
            chomp $line;
            $line =~ s/\r$//;

            if ($line =~ /^AAA,host,(.+)$/)
            {
                $vm_name = $1;
                $vm_names{$vm_name} = 1;
            }
            elsif ($line =~ /^ZZZZ,T(\d+),(\d{2}:\d{2}:\d{2}),(.+)$/)
            {
                my ($t_num, $time, $date_str) = ($1, $2, $3);
                my $tp_date = Time::Piece->strptime($date_str, "%d-%b-%Y");
                $timestamps{$t_num} = $tp_date->ymd . " " . $time;
            }
            elsif ($line =~ /^LPAR,T(\d+),(.+)$/)
            {
                my ($t_num, $lpar_data) = ($1, $2);
                my @lpar_fields = split(/,/, $lpar_data);
                if (defined $timestamps{$t_num} && defined $vm_name)
                {
                    $data{$timestamps{$t_num}}{$vm_name}{physc} = $lpar_fields[1];
                }
            }
            elsif ($line =~ /^CPU_ALL,T(\d+),(.+)$/)
            {
                my ($t_num, $cpu_data) = ($1, $2);
                my @cpu_fields = split(/,/, $cpu_data);
                if (defined $timestamps{$t_num} && defined $vm_name)
                {
                    $data{$timestamps{$t_num}}{$vm_name}{runq} = $cpu_fields[5];
                }
            }
        }
        close $fh;
    }

    # Generate the temporary CSV files
    my @sorted_timestamps = sort keys %data;
    my @sorted_vm_names = sort keys %vm_names;

    # Write headers
    print $physc_out_fh "Time," . join(",", @sorted_vm_names) . "\n";
    print $runq_out_fh "Time," . join(",", @sorted_vm_names) . "\n";

    # Write data
    foreach my $ts (@sorted_timestamps)
    {
        my @physc_row = ($ts);
        my @runq_row = ($ts);
        foreach my $vm (@sorted_vm_names)
        {
            push @physc_row, $data{$ts}{$vm}{physc} // '';
            push @runq_row, $data{$ts}{$vm}{runq} // '';
        }
        print $physc_out_fh join(",", @physc_row) . "\n";
        print $runq_out_fh join(",", @runq_row) . "\n";
    }
}

# Calculate basic statistics (mean, stddev, CV) for a list of values
# Input: reference to an array of numeric values
# Output: hashref { mean, stddev, cv, count } or undef if insufficient data
sub calculate_statistics_for_trend
{
    my ($values_aref) = @_;
    my @defined_values = grep { defined $_ && $_ =~ /^-?[0-9.]+$/ } @{$values_aref};
    my $count = scalar @defined_values;

    return undef if $count == 0;

    my $sum = sum0(@defined_values);
    my $mean = $sum / $count;
    my $stddev = 0;
    my $cv = undef;

    if ($count > 1)
    {
        my $sum_sq_diff = 0;
        foreach my $val (@defined_values)
        {
            $sum_sq_diff += ($val - $mean)**2;
        }
        $stddev = sqrt($sum_sq_diff / ($count - 1));
    }

    if (abs($mean) > $FLOAT_EPSILON)
    {
        $cv = $stddev / $mean;
    }
    elsif ($stddev < $FLOAT_EPSILON && abs($mean) < $FLOAT_EPSILON)
    {
        $cv = 0;
    }

    return {
        mean   => $mean,
        stddev => $stddev,
        cv     => $cv,
        count  => $count,
    };
}

# Calculate linear regression (slope, intercept, R-squared) manually
# Input: reference to an array of [x, y] points, where x and y are numeric.
#        x is typically a time index (0, 1, 2,...), y is the metric value.
# Output: hashref { slope, intercept, r_squared, n_points } or undef if
#         insufficient data (n < 2) or if slope cannot be determined (e.g., all x values are identical).
sub calculate_manual_linear_regression
{
    my ($points_aref) = @_;
    my $n = scalar @{$points_aref};

    # Regression requires at least 2 distinct points to define a line.
    return undef if $n < 2;

    my $sum_x = 0;
    my $sum_y = 0;
    my $sum_xy = 0;
    my $sum_x_squared = 0;
    my $sum_y_squared = 0;

    foreach my $point (@{$points_aref})
    {
        my ($x_val, $y_val) = @{$point};

        # Assuming $x_val and $y_val are already numeric based on upstream processing.
        # If strict validation is needed here, it can be added, but the input
        # preparation logic should ensure numeric data.

        $sum_x += $x_val;
        $sum_y += $y_val;
        $sum_xy += $x_val * $y_val;
        $sum_x_squared += $x_val**2;
        $sum_y_squared += $y_val**2;
    }

    my $slope_calc     = undef;
    my $intercept_calc = undef;
    my $r_squared_calc = undef;

    # Denominator for slope calculation: N * sum(x^2) - (sum(x))^2
    # This is also N * SS_xx (where SS_xx is sum of squares for x)
    my $denominator_slope = ($n * $sum_x_squared) - ($sum_x**2);

    # Check if denominator is too close to zero (implies x values are not distinct enough
    # or only one unique x value if n > 1, which makes slope undefined or infinite).
    if (abs($denominator_slope) > $FLOAT_EPSILON)
    {
        $slope_calc = (($n * $sum_xy) - ($sum_x * $sum_y)) / $denominator_slope;
        $intercept_calc = ($sum_y - ($slope_calc * $sum_x)) / $n;

        # Calculate R-squared (Coefficient of Determination)
        # R^2 = (N * sum(xy) - sum(x) * sum(y))^2 / ((N * sum(x^2) - (sum(x))^2) * (N * sum(y^2) - (sum(y))^2))
        my $numerator_r_sq_squared = (($n * $sum_xy) - ($sum_x * $sum_y))**2;
        my $denominator_r_sq_part_yy = ($n * $sum_y_squared) - ($sum_y**2);

        if (abs($denominator_r_sq_part_yy) > $FLOAT_EPSILON) # Avoid division by zero if all y values are the same
        {
            $r_squared_calc = $numerator_r_sq_squared / ($denominator_slope * $denominator_r_sq_part_yy);
            # Clamp R-squared to [0, 1] due to potential floating point inaccuracies
            $r_squared_calc = 0.0 if defined $r_squared_calc && $r_squared_calc < 0;
            $r_squared_calc = 1.0 if defined $r_squared_calc && $r_squared_calc > 1.0;
        }
        elsif (abs($numerator_r_sq_squared) < $FLOAT_EPSILON**2) # All y are same, and slope is ~0
        {
            # If all y values are identical, the line should perfectly predict them if slope is also ~0.
            $r_squared_calc = 1.0;
        }
        else
        {
            # This case implies an issue or perfect vertical correlation if all x were same (but ss_xx_calc > 0 here)
            # If y is constant, variance of y is 0. If model doesn't perfectly predict this constant, R^2 can be odd.
            # Given y is constant, and slope is non-zero, it means the line isn't horizontal.
            # SS_tot would be 0. SS_res would be >0. R^2 = 1 - SS_res/SS_tot is undefined.
            # However, if $denominator_r_sq_part_yy is near zero, it implies SS_tot is near zero.
            # If the numerator $numerator_r_sq_squared is also near zero, it suggests the model fits perfectly (R^2 = 1).
            # If numerator is not zero but denominator_yy is, it's effectively a poor fit for variation that doesn't exist.
            $r_squared_calc = 0.0; # Default to 0 for poor/undefined fit in this edge scenario
        }
    }
    else
    {
        # Denominator for slope is zero or too small.
        # This happens if all x values are (nearly) identical.
        # Cannot reliably determine a linear trend.
        return undef;
    }

    return {
        slope     => $slope_calc,
        intercept => $intercept_calc,
        r_squared => $r_squared_calc,
        n_points  => $n,
    };
}

sub process_and_store_window_results
{
    my ($window_def_ref, $current_vm_win_buf_href, $vm_overall_win_data_href,
        $all_vm_names_aref, $target_vm_idx_nullable, $target_vm_name_nullable,
        $main_physc_percentile_arg, $calc_peak_arg, $has_runq_data_arg,
        $runq_norm_p_aref_arg, $runq_abs_p_aref_arg,
        $current_runq_avg_method_arg, $current_win_minutes_arg, $current_alpha_for_ema_arg_unused
    ) = @_;

    my ($win_start, $win_end, $win_rep_date) = @{$window_def_ref};
    my $win_idx = -1;
    for (my $i=0; $i < @processing_windows; $i++)
    {
        if ($processing_windows[$i][2]->epoch == $win_rep_date->epoch)
        {
            $win_idx = $i;
            last;
        }
    }
    return unless $win_idx != -1;
    my $window_key = $win_rep_date->ymd('') . "_" . $win_idx;

    my @vms_to_process_this_window_list;
    if (defined $target_vm_idx_nullable)
    {
        push @vms_to_process_this_window_list, $target_vm_name_nullable;
    }
    else
    {
        @vms_to_process_this_window_list = @{$all_vm_names_aref};
    }

    foreach my $vm_name_local (@vms_to_process_this_window_list) # Use local var
    {
        next unless (exists $current_vm_win_buf_href->{$vm_name_local} && exists $current_vm_win_buf_href->{$vm_name_local}{$window_key});
        my $vm_win_data = $current_vm_win_buf_href->{$vm_name_local}{$window_key};

        my $p_label_main = sprintf("P%.2f", $main_physc_percentile_arg);
        $p_label_main =~ s/\.?0+$//;
        $p_label_main = "0" if $p_label_main eq "" && abs($main_physc_percentile_arg-0)<0.001;

        if (exists $vm_win_data->{rolling_physc_avgs} && @{$vm_win_data->{rolling_physc_avgs}})
        {
            my @sorted_physc_avgs = sort {$a <=> $b} grep {defined $_ && $_ =~ /^-?[0-9.]+$/} @{$vm_win_data->{rolling_physc_avgs}};
            if (@sorted_physc_avgs)
            {
                my $physc_perc_val = calculate_percentile(\@sorted_physc_avgs, $main_physc_percentile_arg);
                $vm_overall_win_data_href->{$vm_name_local}{$window_key}{$p_label_main} = defined($physc_perc_val) ? $physc_perc_val : "N/A";
            }
            else
            {
                $vm_overall_win_data_href->{$vm_name_local}{$window_key}{$p_label_main} = "N/A";
            }
        }
        else
        {
            $vm_overall_win_data_href->{$vm_name_local}{$window_key}{$p_label_main} = "N/A";
        }

        if ($calc_peak_arg)
        {
            if (exists $vm_win_data->{raw_physc_for_peak} && @{$vm_win_data->{raw_physc_for_peak}})
            {
                my @defined_peaks = grep {defined $_ && $_ =~ /^-?[0-9.]+$/} @{$vm_win_data->{raw_physc_for_peak}};
                $vm_overall_win_data_href->{$vm_name_local}{$window_key}{'Peak'} = @defined_peaks ? max(@defined_peaks) : "N/A";
            }
            else
            {
                $vm_overall_win_data_href->{$vm_name_local}{$window_key}{'Peak'} = "N/A";
            }
        }
        if ($has_runq_data_arg)
        {
            my @abs_rq_for_perc_cleaned = grep {defined $_ && $_ =~ /^-?[0-9.]+$/} @{$vm_win_data->{abs_rq_values_for_perc}};
            foreach my $p_val (@{$runq_abs_p_aref_arg})
            {
                my $p_num_label = sprintf("%.2f", $p_val);
                $p_num_label =~ s/\.?0+$//;
                $p_num_label = "0" if $p_num_label eq "" && abs($p_val-0)<0.001;
                my $metric_key = "AbsRunQ_P$p_num_label";
                if (@abs_rq_for_perc_cleaned)
                {
                    my @sorted_vals = sort {$a <=> $b} @abs_rq_for_perc_cleaned;
                    my $val = calculate_percentile(\@sorted_vals, $p_val);
                    $vm_overall_win_data_href->{$vm_name_local}{$window_key}{$metric_key} = defined($val) ? sprintf("%.2f", $val) : "N/A";
                }
                else
                {
                    $vm_overall_win_data_href->{$vm_name_local}{$window_key}{$metric_key} = "N/A";
                }
            }
            my @norm_rq_for_perc_cleaned = grep {defined $_ && $_ =~ /^-?[0-9.]+$/} @{$vm_win_data->{norm_rq_values_for_perc}};
            foreach my $p_val (@{$runq_norm_p_aref_arg})
            {
                my $p_num_label = sprintf("%.2f", $p_val);
                $p_num_label =~ s/\.?0+$//;
                $p_num_label = "0" if $p_num_label eq "" && abs($p_val-0)<0.001;
                my $metric_key = "NormRunQ_P$p_num_label";
                if (@norm_rq_for_perc_cleaned)
                {
                    my @sorted_vals = sort {$a <=> $b} @norm_rq_for_perc_cleaned;
                    my $val = calculate_percentile(\@sorted_vals, $p_val);
                    $vm_overall_win_data_href->{$vm_name_local}{$window_key}{$metric_key} = defined($val) ? sprintf("%.2f", $val) : "N/A";
                }
                else
                {
                    $vm_overall_win_data_href->{$vm_name_local}{$window_key}{$metric_key} = "N/A";
                }
            }
        }
    }
}


# --- Existing Subroutines from Original Script ---
sub parse_percentile_list
{
    my ($perc_str, $arg_name) = @_;
    my @percentiles;
    if (defined $perc_str && $perc_str ne '')
    {
        my @raw_percentiles = split /,\s*/, $perc_str;
        foreach my $p (@raw_percentiles)
        {
            if ($p !~ /^[0-9]+(?:\.[0-9]+)?$/ || $p < 0 || $p > 100)
            {
                die "Error: Invalid percentile value '$p' in --$arg_name list. Must be numeric between 0 and 100.\n";
            }
            push @percentiles, $p + 0;
        }
    }
    return @percentiles;
}

sub get_nmon_overall_date_range
{
    my ($nmon_file, $global_start_filter_str, $global_end_filter_str) = @_;
    print STDERR "Scanning NMON file '$nmon_file' for overall effective date range...\n" if ($verbose);
	 # Handle both compressed or uncompressed files.
	 my $fh;
	 if ($nmon_file =~ /\.nmon\.gz$/i) {
        open $fh, "-|", "gzip", "-dc", "--", $nmon_file or do {
				warn "Warning: Could not decompress '$nmon_file': $!. Skipping.";
				next;
		  };
	 }
	 elsif ($nmon_file =~ /\.nmon\.(bz2|bzip2)$/i) {
		  open $fh, "-|", "bzcat", "--", $nmon_file or do {
				warn "Warning: Could not bzcat '$nmon_file': $!. Skipping.";
				next;
		  };
	 }
	 elsif ($nmon_file =~ /\.nmon$/i) {
		  open $fh, '<:encoding(utf8)', $nmon_file or do {
				warn "Warning: Could not open '$nmon_file': $!. Skipping.";
				next;
		  };
	 }
	 else {
		  warn "Warning: Unsupported file type '$nmon_file'. Skipping.";
		  next;
	 }
    my $min_date_obj;
    my $max_date_obj;
    my $header_skipped = 0;
    my $first_data_line_checked = 0;
    my $start_filter_obj;
    my $end_filter_obj;
    if (defined $global_start_filter_str)
    {
        eval { $start_filter_obj = Time::Piece->strptime($global_start_filter_str, "%Y-%m-%d"); };
        if ($@ || (defined $global_start_filter_str && ! (defined $start_filter_obj && $start_filter_obj->isa('Time::Piece') ) ) )
        {
            die "Error parsing global start date filter '$global_start_filter_str': $@\n";
        }
    }
    if (defined $global_end_filter_str)
    {
        eval { $end_filter_obj = Time::Piece->strptime($global_end_filter_str, "%Y-%m-%d"); };
        if ($@ || (defined $global_end_filter_str && ! (defined $end_filter_obj && $end_filter_obj->isa('Time::Piece') ) ) )
        {
            die "Error parsing global end date filter '$global_end_filter_str': $@\n";
        }
    }
    while (my $line = <$fh>)
    {
        chomp $line;
        $line =~ s/\r$//;
        next if $line =~ /^\s*$/;
        if (!$header_skipped && !$first_data_line_checked)
        {
            if ($line =~ /^(Time,|Date,Time,Hostname,ZZZZ)/i)
            {
                $header_skipped = 1;
                next;
            }
            $first_data_line_checked = 1;
        }
        elsif ($header_skipped && $line =~ /^(Time,|Date,Time,Hostname,ZZZZ)/i)
        {
            next;
        }
        if ($line =~ /^(\d{4}-\d{2}-\d{2}[T\s]\d{2}:\d{2}:\d{2})/)
        {
            my $timestamp_str_local = $1;
            $timestamp_str_local =~ s/T/ /;
            my $current_tp;
            eval { $current_tp = Time::Piece->strptime($timestamp_str_local, "%Y-%m-%d %H:%M:%S"); };
            if ($@ || !$current_tp || !$current_tp->isa('Time::Piece'))
            {
                next;
            }
            my $current_date_day_obj = $current_tp->truncate(to => 'day');
            if (defined $start_filter_obj && $current_date_day_obj < $start_filter_obj) { next; }
            if (defined $end_filter_obj   && $current_date_day_obj > $end_filter_obj)   { next; }
            if (!defined $min_date_obj || !$min_date_obj->isa('Time::Piece') || $current_tp < $min_date_obj)
            {
                $min_date_obj = $current_tp;
            }
            if (!defined $max_date_obj || !$max_date_obj->isa('Time::Piece') || $current_tp > $max_date_obj)
            {
                $max_date_obj = $current_tp;
            }
        }
    }
    close $fh;
    if (defined $min_date_obj && $min_date_obj->isa('Time::Piece') && defined $max_date_obj && $max_date_obj->isa('Time::Piece'))
    {
        print STDERR "Effective NMON data range for windowing: " . $min_date_obj->datetime . " to " . $max_date_obj->datetime . "\n";
        my $ret_start = $min_date_obj->truncate(to => 'day');
        my $ret_end   = $max_date_obj->truncate(to => 'day');
        unless (defined $ret_start && $ret_start->isa('Time::Piece') && defined $ret_end && $ret_end->isa('Time::Piece'))
        {
            print STDERR "Error: Truncated date objects became invalid in get_nmon_overall_date_range.\n";
            return (undef, undef);
        }
        return ($ret_start, $ret_end);
    }
    else
    {
        print STDERR "Warning: Could not determine valid min/max dates from NMON file '$nmon_file' after global filters.\n";
        return (undef, undef);
    }
}

sub generate_processing_time_windows
{
    my ($period_start_obj, $period_end_obj, $unit_str, $size_val) = @_;
    my @windows;
    return () unless (defined $period_start_obj && defined $period_end_obj && $period_start_obj->isa('Time::Piece') && $period_end_obj->isa('Time::Piece') && $period_start_obj <= $period_end_obj);
    my $current_window_start = Time::Piece->new($period_start_obj->epoch);
    while ($current_window_start <= $period_end_obj)
    {
        my $current_window_end;
        if ($unit_str eq "days")
        {
            $current_window_end = Time::Piece->new($current_window_start->epoch) + (ONE_DAY() * ($size_val - 1));
        }
        elsif ($unit_str eq "weeks")
        {
            $current_window_end = Time::Piece->new($current_window_start->epoch) + (ONE_WEEK() * $size_val) - ONE_DAY();
        }
        else
        {
            die "Unsupported window unit: $unit_str\n";
        }
        if ($current_window_end > $period_end_obj)
        {
            $current_window_end = Time::Piece->new($period_end_obj->epoch);
        }
        my $representative_date = Time::Piece->new($current_window_end->epoch);
        push @windows, [Time::Piece->new($current_window_start->epoch), Time::Piece->new($current_window_end->epoch), $representative_date];
        my $next_window_start_candidate = $current_window_end + ONE_DAY();
        last if ($next_window_start_candidate > $period_end_obj && $current_window_end >= $period_end_obj);
        $current_window_start = $next_window_start_candidate;
    }
    return @windows;
}

sub get_window_key_for_timestamp
{
    my ($timestamp_obj, $windows_aref, $hint_idx) = @_;
    my $timestamp_day_obj = $timestamp_obj->truncate(to => 'day');
    if (defined $hint_idx && $hint_idx >=0 && $hint_idx < @{$windows_aref})
    {
        my ($win_start, $win_end) = @{$windows_aref->[$hint_idx]};
        if ($timestamp_day_obj >= $win_start && $timestamp_day_obj <= $win_end)
        {
            return $windows_aref->[$hint_idx][2]->ymd('') . "_" . $hint_idx;
        }
    }
    for (my $i=0; $i < @{$windows_aref}; $i++)
    {
        my ($win_start, $win_end) = @{$windows_aref->[$i]};
        if ($timestamp_day_obj >= $win_start && $timestamp_day_obj <= $win_end)
        {
            return $windows_aref->[$i][2]->ymd('') . "_" . $i;
        }
    }
    return undef;
}

sub get_weighted_metric_for_vm
{
    my ($vm_data_by_window_href, $vm_name_local, $metric_key, # Use local var
        $processing_windows_aref, $analysis_ref_obj_arg, $decay_hl_days_arg, $sprintf_fmt_optional) = @_;

    my @metric_window_values;
    if (exists $vm_data_by_window_href->{$vm_name_local})
    {
        foreach my $win_key (sort {
                                my ($ad, $ai) = ($a =~ /^(\d{8})_(\d+)$/);
                                my ($bd, $bi) = ($b =~ /^(\d{8})_(\d+)$/);
                                return ($ad cmp $bd) || ($ai <=> $bi);
                             } keys %{$vm_data_by_window_href->{$vm_name_local}})
        {
            if (exists $vm_data_by_window_href->{$vm_name_local}{$win_key}{$metric_key} &&
                defined $vm_data_by_window_href->{$vm_name_local}{$win_key}{$metric_key} &&
                $vm_data_by_window_href->{$vm_name_local}{$win_key}{$metric_key} ne "N/A" )
            {
                my ($win_date_str_part, $win_idx_part) = ($win_key =~ /^(\d{8})_(\d+)$/);
                if (defined $win_idx_part && $win_idx_part < @{$processing_windows_aref})
                {
                    my $rep_date_obj = $processing_windows_aref->[$win_idx_part][2];
                    unless (defined $rep_date_obj && $rep_date_obj->isa('Time::Piece'))
                    {
                        print STDERR "Warning: Representative date for window key '$win_key' is invalid for VM '$vm_name_local', metric '$metric_key'. Skipping this window point.\n";
                        next;
                    }
                    push @metric_window_values, {
                        value => $vm_data_by_window_href->{$vm_name_local}{$win_key}{$metric_key},
                        date  => $rep_date_obj
                    };
                }
                else
                {
                    print STDERR "Warning: Could not accurately map window key '$win_key' to a processing window definition for VM '$vm_name_local', metric '$metric_key'. Skipping this window point.\n";
                }
            }
        }
    }

    if (@metric_window_values)
    {
        my $final_val_str;
        if ($metric_key eq 'Peak') {
            # For the Peak, we want the absolute maximum value found across all windows.
            my @values = map { $_->{value} } grep { defined $_->{value} && looks_like_number($_->{value}) } @metric_window_values;
            $final_val_str = @values ? max(@values) : "N/A";
        } else {
            # For all other metrics, perform the recency-weighted average.
            $final_val_str = calculate_recency_weighted_average(
                \@metric_window_values, $analysis_ref_obj_arg, $decay_hl_days_arg
            );
        }
        if (defined $sprintf_fmt_optional && defined $final_val_str && $final_val_str ne "N/A" && $final_val_str =~ /^-?[0-9.]+$/)
        {
            return sprintf($sprintf_fmt_optional, $final_val_str + 0);
        }
        return $final_val_str;
    }
    return "N/A";
}

sub calculate_recency_weighted_average
{
    my ($windowed_data_ref, $analysis_ref_obj_arg, $half_life_days_arg) = @_;
    my $sum_weighted_values = 0;
    my $sum_weights = 0;
    return "N/A" if (!defined $analysis_ref_obj_arg || !$analysis_ref_obj_arg->isa('Time::Piece'));
    return "N/A" if (!defined $half_life_days_arg || $half_life_days_arg <= 0);

    my $lambda = log(2) / $half_life_days_arg;

    foreach my $dp_ref (@{$windowed_data_ref})
    {
        my $value_str = $dp_ref->{value};
        next if (!defined $value_str || $value_str eq "N/A" || $value_str !~ /^-?[0-9.]+$/);
        my $value = $value_str + 0;
        my $date_obj = $dp_ref->{date};
        next if (!defined $date_obj || !$date_obj->isa('Time::Piece'));

        my $date_obj_day = $date_obj->truncate(to => 'day');
        my $analysis_ref_day = $analysis_ref_obj_arg->truncate(to => 'day');

        my $days_diff_seconds = $analysis_ref_day->epoch - $date_obj_day->epoch;
        my $days_diff = $days_diff_seconds / ONE_DAY(); # Use seconds from Time::Seconds object
        $days_diff = 0 if $days_diff < 0;

        my $weight = exp(-$lambda * $days_diff);
        $sum_weighted_values += $value * $weight;
        $sum_weights += $weight;
    }

    if ($sum_weights > $FLOAT_EPSILON)
    {
        return sprintf("%.4f", $sum_weighted_values / $sum_weights);
    }
    else
    {
        return "N/A";
    }
}

sub get_vm_index_by_name
{
    my ($vm_name_to_find, $vm_names_list_ref) = @_;
    for (my $i=0; $i < @{$vm_names_list_ref}; $i++)
    {
        if ($vm_names_list_ref->[$i] eq $vm_name_to_find)
        {
            return $i;
        }
    }
    return undef;
}

sub calculate_average
{
    my (@data) = @_;
    my @numbers = grep { defined $_ && looks_like_number($_) } @data;
    return 0 if !@numbers;
    return sum0(@numbers) / scalar(@numbers);
}

sub calculate_percentile
{
    my ($data_ref, $p) = @_;
    my @data_input = @{$data_ref};
    my @data = grep { defined($_) && $_ =~ /^-?[0-9]+(?:\.[0-9]+)?$/ } @data_input;
    my $n = scalar @data;
    return undef if $n == 0;
    @data = sort { $a <=> $b } @data;
    return $data[0] if $n == 1;

    my $rank_fractional = ($p / 100) * ($n - 1);
    my $k = int($rank_fractional);
    my $d = $rank_fractional - $k;

    if ($p == 0) { return $data[0]; }
    if ($p == 100) { return $data[$n-1]; }

    if ($k >= $n - 1)
    {
        return $data[$n - 1];
    }
    elsif ($k < 0)
    {
        return $data[0];
    }
    else
    {
        my $val_k = $data[$k];
        my $val_k_plus_1 = ($k + 1 < $n) ? $data[$k + 1] : $data[$k];
        return $val_k + $d * ($val_k_plus_1 - $val_k);
    }
}

sub calculate_rolling_average_series
{
    my ($data_series_ref, $method, $window_size, $alpha_val) = @_;
    my @output_series;
    my @current_window_sma;
    my $current_ema;

    if ($method eq 'none')
    {
        return $data_series_ref;
    }

    foreach my $value (@{$data_series_ref})
    {
        my $avg_val_to_add = undef;
        if ($method eq 'sma')
        {
            if (defined $value && $value =~ /^-?[0-9.]+$/)
            {
                push @current_window_sma, ($value + 0);
            }
            else
            {
                push @current_window_sma, undef;
            }
            shift @current_window_sma while scalar @current_window_sma > $window_size;
            if (scalar @current_window_sma == $window_size)
            {
                $avg_val_to_add = calculate_average(@current_window_sma);
            }
        }
        elsif ($method eq 'ema')
        {
            if (defined $value && $value =~ /^-?[0-9.]+$/)
            {
                my $numeric_value = $value + 0;
                if (!defined $current_ema)
                {
                    $current_ema = $numeric_value;
                }
                else
                {
                    $current_ema = ($numeric_value * $alpha_val) + ($current_ema * (1 - $alpha_val));
                }
            }
             push @current_window_sma, $value;
             shift @current_window_sma while scalar @current_window_sma > $window_size;
             if (scalar @current_window_sma == $window_size)
             {
                 $avg_val_to_add = $current_ema;
             }
        }
        push @output_series, $avg_val_to_add if defined $avg_val_to_add;
    }
    return \@output_series;
}

sub apply_rounding
{
    my ($value, $increment, $method) = @_;
    return $value unless (defined $value && $value =~ /^-?[0-9]+(?:\.[0-9]+)?$/);
    return $value if $method eq 'none' || !defined $increment || $increment <= $FLOAT_EPSILON;

    my $rounded_value;
    if ($method eq 'standard')
    {
        $rounded_value = int( ($value / $increment) + ( ($value >= 0) ? 0.5 : -0.5) ) * $increment;
    }
    elsif ($method eq 'up')
    {
        $rounded_value = ceil( $value / $increment ) * $increment;
    }
    else
    {
        $rounded_value = $value;
    }
    return $rounded_value;
}

sub get_decimal_places
{
    my ($number_str) = @_;
    $number_str = sprintf("%.15f", $number_str) if ($number_str =~ /e/i);
    if ($number_str =~ /\.(\d+)$/)
    {
        return length($1);
    }
    else
    {
        return 0;
    }
}

# --- define_time_windows_for_state ---
# Subdivides a configuration state window into smaller, regular time-based
# windows for decay and growth analysis.
#
sub define_time_windows_for_state {
    my ($state_obj, $ref_date_str, $unit, $size) = @_;
    my @windows;
    my $start = $state_obj->{start_time};
    my $end   = $state_obj->{end_time};

    my $current_win_start = $start->truncate(to => 'day');

    while ($current_win_start < $end) {
        my $current_win_end;
        if ($unit eq 'days') {
            $current_win_end = $current_win_start + ($size * ONE_DAY()) - 1;
        } else { # weeks
            $current_win_end = $current_win_start + ($size * ONE_WEEK()) - 1;
        }

        $current_win_end = $end if $current_win_end > $end;

        push @windows, {
            start => $current_win_start,
            end   => $current_win_end,
            rep_date => $current_win_end, # Representative date for weighting is the end of the window
        };

        $current_win_start = $current_win_end->truncate(to => 'day') + ONE_DAY();
    }
    return @windows;
}

# --- calculate_metrics_for_period ---
# A generic routine to calculate all relevant metrics (PhysC, RunQ, Peak)
# for a given set of data points and a specific configuration state.
#
sub calculate_metrics_for_period {
    my ($physc_avgs_ref, $peak_physc_val, $norm_runq_raw_ref, $abs_runq_raw_ref) = @_;
    my %metrics;
    
    # --- Optimised PhysC Percentile Calculation ---
    if (@$physc_avgs_ref) {
        # Sort the rolling averages array only ONCE.
        my @sorted_physc_avgs = sort {$a <=> $b} grep {defined $_ && looks_like_number($_)} @$physc_avgs_ref;
        my @final_physc_to_process = @sorted_physc_avgs;

        # If the filter is active, create a sorted slice without re-sorting.
        if (defined $filter_above_perc_value && @sorted_physc_avgs) {
            my $ft = calculate_percentile(\@sorted_physc_avgs, $filter_above_perc_value);
            if (defined $ft) {
                @final_physc_to_process = grep { $_ >= ($ft - $FLOAT_EPSILON) } @sorted_physc_avgs;
            }
        }
        
        # Calculate the final percentile on the (already sorted) final list.
        my $p_val_physc = @final_physc_to_process ? calculate_percentile(\@final_physc_to_process, $percentile) : undef;
        $metrics{"P".clean_perc_label($percentile)} = $p_val_physc;
    }

    # --- Peak and RunQ Calculations (already efficient) ---
    if ($calculate_peak) {
        $metrics{'Peak'} = ($peak_physc_val > 0) ? $peak_physc_val : undef;
    }

    # The RunQ logic now operates on the pre-populated arrays.
    my $norm_data_ref = \@$norm_runq_raw_ref;
    my $abs_data_ref = \@$abs_runq_raw_ref;
    if ($runq_avg_method_str ne 'none') {
        $norm_data_ref = calculate_rolling_average_series($norm_data_ref, $runq_avg_method_str, $window_minutes, $alpha_for_runq_ema);
        $abs_data_ref = calculate_rolling_average_series($abs_data_ref, $runq_avg_method_str, $window_minutes, $alpha_for_runq_ema);
    }
    
    my @sorted_norm = sort {$a <=> $b} @$norm_data_ref;
    my @sorted_abs = sort {$a <=> $b} @$abs_data_ref;
    foreach my $p (@runq_norm_percentiles_to_calc) { $metrics{"NormRunQ_P".clean_perc_label($p)} = @sorted_norm ? calculate_percentile(\@sorted_norm, $p) : undef; }
    foreach my $p (@runq_abs_percentiles_to_calc) { $metrics{"AbsRunQ_P".clean_perc_label($p)} = @sorted_abs ? calculate_percentile(\@sorted_abs, $p) : undef; }

    return \%metrics;
}

# --- stream_data_for_state_from_temp ---
#   - This function implements the efficient streaming part of the design.
#   - It reads from the consolidated temporary files ONCE per state, instead
#     of re-scanning all original NMON files. It handles "long" CSV format from temp files.
sub stream_data_for_state_from_temp
{
    my ($state_obj, $physc_temp_file, $runq_temp_file, $filters_ref) = @_;

    my @data_points;
    my $vm_to_find = $state_obj->{vm_name};

    # Pre-load all relevant RunQ data for the target VM into a hash for quick lookup.
    # This is efficient as it avoids re-scanning the RunQ file for every PhysC point.
    my %runq_data_for_vm;
    if (defined $runq_temp_file && -s $runq_temp_file) {
        open my $rq_fh, '<:encoding(utf8)', $runq_temp_file or die "Could not open temp RunQ file $runq_temp_file: $!";
        my $ln = 0;
        while(my $line = <$rq_fh>) {
            chomp $line; $ln++;
            next if $ln == 1; # Skip header line

            my ($ts_str, $vm_name, $runq_val) = split ',', $line, 3;
            next unless (defined $vm_name && $vm_name eq $vm_to_find);

            $runq_data_for_vm{$ts_str} = (defined $runq_val && looks_like_number($runq_val)) ? $runq_val+0 : undef;
        }
        close $rq_fh;
    }

    # Stream the PhysC file and build the final data points array for the state.
    open my $pc_fh, '<:encoding(utf8)', $physc_temp_file or die "Could not open temp PhysC file $physc_temp_file: $!";
    my $ln = 0;
    while(my $line = <$pc_fh>) {
        chomp $line; $ln++;
        next if $ln == 1; # Skip header line

        my ($ts_str, $vm_name, $physc_val) = split ',', $line, 3;

        # We only care about lines for the VM of the current state object.
        next unless (defined $vm_name && $vm_name eq $vm_to_find);

        my $tp;
        eval { $tp = Time::Piece->strptime($ts_str, "%Y-%m-%d %H:%M:%S"); };
        next if ($@ || !$tp);

        # --- Filtering Logic ---
        # 1. State Window Filter (the primary filter)
        next unless ($tp >= $state_obj->{start_time} && $tp <= $state_obj->{end_time});

        # 2. Global Date Filters (--startdate, --enddate)
        if (defined $filters_ref->{start_date_str} && $tp->ymd('-') lt $filters_ref->{start_date_str}) { next; }
        if (defined $filters_ref->{end_date_str}   && $tp->ymd('-') gt $filters_ref->{end_date_str})   { next; }

        # 3. Weekend Filter (--no-weekends)
        if ($filters_ref->{no_weekends}) {
            my $day_of_week = $tp->day_of_week;
            next if ($day_of_week == 1 || $day_of_week == 7); # Skip Sunday (1) and Saturday (7)
        }

        # 4. Time-of-Day Filter (--online, --batch, --startt/--endt)
        if ($filters_ref->{time_filter_active}) {
            my $line_time = substr($tp->hms(':'), 0, 5);
            my $include_line = 0;
            if ($filters_ref->{time_filter_overnight}) {
                $include_line = 1 if ($line_time ge $filters_ref->{time_filter_start} || $line_time lt $filters_ref->{time_filter_end});
            } else {
                $include_line = 1 if ($line_time ge $filters_ref->{time_filter_start} && $line_time lt $filters_ref->{time_filter_end});
            }
            next unless ($include_line);
        }

        # If the data point passed all filters, add it to the list for this state.
        my $p_val = (defined $physc_val && looks_like_number($physc_val)) ? $physc_val+0 : undef;
        my $r_val = $runq_data_for_vm{$ts_str} // undef; # Look up corresponding RunQ value

        push @data_points, { ts => $ts_str, physc => $p_val, runq => $r_val, tp => $tp };
    }
    close $pc_fh;

    return @data_points;
}

## Streams NMON data to temporary file (Non-Caching Mode with --nmondir) ##
sub process_nmon_directory_to_tempfiles {
    my ($dir, $combined_out_fh) = @_;

    # This function now creates a single, combined data stream.
    print $combined_out_fh "Timestamp,VMName,PhysC,RunQ\n";

    my @nmon_files;
    find( sub {
            return unless -f;
            return unless /\.(nmon|nmon\.gz|nmon\.bz2|nmon\.bzip2)$/i;
            push @nmon_files, $File::Find::name;
        }, $dir
    );
    # No need to die here, as the calling context might handle it.
    return unless @nmon_files;

    foreach my $file_path (sort @nmon_files) {
        my %file_data_points; # Per-file buffer for merging PhysC and RunQ
        my $fh;
        if ($file_path =~ /\.nmon\.gz$/i) {
            open $fh, "-|", "gzip", "-dc", "--", $file_path or do { warn "Warning: Could not decompress '$file_path': $!. Skipping."; next; };
        } elsif ($file_path =~ /\.nmon\.(bz2|bzip2)$/i) {
            open $fh, "-|", "bzcat", "--", $file_path or do { warn "Warning: Could not bzcat '$file_path': $!. Skipping."; next; };
        } else {
            open $fh, '<:encoding(utf8)', $file_path or do { warn "Warning: Could not open '$file_path': $!. Skipping."; next; };
        }

        my $current_vm;
        my %zzzz_map;

        while (my $line = <$fh>) {
            chomp $line; $line =~ s/\r$//;
            my @fields = split ',', $line;
            my $key = $fields[0];

            if ($key eq 'AAA' && $fields[1] eq 'host') { $current_vm = $fields[2]; }
            elsif ($key eq 'ZZZZ') {
                my ($t_num, $time, $date_str) = @fields[1..3];
                my $tp_date;
                eval { $tp_date = Time::Piece->strptime($date_str, "%d-%b-%Y"); };
                if ($@ || !$tp_date) { eval { $tp_date = Time::Piece->strptime($date_str, "%d-%B-%Y"); }; }
                next if ($@ || !$tp_date);
                $zzzz_map{$t_num} = $tp_date->ymd . " " . $time;
            }
            elsif ($key eq 'LPAR') {
                my $t_num = $fields[1];
                if (defined $zzzz_map{$t_num} && defined $current_vm) {
                    my $ts = $zzzz_map{$t_num};
                    my $physc_val = $fields[2];
                    $file_data_points{$ts}{$current_vm}{physc} = $physc_val if defined $physc_val and looks_like_number($physc_val);
                }
            }
            elsif ($key eq 'PROC') {
                my $t_num = $fields[1];
                if (defined $zzzz_map{$t_num} && defined $current_vm) {
                    my $ts = $zzzz_map{$t_num};
                    my $runq_val = $fields[2];
                    $file_data_points{$ts}{$current_vm}{runq} = $runq_val if defined $runq_val and looks_like_number($runq_val);
                }
            }
        }
        close $fh;

        # Stream the buffered data for this file to the combined temp file
        foreach my $ts (sort keys %file_data_points) {
            foreach my $vm (sort keys %{$file_data_points{$ts}}) {
                my $physc_val = $file_data_points{$ts}{$vm}{physc} // '';
                my $runq_val  = $file_data_points{$ts}{$vm}{runq} // '';
                if ($physc_val ne '' || $runq_val ne '') {
                     print $combined_out_fh join(",", $ts, $vm, $physc_val, $runq_val) . "\n";
                }
            }
        }
    }
}

sub generate_run_summary_string {
    my @summary_lines;
    my $label_width = 25;

    push @summary_lines, "\n------------------------------------------------------------------";
    push @summary_lines, "Run Summary";
    push @summary_lines, "------------------------------------------------------------------";

    # Input Method
    my $input_method = defined($nmon_dir) ? "--nmondir ($nmon_dir)" : "--physc-data ($physc_csv_file)";
    push @summary_lines, sprintf("%-".$label_width."s: %s", "Input Method", $input_method);

    # Primary Metric
    my $p_label = sprintf("P%.2f", $percentile);
    $p_label =~ s/\.?0+$//;
    my $primary_metric = sprintf("%s (Percentile: %.2f)", $p_label, $percentile);
    push @summary_lines, sprintf("%-".$label_width."s: %s", "Primary Metric", $primary_metric);

    # Averaging
    my $avg_details = uc($avg_method);
    $avg_details .= sprintf(" (Window: %d min, PhysC Decay: %s)", $window_minutes, $decay_level) if $avg_method eq 'ema';
    $avg_details .= sprintf(" (Window: %d min)", $window_minutes) if $avg_method eq 'sma';
    push @summary_lines, sprintf("%-".$label_width."s: %s", "Averaging Method", $avg_details);

    # RunQ
    if (defined($runq_csv_file) || defined($nmon_dir)) {
        my $runq_decay_str = ($runq_avg_method_str eq 'ema') ? ", Decay: $runq_decay_level_to_use" : "";
        my $runq_details = sprintf("Enabled (SMT: %d, Avg Method: %s%s)", $smt_value, $runq_avg_method_str, $runq_decay_str);
        push @summary_lines, sprintf("%-".$label_width."s: %s", "RunQ Analysis", $runq_details);

        my $norm_perc_str = $runq_norm_perc_str eq '' ? 'none' : $runq_norm_perc_str;
        my $abs_perc_str = $runq_abs_perc_str eq '' ? 'none' : $runq_abs_perc_str;
        push @summary_lines, sprintf("%-".$label_width."s: Norm(%s), Abs(%s)", "RunQ Percentiles", $norm_perc_str, $abs_perc_str);
    }

    # Filters
    my @filters;
    push @filters, "Time ($time_filter_desc)" if $time_filter_active;
    push @filters, "Weekends Excluded" if $no_weekends;
    if (defined $filter_above_perc_value) {
        my $perc_filter_label = sprintf("P%.2f", $filter_above_perc_value);
        $perc_filter_label =~ s/\.?0+$//;
        push @filters, "PhysC Perc (>= $perc_filter_label)";
    }
    push @summary_lines, sprintf("%-".$label_width."s: %s", "Filters Applied", join(', ', @filters)) if @filters;

    # Decay / Growth / Analysis Model
    my $analysis_model_str;
    if ($enable_windowed_decay_internal) {
        $analysis_model_str = "Time-Based Windowed Decay ON";
    } elsif ($decay_over_states_flag) {
        $analysis_model_str = "Hybrid State-Time Decay ON";
    } else {
        $analysis_model_str = "State-Based (Decay OFF)";
    }

    my $growth_status = $enable_growth_prediction ? "Growth Prediction ON" : "Growth Prediction OFF";
    push @summary_lines, sprintf("%-".$label_width."s: %s", "Analysis Model", $analysis_model_str);
    push @summary_lines, sprintf("%-".$label_width."s: %s", "Growth Prediction", $growth_status);

    return join("\n", @summary_lines) . "\n";
}

# --- stream_and_calculate_metrics_from_cache ---
# Reads the unified data cache file, streaming to find all data points
# that fall within the time boundaries of a given configuration state.
# It then calculates all relevant metrics for that collection of points.
#
# Args:
#   1. $state_obj (hash ref): The state object to analyze.
#   2. $cache_data_file (string): The path to the .nfit.cache.data file.
#
# Returns:
#   - A hash reference containing the calculated metrics for the period.
#
sub stream_and_calculate_metrics_from_cache {
    my ($state_obj, $cache_data_file) = @_;

    my @data_points_for_state;
    my $vm_to_find = $state_obj->{vm_name};
    my $start_epoch = $state_obj->{start_time}->epoch;
    my $end_epoch = $state_obj->{end_time}->epoch;

    open my $fh, '<', $cache_data_file or die "FATAL: Could not open data cache file '$cache_data_file': $!";

    # Skip header
    <$fh>;

    while (my $line = <$fh>) {
        # Fast exit if we are past the state's end time
        my $ts_str = substr($line, 0, 19);
        last if $ts_str gt $state_obj->{end_time}->strftime('%Y-%m-%d %H:%M:%S');

        my ($ts, $vm_name, $physc, $runq) = split ',', $line;
        next unless $vm_name eq $vm_to_find;

        my $tp;
        eval { $tp = Time::Piece->strptime($ts, "%Y-%m-%d %H:%M:%S"); };
        next if $@;

        my $current_epoch = $tp->epoch;

        # Check if the data point is within the state's time boundaries
        if ($current_epoch >= $start_epoch && $current_epoch <= $end_epoch) {
            push @data_points_for_state, {
                ts    => $ts,
                physc => (defined $physc && looks_like_number($physc)) ? $physc+0 : undef,
                runq  => (defined $runq && looks_like_number($runq)) ? $runq+0 : undef,
                tp    => $tp,
            };
        }
    }
    close $fh;

    # With the relevant data points now collected in memory for this single state,
    # we can call the existing metric calculation function.
    my $filters_ref = {
        no_weekends => $no_weekends, time_filter_active => $time_filter_active,
        time_filter_start => $start_time_str, time_filter_end => $end_time_str,
        time_filter_overnight => $time_filter_overnight,
    };

    # The calculate_metrics_for_period function can now be simplified as it
    # no longer needs to handle streaming itself, just calculation.
    # We will pass the collected data points to it.

    # You would need to adapt calculate_metrics_for_period to accept the
    # pre-filtered data points. For now, let's assume it does.
    # my $metrics = calculate_metrics_for_period(\@data_points_for_state, $state_obj->{metadata}, $filters_ref);
    # To make this step complete without modifying another function, we can perform the calc here.

    my %metrics;
    if (@data_points_for_state) {
        %metrics = %{ calculate_metrics_for_period(\@data_points_for_state, $state_obj->{metadata}) };
    }

    return \%metrics;
}

sub get_window_index_for_timestamp {
    my ($timestamp_obj, $windows_aref) = @_;
    my $ts_epoch = $timestamp_obj->epoch;
    for (my $i=0; $i < @{$windows_aref}; $i++) {
        my ($win_start, $win_end) = @{$windows_aref->[$i]};
        return $i if ($ts_epoch >= $win_start->epoch && $ts_epoch <= $win_end->epoch);
    }
    return undef;
}


sub parse_csv_line {
    my ($line) = @_;
    my @fields;

    while (length($line)) {
        if ($line =~ s/^"((?:[^"]|"")*)"[,]*//) {
            my $field = $1;
            $field =~ s/""/"/g;  # unescape double quotes
            push @fields, $field;
        } elsif ($line =~ s/^([^",]*)[,]*//) {
            push @fields, $1;
        } else {
            # fallback (shouldn't happen with consistent CSV)
            last;
        }
    }

    return @fields;
}

# --- process_vm_buffer ---
# Analyses the fully buffered data for a single VM, calculating metrics
# for each time window in which the VM has data.
sub process_vm_buffer {
    my ($vm_buffer_aref, $vm_name, $results_agg_href, $states_by_vm, $windows_aref) = @_;

    # This subroutine now generates its own windows instead of receiving them via $windows_aref.

    print STDERR "INFO: Processing " . scalar(@$vm_buffer_aref) . " data points for VM '$vm_name'...\n" if ($verbose);

    # Find all configuration states relevant to THIS VM ONLY (fast)
    my @vm_states = @{$states_by_vm->{$vm_name} || []};
    return unless @vm_states && @$vm_buffer_aref > 0;

    # --- NEW: Step 1 - Determine this VM's specific date range from its data buffer ---
    my $min_epoch_for_vm = $vm_buffer_aref->[0]->{tp}->epoch;
    my $max_epoch_for_vm = $vm_buffer_aref->[-1]->{tp}->epoch;

    # A quick loop is safer than assuming the buffer is perfectly time-sorted.
    foreach my $dp (@$vm_buffer_aref) {
        my $current_epoch = $dp->{tp}->epoch;
        $min_epoch_for_vm = $current_epoch if $current_epoch < $min_epoch_for_vm;
        $max_epoch_for_vm = $current_epoch if $current_epoch > $max_epoch_for_vm;
    }

    my $vm_start_obj = Time::Piece->new($min_epoch_for_vm);
    my $vm_end_obj   = Time::Piece->new($max_epoch_for_vm);

    # --- NEW: Step 2 - Generate processing windows specifically for this VM ---
    my @vm_processing_windows = generate_processing_time_windows(
        $vm_start_obj,
        $vm_end_obj,
        $process_window_unit_str,
        $process_window_size_val
    );

    # If no windows could be generated (e.g., data spans less than one window), exit.
    return unless @vm_processing_windows;

    # --- EXISTING LOGIC: Now applied using the new VM-specific windows ---
    # Group the buffered data points by the index of their time window
    my %window_data_groups;
    foreach my $dp (@$vm_buffer_aref) {
        # Use the new @vm_processing_windows array for mapping
        my $win_idx = get_window_index_for_timestamp($dp->{tp}, \@vm_processing_windows);
        next unless defined $win_idx;
        push @{$window_data_groups{$win_idx}}, $dp;
    }

    # Process each populated time window group for this VM
    foreach my $win_idx (sort { $a <=> $b } keys %window_data_groups) {
        my @window_buffer = @{$window_data_groups{$win_idx}};
        next unless @window_buffer;

        # Take a sample timestamp to find the correct configuration state
        my $sample_tp = $window_buffer[0]->{tp};

        my ($current_state) = grep { $sample_tp->epoch >= $_->{start_epoch} && $sample_tp->epoch <= $_->{end_epoch} } @vm_states;
        next unless $current_state;

        # The existing calculation function is re-used without modification
        my $metrics = calculate_metrics_for_period(\@window_buffer, $current_state->{metadata});

        # Store results in the main aggregator hash
        my $rep_date = $vm_processing_windows[$win_idx][2];
        my $win_key = $rep_date->ymd('') . "_" . $win_idx;
        $results_agg_href->{$vm_name}{'time_windows'}{$win_key} = { metrics => $metrics, date_obj => $rep_date };
    }
}

## --- BEGIN L2 RESULTS CACHE MODIFICATION (NEW SUBROUTINES) --- ##

# ==============================================================================
# Subroutine to generate a canonical key from script arguments for caching.
# ==============================================================================
sub generate_canonical_key
{
    my @key_parts;

    # This subroutine should be called AFTER GetOptions has populated the variables.
    # It systematically checks all options that affect the output.

    # Input Method
    push @key_parts, "--nmondir $nmon_dir" if defined $nmon_dir;
    push @key_parts, "--physc-data $physc_csv_file" if defined $physc_csv_file;
    push @key_parts, "--runq-data $runq_csv_file" if defined $runq_csv_file;
    push @key_parts, "--config $vm_config_file" if defined $vm_config_file;

    # State Management
    push @key_parts, "--include-states $include_states_selector" if (defined $include_states_selector and lc($include_states_selector) ne 'all');

    # Averaging and Decay
    push @key_parts, "--avg-method $avg_method" if defined $avg_method;
    push @key_parts, "--decay $decay_level" if defined $decay_level;
    push @key_parts, "--runq-decay $runq_decay_level_arg" if defined $runq_decay_level_arg;
    push @key_parts, "--window $window_minutes" if defined $window_minutes;

    # RunQ Options
    push @key_parts, "--smt $smt_value" if defined $smt_value;
    push @key_parts, "--runq-norm-perc '$runq_norm_perc_str'" if defined $runq_norm_perc_str;
    push @key_parts, "--runq-abs-perc '$runq_abs_perc_str'" if defined $runq_abs_perc_str;
    push @key_parts, "--runq-avg-method $runq_avg_method_str" if defined $runq_avg_method_str;

    # Filtering
    push @key_parts, "--startdate $start_date_str" if defined $start_date_str;
    push @key_parts, "--enddate $end_date_str" if defined $end_date_str;
    push @key_parts, "--startt $start_time_str" if defined $start_time_str;
    push @key_parts, "--endt $end_time_str" if defined $end_time_str;
    push @key_parts, "--online" if $online_flag;
    push @key_parts, "--batch" if $batch_flag;
    push @key_parts, "--no-weekends" if $no_weekends;
    push @key_parts, "--vm $target_vm_name" if defined $target_vm_name;

    # PhysC Calculation
    push @key_parts, "--percentile $percentile" if defined $percentile;
    push @key_parts, "--peak" if $calculate_peak;
    push @key_parts, "--filter-above-perc $filter_above_perc_value" if defined $filter_above_perc_value;

    # Windowed Recency Decay
    if ($enable_windowed_decay_internal)
    {
        push @key_parts, "--enable-windowed-decay";
        push @key_parts, "--process-window-unit $process_window_unit_str";
        push @key_parts, "--process-window-size $process_window_size_val";
        push @key_parts, "--decay-half-life-days $decay_half_life_days_val";
        push @key_parts, "--analysis-reference-date $analysis_reference_date_str" if defined $analysis_reference_date_str;
    }

    # Growth Prediction
    if ($enable_growth_prediction)
    {
        push @key_parts, "--enable-growth-prediction";
        push @key_parts, "--growth-projection-days $growth_projection_days";
        push @key_parts, "--max-growth-inflation-percent $max_growth_inflation_percent";
    }

    # Rounding
    push @key_parts, "-r" . (defined $round_arg && length $round_arg ? "=$round_arg" : "") if defined $round_arg;
    push @key_parts, "-u" . (defined $roundup_arg && length $roundup_arg ? "=$roundup_arg" : "") if defined $roundup_arg;

    # Normalise by sorting and joining.
    my $canonical_key = join(" ", sort @key_parts);
    return $canonical_key;
}

# ==============================================================================
# Subroutine to look up a result from the L2 cache file.
# Returns: The cached result as a single string, or undef if not found.
# ==============================================================================
sub lookup_cached_result
{
    my ($key, $cache_file) = @_;

    return undef unless (-f $cache_file && -s $cache_file);

    open my $fh, '<:encoding(utf8)', $cache_file
        or warn "Warning: Could not open results cache '$cache_file' for reading: $!" and return undef;

    my $in_target_block = 0;
    my @result_lines;

    while (my $line = <$fh>)
    {
        chomp $line;
        if ($in_target_block)
        {
            if ($line eq '--END--')
            {
                # Found the end of our block, return the collected result.
                close $fh;
                return join("\n", @result_lines);
            }
            push @result_lines, $line;
        }
        elsif ($line eq "[$key]")
        {
            # Found the start of our target block.
            $in_target_block = 1;
        }
    }

    close $fh;
    return undef; # Reached end of file without finding the block or its end.
}

# ==============================================================================
# Subroutine to save a result to the L2 cache file under an exclusive lock.
# ==============================================================================
sub save_result_to_cache
{
    my ($key, $result_string, $cache_file, $lock_file) = @_;

    # Acquire an exclusive lock to prevent race conditions when writing.
    open my $lock_fh, '>', $lock_file or do {
        warn "Warning: Could not create lock file '$lock_file' for writing cache: $!. Skipping cache write.";
        return;
    };
    flock($lock_fh, LOCK_EX) or do {
        warn "Warning: Could not acquire exclusive lock on '$lock_file': $!. Skipping cache write.";
        close $lock_fh;
        return;
    };

    # Open the results cache in append mode.
    open my $fh, '>>:encoding(utf8)', $cache_file or do {
        warn "Warning: Could not open results cache '$cache_file' for appending: $!. Skipping cache write.";
        close $lock_fh; # Release lock
        return;
    };

    # Write the canonical key, the result, and the delimiter.
    print {$fh} "[$key]\n";
    print {$fh} $result_string . "\n"; # result_string already has newlines from the buffer
    print {$fh} "--END--\n";

    close $fh;
    close $lock_fh; # Releases the lock automatically on close.
}

# --- NEW HELPER SUBROUTINE for the non-windowed path ---
sub process_vm_buffer_non_windowed {
    my ($vm_buffer_aref, $vm_name, $results_agg_href, $states_for_vm_aref) = @_;

    return unless @$vm_buffer_aref && @$states_for_vm_aref;
    print STDERR "    - Finalising " . scalar(@$states_for_vm_aref) . " State(s) for VM $vm_name...\n";

    my %state_data_buffers;
    # Group all data points from the buffer into their correct state
    foreach my $dp (@$vm_buffer_aref) {
        foreach my $state (@$states_for_vm_aref) {
            if ($dp->{tp}->epoch >= $state->{start_epoch} && $dp->{tp}->epoch <= $state->{end_epoch}) {
                push @{$state_data_buffers{$state->{state_id}}}, $dp;
                last; # Found the correct state for this datapoint, move to next datapoint
            }
        }
    }

    # Now, calculate metrics for each state that had data
    foreach my $state_id (sort { $a <=> $b } keys %state_data_buffers) {
        my ($state_obj) = grep { $_->{state_id} == $state_id } @$states_for_vm_aref;
        next unless $state_obj;

        my @data_for_this_state = @{$state_data_buffers{$state_id}};

        # Apply time filters (online, batch, no-weekends) here
        my @filtered_data_for_state;
        if ($no_weekends || $time_filter_active) {
            foreach my $dp (@data_for_this_state) {
                 if ($no_weekends) {
                    my $day_of_week = $dp->{tp}->day_of_week;
                    next if ($day_of_week == 1 || $day_of_week == 7);
                }
                if ($time_filter_active) {
                    my $line_time = substr($dp->{tp}->hms(':'), 0, 5);
                    my $include_line = 0;
                    if ($time_filter_overnight) {
                        $include_line = 1 if ($line_time ge $time_filter_start || $line_time lt $time_filter_end);
                    } else {
                        $include_line = 1 if ($line_time ge $time_filter_start && $line_time lt $time_filter_end);
                    }
                    next unless $include_line;
                }
                push @filtered_data_for_state, $dp;
            }
        } else {
            @filtered_data_for_state = @data_for_this_state;
        }

        # Calculate metrics using the filtered data
        my $metrics = calculate_metrics_for_period(\@filtered_data_for_state, $state_obj->{metadata});
        push @{$results_agg_href->{$vm_name}{'states'}}, { metrics => $metrics, state_obj => $state_obj };
    }
}

# Subroutine for the Hybrid State-Time Decay Model
sub synthesize_hybrid_timeseries {
    my ($state_results_aref, $metric_key) = @_;
    my @timeseries_points;

    foreach my $state_result (@$state_results_aref) {
        my $start_obj = $state_result->{state_obj}{start_time}->truncate(to => 'day');
        my $end_obj   = $state_result->{state_obj}{end_time}->truncate(to => 'day');
        my $metric_val = $state_result->{metrics}{$metric_key};

        # Skip if this state didn't have a valid metric
        next unless defined $metric_val && looks_like_number($metric_val);

        my $current_day = $start_obj;
        while ($current_day <= $end_obj) {
            # Add a data point for every day the state was active
            push @timeseries_points, {
                value => $metric_val,
                date  => Time::Piece->new($current_day->epoch) # Use a copy
            };
            $current_day += ONE_DAY;
        }
    }
    return @timeseries_points;
}

# --- get_metrics_for_each_state ---
# A modular subroutine for the non-windowed and hybrid analysis paths.
# It takes a VM's buffered data and its known states, and returns an array
# containing the calculated metrics for each state that had data.
#
# Args:
#   1. $vm_buffer_aref (array ref): A buffer of all data points for a single VM.
#   2. $vm_name (string): The name of the VM being processed.
#   3. $states_for_vm_aref (array ref): The array of state objects for this VM.
#
# Returns:
#   - An array of hashes, where each hash is { metrics => \%metrics, state_obj => $state }
#
sub get_metrics_for_each_state
{
    my ($vm_buffer_aref, $vm_name, $states_for_vm_aref) = @_;

    my @state_results; # Array to hold the results for each state

    return () unless @$vm_buffer_aref && @$states_for_vm_aref;
    my $count = scalar(@$states_for_vm_aref);
    my $label = $count == 1 ? 'state' : 'states';
    print STDERR "    - Profiling performance of $count configuration $label for VM $vm_name...\n";

    my %state_data_buffers;
    # Group all data points from the buffer into their correct state
    foreach my $dp (@$vm_buffer_aref)
    {
        foreach my $state (@$states_for_vm_aref)
        {
            if ($dp->{tp}->epoch >= $state->{start_epoch} && $dp->{tp}->epoch <= $state->{end_epoch})
            {
                push @{$state_data_buffers{$state->{state_id}}}, $dp;
                last; # Found the correct state, move to the next data point
            }
        }
    }

    # Now, calculate metrics for each state that had data
    foreach my $state_id (sort { $a <=> $b } keys %state_data_buffers)
    {
        my ($state_obj) = grep { $_->{state_id} == $state_id } @$states_for_vm_aref;
        next unless $state_obj;

        my @data_for_this_state = @{$state_data_buffers{$state_id}};

        # Apply time filters (online, batch, no-weekends)
        my @filtered_data_for_state;
        if ($no_weekends || $time_filter_active)
        {
            foreach my $dp (@data_for_this_state)
            {
                 if ($no_weekends)
                 {
                    my $day_of_week = $dp->{tp}->day_of_week;
                    next if ($day_of_week == 1 || $day_of_week == 7);
                }
                if ($time_filter_active)
                {
                    my $line_time = substr($dp->{tp}->hms(':'), 0, 5);
                    my $include_line = 0;
                    if ($time_filter_overnight)
                    {
                        $include_line = 1 if ($line_time ge $time_filter_start || $line_time lt $time_filter_end);
                    }
                    else
                    {
                        $include_line = 1 if ($line_time ge $time_filter_start && $line_time lt $time_filter_end);
                    }
                    next unless $include_line;
                }
                push @filtered_data_for_state, $dp;
            }
        }
        else
        {
            @filtered_data_for_state = @data_for_this_state;
        }

        next unless @filtered_data_for_state;

        # Calculate metrics using the filtered data and the existing, robust subroutine
        # --- Start of: single-pass data processing ---
        my @physc_avgs;
        my ($physc_sma_q, $physc_ema_s) = ([], undef);
        my $peak_physc = 0;
        my (@norm_runq_raw, @abs_runq_raw);
        my $smt_for_period = $state_obj->{metadata}{smt} // $DEFAULT_SMT;

        # This single loop calculates rolling averages, finds the peak, and populates RunQ arrays.
        foreach my $dp (@filtered_data_for_state) {
            # Calculate PhysC Rolling Average
            my $avg = calculate_rolling_average($dp->{physc}, $avg_method, $physc_sma_q, \$physc_ema_s, $window_minutes, $alpha_for_physc_ema);
            push @physc_avgs, $avg if defined $avg;

            # Find Peak PhysC
            $peak_physc = $dp->{physc} if (defined $dp->{physc} && $dp->{physc} > $peak_physc);

            # Populate RunQ arrays if applicable
            if (defined $dp->{runq} && defined $dp->{physc} && $dp->{physc} >= $ACTIVE_PHYSC_THRESHOLD) {
                push @abs_runq_raw, $dp->{runq};
                my $eff_lcpus = $dp->{physc} * $smt_for_period;
                if ($eff_lcpus > $FLOAT_EPSILON) {
                    push @norm_runq_raw, ($dp->{runq} / $eff_lcpus);
                }
            }
        }
        
        # Pass the pre-processed arrays to the refactored calculation function.
        my $metrics = calculate_metrics_for_period(
            \@physc_avgs,
            $peak_physc,
            \@norm_runq_raw,
            \@abs_runq_raw
        );
        # --- End: Single-pass data processing ---

        push @state_results, { metrics => $metrics, state_obj => $state_obj };
    }

    return @state_results;
}

# --- calculate_growth_prediction ---
# Performs trend analysis on a time series of metric values and calculates
# a growth adjustment factor based on a linear regression projection. This
# function is now self-contained and used by all aggregated analysis modes.
#
# Args:
#   1. $timeseries_values_aref (array ref): An array of the numeric metric values over time.
#   2. $baseline_val (float): The final aggregated metric value, used as the base for inflation.
#
# Returns:
#   - A list: ($growth_adj_value, $growth_adj_abs_value)
#
sub calculate_growth_prediction
{
    my ($timeseries_values_aref, $baseline_val) = @_;

    my ($growth_adj_value, $growth_adj_abs_value) = (0.0, 0.0);

    # Growth calculation is only possible if there is a positive baseline to inflate.
    return ($growth_adj_value, $growth_adj_abs_value) unless (defined $baseline_val && looks_like_number($baseline_val) && $baseline_val > $FLOAT_EPSILON);

    my @timeseries_points;
    my $time_idx = 0;

    # Create [x, y] points for regression, where x is the time index (0, 1, 2...).
    foreach my $metric_val (@$timeseries_values_aref)
    {
        if (defined $metric_val && looks_like_number($metric_val))
        {
            push @timeseries_points, [$time_idx, $metric_val];
            $time_idx++;
        }
    }

    my $num_hist_periods = scalar(@timeseries_points);

    # Ensure there are enough data points to establish a meaningful trend.
    if ($num_hist_periods >= $GROWTH_MIN_HISTORICAL_PERIODS)
    {
        my @values_for_stats = map { $_->[1] } @timeseries_points;
        my ($mean_val, $stddev_val, $cv_val) = calculate_statistics_for_trend(\@values_for_stats);

        # Only proceed if the data is not excessively volatile (low coefficient of variation).
        if (defined $cv_val && $cv_val < $GROWTH_MAX_CV_THRESHOLD)
        {
            my ($slope, $intercept) = calculate_manual_linear_regression(\@timeseries_points);

            # Only apply inflation if there is a clear positive growth trend.
            if (defined $slope && $slope > $GROWTH_MIN_POSITIVE_SLOPE_THRESHOLD)
            {
                # Project the value N days into the future.
                # Note: For the Hybrid model, which provides daily data points, the projection
                # period logic is simplified as each period is one day. For the standard
                # windowed model, it's scaled by the window size. This logic handles both.
                my $projection_periods = ($process_window_unit_str eq 'weeks')
                                       ? ($growth_projection_days / 7) * (1 / $process_window_size_val)
                                       : ($growth_projection_days / $process_window_size_val);

                # For the daily data from the Hybrid model, we need to adjust the projection period.
                # The simplest way is to check the number of points relative to a known time span,
                # but for now, we assume the projection is in units matching the time series points.
                if (scalar(@$timeseries_values_aref) > $num_hist_periods * 1.5) # Heuristic for daily data
                {
                    $projection_periods = $growth_projection_days;
                }

                my $projected_val = $slope * ($num_hist_periods - 1 + $projection_periods) + $intercept;

                if ($projected_val > $baseline_val)
                {
                    my $inflation_perc = (($projected_val - $baseline_val) / $baseline_val) * 100;
                    $inflation_perc = $max_growth_inflation_percent if $inflation_perc > $max_growth_inflation_percent;
                    $growth_adj_value = ($baseline_val * $inflation_perc / 100);
                    $growth_adj_abs_value = $growth_adj_value;
                }
            }
        }
    }

    return ($growth_adj_value, $growth_adj_abs_value);
}

sub usage
{
    my $script_name = $0;
    $script_name =~ s{.*/}{};
    my $valid_decays_usage = join("|", sort keys %EMA_ALPHAS);
    return <<END_USAGE;
Usage: $script_name --physc-data <file> [options]
   or: $script_name --nmondir <directory> [options]

Analyses NMON data-files or NIMON exported PhysC and RunQ data for capacity planning.
If --enable-windowed-decay is used, applies recency weighting to windowed results,
and can optionally enable growth prediction.

Input Method (mutually exclusive):
  --physc-data, -pc <file> : Path to input NJMON CSV file with PhysC data.
  --runq-data, -rq <file>  : Path to input NJMON CSV file with RunQ data.
  --nmondir <directory>    : Path to a directory of .nmon files to process.
  --config <file>          : Optional. Provides VM metadata (e.g., SMT, MaxCPU).
                             Used primarily with the --physc-data (NJMON) input method.
                             This option is ignored when using --nmondir.

NMON Directory State Management (requires --nmondir):
  --show-states            : Scans NMON files, prints a report of all detected configuration
                             state windows for each VM, and exits.
  --include-states <sel>   : Specifies which state windows to analyze. <sel> is a comma-
                             separated list of state IDs or ranges (e.g., 'all', '1,3',
                             '2-4', '-1' for the most recent). (Default: all)
                             (--show-states and --include-states are mutually exclusive).

Averaging Method (used within each window if decay enabled):
  --avg-method <method>    : Averaging method for PhysC: 'sma' or 'ema'. (Default: $DEFAULT_AVG_METHOD)
  --decay <level>          : If 'ema' for PhysC, specifies decay level: $valid_decays_usage.
                           (Default: $DEFAULT_DECAY_LEVEL). This is also the default decay for RunQ EMA
                           if --runq-decay is not specified.
  --runq-decay <level>     : Optional. If 'ema' for RunQ, specifies its decay level, overriding --decay
                           for RunQ EMA. Accepts: $valid_decays_usage.
  -w, --window <minutes>     : Window for SMA (PhysC or RunQ); conceptual span/stabilisation trigger for EMA.
                           (Default: $DEFAULT_WINDOW_MINUTES min). Also used for RunQ if smoothing enabled.

RunQ Data Options (used with --physc-data or --nmondir):
  --smt <N>                : SMT level for RunQ normalisation (Default: $DEFAULT_SMT).
  --runq-norm-perc <list>  : Comma-separated percentiles for Normalised RunQ (Default: "$DEFAULT_RUNQ_NORM_PERC").
  --runq-abs-perc <list>   : Comma-separated percentiles for Absolute RunQ (Default: "$DEFAULT_RUNQ_ABS_PERC").
  --runq-avg-method <none|sma|ema> : Averaging method for RunQ data before percentile.
                               (Default: $DEFAULT_RUNQ_AVG_METHOD). Uses global -w and applicable decay settings.

Filtering Options (Applied BEFORE windowing if decay enabled):
  -s, --startdate <YYYY-MM-DD> : Ignore data before this date (overall filter).
  -ed, --enddate <YYYY-MM-DD>  : Ignore data after this date (overall filter).
  -startt <HH:MM> / -endt <HH:MM> : Daily time filter.
  -online / -batch           : Shortcut daily time filters.
  -no-weekends               : Exclude data from Saturdays and Sundays.
  -vm, --lpar <name>         : Analyse only the specified VM/LPAR name.

PhysC Calculation Options:
  -p, --percentile <value>   : Final percentile of PhysC (0-100) (Default: $DEFAULT_PERCENTILE).
                               If windowed decay, this is the percentile calculated per window and
                               forms the baseline for growth prediction.
  -k, --peak                 : Calculate peak PhysC value. (If windowed, weighted peak of window peaks).
  --filter-above-perc <N>    : Optional. Filter rolling PhysC values before PXX calc. (Applied per window if decay enabled).

Windowed Recency Decay (Prerequisite for Growth Prediction):
  --enable-windowed-decay    : Enable internal windowed processing with recency decay.
  --process-window-unit <days|weeks> : Unit for processing window size (Default: $DEFAULT_PROCESS_WINDOW_UNIT).
  --process-window-size <N>  : Size of each window in specified units (Default: $DEFAULT_PROCESS_WINDOW_SIZE).
  --decay-half-life-days <N> : Half-life in days for recency weighting (Default: $DEFAULT_DECAY_HALF_LIFE_DAYS).
  --analysis-reference-date <YYYY-MM-DD> : "Current" date for recency calculation
                                (Default: Date of last record in filtered NMON PhysC data).
  --decay-over-states        : Enable Hybrid State-Time Decay Model. This advanced mode first
                             calculates metrics per configuration state, then applies recency
                             decay to those results. It provides a single, aggregated output
                             per VM. Mutually exclusive with --enable-windowed-decay.

Predicted Growth Feature (Requires --enable-windowed-decay or --decay-over-states):
  --enable-growth-prediction         : Enable growth prediction based on per-window primary percentile trend.
  --growth-projection-days <N>       : Project growth for N days into the future (e.g., 30 for 1 month, 90 for ~3 months). (Default: $DEFAULT_GROWTH_PROJECTION_DAYS)
  --max-growth-inflation-percent <X> : Maximum percentage to inflate the baseline by due to growth. (Default: $DEFAULT_MAX_GROWTH_INFLATION_PERCENT)

Rounding Options:
  -r[=increment] / -u[=increment] : Round results (Default increment: $DEFAULT_ROUND_INCREMENT).

Other:
  -h, --help                 : Display this help message.
  -v, --verbose              : Enable verbose output for debugging.
  --version                  : Display script version.
END_USAGE
}
