#!/usr/bin/env python3.9
# -*- coding: utf-8 -*-

"""
nfit-stage-influxdb.py: A tool to query performance and configuration data
from an InfluxDB database and build a local cache for the nFit toolkit.
"""

import argparse
import configparser
import os
import sys
import json
import fcntl
import time
from datetime import datetime, timezone, timedelta
from collections import defaultdict
import tempfile
import subprocess
import csv

# --- Dynamic InfluxDB Client Importing ---
try:
    from influxdb_client import InfluxDBClient
    INFLUXDB_V2_AVAILABLE = True
except ImportError:
    INFLUXDB_V2_AVAILABLE = False

try:
    from influxdb import InfluxDBClient as InfluxDBClientV1
    INFLUXDB_V1_AVAILABLE = True
except ImportError:
    INFLUXDB_V1_AVAILABLE = False

VERSION = '5.25.199.0'

# --- InfluxQL Query Definitions ---
CONFIG_FIELD_MAP = {
    'entitlement': 'entitlement', 'serial_number': 'serial_no', 'smt': 'smtthreads', 'virtual_cpus': 'desired_vcpus',
    'capped': 'capped', 'pool_cpu': 'phys_cpus_pool',
    'pool_id': 'pool_id', 'proc_type': 'processorModel', 'proc_version': 'processorFamily',
    'proc_clock': 'processorMHz'
}

# --- Helper Functions ---
class FcntlLock:
    """A context manager for file locking using the native fcntl module."""
    def __init__(self, lock_path, timeout=10):
        self.lock_path = lock_path
        self.timeout = timeout
        self.lock_file = None

    def __enter__(self):
        start_time = time.time()
        self.lock_file = open(self.lock_path, 'a')
        while time.time() - start_time < self.timeout:
            try:
                fcntl.flock(self.lock_file, fcntl.LOCK_EX | fcntl.LOCK_NB)
                self.lock_file.write(str(os.getpid()))
                self.lock_file.flush()
                return self
            except (IOError, BlockingIOError):
                time.sleep(0.5)
        raise TimeoutError(f"Could not acquire lock on {self.lock_path} within {self.timeout} seconds.")

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.lock_file:
            fcntl.flock(self.lock_file, fcntl.LOCK_UN)
            self.lock_file.close()
            if os.path.exists(self.lock_path):
                os.remove(self.lock_path)

def get_db_client(config_path):
    """Reads config and returns an appropriate InfluxDB client."""
    config = configparser.ConfigParser()
    if not os.path.exists(config_path):
        print(f"Error: Configuration file not found at {config_path}", file=sys.stderr)
        sys.exit(1)
    config.read(config_path)
    db_config = config['database']
    db_version = db_config.getint('VERSION', 1)
    client = None
    if db_version == 2:
        if not INFLUXDB_V2_AVAILABLE:
            print("Error: InfluxDB v2 client not installed. Please run 'pip install influxdb-client'.", file=sys.stderr)
            sys.exit(1)
        client = InfluxDBClient(
            url=f"http://{db_config['HOST']}:{db_config['PORT']}",
            token=db_config['PASSWORD'],
            org=db_config.get('ORG', '-')
        )
    else: # Default to v1
        if not INFLUXDB_V1_AVAILABLE:
            print("Error: InfluxDB v1 client is not installed. Please run 'pip install influxdb'.", file=sys.stderr)
            sys.exit(1)
        client = InfluxDBClientV1(
            host=db_config['HOST'],
            port=db_config['PORT'],
            username=db_config['USER'],
            password=db_config['PASSWORD'],
            database=db_config['DATABASE']
        )
    return client, db_version, db_config

def execute_query(client, query, db_version, verbose=False):
    """Executes a query using the appropriate client version, with timestamped debug output."""
    if verbose:
        timestamp_str = datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC')
        print(f"\n[DEBUG] {timestamp_str} - Executing Query:\n{query}\n")
    if db_version == 2:
        return client.query_api().query(query)
    else:
        return client.query(query)

def parse_utc_string(ts_str):
    """Parses a UTC timestamp string (various formats) into a timezone-aware UTC datetime."""
    ts_str = ts_str.rstrip('Z')
    for fmt in ("%Y-%m-%dT%H:%M:%S.%f", "%Y-%m-%dT%H:%M:%S", "%Y-%m-%d %H:%M:%S.%f", "%Y-%m-%d %H:%M:%S"):
        try:
            return datetime.strptime(ts_str, fmt).replace(tzinfo=timezone.utc)
        except ValueError:
            continue
    raise ValueError(f"Unsupported datetime format: {ts_str}")

def stream_performance_data_optimized(client, db_version, entity_filter, time_filter_v1, tmp_dir, args):
    """
    Streams performance data to temp files, tracks min/max timestamps, and
    uses an external sort to create a master file.
    """
    print("\n## Phase 1: Performance Data Staging")
    temp_files = {}
    temp_file_handles = {}
    min_ts, max_ts = None, None
    discovered_vms = set()

    try:
        perf_query_templates = {
            'physc': 'SELECT "physical_consumed" FROM "cpu_util" WHERE {filter} AND {time_filter} GROUP BY "host"',
            'runq':  'SELECT "run_queue" FROM "kernel" WHERE {filter} AND {time_filter} GROUP BY "host"',
        }

        for metric in ['physc', 'runq']:
            print(f"- Streaming performance metrics ({metric})...", end="", flush=True)
            start_metric_time = time.time()

            temp_file = tempfile.NamedTemporaryFile(mode='w', delete=False, dir=tmp_dir, suffix=f'.{metric}.csv', encoding='utf-8')
            temp_files[metric] = temp_file.name
            temp_file_handles[metric] = temp_file
            if args.verbose:
                print(f"\n    Created temp file for {metric}: {temp_file.name}")

            query = perf_query_templates[metric].format(filter=entity_filter, time_filter=time_filter_v1)
            results = execute_query(client, query, db_version, args.verbose)

            temp_file.write("timestamp,vm_name,value\n")

            for series in results.items():
                # Ensure the series is valid and has a host tag.
                if not (series and series[0] and isinstance(series[0][1], dict) and series[0][1].get('host')):
                    if args.verbose: print("  [DEBUG] Skipping a series with no host tag in performance data.")
                    continue
                vm_name = series[0][1]['host']

                # FINAL POLISH: Reject entries with an empty host tag.
                if not vm_name:
                    if args.verbose: print("  [DEBUG] Skipping performance data series with an empty host tag.")
                    continue

                discovered_vms.add(vm_name)

                for point in series[1]:
                    field_name = next(iter(k for k in point if k != 'time'), None)
                    if field_name and point.get(field_name) is not None:
                        current_ts_obj = parse_utc_string(point['time'])
                        # Track the overall min/max timestamps during the stream.
                        min_ts = min(min_ts, current_ts_obj) if min_ts else current_ts_obj
                        max_ts = max(max_ts, current_ts_obj) if max_ts else current_ts_obj
                        ts = current_ts_obj.strftime('%Y-%m-%d %H:%M:%S')
                        temp_file.write(f"{ts},{vm_name},{point.get(field_name)}\n")

            temp_file.close()
            duration_metric = time.time() - start_metric_time
            print(f" [DONE: {format_duration(duration_metric)}]")

        print("- Sorting and merging all performance data into master cache...", end="", flush=True)
        start_sort_time = time.time()
        master_temp_file = tempfile.NamedTemporaryFile(mode='w', delete=False, dir=tmp_dir, suffix='.master.csv', encoding='utf-8')
        master_temp_filename = master_temp_file.name
        master_temp_file.close()
        create_sorted_master_file(temp_files, master_temp_filename)
        duration_sort = time.time() - start_sort_time
        print(f" [DONE: {format_duration(duration_sort)}]")

        print(f"- Discovered {len(discovered_vms)} VMs with performance data in the target time window.")
        return master_temp_filename, discovered_vms, min_ts, max_ts

    finally:
        # Final cleanup of intermediate temporary files.
        for handle in temp_file_handles.values():
            if not handle.closed: handle.close()
        for temp_file in temp_files.values():
            if os.path.exists(temp_file): os.remove(temp_file)

def create_sorted_master_file(temp_files, master_temp_filename):
    """Orchestrates creating a sorted master file by sorting and joining."""
    sorted_files = {}
    try:
        for metric, temp_file in temp_files.items():
            sorted_file = f"{temp_file}.sorted"
            # Use a large 8GB memory buffer for sort, as requested.
            sort_cmd = ['sort', '-t,', '-k1,1', '-k2,2', '-S', '8G', '-T', os.path.dirname(temp_file), temp_file]
            with open(sorted_file, 'w', encoding='utf-8') as outfile:
                subprocess.run(sort_cmd, stdout=outfile, check=True)
            sorted_files[metric] = sorted_file
        join_sorted_files(sorted_files, master_temp_filename)
    finally:
        for sorted_file in sorted_files.values():
            if os.path.exists(sorted_file): os.remove(sorted_file)

def join_sorted_files(sorted_files, master_temp_filename):
    """Performs a memory-efficient merge-join of multiple sorted CSV files."""
    file_handles = {}
    current_rows = {}
    try:
        for metric, sorted_file in sorted_files.items():
            file_handles[metric] = open(sorted_file, 'r', encoding='utf-8')
            next(file_handles[metric])  # Skip header
            line = file_handles[metric].readline()
            if line:
                parts = line.strip().split(',')
                if len(parts) >= 3:
                    current_rows[metric] = {'key': f"{parts[0]},{parts[1]}", 'value': parts[2]}
        with open(master_temp_filename, 'w', encoding='utf-8') as master_file:
            master_file.write("Timestamp,VMName,PhysC,RunQ\n")
            while current_rows:
                min_key = min(current_rows.values(), key=lambda x: x['key'])['key']
                timestamp, vm_name = min_key.split(',', 1)
                values = {metric: row_data['value'] for metric, row_data in current_rows.items() if row_data['key'] == min_key}
                master_file.write(f"{timestamp},{vm_name},{values.get('physc', '')},{values.get('runq', '')}\n")
                for metric in list(current_rows.keys()):
                    if current_rows.get(metric) and current_rows[metric]['key'] == min_key:
                        line = file_handles[metric].readline()
                        if line:
                            parts = line.strip().split(',')
                            if len(parts) >= 3: current_rows[metric] = {'key': f"{parts[0]},{parts[1]}", 'value': parts[2]}
                            else: del current_rows[metric]
                        else:
                            del current_rows[metric]
    finally:
        for handle in file_handles.values(): handle.close()

def get_config_events_robust(client, db_version, field, measurement, entity_filter, start_date, end_date, verbose=False):
    """Performs a robust two-phase query to capture both the baseline configuration and any changes during the analysis period."""
    events = []
    last_known_values = {}

    # Phase 1: Get the last known value BEFORE the analysis window.
    start_date_obj = datetime.strptime(start_date, '%Y-%m-%d')
    baseline_start_date_obj = start_date_obj - timedelta(days=365)
    baseline_start_str = baseline_start_date_obj.strftime('%Y-%m-%d')
    baseline_time_filter = f"time >= '{baseline_start_str}T00:00:00Z' AND time < '{start_date}T00:00:00Z'"
    baseline_query = f'SELECT LAST("{field}") AS "{field}" FROM "{measurement}" WHERE {entity_filter} AND {baseline_time_filter} GROUP BY "host"'

    baseline_results = execute_query(client, baseline_query, db_version, verbose)
    for series in baseline_results.items():
        if 'host' in series[0][1] and series[0][1]['host']:
            vm_name = series[0][1]['host']
            points = list(series[1])
            if points:
                value = points[0].get(field)
                if value is not None:
                    ts = int(start_date_obj.replace(tzinfo=timezone.utc).timestamp())
                    events.append({'ts': ts, 'host': vm_name, 'value': value})
                    last_known_values[vm_name] = value

    # Phase 2: Get all changes DURING the analysis window using sparse sampling.
    analysis_time_filter = f"time >= '{start_date}T00:00:00Z' AND time <= '{end_date}T23:59:59Z'"
    changes_query = f'SELECT LAST("{field}") AS "{field}" FROM "{measurement}" WHERE {entity_filter} AND {analysis_time_filter} GROUP BY time(6h), "host" ORDER BY time ASC'
    change_results = execute_query(client, changes_query, db_version, verbose)

    for series in change_results.items():
        if 'host' in series[0][1] and series[0][1]['host']:
            vm_name = series[0][1]['host']
            for point in series[1]:
                current_value = point.get(field)
                if current_value is not None and current_value != last_known_values.get(vm_name):
                    events.append({'ts': int(parse_utc_string(point['time']).timestamp()), 'host': vm_name, 'value': current_value})
                    last_known_values[vm_name] = current_value
    return events

def load_fallback_config(config_path, verbose=False):
    """Loads VM configuration from a CSV file for fallback."""
    if not config_path or not os.path.exists(config_path):
        if verbose and config_path: print(f"Info: Fallback config file not found at {config_path}")
        return {}
    config_data = {}
    try:
        with open(config_path, mode='r', encoding='utf-8-sig') as infile:
            reader = csv.DictReader(infile)
            for row in reader:
                if row.get('hostname'): config_data[row['hostname']] = row
    except Exception as e:
        print(f"WARNING: Could not read fallback config file {config_path}. Error: {e}", file=sys.stderr)
    return config_data

def format_duration(seconds):
    """Formats a duration in seconds into a human-readable mm:ss.xxs string."""
    if seconds < 60:
        return f"{seconds:.2f}s"
    minutes = int(seconds // 60)
    remaining_seconds = seconds % 60
    return f"{minutes}m {remaining_seconds:.2f}s"

# ==============================================================================
# HELPER FUNCTION: _calculate_max_capacity
# PURPOSE: Calculates the maximum potential CPU for an LPAR based on its
#          configuration state (capped status, virtual CPUs, and pool size).
# ARGS:
#   metadata_dict (dict): A dictionary containing the LPAR's metadata.
# RETURNS:
#   The calculated max_capacity value (float).
# ==============================================================================
def _calculate_max_capacity(metadata_dict):
    """
    Calculates the maximum potential CPU for an LPAR from its metadata.
    """
    # Safely get values, defaulting to 0 if missing or not a valid number.
    try:
        is_capped = int(metadata_dict.get('capped', 0))
    except (ValueError, TypeError):
        is_capped = 0

    try:
        entitlement = float(metadata_dict.get('entitlement', 0.0))
    except (ValueError, TypeError):
        entitlement = 0.0

    try:
        virtual_cpus = int(metadata_dict.get('virtual_cpus', 0))
    except (ValueError, TypeError):
        virtual_cpus = 0

    try:
        pool_cpu = int(metadata_dict.get('pool_cpu', 0))
    except (ValueError, TypeError):
        pool_cpu = 0

    # For a capped LPAR, the maximum is strictly its entitlement.
    if is_capped:
        return entitlement

    # For an uncapped LPAR, the maximum is the lesser of its virtual CPUs
    # or the total physical CPUs in its shared pool.
    if pool_cpu > 0 and virtual_cpus > 0:
        return float(min(virtual_cpus, pool_cpu))

    # Fallback for uncapped LPARs where pool size is not available.
    return float(virtual_cpus)

def main():
    """Main function for the nfit-stage-influxdb script."""
    # --- Argument Parsing ---
    parser = argparse.ArgumentParser(description="Query InfluxDB and stage data for nfit analysis.", formatter_class=argparse.RawTextHelpFormatter)
    target_group = parser.add_mutually_exclusive_group(required=True)
    target_group.add_argument("--mgsys", "--system", "--serial", "--host", help="A comma-separated list of managed system serial numbers.")
    target_group.add_argument("--vms", help="A comma-separated list of specific VM hostnames.")
    target_group.add_argument("--vmfile", help="Path to a file with a list of VM hostnames.")
    parser.add_argument("--startd", "--start-date", help="Start date (YYYY-MM-DD).")
    parser.add_argument("--endd", "--end-date", help="End date (YYYY-MM-DD).")
    parser.add_argument("--days", type=int, help="Number of recent days to query.")
    parser.add_argument("--datadir", help="Optional path to a cache directory.")
    parser.add_argument("--cleanup", action="store_true", help="Delete existing cache files before running.")
    parser.add_argument("--preserve-results", action="store_true", help="When using --cleanup, do not delete the L2 results cache.")
    parser.add_argument("--config", help="Optional path to a CSV configuration file for fallback data.")
    parser.add_argument("-v", "--verbose", action="store_true", help="Enable verbose output.")
    parser.add_argument('--version', action='version', version=f'%(prog)s {VERSION}')
    args = parser.parse_args()

    # --- Initial Setup & Timing ---
    script_start_time = time.time()

    # --- Banner ---
    print(f"nfit-stage-influxdb v{VERSION} -- InfluxDB Caching Tool")
    print("----------------------------------------------------------------------")
    print(f"Run started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S %Z')}")

    if args.days and (args.startd or args.endd): parser.error("--days cannot be used with --startd or --endd.")
    if not args.days and not (args.startd and args.endd): parser.error("Must specify a time window using --days or both --startd and --endd.")

    if args.days:
        if args.days <= 0: parser.error("--days must be a positive number.")
        end_date = datetime.now(timezone.utc)
        start_date = end_date - timedelta(days=args.days)
        startd_for_query, endd_for_query = start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d')
    else:
        startd_for_query, endd_for_query = args.startd, args.endd

    script_root = os.path.dirname(os.path.abspath(__file__))
    config_path = os.path.join(script_root, 'etc', 'influxdb.conf')
    client, db_version, _ = get_db_client(config_path)

    cache_dir = args.datadir if args.datadir else os.path.join(script_root, 'stage')
    tmp_dir = os.path.join(script_root, 'tmp')
    os.makedirs(cache_dir, exist_ok=True)
    os.makedirs(tmp_dir, exist_ok=True)

    if args.mgsys: print(f"Target: Managed System(s) '{args.mgsys}'")
    else: print(f"Target: VMs from --vms or --vmfile argument")
    print(f"Time Window: {startd_for_query} to {endd_for_query}")

    lock_path = os.path.join(cache_dir, '.nfit.cache.lock')
    master_temp_filename = None

    try:
        with FcntlLock(lock_path):
            print("\nAcquired cache lock.")
            if args.cleanup:
                # Cleanup logic can be expanded here
                print("Cleanup requested.")

            # Define the entity filter based on user input
            if args.mgsys:
                mgsys_list = [s.strip() for s in args.mgsys.split(',')]
                entity_filter = "(" + " OR ".join([f"\"serial_no\" = '{s}'" for s in mgsys_list]) + ")"
            else:
                vm_list = [s.strip() for s in args.vms.split(',')] if args.vms else [line.strip() for line in open(args.vmfile) if line.strip()]
                entity_filter = "(" + " OR ".join([f"host = '{vm}'" for vm in vm_list]) + ")"

           # Convert date strings to nanosecond epoch timestamps for the InfluxDB v1 query
            start_dt = datetime.strptime(f'{startd_for_query}T00:00:00Z', '%Y-%m-%dT%H:%M:%SZ').replace(tzinfo=timezone.utc)
            end_dt = datetime.strptime(f'{endd_for_query}T23:59:59Z', '%Y-%m-%dT%H:%M:%SZ').replace(tzinfo=timezone.utc)

            start_ns = int(start_dt.timestamp() * 1_000_000_000)
            end_ns = int(end_dt.timestamp() * 1_000_000_000)

            time_filter_v1 = f"time >= {start_ns} AND time <= {end_ns}"

            if args.verbose:
                print(f"\n[DEBUG] Generated InfluxDB v1 time filter: {time_filter_v1}\n")


            master_temp_filename, discovered_vms, min_ts, max_ts = stream_performance_data_optimized(
                client, db_version, entity_filter, time_filter_v1, tmp_dir, args
            )

            if not discovered_vms:
                print("\nNo VMs with performance data found in the specified time window. Nothing to do.")
                sys.exit(0)

            # --- Phase 2: Configuration State Gathering (Robust) ---
            print("\n## Phase 2: Configuration State Discovery")
            phase2_start_time = time.time()
            all_config_events = defaultdict(list)
            numeric_fields = {'entitlement': 'cpu_util', 'smtthreads': 'config', 'desired_vcpus': 'lpar_format1', 'pool_id': 'processor_pool', 'phys_cpus_pool': 'processor_pool', 'processorMHz': 'config'}
            non_numeric_fields = {'serial_no': 'server', 'capped': 'partition_type', 'processorModel': 'config', 'processorFamily': 'config'}
            config_fields = {**numeric_fields, **non_numeric_fields}

            for field, measurement in config_fields.items():
                change_events = get_config_events_robust(client, db_version, field, measurement, entity_filter, startd_for_query, endd_for_query, args.verbose)
                print(f"- Attribute '{field}' (anchor: {startd_for_query}): identified {len(change_events)} states.")
                internal_metric_key = next((key for key, val in CONFIG_FIELD_MAP.items() if val == field), field)
                for event in change_events:
                    all_config_events[event['host']].append({'ts': event['ts'], 'key': internal_metric_key, 'value': event['value']})
            print(f"[DONE: {format_duration(time.time() - phase2_start_time)}]")

            # --- Phase 3: Building State Timelines ---
            print("\n## Phase 3: Building State Timelines")
            print("- Building state timelines from all collected configuration events.")
            phase3_start_time = time.time()
            all_states = {}
            for vm_name, events in all_config_events.items():
                if not events: continue
                events.sort(key=lambda x: x['ts'])
                state_list = []
                last_known_metadata = {}
                current_state_start_epoch = None
                event_timeline = defaultdict(dict)
                for event in events: event_timeline[event['ts']][event['key']] = event['value']

                for i, ts in enumerate(sorted(event_timeline.keys())):
                    metadata_before_change = last_known_metadata.copy()
                    last_known_metadata.update(event_timeline[ts])
                    if i == 0:
                        current_state_start_epoch = ts
                    elif json.dumps(last_known_metadata, sort_keys=True) != json.dumps(metadata_before_change, sort_keys=True):
                        # Before adding the state, enrich it with max_capacity
                        metadata_before_change['max_capacity'] = _calculate_max_capacity(metadata_before_change)
                        state_list.append({'start_epoch': current_state_start_epoch, 'end_epoch': ts - 1, 'metadata': metadata_before_change})
                        current_state_start_epoch = ts
                if current_state_start_epoch is not None:
                    final_end_epoch = max(current_state_start_epoch, int(max_ts.timestamp()))
                    # Also enrich the final state before adding it
                    last_known_metadata['max_capacity'] = _calculate_max_capacity(last_known_metadata)
                    state_list.append({'start_epoch': current_state_start_epoch, 'end_epoch': final_end_epoch, 'metadata': last_known_metadata})
                all_states[vm_name] = state_list

            # --- Phase 4: Writing Final Cache Files ---
            print("\n## Phase 4: Writing Final Cache Files")
            phase4_start_time = time.time()
            fallback_config_data = load_fallback_config(args.config, args.verbose) if args.config else {}
            if fallback_config_data and os.path.exists(args.config):
                config_mtime = os.path.getmtime(args.config)
                if config_mtime < datetime.strptime(startd_for_query, '%Y-%m-%d').timestamp():
                    print(f"WARNING: The provided config file ({args.config}) was last modified on {datetime.fromtimestamp(config_mtime).strftime('%Y-%m-%d')}, which is before the analysis start date.", file=sys.stderr)

            data_by_serial = defaultdict(lambda: {'states': {}})
            vm_to_serial_map = {}
            for vm, states in all_states.items():
                if states:
                    serial = states[0].get('metadata', {}).get('serial_number', 'UNKNOWN_SERIAL')
                    vm_to_serial_map[vm] = serial
                    data_by_serial[serial]['states'][vm] = states

            # Handle VMs discovered from performance data that have no configuration in InfluxDB.
            # This logic can only reliably assign these 'orphan' VMs if a single managed system is the target.
            vms_with_missing_config = discovered_vms - set(vm_to_serial_map.keys())
            if vms_with_missing_config:
                if args.mgsys and len(mgsys_list) == 1:
                    authoritative_serial = mgsys_list[0]
                    print(f"- Assigning {len(vms_with_missing_config)} VMs with missing config to system: {authoritative_serial}")
                    for vm in vms_with_missing_config:
                        fallback_data = fallback_config_data.get(vm)
                        if fallback_data:
                            print(f"  INFO: Configuration for '{vm}' not in InfluxDB. Using fallback from --config.", file=sys.stderr)
                            metadata = { "entitlement": float(fallback_data.get('entitledcpu', 0.1)), "smt": int(fallback_data.get('smt', 8)), "virtual_cpus": int(fallback_data.get('maxcpu', 1)), "serial_number": fallback_data.get('serial', authoritative_serial), "capped": int(fallback_data.get('capped', 0)), "pool_id": int(fallback_data.get('procpool_id', 0)), "pool_cpu": int(fallback_data.get('pool_cpu', 0)), "proc_clock": float(fallback_data.get('proc_clock', 0.0)), "proc_type": fallback_data.get('model', 'Unknown'), "proc_version": fallback_data.get('osversion', 'Unknown')}
                        else:
                            print(f"  WARNING: Configuration for '{vm}' not found. Creating synthetic state.", file=sys.stderr)
                            metadata = { "entitlement": 0.1, "smt": 8, "virtual_cpus": 1, "pool_id": 0, "pool_cpu": 0, "proc_clock": 0.0, "serial_number": authoritative_serial, "capped": 0, "proc_type": "Unknown", "proc_version": "Unknown"}
                        # Add the synthetic state to the main data structure
                        synthetic_state = [{'start_epoch': int(min_ts.timestamp()), 'end_epoch': int(max_ts.timestamp()), 'metadata': metadata}]
                        data_by_serial[authoritative_serial]['states'][vm] = synthetic_state
                elif vms_with_missing_config:
                    print(f"WARNING: Found {len(vms_with_missing_config)} VMs with performance data but no configuration. These will be skipped as they cannot be reliably assigned to a multi-system target.", file=sys.stderr)
                    if args.verbose:
                        for vm in sorted(list(vms_with_missing_config)): print(f"  - Skipped VM: {vm}")

            print("- Writing cache files for each discovered serial number.")
            if not data_by_serial:
                print("No complete VM configuration states could be built. No cache files will be written.", file=sys.stderr)

            for serial, data in data_by_serial.items():
                print(f"  - Processing serial: {serial}")
                serial_cache_dir = os.path.join(cache_dir, serial)
                os.makedirs(serial_cache_dir, exist_ok=True)

                if args.cleanup:
                    print(f"    - Performing cleanup for {serial}...")

                    # Create a failsafe backup of the history file
                    history_file = os.path.join(serial_cache_dir, '.nfit.history.json')
                    if os.path.exists(history_file):
                        backup_file = history_file + ".bak"
                        try:
                            import shutil
                            shutil.copy2(history_file, backup_file)
                            print(f"      - Created backup of history file: {backup_file}")
                        except Exception as e:
                            print(f"      - WARNING: Could not create history backup for {serial}. Halting cleanup for safety. Error: {e}", file=sys.stderr)
                            continue # Skip to next serial if backup fails

                    # Define files for selective deletion
                    files_to_delete = [
                        '.nfit.cache.data',
                        '.nfit.cache.states',
                    ]

                    if not args.preserve_results:
                        files_to_delete.append('.nfit.cache.results')
                        print("      - Removing L2 results cache.")
                    else:
                        print("      - Preserving L2 results cache (--preserve-results).")

                    for filename in files_to_delete:
                        path_to_delete = os.path.join(serial_cache_dir, filename)
                        try:
                            if os.path.isfile(path_to_delete):
                                os.remove(path_to_delete)
                            elif os.path.isdir(path_to_delete):
                                import shutil
                                shutil.rmtree(path_to_delete)
                        except Exception as e:
                            print(f"      - WARNING: Could not remove {path_to_delete}. Error: {e}", file=sys.stderr)

                # --- Create Performance Data Cache (.nfit.cache.data) ---
                perf_cache_start = time.time()
                cache_data_path = os.path.join(serial_cache_dir, '.nfit.cache.data')
                vms_for_this_serial = set(data['states'].keys())

                with open(master_temp_filename, 'r', encoding='utf-8') as master_fh, \
                     open(cache_data_path, 'w', encoding='utf-8') as final_cache_file:
                    final_cache_file.write(master_fh.readline()) # Write header
                    for line in master_fh:
                        try:
                            if line.split(',', 2)[1] in vms_for_this_serial:
                                final_cache_file.write(line)
                        except IndexError:
                            continue # Ignore malformed lines
                print(f"    - Performance data cache created. [DONE: {format_duration(time.time() - perf_cache_start)}]")

                # --- Create Configuration State Cache (.nfit.cache.states) ---
                state_cache_start = time.time()
                cache_states_path = os.path.join(serial_cache_dir, '.nfit.cache.states')
                with open(cache_states_path, 'w') as f:
                    json.dump(data['states'], f, indent=2, allow_nan=False)
                print(f"    - Configuration state cache (JSON) created. [DONE: {format_duration(time.time() - state_cache_start)}]")

                # --- Write Manifest and ID Files ---
                with open(os.path.join(serial_cache_dir, '.nfit.cache.manifest'), 'w') as f:
                    f.write(f"cache_build_status: success\nbuild_timestamp: {datetime.now(timezone.utc).isoformat()}\n")

                with open(os.path.join(serial_cache_dir, '.nfit_stage_id'), 'w') as f:
                    f.write(f"Created by nfit-stage-influxdb.py v{VERSION} for system {serial}\n")

                print(f"    - InfluxDB cache location: {serial_cache_dir}")

            print(f"[DONE: {format_duration(time.time() - phase4_start_time)}]")

    except Exception as e:
        print(f"\nAn unexpected error occurred: {e}", file=sys.stderr)
        # Attempt to clean up the lock file in case of an unhandled error.
        if 'lock_path' in locals() and os.path.exists(lock_path): os.remove(lock_path)
        raise
    finally:
        if master_temp_filename and os.path.exists(master_temp_filename):
            if args.verbose:
                print(f"\nCleaning up master temporary file: {master_temp_filename}")
            os.remove(master_temp_filename)

        # --- Footer ---
        print("----------------------------------------------------------------------")
        total_duration = time.time() - script_start_time
        print(f"Staging complete. Total execution time: {format_duration(total_duration)}")

if __name__ == "__main__":
    main()
